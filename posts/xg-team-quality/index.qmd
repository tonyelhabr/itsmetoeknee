---
title: Should we account for team quality in an xG model?
description: "F**king around (with an xG model) and finding out"
date: 2023-12-31
categories:
  - r
  - soccer
image: elo-pdp.png
execute: 
  code-fold: false
  eval: false
  include: true
  echo: true
---

## Introduction

"Should we account for team quality in an [xG model](https://theanalyst.com/na/2021/07/what-are-expected-goals-xg/)?" From a purely philosophical point of view, my opinion is "no"--I think that xG models should be a purely "descriptive", agnostic to player and team abilities, as well as other non-event factors such as game state.

But I thought it might be fun to entertain the question from a quantitative perspective. If we add features for team strength to an xG model, can we meaningfully improve the model's overall predictive performance and [calibration](https://tonyelhabr.rbind.io/posts/probability-calibration/) (with respect to team quality)?

### Motivation

This write-up is inspired by [Ryan Brill](https://twitter.com/RyanBrill_)'s recent [presentation on fourth-down decision-making in the National Football League (NFL)](https://youtu.be/uS4XxQ0LVfE?si=BnmzeePnk3R5uiY3&t=361). He points out that [expected points (EP)](https://www.nfeloapp.com/analysis/expected-points-added-epa-nfl/) models in the NFL have a [selection bias](https://en.wikipedia.org/wiki/Selection_bias) problem--they tend to under-rate the probability of a positive outcome for "good" teams and over-rate such outcomes for "bad" teams.

Expected goals (xG) in soccer also show signs of this phenomenon. [Lars Maurath](https://github.com/larsmaurath) has [a great deep-dive](https://www.thesignificantgame.com/portfolio/do-naive-xg-models-underestimate-expected-goals-for-top-teams/) looking into expected goals under-estimation for strong teams in the [Big 5 European leagues](https://fbref.com/en/comps/Big5/Big-5-European-Leagues-Stats). The plot below (copied shamelessly from Lars' post) shows that a naive xG model consistently under-predicts Barcelona's goals over the course of the season, for seasons from 2007/08 to 2018/19. Even [StatsBomb](https://statsbomb.com/)'s model--which is more sophisticated--tends to underestimate the true cumulative goal total!

![](barcelona-cumulative-xg.png)

## Analysis and Results

### Data

I'll be using event data that I've ingested with the [`{socceraction}` package](https://github.com/ML-KULeuven/socceraction) (which I've made available [here](https://github.com/tonyelhabr/socceraction-streamlined/releases)!) for the 2013/14 through 2022/23 [English Premier League (EPL)](https://www.premierleague.com/) seasons. I'll focus on just "open-play" shots, i.e. shots excluding penalties and those taken from set pieces.

But we also need some measure of team quality. To that end, I've scraped [Elo](https://en.wikipedia.org/wiki/Elo_rating_system) ratings from [ClubElo](http://clubelo.com/). I've chosen Elo because provides a intuitive, sport-agnostic measure of relative skill. Also, it is calculated independent of the events that take place in a game, so correlation with measures of shot volume, quality, etc. are only coincidental. (It was also fairly easy to retrieve!)

```{r}
#| label: setup
#| code-fold: true
#| code-summary: Package imports and other setup

## Data retrieval
library(curl)
library(arrow)
library(qs) ## local dev

## Data manipulation
library(dplyr)
library(purrr)
library(lubridate)

## Modeling
library(rsample)
library(recipes)
library(parsnip)
library(workflows)
library(hardhat)

## Model tuning
library(tune)
library(dials)
library(workflowsets)
library(finetune)

## Model diagnostics
library(rlang)
library(yardstick)
library(pdp)
library(vip)

## Plotting
library(ggplot2)
library(sysfonts)
library(showtext)
library(ggtext)
library(htmltools)
library(scales)

PROJ_DIR <- 'posts/xg-team-quality'

TAG_LABEL <- htmltools::tagList(
  htmltools::tags$span(htmltools::HTML(enc2utf8("&#xf099;")), style = 'font-family:fb'),
  htmltools::tags$span("@TonyElHabr"),
)
SUBTITLE_LABEL <- 'English Premier League, 2012/13 - 2022/23'
PLOT_RESOLUTION <- 300
WHITISH_FOREGROUND_COLOR <- 'white'
COMPLEMENTARY_FOREGROUND_COLOR <- '#cbcbcb' # '#f1f1f1'
BLACKISH_BACKGROUND_COLOR <- '#1c1c1c'
COMPLEMENTARY_BACKGROUND_COLOR <- '#4d4d4d'
FONT <- 'Titillium Web'
sysfonts::font_add_google(FONT, FONT)
## https://github.com/tashapiro/tanya-data-viz/blob/main/chatgpt-lensa/chatgpt-lensa.R for twitter logo
sysfonts::font_add('fb', 'Font Awesome 6 Brands-Regular-400.otf')
showtext::showtext_auto()
showtext::showtext_opts(dpi = PLOT_RESOLUTION)

ggplot2::theme_set(ggplot2::theme_minimal())
ggplot2::theme_update(
  text = ggplot2::element_text(family = FONT),
  title = ggplot2::element_text(size = 20, color = WHITISH_FOREGROUND_COLOR),
  plot.title = ggtext::element_markdown(face = 'bold', size = 20, color = WHITISH_FOREGROUND_COLOR),
  plot.title.position = 'plot',
  plot.subtitle = ggtext::element_markdown(size = 16, color = COMPLEMENTARY_FOREGROUND_COLOR),
  axis.text = ggplot2::element_text(color = WHITISH_FOREGROUND_COLOR, size = 14),
  # axis.title = ggplot2::element_text(size = 14, color = WHITISH_FOREGROUND_COLOR, face = 'bold', hjust = 0.99),
  axis.title.x = ggtext::element_markdown(size = 14, color = WHITISH_FOREGROUND_COLOR, face = 'bold', hjust = 0.99),
  axis.title.y = ggtext::element_markdown(size = 14, color = WHITISH_FOREGROUND_COLOR, face = 'bold', hjust = 0.99),
  axis.line = ggplot2::element_blank(),
  strip.text = ggplot2::element_text(size = 14, color = WHITISH_FOREGROUND_COLOR, face = 'bold', hjust = 0),
  legend.position = 'top',
  legend.text = ggplot2::element_text(size = 12, color = WHITISH_FOREGROUND_COLOR, face = 'plain'),
  legend.title = ggplot2::element_text(size = 12, color = WHITISH_FOREGROUND_COLOR, face = 'bold'),
  panel.grid.major = ggplot2::element_line(color = COMPLEMENTARY_BACKGROUND_COLOR),
  panel.grid.minor = ggplot2::element_line(color = COMPLEMENTARY_BACKGROUND_COLOR),
  panel.grid.minor.x = ggplot2::element_blank(),
  panel.grid.minor.y = ggplot2::element_blank(),
  plot.margin = ggplot2::margin(10, 20, 10, 20),
  plot.background = ggplot2::element_rect(fill = BLACKISH_BACKGROUND_COLOR, color = BLACKISH_BACKGROUND_COLOR),
  plot.caption = ggtext::element_markdown(color = WHITISH_FOREGROUND_COLOR, hjust = 0, size = 10, face = 'plain'),
  plot.caption.position = 'plot',
  plot.tag = ggtext::element_markdown(size = 10, color = WHITISH_FOREGROUND_COLOR, hjust = 1),
  plot.tag.position = c(0.99, 0.01),
  panel.spacing.x = grid::unit(2, 'lines'),
  panel.background = ggplot2::element_rect(fill = BLACKISH_BACKGROUND_COLOR, color = BLACKISH_BACKGROUND_COLOR)
)
```

```{r}
#| label: data-pull
#| code-fold: true
#| code-summary: Retrieve and wrangle data
read_parquet_from_url <- function(url) {
  load <- curl::curl_fetch_memory(url)
  arrow::read_parquet(load$content)
}

REPO <- 'tonyelhabr/socceraction-streamlined'
read_socceraction_parquet_release <- function(name, tag) {
  url <- sprintf('https://github.com/%s/releases/download/%s/%s.parquet', REPO, tag, name)
  read_parquet_from_url(url)
}

read_socceraction_parquet_releases <- function(name, tag = 'data-processed') {
  purrr::map_dfr(
    2013:2022,
    \(season_start_year) {
      basename <- sprintf('8-%s-%s', season_start_year, name)
      message(basename)
      read_socceraction_parquet_release(basename, tag = tag)
    }
  )
}

read_socceraction_parquet <- function(name, branch = 'main') {
  url <- sprintf('https://github.com/%s/raw/%s/%s.parquet', REPO, branch, name)
  read_parquet_from_url(url)
}

x <- read_socceraction_parquet_releases('x')
y <- read_socceraction_parquet_releases('y')
actions <- read_socceraction_parquet_releases('actions')
games <- read_socceraction_parquet_releases('games') |> 
  dplyr::mutate(
    date = lubridate::date(game_date)
  )
team_elo <- read_socceraction_parquet('data/final/8/2013-2022/clubelo-ratings')

open_play_shots <- games |>
  dplyr::transmute(
    season_id,
    game_id,
    date,
    home_team_id,
    away_team_id
  ) |> 
  dplyr::inner_join(
    x |> 
      dplyr::filter(type_shot_a0 == 1) |> 
      dplyr::select(
        game_id,
        action_id,
        
        ## features
        start_x_a0,
        start_y_a0,
        start_dist_to_goal_a0,
        start_angle_to_goal_a0,
        type_dribble_a1,
        type_pass_a1,
        type_cross_a1,
        type_corner_crossed_a1,
        type_shot_a1,
        type_freekick_crossed_a1,
        bodypart_foot_a0,
        bodypart_head_a0,
        bodypart_other_a0
      ) |> 
      dplyr::mutate(
        dplyr::across(-c(game_id, action_id), as.integer)
      ),
    by = dplyr::join_by(game_id),
    relationship = 'many-to-many'
  ) |> 
  dplyr::inner_join(
    y |> 
      dplyr::transmute(
        game_id, 
        action_id,
        scores = ifelse(scores, 'yes', 'no') |> factor(levels = c('yes', 'no'))
      ),
    by = dplyr::join_by(game_id, action_id)
  ) |> 
  dplyr::inner_join(
    actions |> 
      dplyr::select(
        game_id,
        action_id,
        team_id,
        player_id
      ),
    by = dplyr::join_by(game_id, action_id)
  ) |> 
  dplyr::left_join(
    team_elo |> dplyr::select(date, home_team_id = team_id, home_elo = elo),
    by = dplyr::join_by(date, home_team_id)
  ) |> 
  dplyr::left_join(
    team_elo |> dplyr::select(date, away_team_id = team_id, away_elo = elo),
    by = dplyr::join_by(date, away_team_id)
  ) |> 
  dplyr::transmute(
    date,
    season_id,
    game_id,
    team_id,
    opponent_team_id = ifelse(team_id == home_team_id, away_team_id, home_team_id),
    action_id,
    
    scores,
    
    elo = ifelse(team_id == home_team_id, home_elo, away_elo),
    opponent_elo = ifelse(team_id == home_team_id, away_elo, home_elo),
    elo_diff = elo - opponent_elo,
    
    start_dist_to_goal_a0,
    start_angle_to_goal_a0,
    type_dribble_a1,
    type_pass_a1,
    type_cross_a1,
    type_corner_crossed_a1,
    type_shot_a1,
    type_freekick_crossed_a1,
    bodypart_foot_a0,
    bodypart_head_a0,
    bodypart_other_a0
  )
```

```{r}
#| label: data-pull-save
#| include: false
# library(qs)
qs::qsave(open_play_shots, file.path(PROJ_DIR, 'open_play_shots.qs'))
```

```{r}
#| label: data-pull-read
#| include: false
open_play_shots <- qs::qread(file.path(PROJ_DIR, 'open_play_shots.qs'))
```

The open play shot data looks like this.

```{r}
#| label: glimpse-data-pull
#| code-fold: false
dplyr::glimpse(open_play_shots)
#> Rows: 92,322
#> Columns: 21
#> $ date                     <date> 2012-08-18, 2012-08-18, 2012-08-18, 2012-…
#> $ season_id                <int> 2013, 2013, 2013, 2013, 2013, 2013, 2013, …
#> $ game_id                  <int> 614051, 614051, 614051, 614051, 614051, 61…
#> $ team_id                  <int> 16, 13, 16, 13, 13, 13, 13, 13, 13, 13, 16…
#> $ opponent_team_id         <dbl> 13, 16, 13, 16, 16, 16, 16, 16, 16, 16, 13…
#> $ action_id                <int> 109, 123, 276, 436, 502, 517, 541, 573, 61…
#> $ scores                   <fct> no, no, no, no, no, no, no, no, no, no, no…
#> $ elo                      <dbl> 1669.172, 1827.481, 1669.172, 1827.481, 18…
#> $ opponent_elo             <dbl> 1827.481, 1669.172, 1827.481, 1669.172, 16…
#> $ elo_diff                 <dbl> -158.3093, 158.3093, -158.3093, 158.3093, …
#> $ start_dist_to_goal_a0    <int> 12, 27, 21, 15, 8, 16, 29, 10, 26, 32, 30,…
#> $ start_angle_to_goal_a0   <int> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, …
#> $ type_dribble_a1          <int> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, …
#> $ type_pass_a1             <int> 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, …
#> $ type_cross_a1            <int> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, …
#> $ type_corner_crossed_a1   <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
#> $ type_shot_a1             <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
#> $ type_freekick_crossed_a1 <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
#> $ bodypart_foot_a0         <int> 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, …
#> $ bodypart_head_a0         <int> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, …
#> $ bodypart_other_a0        <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
```

We'll set aside the shots from the 7 seasons from 2013/14 to 2019/20 for our training set, and the 3 seasons from 2020/21 to 2022/23 for our test set.

```{r}
#| label: split-data
#| code-fold: true
#| code-summary: Split the data for modeling
split <- rsample::make_splits(
  open_play_shots |> dplyr::filter(season_id %in% c(2013L:2019L)),
  open_play_shots |> dplyr::filter(season_id %in% c(2020L:2022L))
)

train <- rsample::training(split)
test <- rsample::testing(split)
```

It's worth spotlighting Elo a bit more, since it's the novel feature here. Below is a look at the distribution of pre-match Elo over the course of the entire data set.

```{r}
#| label: elo-hist
#| include: false
elo_hist <- open_play_shots |> 
  dplyr::distinct(game_id, elo) |> 
  ggplot2::ggplot() +
  ggplot2::aes(
    x = elo
  ) +
  ggplot2::geom_histogram(binwidth = 25, fill = 'white') +
  ggplot2::labs(
    title = 'Distribution of Elo',
    subtitle = SUBTITLE_LABEL,
    y = 'Count',
    x = 'Elo',
    caption =  'Each point represents the pre-match Elo of a given team.<br/>There are 10 seasons x 380 matches x 2 sides = 7.6k data points in total.'
  )
ggplot2::ggsave(
  elo_hist,
  filename = file.path(PROJ_DIR, 'elo-hist.png'),
  device = 'png',
  width = 7,
  height = 7 / 1.5
)
```

![](elo-hist.png)

To make Elo values feel a bit more tangible, note that:

-   Man City tends to sustain an Elo greater than 1850 (arguably the best team in the EPL for the past decade).
-   Bottom-table teams tend to have Elos less than 1650.

```{r}
#| label: elo-diff-hist
#| include: false
elo_diff_hist <- open_play_shots |> 
  dplyr::distinct(game_id, elo_diff) |> 
  ggplot2::ggplot() +
  ggplot2::aes(
    x = elo_diff
  ) +
  ggplot2::geom_histogram(binwidth = 25, fill = 'white') +
  ggplot2::theme(
    plot.title = ggtext::element_markdown(size = 19)
  ) +
  ggplot2::labs(
    title = 'Distribution of the difference in Elo between opponents',
    subtitle = SUBTITLE_LABEL,
    y = 'Count',
    x = 'Elo',
    caption = 'Each point represents the pre-match difference between two teams.<br/>There are 10 seasons x 380 matches x 2 sides = 7.6k data points in total.'
  )
ggplot2::ggsave(
  elo_diff_hist,
  filename = file.path(PROJ_DIR, 'elo-diff-hist.png'),
  device = 'png',
  width = 7,
  height = 7 / 1.5
)
```

The pre-match difference team Elos follows a normal-ish distribution, with most values falling within the ±300 range.

![](elo-diff-hist.png)

### Model Training

The feature set for our "base" xG model consists of the following:

-   location of the shot (distance and angle of the shot to the center of the goal mouth)[^1].
-   type of action leading to the shot.
-   body part with which the shot was taken.

[^1]: One might get a slightly more performant by adding the `x` and `y` coordinates of the shot--to implicitly account for right-footed bias, for example--but I actually prefer not to add those in the model. Such terms can result in slight [over-fitting](https://en.wikipedia.org/wiki/Overfitting), in the presence of other features that provide information about the location of the shot, such as distance and angle. (This is the classical ["bias-variance" trade-off](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff).)

These features are essentially what [all xG models](https://fbref.com/en/expected-goals-model-explained/) have in common, although the exact implementation differs. Data providers such as [Opta also account for information](https://theanalyst.com/na/2023/08/what-is-expected-goals-xg/) that is not captured in traditional event data, such as the position of the goalkeeper.

For the team-quality-adjusted (or "Elo-augmented") model, I'll add two additional features:

-   `elo`: the Elo of the team of the shot-taker.
-   `elo_diff`: the difference in the ELO of the shot-taking team and the opposing team.

The former is meant to capture the opponent-agnostic quality of a team, while the latter captures the quality of the team relative to their opponent.

```{r}
#| label: model-recipes
#| code-fold: true
#| code-summary: Setting up the models
rec_elo <- recipes::recipe(
  scores ~ 
    elo +
    elo_diff +
    start_dist_to_goal_a0 +
    start_angle_to_goal_a0 +
    type_dribble_a1 +
    type_pass_a1 +
    type_cross_a1 +
    type_corner_crossed_a1 +
    type_shot_a1 +
    type_freekick_crossed_a1 +
    bodypart_foot_a0 +
    bodypart_head_a0 +
    bodypart_other_a0,
  data = train
)

rec_base <- rec_elo |> 
  recipes::step_rm(elo, elo_diff)
```

I'll be using [xgboost](https://xgboost.readthedocs.io/en/stable/) for our model, the state-of-the-art framework for tabular machine learning tasks. (It's also the type of model used by providers like Opta.) I'll choose [hyperparameters](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)) using [an efficient grid search](https://finetune.tidymodels.org/index.html), evaluating models with the [Brier skill score (BSS)](https://en.wikipedia.org/wiki/Brier_score#Brier_Skill_Score_(BSS)). For the reference Brier score for BSS, I'll use a dummy model that predicts 12% conversion for all shots.[^2]

[^2]: 12% is approximately the observed shot conversion rate in the whole data set.

::: {.callout-note collapse="true"}
## Brier skill score (BSS)

If this isn't the first blog post you've read of mine, you probably know that [I love to use BSS](https://tonyelhabr.rbind.io/posts/opta-xg-model-calibration) for classification tasks, especially for xG models.

But why BSS? Simply put, [Brier scores](https://en.wikipedia.org/wiki/Brier_score) are known as [the best evaluation metric](https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/) to use for classification tasks where you're purely interested in probabilities. And BSS goes one step beyond Brier scores, forcing one to contextualize the model evaluation with a reasonable baseline. In the context of xG, BSS helps us directly see whether our fitted model is better than a naive prediction, such as guessing that all shots convert at the observed shot conversion rate.

Keep in mind that a higher BSS is ideal. A perfect model would have a BSS of 1; a model that is no better than a reference model would have a BSS of 0.
:::

```{r}
#| label: brier-skill-score-implementation
#| code-fold: true
#| code-summary: Functions for Brier skill score
## See also: probability-calibration
brier_skill_score <- function(data, ...) {
  UseMethod('brier_skill_score')
}

brier_skill_score <- yardstick::new_prob_metric(
  brier_skill_score, 
  direction = 'maximize'
)

bss <- function(
    truth, 
    estimate, 
    ref_estimate, 
    event_level,
    case_weights,
    ...
) {
  
  if (length(estimate) == 1) {
    estimate <- rep(estimate, length(truth))
  }
  
  if (length(ref_estimate) == 1) {
    ref_estimate <- rep(ref_estimate, length(truth))
  }
  
  estimate_brier_score <- brier_class_vec(
    truth = truth,
    estimate = estimate,
    event_level = event_level,
    case_weights = case_weights,
    ...
  )
  
  ref_brier_score <- brier_class_vec(
    truth = truth,
    estimate = ref_estimate,
    event_level = event_level,
    case_weights = case_weights,
    ...
  )
  
  1 - (estimate_brier_score / ref_brier_score)
}

brier_skill_score_estimator_impl <- function(
    truth, 
    estimate, 
    ref_estimate, 
    event_level,
    case_weights
) {
  bss(
    truth = truth,
    estimate = estimate,
    ref_estimate = ref_estimate,
    event_level = event_level,
    case_weights = case_weights
  )
}

brier_skill_score_vec <- function(
    truth, 
    estimate, 
    ref_estimate, 
    na_rm = TRUE, 
    event_level = yardstick:::yardstick_event_level(),
    case_weights = NULL, 
    ...
) {
  
  yardstick:::abort_if_class_pred(truth)
  
  estimator <- yardstick::finalize_estimator(
    truth, 
    metric_class = 'brier_skill_score'
  )
  
  yardstick::check_prob_metric(truth, estimate, case_weights, estimator)
  
  if (na_rm) {
    result <- yardstick::yardstick_remove_missing(truth, estimate, case_weights)
    
    truth <- result$truth
    estimate <- result$estimate
    case_weights <- result$case_weights
  } else if (yardstick::yardstick_any_missing(truth, estimate, case_weights)) {
    return(NA_real_)
  }
  
  brier_skill_score_estimator_impl(
    truth = truth,
    estimate = estimate,
    ref_estimate = ref_estimate,
    event_level = event_level,
    case_weights = case_weights
  )
}

brier_skill_score.data.frame <- function(
    data, 
    truth, 
    ...,
    na_rm = TRUE,
    event_level = yardstick:::yardstick_event_level(),
    case_weights = NULL,
    
    ref_estimate = 0.5,
    name = 'brier_skill_score'
) {
  yardstick::prob_metric_summarizer(
    name = name,
    fn = brier_skill_score_vec,
    data = data,
    truth = !!rlang::enquo(truth),
    ...,
    na_rm = na_rm,
    event_level = event_level,
    case_weights = !!rlang::enquo(case_weights),
    fn_options = list(
      ref_estimate = ref_estimate
    )
  )
}

xg_brier_skill_score <- function(data, ...) {
  UseMethod('xg_brier_skill_score')
}

# REF_ESTIMATE <- open_play_shots |>
#   dplyr::summarize(goal_rate = sum(scores == 'yes') / dplyr::n()) |>
#   dplyr::pull(goal_rate)
REF_ESTIMATE <- 0.12

xg_brier_skill_score.data.frame <- function(...) {
  brier_skill_score(
    ref_estimate = REF_ESTIMATE,
    name = 'xg_brier_skill_score',
    ...
  )
}

xg_brier_skill_score <- yardstick::new_prob_metric(
  xg_brier_skill_score,
  direction = 'maximize'
)
```

```{r}
#| label: tune-models
#| code-fold: true
#| code-summary: Tuning the xG models
## Useful reference: https://jlaw.netlify.app/2022/01/24/predicting-when-kickers-get-iced-with-tidymodels/
TREES <- 500
LEARN_RATE <- 0.01
define_xgboost_spec <- function(...) {
  parsnip::boost_tree(
    trees = !!TREES,
    learn_rate = !!LEARN_RATE,
    tree_depth = tune::tune(),
    min_n = tune::tune(), 
    loss_reduction = tune::tune(),
    sample_size = tune::tune(), 
    mtry = tune::tune(),
    stop_iter = tune::tune()
  ) |>
    parsnip::set_engine('xgboost', ...) |> 
    parsnip::set_mode('classification')
}

spec_base <- define_xgboost_spec()

elo_features <- rec_elo |> recipes::prep() |> recipes::juice() |> colnames()
spec_elo <- define_xgboost_spec(
  monotone_constraints = !!ifelse(elo_features %in% c('elo', 'elo_diff'), 1, 0)
)

grid <- dials::grid_latin_hypercube(
  dials::tree_depth(),
  dials::min_n(),
  dials::loss_reduction(),
  sample_size = dials::sample_prop(),
  dials::finalize(dials::mtry(), train),
  dials::stop_iter(range = c(10L, 50L)),
  size = 50
)

wf_sets <- workflowsets::workflow_set(
  preproc = list(
    base = rec_base, 
    elo = rec_elo
  ),
  ## Separate specs because we want monotonic constraints on the Elo model
  models = list(
    model = spec_base,
    model = spec_elo
  ),
  cross = FALSE
)

control <- finetune::control_race(
  save_pred = TRUE,
  # burn_in = 2,
  parallel_over = 'everything',
  save_workflow = TRUE,
  verbose = TRUE
)

set.seed(42)
train_folds <- rsample::vfold_cv(train, strata = scores, v = 5)

tuned_results <- workflowsets::workflow_map(
  wf_sets,
  fn = 'tune_race_anova',
  grid = grid,
  control = control,
  metrics = yardstick::metric_set(xg_brier_skill_score),
  resamples = train_folds,
  seed = 42
)
```

```{r}
#| label: tune-models-save
#| include: false
qs::qsave(tuned_results, file.path(PROJ_DIR, 'tuned_results2.qs'))
```

```{r}
#| label: tune-models-read
#| include: false
tuned_results <- qs::qread(file.path(PROJ_DIR, 'tuned_results2.qs'))
```

```{r}
#| label: best-sets
#| code-fold: true
#| code-summary: Choosing best hyper-parameters
MODEL_TYPES <- c(
  'base_model' = 'Base',
  'elo_model' = 'Elo-augmented'
)
select_best_model <- function(tuned_results, model_type) {
  tuned_results |>
    workflowsets::extract_workflow_set_result(model_type) |> 
    ## ugh, i don't like when models are chosen with super small `min_n`
    ##   since they tend to be more naive. i'll do my own version of `select_best` here.
    # tune::select_best(metric = 'xg_brier_skill_score') |>
    tune::show_best('xg_brier_skill_score') |> 
    dplyr::filter(min_n >= 5) |> 
    dplyr::slice(1) |> 
    dplyr::transmute(
      model_type = MODEL_TYPES[model_type],
      mtry,
      min_n,
      tree_depth, 
      loss_reduction,
      sample_size,
      stop_iter
    )
}

best_base_set <- select_best_model(tuned_results, 'base_model')
best_elo_set <- select_best_model(tuned_results, 'elo_model')

dplyr::bind_rows(
  best_base_set,
  best_elo_set
) |> 
  dplyr::transmute(
    model_type,
    mtry,
    min_n,
    tree_depth, 
    loss_reduction,
    sample_size,
    stop_iter
  ) |> 
  knitr::kable()
```

::: callout-warning
## Model tuning procedure

This post isn't meant to be so much about the why's and how's of model tuning and training, so I've spared commentary on the code.
:::

::: {.callout-note collapse="true"}
## Model hyperparameters

For reproducibility, the chosen hyperparameters are as follows.

| model_type    | mtry | min_n | tree_depth | loss_reduction | sample_size | stop_iter |
|:--------------|-----:|------:|-----------:|---------------:|------------:|----------:|
| Base          |   10 |    10 |          5 |      0.6247507 |   0.4397286 |        18 |
| Elo-augmented |   10 |    10 |          5 |      0.6247507 |   0.4397286 |        18 |

The number of `trees` and `learning_rate` were pre-defined to be 500 and 0.01 respectively.
:::

Finally, we fit singular models on the entire training set. These are the models that we'll use to evaluate the effect of team quality on xG.[^3]

[^3]: The hyperparameter search evaluated models fit on portions (or "folds") of the training data in a [cross-validation procedure](https://scikit-learn.org/stable/modules/cross_validation.html). We should fit one "final" model with all of the training data, per [best practice](https://tune.tidymodels.org/reference/last_fit.html).

```{r}
#| label: last-fits
#| code-fold: true
#| code-summary: Fit models on entire training set after choosing best hyper-parameters
finalize_tuned_results <- function(tuned_results, model_type) {
  best_set <- select_best_model(tuned_results, model_type)
  tuned_results |>
    hardhat::extract_workflow(model_type) |>
    tune::finalize_workflow(best_base_set) |> 
    tune::last_fit(
      split,
      metrics = yardstick::metric_set(xg_brier_skill_score)
    )
}
last_base_fit <- finalize_tuned_results(tuned_results, 'base_model')
last_elo_fit <- finalize_tuned_results(tuned_results, 'elo_model')
```

```{r}
#| label: last-fits-save
#| include: false
qs::qsave(last_base_fit, file.path(PROJ_DIR, 'last_base_fit.qs'))
qs::qsave(last_elo_fit, file.path(PROJ_DIR, 'last_elo_fit.qs'))
```

```{r}
#| label: last-fits-read
#| include: false
last_base_fit <- qs::qread(file.path(PROJ_DIR, 'last_base_fit.qs'))
last_elo_fit <- qs::qread(file.path(PROJ_DIR, 'last_elo_fit.qs'))
```

### Feature Importance

We should look to see that the xG models are behaving is expected. One way of doing so is to look at the feature importance.

In bespoke models, feature importance can be enlightening, as it tell us which features are contributing most to the predicted outcomes. In this case, I know that shot distance should be the most important feature (by far), as this is found to be the most important features in other similar "basic" public xG models, such as [this](https://github.com/AnshChoudhary/xGModel/blob/main/expected-goals-model-xg.ipynb) and [this](https://github.com/ML-KULeuven/soccer_xg/blob/master/notebooks/4-creating-custom-xg-pipelines.ipynb).

```{r}
#| label: var-imp-base
#| include: false
extract_n_features_from_last_fit <- function(last_fit) {
  last_fit |>
    dplyr::pull(.workflow) |> 
    purrr::pluck(1) |> 
    workflows::extract_recipe() |> 
    base::summary() |> 
    dplyr::filter(role == 'predictor') |> 
    nrow()
}

FEATURE_LABELS <- c(
  'elo' = 'Elo',
  'elo_diff' = 'Elo difference',
  'start_dist_to_goal_a0' = 'Distance to goal',
  'start_angle_to_goal_a0' = 'Angle to goal',
  'type_dribble_a1' = 'Prev. action = dribble',
  'type_pass_a1' = 'Prev. action = pass',
  'type_cross_a1' = 'Prev. action = cross',
  'type_corner_crossed_a1' = 'Prev. action = crossed corner',
  'type_shot_a1' = 'Prev. action = shot',
  'type_freekick_crossed_a1' = 'Prev. action = crossed freekick',
  'bodypart_foot_a0' = 'Bodypart = foot',
  'bodypart_head_a0' = 'Bodypart = head',
  'bodypart_other_a0' = 'Bodypart = other'
)


plot_var_imp_from_last_importance <- function(last_fit) {
  
  n_features <- extract_n_features_from_last_fit(last_fit)
  last_fit |> 
    workflows::extract_fit_parsnip() |>
    vip::vi(
      method = 'model'
    ) |> 
    dplyr::transmute(
      term = forcats::fct_reorder(FEATURE_LABELS[Variable], Importance),
      importance = Importance
    ) |> 
    ggplot2::ggplot() +
    ggplot2::aes(
      x = importance,
      y = term
    ) +
    ggplot2::geom_segment(
      ggplot2::aes(
        group = term,
        x = 0,
        xend = importance,
        y = term,
        yend = term
      ),
      color = WHITISH_FOREGROUND_COLOR
    ) +
    ggplot2::geom_point(
      size = 3,
      color = WHITISH_FOREGROUND_COLOR
    ) +
    ggplot2::geom_text(
      family = FONT,
      color = WHITISH_FOREGROUND_COLOR,
      ggplot2::aes(label = scales::percent(importance, accuracy = 0.1)),
      size = 12 / ggplot2::.pt,
      hjust = 0,
      nudge_x = 0.05
    ) +
    ggplot2::scale_x_continuous(
      labels = scales::percent,
      limits = c(0.0, 1)
    ) +
    ggplot2::labs(
      title = 'Feature importance',
      x = 'Feature importance',
      y = NULL
    ) +
    ggplot2::theme(
      panel.grid.major.y = ggplot2::element_blank()
    )
}

var_imp_base <- plot_var_imp_from_last_importance(last_base_fit) +
  ggplot2::labs(
    subtitle = 'Base xG model'
  )

ggplot2::ggsave(
  var_imp_base,
  filename = file.path(PROJ_DIR, 'var-imp-base.png'),
  width = 7,
  height = 7
)
```

![](var-imp-base.png)

Indeed, this is exactly what we see with the base xG model.

Now, if we make the same plot for our team-quality-adjusted xG model, we see that Elo and Elo difference are the second and third most important features! That's pretty interesting.

```{r}
#| label: var-imp-elo
var_imp_elo <- plot_var_imp_from_last_importance(last_elo_fit) +
  ggplot2::labs(
    subtitle = 'Elo-augmented xG model'
  )

ggplot2::ggsave(
  var_imp_elo,
  filename = file.path(PROJ_DIR, 'var-imp-elo.png'),
  width = 7,
  height = 7
)
```

Further, we can verify that the Elo-augmented model predicts xG in a manner that matches intuition using [partial dependence plots](https://christophm.github.io/interpretable-ml-book/pdp.html) (PDP). We should see that the augmented model predicts higher xG for teams with higher Elo.[^4]

[^4]: Partial dependence plots aren't quite the gold standard for model interpretability, but I find them useful for getting a sense of the orientation and magnitude of a feature's effect, particularly for non-linear modeling techniques like gradient boosting.

```{r}
#| label: elo-pdp-plot
#| include: false
## Don't use too much data so as to speed up the `pdp::partial` call.
sampled_test <- test |> 
  dplyr::filter(season_id == 2022)

baked_elo_data <- rec_elo |>
  recipes::prep() |>
  recipes::bake(new_data = sampled_test) |> 
  ## pdp::partial complains if the target variable is included
  dplyr::select(-scores)

elo_model_object <- hardhat::extract_fit_engine(last_elo_fit)
elo_pdp <- pdp::partial(
  elo_model_object,
  train = baked_elo_data,
  pred.var = 'elo',
  type = 'classification',
  plot = FALSE,
  prob = TRUE,
  trim.outliers = TRUE
)

elo_pdp_plot <- tibble::as_tibble(elo_pdp) |> 
  ggplot2::ggplot() +
  ggplot2::aes(
    x = elo,
    y = yhat
  ) +
  ggplot2::geom_path(
    color = 'white',
    linewidth = 1.5
  ) +
  ggplot2::scale_y_continuous(
    labels = scales::percent
  ) +
  ggplot2::labs(
    title = 'Average marginal effect of Elo on predicted xG',
    subtitle = 'Elo-augmented xG model',
    x = 'Elo',
    y = 'xG'
  )

ggplot2::ggsave(
  elo_pdp_plot,
  filename = file.path(PROJ_DIR, 'elo-pdp.png'),
  device = 'png',
  width = 7,
  height = 7 / 1.5
)
```

![](elo-pdp.png)

::: callout-note
Keep in mind that this shows just the average effect. On individual shots, the role of Elo could be stronger or weaker. And the range of the curve is constrained by the averaging--individual model outputs may be much lower than 12% or higher than 16% (i.e. the extremes of the y-axis).
:::

The PDP for the Elo difference feature looks similar, so I've omitted it. Nonetheless, we can go one layer deeper and look at the average marginal effect of Elo and Elo difference simultaneously.[^5]

[^5]: Note that the average marginal effect in the 2-D PDP below is shown via color instead of the y-axis.

```{r}
#| label: elo-pdp-2d-plot
#| include: false
elo_pdp_2d <- pdp::partial(
  elo_model_object,
  train = baked_elo_data,
  pred.var = c('elo', 'elo_diff'),
  type = 'classification',
  plot = FALSE,
  prob = TRUE,
  trim.outliers = TRUE
)

elo_pdp_2d_plot <- tibble::as_tibble(elo_pdp_2d) |> 
  ggplot2::ggplot() +
  ggplot2::aes(
    x = elo,
    y = elo_diff
  ) +
  ggplot2::geom_tile(
    ggplot2::aes(
      fill = yhat
    )
  ) +
  ggplot2::guides(
    fill = ggplot2::guide_legend(
      title = 'Model prediction (xG)'
    )
  ) +
  ggplot2::scale_fill_viridis_c(
    option = 'B',
    begin = 0.2,
    end = 0.9
  ) +
  ggplot2::labs(
    title = 'Average marginal effect of Elo and Elo difference<br/>on predicted xG',
    subtitle = 'Elo-augmented xG model',
    x = 'Elo',
    y = 'Elo difference'
  ) + 
  ggplot2::theme(
    panel.grid.major.x = ggplot2::element_blank(),
    panel.grid.major.y = ggplot2::element_blank(),
    axis.ticks.x = ggplot2::element_line(color = WHITISH_FOREGROUND_COLOR),
    axis.ticks.y = ggplot2::element_line(color = WHITISH_FOREGROUND_COLOR),
    axis.ticks.length = grid::unit(0.3, 'cm')
  )

ggplot2::ggsave(
  elo_pdp_2d_plot,
  filename = file.path(PROJ_DIR, 'elo-pdp-2d.png'),
  device = 'png',
  width = 7,
  height = 7
)
```

![](elo-pdp-2d.png)

As we should expect, predicted shot conversion is higher for strong teams and for teams that are stronger than their opponent.

### Model Evaluation

Let's take a look at whether the Elo-augmented model has a higher BSS than the "base" model.

```{r}
#| label: test-set-metrics
#| include: false
dplyr::bind_rows(
  tune::collect_metrics(last_base_fit) |> dplyr::mutate(model_type = 'Base'),
  tune::collect_metrics(last_elo_fit) |> dplyr::mutate(model_type = 'Elo-augmented')
) |> 
  dplyr::transmute(
    model_type,
    brier_skill_score = .estimate
  ) |> 
  knitr::kable(digits = 3)
```

| model_type    | brier_skill_score |
|:--------------|------------------:|
| Base          |             0.107 |
| Elo-augmented |             0.106 |

So, alas, it looks like there's basically no difference between the models![^6]

[^6]: One could run a bootstrap analysis to prove this with more statistical rigor, but I'll leave that as an exercise for the eager reader.

But BSS is just one, wholistic measure. We also wanted to look at the calibration of an xG model that accounts for team quality might look compared to that of a traditional xG model. To put that into context, let's circle back to the premise that xG models under-estimate the shot conversion of strong teams.

```{r}
#| label: test-set-calibration
#| include: false

# elo_breaks <- c(1400, 1650, 1700, 1750, 1800, 1850, 1900, 2000, 2100)
elo_breaks <- c(1400, 1650, 1700, 1750, 1850, 1950, 2100)
# elo_labels <- c(
#   paste0('<=', min(init_elo_breaks)), 
#   init_elo_breaks[1:(length(init_elo_breaks) - 1)], 
#   paste0('>', max(init_elo_breaks))
# )
preds <- dplyr::bind_rows(
  # tune::collect_predictions(last_base_fit) |> dplyr::mutate(model_type = 'base'),
  # tune::collect_predictions(last_elo_fit) |> dplyr::mutate(model_type = 'elo')
  broom::augment(last_base_fit) |> dplyr::mutate(model_type = 'base'),
  broom::augment(last_elo_fit) |> dplyr::mutate(model_type = 'elo')
) |> 
  dplyr::transmute(
    model_type,
    elo,
    elo_group = cut(
      elo, 
      breaks = elo_breaks, 
      # labels = elo_labels,
      dig.lab = 4,
      include.lower = TRUE
    ),
    scores,
    .pred_yes
  )
# preds |> count(elo_group)

calibration <- preds |> 
  tidyr::nest(data = -c(model_type, elo_group)) |> 
  dplyr::mutate(
    calibration = purrr::map(
      data,
      \(.x) probably::cal_plot_breaks(
        .x,
        truth = scores,
        estimate = .pred_yes
      ) |> 
        purrr::pluck('data')
    )
  ) |> 
  dplyr::select(model_type, elo_group, calibration) |> 
  tidyr::unnest(calibration)

# elo_levels <- levels(preds$elo_group)
# elo_pal <- setNames(
#   viridisLite::turbo(
#     length(elo_levels) + 2
#   )[2:(2 + length(elo_levels))],
#   elo_levels
# )

CAL_COLORS <- c(
  'Base' = '#ef426f', 
  'Elo-augmented' = '#00b2a9'
)

compared_calibration_plot <- calibration |> 
  ggplot2::ggplot() +
  ggplot2::aes(
    x = predicted_midpoint, 
    y = event_rate,
    color = model_type
  ) +
  ggplot2::facet_wrap(~elo_group, scales = 'fixed') +
  ggplot2::geom_line() +
  ggplot2::geom_point(
    ggplot2::aes(size = total)
  ) +
  ggplot2::scale_color_manual(
    values = CAL_COLORS
  ) +
  ggplot2::guides(
    color = ggplot2::guide_legend(
      title = '',
      override.aes = list(linewidth = 3)
    ),
    size = ggplot2::guide_legend(
      title = '# of shots',
      override.aes = list(color = WHITISH_FOREGROUND_COLOR)
    )
  ) +
  ggplot2::geom_abline(
    color = WHITISH_FOREGROUND_COLOR, 
    linetype = 2
  ) +
  ggplot2::scale_x_continuous(
    limits = c(0, 1),
    expand = c(0.01, 0.01)
  ) +
  ggplot2::scale_y_continuous(
    labels = scales::percent,
    limits = c(0, 1),
    expand = c(0.01, 0.01)
  ) +
  ggplot2::geom_segment(
    inherit.aes = FALSE,
    data = data.frame(
      elo_group = '(1400,1650]',
      x = 0.45,
      xend = 0.35,
      y = 0.6,
      yend = 0.7
    ),
    arrow = grid::arrow(length = grid::unit(6, 'pt'), type = 'closed'),
    color = WHITISH_FOREGROUND_COLOR,
    linewidth = 1,
    ggplot2::aes(
      x = x,
      xend = xend,
      y = y,
      yend = yend
    )
  ) +
  ggplot2::geom_segment(
    inherit.aes = FALSE,
    data = data.frame(
      elo_group = '(1400,1650]',
      x = 0.55,
      xend = 0.65,
      y = 0.4,
      yend = 0.3
    ),
    arrow = grid::arrow(length = grid::unit(6, 'pt'), type = 'closed'),
    color = WHITISH_FOREGROUND_COLOR,
    linewidth = 1,
    ggplot2::aes(
      x = x,
      xend = xend,
      y = y,
      yend = yend
    )
  ) +
  ggtext::geom_richtext(
    inherit.aes = FALSE,
    data = data.frame(
      elo_group = '(1400,1650]',
      x = 0.35,
      y = 0.75,
      label = '*Model under-predicts*'
    ),
    fill = NA, label.color = NA,
    label.padding = grid::unit(rep(0, 1), 'pt'),
    color = WHITISH_FOREGROUND_COLOR,
    family = FONT,
    hjust = 1,
    vjust = 1.1,
    size = 10 / ggplot2::.pt,
    ggplot2::aes(
      x = x,
      y = y,
      label = label
    )
  ) +
  ggtext::geom_richtext(
    inherit.aes = FALSE,
    data = data.frame(
      elo_group = '(1400,1650]',
      x = 0.65,
      y = 0.25,
      label = '*Model over-predicts*'
    ),
    fill = NA, label.color = NA,
    label.padding = grid::unit(rep(0, 1), 'pt'),
    color = WHITISH_FOREGROUND_COLOR,
    family = FONT,
    hjust = 0,
    vjust = -0.1,
    size = 10 / ggplot2::.pt,
    ggplot2::aes(
      x = x,
      y = y,
      label = label
    )
  ) +
  ggplot2::labs(
    title = 'Calibration of xG models',
    y = 'Actual shot conversion rate',
    x = 'Model prediction (xG)'
  )
compared_calibration_plot

ggplot2::ggsave(
  compared_calibration_plot,
  filename = file.path(PROJ_DIR, 'compared-calibration.png'),
  width = 10,
  height = 7
)
```

The table above indeed provides evidence for the miscalibration phenomenon--the "base" xG model tends to predict higher xG compared to actual goals scores for teams with lower Elo compared to higher Elo.

## Discussion

## Conclusion
