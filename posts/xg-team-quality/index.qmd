---
title: Should we account for team quality in an xG model?
description: "F**king around (with an xG model) and finding out"
date: 2023-12-31
categories:
  - r
  - soccer
image: elo-pdp.png
execute: 
  code-fold: false
  eval: false
  include: true
  echo: true
---

## Introduction

"Should we account for team quality in an [xG model](https://theanalyst.com/na/2021/07/what-are-expected-goals-xg/)?" From a purely philosophical point of view, my opinion is "no"--I think that xG models should be a purely "descriptive", agnostic to player and team abilities, as well as other non-event factors such as game state.

But I thought it might be fun to entertain the question from a quantitative perspective. If we add features for team strength to an xG model, can we meaningfully improve the model's overall predictive performance and [calibration](https://tonyelhabr.rbind.io/posts/probability-calibration/) (with respect to team quality)?

### Motivation

This write-up is inspired by [Ryan Brill](https://twitter.com/RyanBrill_)'s recent [presentation on fourth-down decision-making in the National Football League (NFL)](https://youtu.be/uS4XxQ0LVfE?si=BnmzeePnk3R5uiY3&t=361). He points out that [expected points (EP)](https://www.nfeloapp.com/analysis/expected-points-added-epa-nfl/) models in the NFL have a [selection bias](https://en.wikipedia.org/wiki/Selection_bias) problem--they tend to under-rate the probability of a positive outcome for "good" teams and over-rate such outcomes for "bad" teams.

Expected goals (xG) in soccer also show signs of this phenomenon. [Lars Maurath](https://github.com/larsmaurath) has [a great deep-dive](https://www.thesignificantgame.com/portfolio/do-naive-xg-models-underestimate-expected-goals-for-top-teams/) looking into expected goals under-estimation for strong teams in the [Big 5 European leagues](https://fbref.com/en/comps/Big5/Big-5-European-Leagues-Stats). The plot below (copied shamelessly from Lars' post) shows that a naive xG model consistently under-predicts Barcelona's goals over the course of the season, for seasons from 2007/08 to 2018/19. Even [StatsBomb](https://statsbomb.com/)'s model--which is more sophisticated--tends to underestimate the true cumulative goal total!

![](barcelona-cumulative-xg.png)

## Analysis and Results

### Data

I'll be using event data that I've ingested with the [`{socceraction}` package](https://github.com/ML-KULeuven/socceraction) (which I've made available [here](https://github.com/tonyelhabr/socceraction-streamlined/releases)!) for the 2013/14 through 2022/23 [English Premier League (EPL)](https://www.premierleague.com/) seasons. I'll focus on just "open-play" shots, i.e. shots excluding penalties and those taken from set pieces.

But we also need some measure of team quality. To that end, I've scraped [Elo](https://en.wikipedia.org/wiki/Elo_rating_system) ratings from [ClubElo](http://clubelo.com/). I've chosen Elo because provides a intuitive, sport-agnostic measure of relative skill. Also, it is calculated independent of the events that take place in a game, so correlation with measures of shot volume, quality, etc. are only coincidental. (It was also fairly easy to retrieve!)

```{r}
#| label: setup
#| code-fold: true
#| code-summary: Package imports and other setup

## Data retrieval
library(curl)
library(arrow)
library(qs) ## local dev

## Data manipulation
library(dplyr)
library(purrr)
library(lubridate)

## Modeling
library(rsample)
library(recipes)
library(parsnip)
library(workflows)
library(hardhat)

## Model tuning
library(tune)
library(dials)
library(workflowsets)
library(finetune)

## Model diagnostics
library(rlang)
library(yardstick)
library(pdp)
library(vip)

## Plotting
library(ggplot2)
library(sysfonts)
library(showtext)
library(ggtext)
library(htmltools)
library(scales)

PROJ_DIR <- 'posts/xg-team-quality'

TAG_LABEL <- htmltools::tagList(
  htmltools::tags$span(htmltools::HTML(enc2utf8("&#xf099;")), style = 'font-family:fb'),
  htmltools::tags$span("@TonyElHabr"),
)
SUBTITLE_LABEL <- 'English Premier League, 2012/13 - 2022/23'
PLOT_RESOLUTION <- 300
WHITISH_FOREGROUND_COLOR <- 'white'
COMPLEMENTARY_FOREGROUND_COLOR <- '#cbcbcb' # '#f1f1f1'
BLACKISH_BACKGROUND_COLOR <- '#1c1c1c'
COMPLEMENTARY_BACKGROUND_COLOR <- '#4d4d4d'
FONT <- 'Titillium Web'
sysfonts::font_add_google(FONT, FONT)
## https://github.com/tashapiro/tanya-data-viz/blob/main/chatgpt-lensa/chatgpt-lensa.R for twitter logo
sysfonts::font_add('fb', 'Font Awesome 6 Brands-Regular-400.otf')
showtext::showtext_auto()
showtext::showtext_opts(dpi = PLOT_RESOLUTION)

ggplot2::theme_set(ggplot2::theme_minimal())
ggplot2::theme_update(
  text = ggplot2::element_text(family = FONT),
  title = ggplot2::element_text(size = 20, color = WHITISH_FOREGROUND_COLOR),
  plot.title = ggtext::element_markdown(face = 'bold', size = 20, color = WHITISH_FOREGROUND_COLOR),
  plot.title.position = 'plot',
  plot.subtitle = ggtext::element_markdown(size = 16, color = COMPLEMENTARY_FOREGROUND_COLOR),
  axis.text = ggplot2::element_text(color = WHITISH_FOREGROUND_COLOR, size = 14),
  # axis.title = ggplot2::element_text(size = 14, color = WHITISH_FOREGROUND_COLOR, face = 'bold', hjust = 0.99),
  axis.title.x = ggtext::element_markdown(size = 14, color = WHITISH_FOREGROUND_COLOR, face = 'bold', hjust = 0.99),
  axis.title.y = ggtext::element_markdown(size = 14, color = WHITISH_FOREGROUND_COLOR, face = 'bold', hjust = 0.99),
  axis.line = ggplot2::element_blank(),
  strip.text = ggplot2::element_text(size = 14, color = WHITISH_FOREGROUND_COLOR, face = 'bold', hjust = 0),
  legend.position = 'top',
  legend.text = ggplot2::element_text(size = 12, color = WHITISH_FOREGROUND_COLOR, face = 'plain'),
  legend.title = ggplot2::element_text(size = 12, color = WHITISH_FOREGROUND_COLOR, face = 'bold'),
  panel.grid.major = ggplot2::element_line(color = COMPLEMENTARY_BACKGROUND_COLOR),
  panel.grid.minor = ggplot2::element_line(color = COMPLEMENTARY_BACKGROUND_COLOR),
  panel.grid.minor.x = ggplot2::element_blank(),
  panel.grid.minor.y = ggplot2::element_blank(),
  plot.margin = ggplot2::margin(10, 20, 10, 20),
  plot.background = ggplot2::element_rect(fill = BLACKISH_BACKGROUND_COLOR, color = BLACKISH_BACKGROUND_COLOR),
  plot.caption = ggtext::element_markdown(color = WHITISH_FOREGROUND_COLOR, hjust = 0, size = 10, face = 'plain'),
  plot.caption.position = 'plot',
  plot.tag = ggtext::element_markdown(size = 10, color = WHITISH_FOREGROUND_COLOR, hjust = 1),
  plot.tag.position = c(0.99, 0.01),
  panel.spacing.x = grid::unit(2, 'lines'),
  panel.background = ggplot2::element_rect(fill = BLACKISH_BACKGROUND_COLOR, color = BLACKISH_BACKGROUND_COLOR)
)
```

```{r}
#| label: data-pull
#| code-fold: true
#| code-summary: Retrieve and wrangle data
read_parquet_from_url <- function(url) {
  load <- curl::curl_fetch_memory(url)
  arrow::read_parquet(load$content)
}

REPO <- 'tonyelhabr/socceraction-streamlined'
read_socceraction_parquet_release <- function(name, tag) {
  url <- sprintf('https://github.com/%s/releases/download/%s/%s.parquet', REPO, tag, name)
  read_parquet_from_url(url)
}

read_socceraction_parquet_releases <- function(name, tag = 'data-processed') {
  purrr::map_dfr(
    2013:2022,
    \(season_start_year) {
      basename <- sprintf('8-%s-%s', season_start_year, name)
      message(basename)
      read_socceraction_parquet_release(basename, tag = tag)
    }
  )
}

read_socceraction_parquet <- function(name, branch = 'main') {
  url <- sprintf('https://github.com/%s/raw/%s/%s.parquet', REPO, branch, name)
  read_parquet_from_url(url)
}

x <- read_socceraction_parquet_releases('x')
y <- read_socceraction_parquet_releases('y')
actions <- read_socceraction_parquet_releases('actions')
games <- read_socceraction_parquet_releases('games') |> 
  dplyr::mutate(
    date = lubridate::date(game_date)
  )
team_elo <- read_socceraction_parquet('data/final/8/2013-2022/clubelo-ratings')

open_play_shots <- games |>
  dplyr::transmute(
    season_id,
    game_id,
    date,
    home_team_id,
    away_team_id
  ) |> 
  dplyr::inner_join(
    x |> 
      dplyr::filter(type_shot_a0 == 1) |> 
      dplyr::select(
        game_id,
        action_id,
        
        ## features
        start_x_a0,
        start_y_a0,
        start_dist_to_goal_a0,
        start_angle_to_goal_a0,
        type_dribble_a1,
        type_pass_a1,
        type_cross_a1,
        type_corner_crossed_a1,
        type_shot_a1,
        type_freekick_crossed_a1,
        bodypart_foot_a0,
        bodypart_head_a0,
        bodypart_other_a0
      ) |> 
      dplyr::mutate(
        dplyr::across(-c(game_id, action_id), as.integer)
      ),
    by = dplyr::join_by(game_id),
    relationship = 'many-to-many'
  ) |> 
  dplyr::inner_join(
    y |> 
      dplyr::transmute(
        game_id, 
        action_id,
        scores = ifelse(scores, 'yes', 'no') |> factor(levels = c('yes', 'no'))
      ),
    by = dplyr::join_by(game_id, action_id)
  ) |> 
  dplyr::inner_join(
    actions |> 
      dplyr::select(
        game_id,
        action_id,
        team_id,
        player_id
      ),
    by = dplyr::join_by(game_id, action_id)
  ) |> 
  dplyr::left_join(
    team_elo |> dplyr::select(date, home_team_id = team_id, home_elo = elo),
    by = dplyr::join_by(date, home_team_id)
  ) |> 
  dplyr::left_join(
    team_elo |> dplyr::select(date, away_team_id = team_id, away_elo = elo),
    by = dplyr::join_by(date, away_team_id)
  ) |> 
  dplyr::transmute(
    date,
    season_id,
    game_id,
    team_id,
    opponent_team_id = ifelse(team_id == home_team_id, away_team_id, home_team_id),
    action_id,
    
    scores,
    
    elo = ifelse(team_id == home_team_id, home_elo, away_elo),
    opponent_elo = ifelse(team_id == home_team_id, away_elo, home_elo),
    elo_diff = elo - opponent_elo,
    
    start_dist_to_goal_a0,
    start_angle_to_goal_a0,
    type_dribble_a1,
    type_pass_a1,
    type_cross_a1,
    type_corner_crossed_a1,
    type_shot_a1,
    type_freekick_crossed_a1,
    bodypart_foot_a0,
    bodypart_head_a0,
    bodypart_other_a0
  )
```

```{r}
#| label: data-pull-save
#| include: false
# library(qs)
qs::qsave(open_play_shots, file.path(PROJ_DIR, 'open_play_shots.qs'))
```

```{r}
#| label: data-pull-read
#| include: false
open_play_shots <- qs::qread(file.path(PROJ_DIR, 'open_play_shots.qs'))
```

The open play shot data looks like this.

```{r}
#| label: glimpse-data-pull
#| code-fold: false
dplyr::glimpse(open_play_shots)
#> Rows: 92,322
#> Columns: 21
#> $ date                     <date> 2012-08-18, 2012-08-18, 2012-08-18, 2012-…
#> $ season_id                <int> 2013, 2013, 2013, 2013, 2013, 2013, 2013, …
#> $ game_id                  <int> 614051, 614051, 614051, 614051, 614051, 61…
#> $ team_id                  <int> 16, 13, 16, 13, 13, 13, 13, 13, 13, 13, 16…
#> $ opponent_team_id         <dbl> 13, 16, 13, 16, 16, 16, 16, 16, 16, 16, 13…
#> $ action_id                <int> 109, 123, 276, 436, 502, 517, 541, 573, 61…
#> $ scores                   <fct> no, no, no, no, no, no, no, no, no, no, no…
#> $ elo                      <dbl> 1669.172, 1827.481, 1669.172, 1827.481, 18…
#> $ opponent_elo             <dbl> 1827.481, 1669.172, 1827.481, 1669.172, 16…
#> $ elo_diff                 <dbl> -158.3093, 158.3093, -158.3093, 158.3093, …
#> $ start_dist_to_goal_a0    <int> 12, 27, 21, 15, 8, 16, 29, 10, 26, 32, 30,…
#> $ start_angle_to_goal_a0   <int> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, …
#> $ type_dribble_a1          <int> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, …
#> $ type_pass_a1             <int> 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, …
#> $ type_cross_a1            <int> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, …
#> $ type_corner_crossed_a1   <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
#> $ type_shot_a1             <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
#> $ type_freekick_crossed_a1 <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
#> $ bodypart_foot_a0         <int> 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, …
#> $ bodypart_head_a0         <int> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, …
#> $ bodypart_other_a0        <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
```

We'll set aside the shots from the 7 seasons from 2013/14 to 2019/20 for our training set, and the 3 seasons from 2020/21 to 2022/23 for our test set.

```{r}
#| label: split-data
#| code-fold: true
#| code-summary: Split the data for modeling
split <- rsample::make_splits(
  open_play_shots |> dplyr::filter(season_id %in% c(2013L:2019L)),
  open_play_shots |> dplyr::filter(season_id %in% c(2020L:2022L))
)

train <- rsample::training(split)
test <- rsample::testing(split)
```

It's worth spotlighting Elo a bit more, since it's the novel feature here. Below is a look at the distribution of pre-match Elo over the course of the entire data set.

```{r}
#| label: elo-hist
#| include: false
elo_hist <- open_play_shots |> 
  dplyr::distinct(game_id, elo) |> 
  ggplot2::ggplot() +
  ggplot2::aes(
    x = elo
  ) +
  ggplot2::geom_histogram(binwidth = 25, fill = 'white') +
  ggplot2::labs(
    title = 'Distribution of Elo',
    subtitle = SUBTITLE_LABEL,
    y = 'Count',
    x = 'Elo',
    caption =  'Each point represents the pre-match Elo of a given team.<br/>There are 10 seasons x 380 matches x 2 sides = 7.6k data points in total.'
  )
ggplot2::ggsave(
  elo_hist,
  filename = file.path(PROJ_DIR, 'elo-hist.png'),
  device = 'png',
  width = 7,
  height = 7 / 1.5
)
```

![](elo-hist.png)

To make Elo values feel a bit more tangible, note that:

-   Man City tends to sustain an Elo greater than 1850 (arguably the best team in the EPL for the past decade).
-   Bottom-table teams tend to have Elos less than 1650.

```{r}
#| label: elo-diff-hist
#| include: false
elo_diff_hist <- open_play_shots |> 
  dplyr::distinct(game_id, elo_diff) |> 
  ggplot2::ggplot() +
  ggplot2::aes(
    x = elo_diff
  ) +
  ggplot2::geom_histogram(binwidth = 25, fill = 'white') +
  ggplot2::theme(
    plot.title = ggtext::element_markdown(size = 19)
  ) +
  ggplot2::labs(
    title = 'Distribution of the difference in Elo between opponents',
    subtitle = SUBTITLE_LABEL,
    y = 'Count',
    x = 'Elo',
    caption = 'Each point represents the pre-match difference between two teams.<br/>There are 10 seasons x 380 matches x 2 sides = 7.6k data points in total.'
  )
ggplot2::ggsave(
  elo_diff_hist,
  filename = file.path(PROJ_DIR, 'elo-diff-hist.png'),
  device = 'png',
  width = 7,
  height = 7 / 1.5
)
```

The pre-match difference team Elos follows a normal-ish distribution, with most values falling within the ±300 range.

![](elo-diff-hist.png)

### Model Training

The feature set for our "base" xG model consists of the following:

-   location of the shot (distance and angle of the shot to the center of the goal mouth)[^1].
-   type of action leading to the shot.
-   body part with which the shot was taken.

[^1]: One might get a slightly more performant by adding the `x` and `y` coordinates of the shot--to implicitly account for right-footed bias, for example--but I actually prefer not to add those in the model. Such terms can result in slight [over-fitting](https://en.wikipedia.org/wiki/Overfitting), in the presence of other features that provide information about the location of the shot, such as distance and angle. (This is the classical ["bias-variance" trade-off](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff).)

These features are essentially what [all xG models](https://fbref.com/en/expected-goals-model-explained/) have in common, although the exact implementation differs. Data providers such as [Opta also account for information](https://theanalyst.com/na/2023/08/what-is-expected-goals-xg/) that is not captured in traditional event data, such as the position of the goalkeeper.

For the team-quality-adjusted (or "Elo-augmented") model, I'll add two additional features:

-   `elo`: the Elo of the team of the shot-taker.
-   `elo_diff`: the difference in the ELO of the shot-taking team and the opposing team.

The former is meant to capture the opponent-agnostic quality of a team, while the latter captures the quality of the team relative to their opponent.

```{r}
#| label: model-recipes
#| code-fold: true
#| code-summary: Setting up the models
rec_elo <- recipes::recipe(
  scores ~ 
    elo +
    elo_diff +
    start_dist_to_goal_a0 +
    start_angle_to_goal_a0 +
    type_dribble_a1 +
    type_pass_a1 +
    type_cross_a1 +
    type_corner_crossed_a1 +
    type_shot_a1 +
    type_freekick_crossed_a1 +
    bodypart_foot_a0 +
    bodypart_head_a0 +
    bodypart_other_a0,
  data = train
)

rec_base <- rec_elo |> 
  recipes::step_rm(elo, elo_diff)
```

I'll be using [xgboost](https://xgboost.readthedocs.io/en/stable/) for our model, the state-of-the-art framework for tabular machine learning tasks. (It's also the type of model used by providers like Opta.) I'll choose [hyperparameters](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)) using [an efficient grid search](https://finetune.tidymodels.org/index.html), evaluating models with the [Brier skill score (BSS)](https://en.wikipedia.org/wiki/Brier_score#Brier_Skill_Score_(BSS)). For the reference Brier score for BSS, I'll use a dummy model that predicts 12% conversion for all shots.[^2]

[^2]: 12% is approximately the observed shot conversion rate in the whole data set.

::: {.callout-note collapse="true"}
## Brier skill score (BSS)

If this isn't the first blog post you've read of mine, you probably know that [I love to use BSS](https://tonyelhabr.rbind.io/posts/opta-xg-model-calibration) for classification tasks, especially for xG models.

But why BSS? Simply put, [Brier scores](https://en.wikipedia.org/wiki/Brier_score) are known as [the best evaluation metric](https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/) to use for classification tasks where you're purely interested in probabilities. And BSS goes one step beyond Brier scores, forcing one to contextualize the model evaluation with a reasonable baseline. In the context of xG, BSS helps us directly see whether our fitted model is better than a naive prediction, such as guessing that all shots convert at the observed shot conversion rate.

Keep in mind that a higher BSS is ideal. A perfect model would have a BSS of 1; a model that is no better than a reference model would have a BSS of 0.
:::

```{r}
#| label: brier-skill-score-implementation
#| code-fold: true
#| code-summary: Functions for Brier skill score
## See also: probability-calibration
brier_skill_score <- function(data, ...) {
  UseMethod('brier_skill_score')
}

brier_skill_score <- yardstick::new_prob_metric(
  brier_skill_score, 
  direction = 'maximize'
)

bss <- function(
    truth, 
    estimate, 
    ref_estimate, 
    event_level,
    case_weights,
    ...
) {
  
  if (length(estimate) == 1) {
    estimate <- rep(estimate, length(truth))
  }
  
  if (length(ref_estimate) == 1) {
    ref_estimate <- rep(ref_estimate, length(truth))
  }
  
  estimate_brier_score <- brier_class_vec(
    truth = truth,
    estimate = estimate,
    event_level = event_level,
    case_weights = case_weights,
    ...
  )
  
  ref_brier_score <- brier_class_vec(
    truth = truth,
    estimate = ref_estimate,
    event_level = event_level,
    case_weights = case_weights,
    ...
  )
  
  1 - (estimate_brier_score / ref_brier_score)
}

brier_skill_score_estimator_impl <- function(
    truth, 
    estimate, 
    ref_estimate, 
    event_level,
    case_weights
) {
  bss(
    truth = truth,
    estimate = estimate,
    ref_estimate = ref_estimate,
    event_level = event_level,
    case_weights = case_weights
  )
}

brier_skill_score_vec <- function(
    truth, 
    estimate, 
    ref_estimate, 
    na_rm = TRUE, 
    event_level = yardstick:::yardstick_event_level(),
    case_weights = NULL, 
    ...
) {
  
  yardstick:::abort_if_class_pred(truth)
  
  estimator <- yardstick::finalize_estimator(
    truth, 
    metric_class = 'brier_skill_score'
  )
  
  yardstick::check_prob_metric(truth, estimate, case_weights, estimator)
  
  if (na_rm) {
    result <- yardstick::yardstick_remove_missing(truth, estimate, case_weights)
    
    truth <- result$truth
    estimate <- result$estimate
    case_weights <- result$case_weights
  } else if (yardstick::yardstick_any_missing(truth, estimate, case_weights)) {
    return(NA_real_)
  }
  
  brier_skill_score_estimator_impl(
    truth = truth,
    estimate = estimate,
    ref_estimate = ref_estimate,
    event_level = event_level,
    case_weights = case_weights
  )
}

brier_skill_score.data.frame <- function(
    data, 
    truth, 
    ...,
    na_rm = TRUE,
    event_level = yardstick:::yardstick_event_level(),
    case_weights = NULL,
    
    ref_estimate = 0.5,
    name = 'brier_skill_score'
) {
  yardstick::prob_metric_summarizer(
    name = name,
    fn = brier_skill_score_vec,
    data = data,
    truth = !!rlang::enquo(truth),
    ...,
    na_rm = na_rm,
    event_level = event_level,
    case_weights = !!rlang::enquo(case_weights),
    fn_options = list(
      ref_estimate = ref_estimate
    )
  )
}

xg_brier_skill_score <- function(data, ...) {
  UseMethod('xg_brier_skill_score')
}

# REF_ESTIMATE <- open_play_shots |>
#   dplyr::summarize(goal_rate = sum(scores == 'yes') / dplyr::n()) |>
#   dplyr::pull(goal_rate)
REF_ESTIMATE <- 0.12

xg_brier_skill_score.data.frame <- function(...) {
  brier_skill_score(
    ref_estimate = REF_ESTIMATE,
    name = 'xg_brier_skill_score',
    ...
  )
}

xg_brier_skill_score <- yardstick::new_prob_metric(
  xg_brier_skill_score,
  direction = 'maximize'
)
```

```{r}
#| label: tune-models
#| code-fold: true
#| code-summary: Tuning the xG models
## Useful reference: https://jlaw.netlify.app/2022/01/24/predicting-when-kickers-get-iced-with-tidymodels/
TREES <- 500
LEARN_RATE <- 0.01
spec <- parsnip::boost_tree(
  trees = !!TREES,
  learn_rate = !!LEARN_RATE,
  tree_depth = tune::tune(),
  min_n = tune::tune(), 
  loss_reduction = tune::tune(),
  sample_size = tune::tune(), 
  mtry = tune::tune(),
  stop_iter = tune::tune()
) |>
  parsnip::set_engine('xgboost') |> 
  parsnip::set_mode('classification')

grid <- dials::grid_latin_hypercube(
  dials::tree_depth(),
  dials::min_n(range = c(5L, 40L)),
  dials::loss_reduction(),
  sample_size = dials::sample_prop(),
  dials::finalize(dials::mtry(), train),
  dials::stop_iter(range = c(10L, 50L)),
  size = 50
)

wf_sets <- workflowsets::workflow_set(
  preproc = list(
    base = rec_base, 
    elo = rec_elo
  ),
  models = list(
    model = spec
  ),
  cross = FALSE
)

control <- finetune::control_race(
  save_pred = TRUE,
  parallel_over = 'everything',
  save_workflow = TRUE,
  verbose = TRUE
)

set.seed(42)
train_folds <- rsample::vfold_cv(train, strata = scores, v = 5)

tuned_results <- workflowsets::workflow_map(
  wf_sets,
  fn = 'tune_race_anova',
  grid = grid,
  control = control,
  metrics = yardstick::metric_set(xg_brier_skill_score),
  resamples = train_folds,
  seed = 42
)
```

```{r}
#| label: tune-models-save
#| include: false
qs::qsave(tuned_results, file.path(PROJ_DIR, 'tuned_results2.qs'))
```

```{r}
#| label: tune-models-read
#| include: false
tuned_results <- qs::qread(file.path(PROJ_DIR, 'tuned_results2.qs'))
```

```{r}
#| label: best-sets
#| code-fold: true
#| code-summary: Choosing best hyper-parameters
MODEL_TYPES <- c(
  'base_model' = 'Base',
  'elo_model' = 'Elo-augmented'
)
select_best_model <- function(tuned_results, model_type) {
  tuned_results |>
    workflowsets::extract_workflow_set_result(model_type) |> 
    tune::select_best(metric = 'xg_brier_skill_score') |>
    dplyr::filter(min_n >= 5) |> 
    dplyr::slice(1) |> 
    dplyr::transmute(
      model_type = MODEL_TYPES[model_type],
      mtry,
      min_n,
      tree_depth, 
      loss_reduction,
      sample_size,
      stop_iter
    )
}

best_base_set <- select_best_model(tuned_results, 'base_model')
best_elo_set <- select_best_model(tuned_results, 'elo_model')

dplyr::bind_rows(
  best_base_set,
  best_elo_set
) |> 
  dplyr::transmute(
    model_type,
    mtry,
    min_n,
    tree_depth, 
    loss_reduction,
    sample_size,
    stop_iter
  ) |> 
  knitr::kable()
```

::: callout-warning
## Model tuning procedure

This post isn't meant to be so much about the why's and how's of model tuning and training, so I've spared commentary on the code.
:::

::: {.callout-note collapse="true"}
## Model hyperparameters

For reproducibility, the chosen hyperparameters are as follows. By coincidence, the same hyper-parameters are chosen. (Only 50 combinations were evaluated.)

| model_type    | mtry | min_n | tree_depth | loss_reduction | sample_size | stop_iter |
|:--------------|-----:|------:|-----------:|---------------:|------------:|----------:|
| Base          |    7 |    17 |          6 |              0 |   0.4641502 |        50 |
| Elo-augmented |    7 |    17 |          6 |              0 |   0.4641502 |        50 |

The number of `trees` and `learning_rate` were pre-defined to be 500 and 0.01 respectively.
:::

Finally, we fit singular models on the entire training set. These are the models that we'll use to evaluate the effect of team quality on xG.[^3]

[^3]: The hyperparameter search evaluated models fit on portions (or "folds") of the training data in a [cross-validation procedure](https://scikit-learn.org/stable/modules/cross_validation.html). We should fit one "final" model with all of the training data, per [best practice](https://tune.tidymodels.org/reference/last_fit.html).

```{r}
#| label: last-fits
#| code-fold: true
#| code-summary: Fit models on entire training set after choosing best hyper-parameters
finalize_tuned_results <- function(tuned_results, model_type) {
  best_set <- select_best_model(tuned_results, model_type)
  tuned_results |>
    hardhat::extract_workflow(model_type) |>
    tune::finalize_workflow(best_base_set) |> 
    tune::last_fit(
      split,
      metrics = yardstick::metric_set(xg_brier_skill_score)
    )
}
last_base_fit <- finalize_tuned_results(tuned_results, 'base_model')
last_elo_fit <- finalize_tuned_results(tuned_results, 'elo_model')
```

```{r}
#| label: last-fits-save
#| include: false
qs::qsave(last_base_fit, file.path(PROJ_DIR, 'last_base_fit.qs'))
qs::qsave(last_elo_fit, file.path(PROJ_DIR, 'last_elo_fit.qs'))
```

```{r}
#| label: last-fits-read
#| include: false
last_base_fit <- qs::qread(file.path(PROJ_DIR, 'last_base_fit.qs'))
last_elo_fit <- qs::qread(file.path(PROJ_DIR, 'last_elo_fit.qs'))
```

### Feature Importance

We should look to see that the xG models are behaving is expected. One way of doing so is to look at the feature importance.

In bespoke models, feature importance can be enlightening, as it tell us which features are contributing most to the predicted outcomes. In this case, I know that shot distance should be the most important feature (by far), as this is found to be the most important features in other similar "basic" public xG models, such as [this](https://github.com/AnshChoudhary/xGModel/blob/main/expected-goals-model-xg.ipynb) and [this](https://github.com/ML-KULeuven/soccer_xg/blob/master/notebooks/4-creating-custom-xg-pipelines.ipynb).

I like to use [SHAP values](https://christophm.github.io/interpretable-ml-book/shap.html) for feature importance since [they have nice "local" explanations and are found to be consistent in their "global" structure](https://liuyanguu.github.io/post/2019/07/18/visualization-of-shap-for-xgboost/).

::: {.callout-note collapse="true"}
## SHAP values

SHAP values quantify the impact of each feature in a predictive model on the model's output. For a binary classification task like ours, SHAP values are expressed on a 0-1 scale.

A SHAP value of 0.9 for a specific feature in a binary classification model doesn't mean that the feature contributes 90% to the prediction, nor does it directly imply that the predicted probability is 90%. Instead, it indicates that this particular feature contributes a value of 0.9 to the shift from the baseline prediction (usually the average of the training data's output) to the specific model output for that observation.
:::

```{r}
#| label: var-imp-base
#| include: false
FEATURE_LABELS <- c(
  'elo' = 'Elo',
  'elo_diff' = 'Elo difference',
  'start_dist_to_goal_a0' = 'Distance to goal',
  'start_angle_to_goal_a0' = 'Angle to goal',
  'type_dribble_a1' = 'Prev. action = dribble',
  'type_pass_a1' = 'Prev. action = pass',
  'type_cross_a1' = 'Prev. action = cross',
  'type_corner_crossed_a1' = 'Prev. action = crossed corner',
  'type_shot_a1' = 'Prev. action = shot',
  'type_freekick_crossed_a1' = 'Prev. action = crossed freekick',
  'bodypart_foot_a0' = 'Bodypart = foot',
  'bodypart_head_a0' = 'Bodypart = head',
  'bodypart_other_a0' = 'Bodypart = other'
)

## Don't use too much data so as to speed up the `pdp::partial` call.
sampled_test <- test |> 
  dplyr::filter(season_id == 2022)

bake_recipe <- function(rec, new_data) {
  rec |>
    recipes::prep() |>
    recipes::bake(new_data = new_data) |> 
    ## pdp::partial complains if the target variable is included
    dplyr::select(-scores)
}

baked_base_data <- bake_recipe(rec_base, sampled_test)
baked_elo_data <- bake_recipe(rec_elo, sampled_test)

base_model_object <- hardhat::extract_fit_engine(last_base_fit)
elo_model_object <- hardhat::extract_fit_engine(last_elo_fit)

shap_long_base <- SHAPforxgboost::shap.prep(
  xgb_model = base_model_object,
  X_train = as.matrix(baked_base_data)
)

shap_long_elo <- SHAPforxgboost::shap.prep(
  xgb_model = elo_model_object,
  X_train = as.matrix(baked_elo_data)
)

get_mean_abs_shap <- function(shap_long) {
  SHAPforxgboost:::shap.importance(shap_long) |> 
    tibble::as_tibble() |> 
    dplyr::transmute(
      ## shap.importance returns a factor!
      variable = forcats::fct_reorder(FEATURE_LABELS[as.character(variable)], mean_abs_shap),
      mean_abs_shap
    )
}

mean_abs_shap_base <- get_mean_abs_shap(shap_long_base)
mean_abs_shap_elo <- get_mean_abs_shap(shap_long_elo)

mean_abs_shap_constants <- list(
  ggplot2::scale_x_continuous(
    limits = c(0, 1)
  ),
  ggplot2::labs(
    x = 'mean(|SHAP value|)',
    y = NULL,
    caption = 'mean(|SHAP value|) = average impact on model output magnitude.'
  ),
  ggplot2::theme(
    panel.grid.major.y = ggplot2::element_blank()
  )
)

mean_abs_shap_base_plot <- mean_abs_shap_base |> 
  ggplot2::ggplot() +
  ggplot2::aes(
    x = mean_abs_shap,
    y = variable
  ) +
  ggplot2::geom_col(
    ggplot2::aes(
      x = mean_abs_shap,
      y = variable
    ),
    fill = WHITISH_FOREGROUND_COLOR
  ) +
  ggplot2::geom_text(
    family = FONT,
    color = WHITISH_FOREGROUND_COLOR,
    ggplot2::aes(label = scales::number(mean_abs_shap, accuracy = 0.01)),
    size = 12 / ggplot2::.pt,
    hjust = 0,
    nudge_x = 0.02
  ) +
  mean_abs_shap_constants +
  ggplot2::labs(
    title = 'Feature importance',
    subtitle = 'Base xG model'
  )

ggplot2::ggsave(
  mean_abs_shap_base_plot,
  filename = file.path(PROJ_DIR, 'var-imp-base.png'),
  width = 7,
  height = 7
)

MODEL_TYPE_PAL <- c(
  'Base' = '#ffbf00', 
  'Elo-augmented' = '#e83f6f'
)
mean_abs_shap_compared <- dplyr::bind_rows(
  mean_abs_shap_base |> dplyr::mutate(model_type = 'Base'),
  mean_abs_shap_elo |> dplyr::mutate(model_type = 'Elo-augmented')
) |> 
  tidyr::complete(model_type, variable) |> 
  dplyr::transmute(
    model_type,
    variable = factor(variable, levels(mean_abs_shap_elo$variable)),
    mean_abs_shap
  )
```

![](var-imp-base.png)

Indeed, this is exactly what we see with the base xG model.

Now, if we make the same plot for our team-quality-adjusted xG model, we see that Elo and Elo difference terms are in the middle of the pack in terms of feature importnace. That's pretty interesting.

```{r}
#| label: var-imp-compared
#| include: false
mean_abs_shap_compared_plot <- mean_abs_shap_compared |> 
  ggplot2::ggplot() +
  ggplot2::aes(
    x = mean_abs_shap,
    y = variable,
    fill = model_type
  ) +
  ggplot2::geom_col(
    ggplot2::aes(
      x = mean_abs_shap,
      y = variable
    ),
    position = ggplot2::position_dodge2(padding = 0.2, reverse = TRUE)
  ) +
  ggplot2::geom_text(
    data = mean_abs_shap_compared |> dplyr::filter(model_type == 'Base'),
    family = FONT,
    ggplot2::aes(
      color = model_type,
      label = scales::number(mean_abs_shap, accuracy = 0.01)
    ),
    size = 10 / ggplot2::.pt,
    hjust = 0,
    nudge_x = 0.02,
    nudge_y = +0.25
  ) +
  ggplot2::geom_text(
    data = mean_abs_shap_compared |> dplyr::filter(model_type == 'Elo-augmented'),
    family = FONT,
    ggplot2::aes(
      color = model_type,
      label = scales::number(mean_abs_shap, accuracy = 0.01)
    ),
    size = 10 / ggplot2::.pt,
    hjust = 0,
    nudge_x = 0.02,
    nudge_y = -0.25
  ) +
  ggplot2::scale_color_manual(values = MODEL_TYPE_PAL) +
  ggplot2::scale_fill_manual(values = MODEL_TYPE_PAL) +
  ggplot2::guides(
    color = 'none',
    fill = 'none'
  ) +
  mean_abs_shap_constants +
  ggplot2::labs(
    title = 'Feature importance comparison',
    subtitle = glue::glue('<b><span style="color:{MODEL_TYPE_PAL["Base"]}">Base</span></b> vs. <b><span style="color:{MODEL_TYPE_PAL["Elo-augmented"]}">Elo-augmented</span></b> xG model')
  )
mean_abs_shap_compared_plot

ggplot2::ggsave(
  mean_abs_shap_compared_plot,
  filename = file.path(PROJ_DIR, 'var-imp-compared.png'),
  width = 7,
  height = 7
)
```

Further, we can verify that the Elo-augmented model predicts xG in a manner that matches intuition using [partial dependence plots](https://christophm.github.io/interpretable-ml-book/pdp.html) (PDP). We should see that the augmented model predicts higher xG for teams with higher Elo.[^4]

[^4]: Partial dependence plots aren't quite the gold standard for model interpretability, but I find them useful for getting a sense of the orientation and magnitude of a feature's effect, particularly for non-linear modeling techniques like gradient boosting.

```{r}
#| label: elo-pdp-plot
#| include: false
elo_pdp <- pdp::partial(
  elo_model_object,
  train = baked_elo_data,
  pred.var = 'elo',
  type = 'classification',
  plot = FALSE,
  prob = TRUE,
  trim.outliers = TRUE
)

elo_pdp_plot <- tibble::as_tibble(elo_pdp) |> 
  ggplot2::ggplot() +
  ggplot2::aes(
    x = elo,
    y = yhat
  ) +
  ggplot2::geom_path(
    color = 'white',
    linewidth = 1.5
  ) +
  ggplot2::scale_y_continuous(
    labels = scales::percent
  ) +
  ggplot2::labs(
    title = 'Average marginal effect of Elo on predicted xG',
    subtitle = 'Elo-augmented xG model',
    x = 'Elo',
    y = 'xG'
  )

ggplot2::ggsave(
  elo_pdp_plot,
  filename = file.path(PROJ_DIR, 'elo-pdp.png'),
  device = 'png',
  width = 7,
  height = 7 / 1.5
)
```

![](elo-pdp.png)

::: {.callout-note collapse="true"}
Keep in mind that this shows just the average effect. On individual shots, the role of Elo could be stronger or weaker. And the range of the curve is constrained by the averaging--individual model outputs may be much lower than 12% or higher than 16% (i.e. the extremes of the y-axis).
:::

The PDP for the Elo difference feature looks similar, so I've omitted it. Nonetheless, we can go one layer deeper and look at the average marginal effect of Elo and Elo difference simultaneously.[^5]

[^5]: Note that the average marginal effect in the 2-D PDP below is shown via color instead of the y-axis.

```{r}
#| label: elo-pdp-2d-plot
#| include: false
elo_pdp_2d <- pdp::partial(
  elo_model_object,
  train = baked_elo_data,
  pred.var = c('elo', 'elo_diff'),
  type = 'classification',
  plot = FALSE,
  prob = TRUE,
  trim.outliers = TRUE
)

elo_pdp_2d_plot <- tibble::as_tibble(elo_pdp_2d) |> 
  ggplot2::ggplot() +
  ggplot2::aes(
    x = elo,
    y = elo_diff
  ) +
  ggplot2::geom_tile(
    ggplot2::aes(
      fill = yhat
    )
  ) +
  ggplot2::guides(
    fill = ggplot2::guide_legend(
      title = 'Model prediction (xG)'
    )
  ) +
  ggplot2::scale_fill_viridis_c(
    option = 'B',
    begin = 0.2,
    end = 0.9
  ) +
  ggplot2::labs(
    title = 'Average marginal effect of Elo and Elo difference<br/>on predicted xG',
    subtitle = 'Elo-augmented xG model',
    x = 'Elo',
    y = 'Elo difference'
  ) + 
  ggplot2::theme(
    panel.grid.major.x = ggplot2::element_blank(),
    panel.grid.major.y = ggplot2::element_blank(),
    axis.ticks.x = ggplot2::element_line(color = WHITISH_FOREGROUND_COLOR),
    axis.ticks.y = ggplot2::element_line(color = WHITISH_FOREGROUND_COLOR),
    axis.ticks.length = grid::unit(0.3, 'cm')
  )

ggplot2::ggsave(
  elo_pdp_2d_plot,
  filename = file.path(PROJ_DIR, 'elo-pdp-2d.png'),
  device = 'png',
  width = 7,
  height = 7
)
```

![](elo-pdp-2d.png)

As we should expect, predicted shot conversion is higher for strong teams and for teams that are stronger than their opponent.

```{r}
#| label: shap
#| include: false
shap_long <- SHAPforxgboost::shap.prep(
  xgb_model = elo_model_object, 
  X_train = as.matrix(baked_elo_data)
)
shap.plot.summary(shap_long)
shap_pdp <- SHAPforxgboost::shap.plot.dependence(
  data_long = shap_long, 
  x = 'elo_diff', 
  y = 'elo_diff', 
  color_feature = 'elo_diff'
)
shap_pdp
```

### Model Evaluation

Let's take a look at whether the Elo-augmented model has a higher BSS than the "base" model.

```{r}
#| label: test-set-metrics
#| include: false
dplyr::bind_rows(
  tune::collect_metrics(last_base_fit) |> dplyr::mutate(model_type = 'Base'),
  tune::collect_metrics(last_elo_fit) |> dplyr::mutate(model_type = 'Elo-augmented')
) |> 
  dplyr::transmute(
    model_type,
    brier_skill_score = .estimate
  ) |> 
  knitr::kable(digits = 3)
```

| model_type    | brier_skill_score |
|:--------------|------------------:|
| Base          |             0.107 |
| Elo-augmented |             0.106 |

So, alas, it looks like there's basically no difference between the models![^6]

[^6]: One could run a bootstrap analysis to prove this with more statistical rigor, but I'll leave that as an exercise for the eager reader.

But BSS is just one, wholistic measure. We also wanted to look at the calibration of an xG model that accounts for team quality might look compared to that of a traditional xG model. To put that into context, let's circle back to the premise that xG models under-estimate the shot conversion of strong teams.

```{r}
#| label: test-set-calibration
#| include: false

# elo_breaks <- c(1400, 1650, 1700, 1750, 1800, 1850, 1900, 2000, 2100)
elo_breaks <- c(1400, 1650, 1700, 1750, 1850, 1950, 2100)
# elo_labels <- c(
#   paste0('<=', min(init_elo_breaks)), 
#   init_elo_breaks[1:(length(init_elo_breaks) - 1)], 
#   paste0('>', max(init_elo_breaks))
# )
preds <- dplyr::bind_rows(
  # tune::collect_predictions(last_base_fit) |> dplyr::mutate(model_type = 'base'),
  # tune::collect_predictions(last_elo_fit) |> dplyr::mutate(model_type = 'elo')
  broom::augment(last_base_fit) |> dplyr::mutate(model_type = 'base'),
  broom::augment(last_elo_fit) |> dplyr::mutate(model_type = 'elo')
) |> 
  dplyr::transmute(
    model_type,
    elo,
    elo_group = cut(
      elo, 
      breaks = elo_breaks, 
      # labels = elo_labels,
      dig.lab = 4,
      include.lower = TRUE
    ),
    scores,
    .pred_yes
  )
# preds |> count(elo_group)

calibration <- preds |> 
  tidyr::nest(data = -c(model_type, elo_group)) |> 
  dplyr::mutate(
    calibration = purrr::map(
      data,
      \(.x) probably::cal_plot_breaks(
        .x,
        truth = scores,
        estimate = .pred_yes
      ) |> 
        purrr::pluck('data')
    )
  ) |> 
  dplyr::select(model_type, elo_group, calibration) |> 
  tidyr::unnest(calibration)

# elo_levels <- levels(preds$elo_group)
# elo_pal <- setNames(
#   viridisLite::turbo(
#     length(elo_levels) + 2
#   )[2:(2 + length(elo_levels))],
#   elo_levels
# )

compared_calibration_plot <- calibration |> 
  ggplot2::ggplot() +
  ggplot2::aes(
    x = predicted_midpoint, 
    y = event_rate,
    color = model_type
  ) +
  ggplot2::facet_wrap(~elo_group, scales = 'fixed') +
  ggplot2::geom_line() +
  ggplot2::geom_point(
    ggplot2::aes(size = total)
  ) +
  ggplot2::scale_color_manual(
    values = MODEL_TYPE_PAL
  ) +
  ggplot2::guides(
    color = ggplot2::guide_legend(
      title = '',
      override.aes = list(linewidth = 3)
    ),
    size = ggplot2::guide_legend(
      title = '# of shots',
      override.aes = list(color = WHITISH_FOREGROUND_COLOR)
    )
  ) +
  ggplot2::geom_abline(
    color = WHITISH_FOREGROUND_COLOR, 
    linetype = 2
  ) +
  ggplot2::scale_x_continuous(
    limits = c(0, 1),
    expand = c(0.01, 0.01)
  ) +
  ggplot2::scale_y_continuous(
    labels = scales::percent,
    limits = c(0, 1),
    expand = c(0.01, 0.01)
  ) +
  ggplot2::geom_segment(
    inherit.aes = FALSE,
    data = data.frame(
      elo_group = '(1400,1650]',
      x = 0.45,
      xend = 0.35,
      y = 0.6,
      yend = 0.7
    ),
    arrow = grid::arrow(length = grid::unit(6, 'pt'), type = 'closed'),
    color = WHITISH_FOREGROUND_COLOR,
    linewidth = 1,
    ggplot2::aes(
      x = x,
      xend = xend,
      y = y,
      yend = yend
    )
  ) +
  ggplot2::geom_segment(
    inherit.aes = FALSE,
    data = data.frame(
      elo_group = '(1400,1650]',
      x = 0.55,
      xend = 0.65,
      y = 0.4,
      yend = 0.3
    ),
    arrow = grid::arrow(length = grid::unit(6, 'pt'), type = 'closed'),
    color = WHITISH_FOREGROUND_COLOR,
    linewidth = 1,
    ggplot2::aes(
      x = x,
      xend = xend,
      y = y,
      yend = yend
    )
  ) +
  ggtext::geom_richtext(
    inherit.aes = FALSE,
    data = data.frame(
      elo_group = '(1400,1650]',
      x = 0.35,
      y = 0.75,
      label = '*Model under-predicts*'
    ),
    fill = NA, label.color = NA,
    label.padding = grid::unit(rep(0, 1), 'pt'),
    color = WHITISH_FOREGROUND_COLOR,
    family = FONT,
    hjust = 1,
    vjust = 1.1,
    size = 10 / ggplot2::.pt,
    ggplot2::aes(
      x = x,
      y = y,
      label = label
    )
  ) +
  ggtext::geom_richtext(
    inherit.aes = FALSE,
    data = data.frame(
      elo_group = '(1400,1650]',
      x = 0.65,
      y = 0.25,
      label = '*Model over-predicts*'
    ),
    fill = NA, label.color = NA,
    label.padding = grid::unit(rep(0, 1), 'pt'),
    color = WHITISH_FOREGROUND_COLOR,
    family = FONT,
    hjust = 0,
    vjust = -0.1,
    size = 10 / ggplot2::.pt,
    ggplot2::aes(
      x = x,
      y = y,
      label = label
    )
  ) +
  ggplot2::labs(
    title = 'Calibration of xG models',
    y = 'Actual shot conversion rate',
    x = 'Model prediction (xG)'
  )
compared_calibration_plot

ggplot2::ggsave(
  compared_calibration_plot,
  filename = file.path(PROJ_DIR, 'compared-calibration.png'),
  width = 10,
  height = 7
)
```

The table above indeed provides evidence for the miscalibration phenomenon--the "base" xG model tends to predict higher xG compared to actual goals scores for teams with lower Elo compared to higher Elo.

## Discussion

We've seen that there is no clear evidence that accounting for team quality improves an xG model. Sure, it can provide value to an xG model--see the feature importance for the Elo-augmented model--but it's effectively just providing a difference kind of nuance that would be otherwise captures by other traditional xG features.

Sso naturally one may ask--what about player quality? Or goalkeeper quality? While I do think those would be a little more useful in terms of making a better xG model, the better question is whether we should be accounting for these kinds of "quality" features at all. I think "no", mostly due to the its typical and potential applications.

An xG model cannot be used to "intervene" with shots in real-time like a fourth-down model might be "actively" used to make a choice in the NFL. An NFL coach probably wants a model to incorporate their team's strength so as to make an informed decision about whether to go for it.

In contrast, xG models are most commonly used in a "passive" manner, to understand how a team or a specific player might be performing compared to expectation. Soccer coaches and analysts often look at the actual goals scored minus the expected goals accumulated (the "residual") to diagnose under- and over-performance. Yes, they also look at expected goals in a more broad manner to get a sense of where and how they can get better shots, but that is not real-time in the same sense that a fourth-down decision is made in real-time.

Arguably incorporating team or player quality into an xG model can make interpretation more difficult, at least in the manner that we typically use xG to contextualize a game or a season. (xGods please forgive me for looking at single-game xG.) As an extreme example, let's say that we add an "Is Messi?" feature to an xG model (not unlike what Ryan suggested with Justin Tucker in his presentation) to achieve a slightly more accurate xG model. (The model will learn that Messi has historically converted shots into goals at rates higher than nearly all other players.) But then your game- and season-level analysis may need change.

I may look at an Inter Miami game where they "lost" on actual goals, let's say 2 to 1, AND "lost" on xG, let's say 0.9 to 1.7, and think "oh, that's a fair result". But if Messi took 4 shots and we're using an expected goals model with the "Is Messi?" feature, the Inter Miami xG might appear to be 2.2, perhaps leading to a different take on the game---"that was Inter Miami's game, but they failed to convert on some key chances".

tldr: Building in "quality" features into a model used to evaluate quality (of shots) arguably leads to circular reasoning. I prefer xG to be agnostic to team or player quality, at least in the current paradigm of soccer analytics, where we often use xG to evaluate the quality of teams and players.

## Conclusion
