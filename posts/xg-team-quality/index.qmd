---
title: Should we account for team quality in an xG model?
description: "F**king around (with an xG model) and finding out"
date: 2023-12-31
categories:
  - r
  - soccer
image: image.png
draft: true
execute: 
  code-fold: false
  eval: false
  include: true
  echo: true
---

## Introduction

"Should we account for team quality in an [xG model](https://theanalyst.com/na/2021/07/what-are-expected-goals-xg/)?" From a purely philosophical point of view, my opinion is "no". (See Discussion section later.) But I thought it might be fun to entertain the question from a quantitative perspective. If we add features for team strength to an xG model, can we improve the model's (1) predictive performance and (2) [calibration](/posts/probability-calibration)?

### Motivation

This write-up is inspired by [Ryan Brill](https://twitter.com/RyanBrill_)'s recent [presentation on fourth-down decision-making in the National Football League (NFL)](https://youtu.be/uS4XxQ0LVfE?si=BnmzeePnk3R5uiY3&t=361). He points out that [expected points (EP)](https://www.nfeloapp.com/analysis/expected-points-added-epa-nfl/) models in the NFL have a [selection bias](https://en.wikipedia.org/wiki/Selection_bias) problem--they tend to under-rate the probability of a positive outcome for "good" teams and over-rate such outcomes for "bad" teams.

For those of us that have a good sense of how expected goals (xG) work in soccer, we know that this phenomenon also occurs in our beloved sport. [Lars Maurath](https://github.com/larsmaurath) has [a great deep-dive](https://www.thesignificantgame.com/portfolio/do-naive-xg-models-underestimate-expected-goals-for-top-teams/) looking into expected goals underestimation for strong teams in the [Big 5 European leagues](https://fbref.com/en/comps/Big5/Big-5-European-Leagues-Stats). The plot below (copied shamelessly from Lars' post) shows that a naive xG model consistently under-predicts Barcelona's goals over the course of the season, for seasons from 2007/08 to 2018/19. Even [StatsBomb](https://statsbomb.com/)'s model--which is more sophisticated--tends to underestimate the true cumulative goal total!

![](barcelona-cumulative-xg.png)

## Analysis and Results

### Data

I'll be using event data that I've ingested with the [`{socceraction}` package](https://github.com/ML-KULeuven/socceraction) (which I've made available [here](https://github.com/tonyelhabr/socceraction-streamlined/releases)!) for the 2013/14 through 2022/23 [English Premier League (EPL)](https://www.premierleague.com/) seasons. We'll focus on just "open-play" shots, i.e. shots excluding penalties and those taken from set pieces.

As for the measure of team quality, I've scraped [Elo](https://en.wikipedia.org/wiki/Elo_rating_system) ratings from [ClubElo](http://clubelo.com/). I've chosen Elo because provides a intuitive, sport-agnostic measure of relative skill. Also, it is calculated independent of the events that take place in a game, so correlation with measures of shot volume, quality, etc. are only coincidental. (It was also fairly easy to retrieve!)

```{r}
#| label: setup
#| code-fold: true
#| code-summary: Libraries

## Data retrieval
library(curl)
library(arrow)
library(qs) ## local dev

## Data manipulation
library(dplyr)
library(purrr)
library(lubridate)

## Modeling
library(rsample)
library(recipes)
library(parsnip)
library(workflows)
library(hardhat)

## Model tuning
library(tune)
library(dials)
library(workflowsets)
library(finetune)

## Model diagnostics
library(rlang)
library(yardstick)
library(pdp)
library(vip)

## Plotting
library(ggplot2)
library(scales)
```

```{r}
#| label: data-pull
#| code-fold: true
#| code-summary: Retrieve and wrangle data

read_parquet_from_url <- function(url) {
  load <- curl::curl_fetch_memory(url)
  arrow::read_parquet(load$content)
}

REPO <- 'tonyelhabr/socceraction-streamlined'
read_socceraction_parquet_release <- function(name, tag) {
  url <- sprintf('https://github.com/%s/releases/download/%s/%s.parquet', REPO, tag, name)
  read_parquet_from_url(url)
}

read_socceraction_parquet_releases <- function(name, tag = 'data-processed') {
  purrr::map_dfr(
    2013:2022,
    \(season_start_year) {
      basename <- sprintf('8-%s-%s', season_start_year, name)
      message(basename)
      read_socceraction_parquet_release(basename, tag = tag)
    }
  )
}

read_socceraction_parquet <- function(name, branch = 'main') {
  url <- sprintf('https://github.com/%s/raw/%s/%s.parquet', REPO, branch, name)
  read_parquet_from_url(url)
}

x <- read_socceraction_parquet_releases('x')
y <- read_socceraction_parquet_releases('y')
actions <- read_socceraction_parquet_releases('actions')
games <- read_socceraction_parquet_releases('games') |> 
  dplyr::mutate(
    date = lubridate::date(game_date)
  )
team_elo <- read_socceraction_parquet('data/final/8/2013-2022/clubelo-ratings')

open_play_shots <- games |>
  dplyr::transmute(
    season_id,
    game_id,
    date,
    home_team_id,
    away_team_id
  ) |> 
  dplyr::inner_join(
    x |> 
      dplyr::filter(type_shot_a0 == 1) |> 
      dplyr::select(
        game_id,
        action_id,
        
        ## features
        start_x_a0,
        start_y_a0,
        start_dist_to_goal_a0,
        start_angle_to_goal_a0,
        type_dribble_a1,
        type_pass_a1,
        type_cross_a1,
        type_corner_crossed_a1,
        type_shot_a1,
        type_freekick_crossed_a1,
        bodypart_foot_a0,
        bodypart_head_a0,
        bodypart_other_a0
      ) |> 
      dplyr::mutate(
        dplyr::across(-c(game_id, action_id), as.integer)
      ),
    by = dplyr::join_by(game_id),
    relationship = 'many-to-many'
  ) |> 
  dplyr::inner_join(
    y |> 
      dplyr::transmute(
        game_id, 
        action_id,
        scores = ifelse(scores, 'yes', 'no') |> factor(levels = c('yes', 'no'))
      ),
    by = dplyr::join_by(game_id, action_id)
  ) |> 
  dplyr::inner_join(
    actions |> 
      dplyr::select(
        game_id,
        action_id,
        team_id,
        player_id
      ),
    by = dplyr::join_by(game_id, action_id)
  ) |> 
  dplyr::left_join(
    team_elo |> dplyr::select(date, home_team_id = team_id, home_elo = elo),
    by = dplyr::join_by(date, home_team_id)
  ) |> 
  dplyr::left_join(
    team_elo |> dplyr::select(date, away_team_id = team_id, away_elo = elo),
    by = dplyr::join_by(date, away_team_id)
  ) |> 
  dplyr::transmute(
    date,
    season_id,
    game_id,
    team_id,
    opponent_team_id = ifelse(team_id == home_team_id, away_team_id, home_team_id),
    action_id,

    scores,
    
    elo = ifelse(team_id == home_team_id, home_elo, away_elo),
    opponent_elo = ifelse(team_id == home_team_id, away_elo, home_elo),
    elo_diff = elo - opponent_elo,
    
    start_dist_to_goal_a0,
    start_angle_to_goal_a0,
    type_dribble_a1,
    type_pass_a1,
    type_cross_a1,
    type_corner_crossed_a1,
    type_shot_a1,
    type_freekick_crossed_a1,
    bodypart_foot_a0,
    bodypart_head_a0,
    bodypart_other_a0
  )
```

```{r}
#| label: data-pull-save
#| include: false
# library(qs)
PROJ_DIR <- 'posts/xg-team-quality'
qs::qsave(open_play_shots, file.path(PROJ_DIR, 'open_play_shots.qs'))
```

```{r}
#| label: data-pull-read
#| include: false
PROJ_DIR <- 'posts/xg-team-quality'
open_play_shots <- qs::qread(file.path(PROJ_DIR, 'open_play_shots.qs'))
```

The open play shot data looks like this.

```{r}
#| label: glimpse-data-pull
#| code-fold: false
dplyr::glimpse(open_play_shots)
#> Rows: 92,322
#> Columns: 21
#> $ date                     <date> 2012-08-18, 2012-08-18, 2012-08-18, 2012-…
#> $ season_id                <int> 2013, 2013, 2013, 2013, 2013, 2013, 2013, …
#> $ game_id                  <int> 614051, 614051, 614051, 614051, 614051, 61…
#> $ team_id                  <int> 16, 13, 16, 13, 13, 13, 13, 13, 13, 13, 16…
#> $ opponent_team_id         <dbl> 13, 16, 13, 16, 16, 16, 16, 16, 16, 16, 13…
#> $ action_id                <int> 109, 123, 276, 436, 502, 517, 541, 573, 61…
#> $ scores                   <fct> no, no, no, no, no, no, no, no, no, no, no…
#> $ elo                      <dbl> 1669.172, 1827.481, 1669.172, 1827.481, 18…
#> $ opponent_elo             <dbl> 1827.481, 1669.172, 1827.481, 1669.172, 16…
#> $ elo_diff                 <dbl> -158.3093, 158.3093, -158.3093, 158.3093, …
#> $ start_dist_to_goal_a0    <int> 12, 27, 21, 15, 8, 16, 29, 10, 26, 32, 30,…
#> $ start_angle_to_goal_a0   <int> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, …
#> $ type_dribble_a1          <int> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, …
#> $ type_pass_a1             <int> 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, …
#> $ type_cross_a1            <int> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, …
#> $ type_corner_crossed_a1   <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
#> $ type_shot_a1             <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
#> $ type_freekick_crossed_a1 <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
#> $ bodypart_foot_a0         <int> 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, …
#> $ bodypart_head_a0         <int> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, …
#> $ bodypart_other_a0        <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
```

We'll set aside the shots from the 7 seasons from 2013/14 to 2019/20 for our training set, and the 3 seasons from 2020/21 to 2022/23 for our test set.

```{r}
#| label: split-data
#| code-fold: true
#| code-summary: Split the data for modeling
split <- rsample::make_splits(
  open_play_shots |> dplyr::filter(season_id %in% c(2013L:2019L)),
  open_play_shots |> dplyr::filter(season_id %in% c(2020L:2022L))
)

train <- rsample::training(split)
test <- rsample::testing(split)
```

## Model Training

For features, we'll use the following for our "base" model:

-   location of the shot (`start_dist_to_goal_a0` and `start_angle_to_goal_a0`)[^1]
-   type of action leading to the shot (`type_dribble_a1`, etc.)
-   body part with which the shot was taken (`bodypart_foot_a0`, etc.).

[^1]: One might get a slightly more performant by adding the `x` and `y` coordinates of the shot (to implicitly account for right-footed shots, inswinging shots, etc.), but I actually prefer not to add those in the model. Such terms can result in slight over-fitting, in the presence of other features that provide information about the location of the shot, such as distance and angle (This is the classical ["bias-variance" trade-off](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff).)

These features are essentially what [all xG models](https://fbref.com/en/expected-goals-model-explained/) have in common, although the exact implementation differs. Data providers such as [Opta also account for information](https://theanalyst.com/na/2023/08/what-is-expected-goals-xg/) that is not captured in traditional event data, such as the position of the goalkeeper.

For the Elo-augmented model, we'll add two additional features:

-   `elo`: the Elo of the team of the shot-taker
-   `elo_diff`: the difference in the ELO of the shot-taking team and the opposing team.

The former is meant to capture the opponent-agnostic quality of a team, while the latter captures the quality of the team relative to their opponent.

```{r}
#| label: model-recipes
#| code-fold: true
#| code-summary: Setting up the model
rec_elo <- recipes::recipe(
  scores ~ 
    elo +
    elo_diff +
    start_dist_to_goal_a0 +
    start_angle_to_goal_a0 +
    type_dribble_a1 +
    type_pass_a1 +
    type_cross_a1 +
    type_corner_crossed_a1 +
    type_shot_a1 +
    type_freekick_crossed_a1 +
    bodypart_foot_a0 +
    bodypart_head_a0 +
    bodypart_other_a0,
  data = train
)

rec_base <- rec_elo |> 
  recipes::step_rm(elo, elo_diff)
```

We'll be using [xgboost](https://xgboost.readthedocs.io/en/stable/) for our model, the state-of-the-art for tabular machine learning tasks. (It's also the type of model used by providers like Opta.) We'll choose [hyperparameters](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)) using [an efficient grid search](https://finetune.tidymodels.org/index.html), evaluating models with the [F-score](https://en.wikipedia.org/wiki/F-score).[^2]

[^2]: The F-score is a good evaluation metric when we don't have a strong opinion on whether false positives or false negatives are "worse". Further, it has desireable properties for "imbalanced" data sets such as this one, where the positive class (i.e. the shot is made) is fairly infrequent (about \~11.5% of all shots).

```{r}
#| label: brier-skill-score-implementation
#| code-fold: true
#| code-summary: Brier skill score implementation
## See also: /posts/probability-calibrations
brier_skill_score <- function(data, ...) {
  UseMethod('brier_skill_score')
}

brier_skill_score <- yardstick::new_prob_metric(
  brier_skill_score, 
  direction = 'maximize'
)

bss <- function(
    truth, 
    estimate, 
    ref_estimate, 
    event_level,
    case_weights,
    ...
) {
  
  if (length(estimate) == 1) {
    estimate <- rep(estimate, length(truth))
  }
  
  if (length(ref_estimate) == 1) {
    ref_estimate <- rep(ref_estimate, length(truth))
  }
  
  estimate_brier_score <- brier_class_vec(
    truth = truth,
    estimate = estimate,
    event_level = event_level,
    case_weights = case_weights,
    ...
  )
  
  ref_brier_score <- brier_class_vec(
    truth = truth,
    estimate = ref_estimate,
    event_level = event_level,
    case_weights = case_weights,
    ...
  )
  
  1 - (estimate_brier_score / ref_brier_score)
}

brier_skill_score_estimator_impl <- function(
    truth, 
    estimate, 
    ref_estimate, 
    event_level,
    case_weights
) {
  bss(
    truth = truth,
    estimate = estimate,
    ref_estimate = ref_estimate,
    event_level = event_level,
    case_weights = case_weights
  )
}

brier_skill_score_vec <- function(
    truth, 
    estimate, 
    ref_estimate, 
    na_rm = TRUE, 
    event_level = yardstick:::yardstick_event_level(),
    case_weights = NULL, 
    ...
) {
  
  yardstick:::abort_if_class_pred(truth)
  
  estimator <- yardstick::finalize_estimator(
    truth, 
    metric_class = 'brier_skill_score'
  )
  
  yardstick::check_prob_metric(truth, estimate, case_weights, estimator)
  
  if (na_rm) {
    result <- yardstick::yardstick_remove_missing(truth, estimate, case_weights)
    
    truth <- result$truth
    estimate <- result$estimate
    case_weights <- result$case_weights
  } else if (yardstick::yardstick_any_missing(truth, estimate, case_weights)) {
    return(NA_real_)
  }
  
  brier_skill_score_estimator_impl(
    truth = truth,
    estimate = estimate,
    ref_estimate = ref_estimate,
    event_level = event_level,
    case_weights = case_weights
  )
}

brier_skill_score.data.frame <- function(
    data, 
    truth, 
    ...,
    ref_estimate = 0.5,
    na_rm = TRUE,
    event_level = yardstick:::yardstick_event_level(),
    case_weights = NULL
) {
  yardstick::prob_metric_summarizer(
    name = 'brier_skill_score',
    fn = brier_skill_score_vec,
    data = data,
    truth = !!rlang::enquo(truth),
    ...,
    na_rm = na_rm,
    event_level = event_level,
    case_weights = !!rlang::enquo(case_weights),
    fn_options = list(
      ref_estimate = ref_estimate
    )
  )
}

xg_brier_skill_score <- function(data, ...) {
  UseMethod('xg_brier_skill_score')
}

# REF_ESTIMATE <- open_play_shots |>
#   dplyr::summarize(goal_rate = sum(scores == 'yes') / dplyr::n()) |>
#   dplyr::pull(goal_rate)
REF_ESTIMATE <- 0.12

xg_brier_skill_score.data.frame <- function(...) {
  brier_skill_score(
    ref_estimate = REF_ESTIMATE,
    ...
  )
}

xg_brier_skill_score <- yardstick::new_prob_metric(
  xg_brier_skill_score,
  direction = 'maximize'
)
```

```{r}
#| label: tune-models
#| code-fold: true
#| code-summary: Tune model
TREES <- 500
LEARN_RATE <- 0.01
define_xgboost_spec <- function(...) {
  parsnip::boost_tree(
    trees = TREES,
    learn_rate = LEARN_RATE,
    tree_depth = tune::tune(),
    min_n = tune::tune(), 
    loss_reduction = tune::tune(),
    sample_size = tune::tune(), 
    mtry = tune::tune(),
    stop_iter = tune::tune()
  ) |>
    parsnip::set_engine('xgboost', ...) |> 
    parsnip::set_mode('classification')
}

spec_base <- define_xgboost_spec()

elo_features <- rec_elo |> recipes::prep() |> recipes::juice() |> colnames()
spec_elo <- define_xgboost_spec(
  monotone_constraints = !!ifelse(elo_features %in% c('elo', 'elo_diff'), 1, 0)
)

grid <- dials::grid_latin_hypercube(
  dials::tree_depth(),
  dials::min_n(),
  dials::loss_reduction(),
  sample_size = dials::sample_prop(),
  dials::finalize(dials::mtry(), train),
  dials::stop_iter(range = c(10L, 50L)),
  size = 50
)

wf_sets <- workflowsets::workflow_set(
  preproc = list(
    base = rec_base, 
    elo = rec_elo
  ),
  ## Separate specs because we want monotonic constraints on the Elo model
  models = list(
    model = spec_base,
    model = spec_elo
  ),
  cross = FALSE
)

control <- finetune::control_race(
  save_pred = TRUE,
  parallel_over = 'everything',
  save_workflow = TRUE,
  verbose = TRUE,
  verbose_elim = TRUE
)

set.seed(42)
train_folds <- rsample::vfold_cv(train, strata = scores, v = 5)

tuned_results <- workflowsets::workflow_map(
  wf_sets,
  fn = 'tune_race_anova',
  grid = grid,
  control = control,
  # metrics = yardstick::metric_set(yardstick::f_meas),
  metrics = yardstick::metric_set(xg_brier_skill_score),
  resamples = train_folds,
  seed = 42
)
```

```{r}
#| label: tune-models-save
#| include: false
qs::qsave(tuned_results, file.path(PROJ_DIR, 'tuned_results.qs'))
```

```{r}
#| label: tune-models-save
#| include: false
tuned_results <- qs::qread(file.path(PROJ_DIR, 'tuned_results.qs'))
```

```{r}
#| label: best-sets
#| code-fold: true
#| code-summary: Choosing best hyper-parameters
best_base_set <- tuned_results |>
  extract_workflow_set_result('base_model') |> 
  select_best(metric = 'f_meas')

best_elo_set <- tuned_results |>
  extract_workflow_set_result('elo_model') |> 
  select_best(metric = 'f_meas')

dplyr::bind_rows(
  best_base_set |> dplyr::mutate(model_type = 'base'),
  best_elo_set |> dplyr::mutate(model_type = 'elo')
) |> 
  dplyr::transmute(
    model_type,
    mtry,
    min_n,
    tree_depth, 
    loss_reduction,
    sample_size,
    stop_iter
  ) |> 
  knitr::kable()
```

::: {.callout-note collapse="true"}
## Model Hyperparameters

For reproducibility, the chosen hyperparameters are as follows.

| model_type | mtry | min_n | tree_depth | loss_reduction | sample_size | stop_iter |
|:-----------|-----:|------:|-----------:|---------------:|------------:|----------:|
| base       |   13 |    35 |          7 |      0.0010687 |   0.2217629 |        11 |
| elo        |   14 |     9 |         14 |      0.0000274 |   0.8725705 |        14 |

The number of `trees` and `learning_rate` were pre-defined to be 500 and 0.01 respectively.
:::

Finally, we fit singular models on the entire training set. These are the models that we'll use to evaluate the effect of team quality on xG.[^3]

[^3]: The hyperparameter search evaluated models fit on portions (or "folds") of the training data in a [cross-validation procedure](https://scikit-learn.org/stable/modules/cross_validation.html). We should fit one "final" model with all of the training data, per [best practice](https://tune.tidymodels.org/reference/last_fit.html).

```{r}
#| label: last-fits
#| code-fold: true
#| code-summary: Fit models on entire training set after choosing best hyper-parameters
last_base_fit <- tuned_results |>
  extract_workflow('base_model') |>
  finalize_workflow(best_base_set) |> 
  last_fit(
    split,
    metrics = yardstick::metric_set(xg_brier_skill_score)
  )

last_elo_fit <- tuned_results |>
  extract_workflow('elo_model') |>
  finalize_workflow(best_elo_set) |> 
  last_fit(
    split,
    metrics = yardstick::metric_set(xg_brier_skill_score)
  )
```

```{r}
#| label: tune-models-save
#| include: false
qs::qsave(last_base_fit, file.path(PROJ_DIR, 'last_base_fit.qs'))
qs::qsave(last_elo_fit, file.path(PROJ_DIR, 'last_elo_fit.qs'))
```

```{r}
#| label: tune-models-save
#| include: false
last_base_fit <- qs::qread(file.path(PROJ_DIR, 'last_base_fit.qs'))
last_elo_fit <- qs::qread(file.path(PROJ_DIR, 'last_elo_fit.qs'))
```

## Model Evaluation

Let's take a look at whether the Elo-augmented model has a higher BSS than the "base" model.

```{r}
dplyr::bind_rows(
  tune::collect_metrics(last_base_fit) |> dplyr::mutate(model_type = 'base'),
  tune::collect_metrics(last_elo_fit) |> dplyr::mutate(model_type = 'elo')
) |> 
  dplyr::transmute(
    model_type,
    brier_skill_score = .estimate
  ) |> 
  knitr::kable(digits = 3)
```

| model_type | brier_skill_score |
|:-----------|------------------:|
| base       |             0.104 |
| elo        |             0.092 |

So, alas, it looks like there's effectively no difference between the models.[^4] In fact, the team quality adjusted model performs worse on the test set!

[^4]: One could run a bootstrap analysis to prove this with more statistical rigor, but I'll leave that as an exercise for the eager reader.

But BSS is just one, wholistic measure. We also wanted to look at the how calibration of an xG model that accounts for team quality might look compared to that of a traditional xG model.

## Discussion

## Conclusion
