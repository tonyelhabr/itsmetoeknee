---
title: Calibrating Binary Probabilities
description: Using calibration to improve classifier model performance
date: 2023-09-11
date-modified: 2023-09-12
categories:
  - r
  - soccer
image: compared-calibration.png
execute: 
  code-fold: false
  eval: false
  include: true
  echo: true
---

## Introduction

Ever grappled with a [classification](https://en.wikipedia.org/wiki/Statistical_classification) model that consistently over-predicts or under-predicts an event? Your first thought might be to re-evaluate the model's features or framework. But what if tweaking the model isn't an option, either due to a lack of resources or access restrictions? The good news is, there's another way--it's called [**calibration**](https://www.tidyverse.org/blog/2022/11/model-calibration/).

Calibration falls in the "post-processing" step of predictive modeling.[^1] We modify the output of a model using nothing but the model predictions and labels of the output. We do that by, you guess it, fitting another model, often called [a "calibrator"](https://scikit-learn.org/stable/modules/calibration.html#calibrating-a-classifier).

[^1]: The [`{tidymodels}` guide](https://www.tidymodels.org/learn/models/calibration) breaks down the traditional modeling workflow into three steps:

> -   the pre-processing stage (e.g., feature engineering, normalization, etc.)
> -   model fitting (actually training the model)
> -   post-processing (such as optimizing a probability threshold)

One of my favorite (and relatively new) packages in the [`{tidymodels}` ecosystem](https://www.tidymodels.org/) is the [`{probably}`](https://probably.tidymodels.org/) package. It provides functions that make it fairly straightforward to do calibration, even for those who are new to the concept. So let's use `{probably}` to demonstrate the power of calibration.

## Calibrating a Binary Classifier

### Data

I'll be using the pre-match win probabilities from [FiveThirtyEight](https://data.fivethirtyeight.com/#soccer-spi) ([RIP](https://twitter.com/ryanabest/status/1672013407908667392)). Specifically, I'll subset their plethora of historical projections to two women's leagues: the [Women's Super League (WSL)](https://en.wikipedia.org/wiki/Women%27s_Super_League) in England and the [National Women's Soccer League (NWSL)](https://en.wikipedia.org/wiki/National_Women%27s_Soccer_League) in the U.S. I have a suspicion that the model probabilities are not as calibrated as they could be, as [has been observed that gender-agnostic models](https://statsbomb.com/articles/soccer/analytics-and-modelling-in-womens-football/) in soccer can be less performant for the women's game.

To keep things simple, I'm going to treat this as a [binary classification](https://en.wikipedia.org/wiki/Binary_classification) task, where matches are the "positive" outcome (`"yes"`), and losses and draws are grouped as the "negative" outcome (`"no"`).

```{r}
#| label: data-pull
#| code-fold: true
#| code-summary: Retrieve data
library(readr)
library(dplyr)
matches <- readr::read_csv('https://projects.fivethirtyeight.com/soccer-api/club/spi_matches.csv') |> 
  dplyr::filter(
    !is.na(score1), ## match is completed
    league %in% c(
      "FA Women's Super League",
      "National Women's Soccer League"
    )
  ) |> 
  dplyr::transmute(
    league,
    season,
    date,
    team1,
    team2,
    target = factor(ifelse(score1 > score2, 'yes', 'no')),
    ## .pred_ is sort of a convention among {tidymodels} docs
    .pred_yes = prob1,
    .pred_no = 1- .pred_yes 
  )
```

```{r}
#| label: save-matches
#| include: false
PROJ_DIR <- 'posts/probability-calibration'
qs::qsave(matches, file.path(PROJ_DIR, 'matches.qs'))
```

```{r}
#| label: show-matches
#| code-fold: false
matches
#> # A tibble: 1,358 × 6
#>    date       team1               team2             target .pred_yes .pred_no
#>    <date>     <chr>               <chr>             <fct>      <dbl>    <dbl>
#>  1 2016-07-09 Liverpool Women     Reading           yes        0.439    0.561
#>  2 2016-07-10 Arsenal Women       Notts County Lad… yes        0.357    0.643
#>  3 2016-07-10 Chelsea FC Women    Birmingham City   no         0.480    0.520
#>  4 2016-07-16 Liverpool Women     Notts County Lad… no         0.429    0.571
#>  5 2016-07-17 Chelsea FC Women    Arsenal Women     no         0.412    0.588
#>  6 2016-07-24 Reading             Birmingham City   no         0.382    0.618
#>  7 2016-07-24 Notts County Ladies Manchester City … no         0.308    0.692
#>  8 2016-07-31 Reading             Notts County Lad… no         0.407    0.593
#>  9 2016-07-31 Arsenal Women       Liverpool Women   no         0.435    0.565
#> 10 2016-08-03 Reading             Manchester City … no         0.306    0.694
#> # ℹ 1,348 more rows
```

### Diagnosis

We start with the ["diagnosis"](https://www.tidymodels.org/learn/models/calibration/#but-is-it-calibrated) phase: "How well do the original probabilities perform?" To evaluate this, we can use one of the several `probably::cal_plot_*` functions. In this case, we'll use `cal_plot_breaks()`.[^2]

[^2]: Sticking with the basics, I use the defaults for `num_breaks` (10) and `conf_level` (0.9).

This function neatly divides the range of predicted probabilities from zero to one into distinct bins. For each bin, it calculates the observed event rate using data that has probabilities falling within that bin's range. Ideally, if our predictions are calibrated, the curve produced should match up with a straight diagonal line, i.e. a 45-degree slope passing through (0,0) and (1,1). As a bonus, the `probably::cal_plot_*` family of functions even provides confidence intervals about the calibration curve.

```{r}
#| label: cal_plot_breaks
#| code-fold: false
library(probably)
packageVersion('probably')
#> [1] ‘1.0.1.9000’

matches |> 
  probably::cal_plot_breaks(
    truth = target,
    ## the "_yes" in `.pred_yes` must match one of the values in `target`
    estimate = .pred_yes,
    ## because "yes" is the second event level ("no" is the first)
    event_level = 'second'
  )
```

`{probably}` offers some really neat automatic plotting of calibration curves. While I'd suggest giving them a try, I like to make my curves in a certain manner. In particular, instead of using a "rug", I like showing sample sizes via points on the curve.

```{r}
#| label: cal_plot_breaks-plot
#| include: false
library(purrr)

raw_calibration <- matches |> 
  probably::cal_plot_breaks(
    truth = target,
    estimate = .pred_yes,
    event_level = 'second'
  ) |> 
  purrr::pluck('data')

library(ggplot2)
library(sysfonts)
library(showtext)
library(ggtext)
library(htmltools)

CAPTION_LABEL <- '**Data**: FiveThirtyEight.' # <br/>Point size is proportional to number of matches.'
SUBTITLE_LABEL <- 'WSL 2016-2022, NWSL 2017-2023'
TAG_LABEL <- htmltools::tagList(
  htmltools::tags$span(htmltools::HTML(enc2utf8("&#xf099;")), style = 'font-family:fb'),
  htmltools::tags$span("@TonyElHabr"),
)
common_calibration_plot_layers <- function(...) {
  list(
    ...,
    ggplot2::geom_abline(
      color = WHITISH_FOREGROUND_COLOR, 
      linetype = 2
    ),
    ggplot2::scale_x_continuous(
      limits = c(0, 1),
      expand = c(0.01, 0.01)
    ),
    ggplot2::scale_y_continuous(
      limits = c(0, 1),
      expand = c(0.01, 0.01)
    ),
    ggplot2::guides(
      size = ggplot2::guide_legend(
        title = '# of matches'
      )
    ),
    ggplot2::geom_segment(
      data = data.frame(
        x = 0.45,
        xend = 0.35,
        y = 0.6,
        yend = 0.7
      ),
      arrow = grid::arrow(length = grid::unit(6, 'pt'), type = 'closed'),
      color = WHITISH_FOREGROUND_COLOR,
      linewidth = 1,
      ggplot2::aes(
        x = x,
        xend = xend,
        y = y,
        yend = yend
      )
    ),
    ggplot2::geom_segment(
      data = data.frame(
        x = 0.55,
        xend = 0.65,
        y = 0.4,
        yend = 0.3
      ),
      arrow = grid::arrow(length = grid::unit(6, 'pt'), type = 'closed'),
      color = WHITISH_FOREGROUND_COLOR,
      linewidth = 1,
      ggplot2::aes(
        x = x,
        xend = xend,
        y = y,
        yend = yend
      )
    ),
    ggtext::geom_richtext(
      data = data.frame(
        x = 0.35,
        y = 0.75,
        label = '*Model under-predicts*'
      ),
      fill = NA, label.color = NA,
      label.padding = grid::unit(rep(0, 1), 'pt'),
      color = WHITISH_FOREGROUND_COLOR,
      family = FONT,
      hjust = 1,
      vjust = 1.1,
      size = 14 / ggplot2::.pt,
      ggplot2::aes(
        x = x,
        y = y,
        label = label
      )
    ),
    ggtext::geom_richtext(
      data = data.frame(
        x = 0.65,
        y = 0.25,
        label = '*Model over-predicts*'
      ),
      fill = NA, label.color = NA,
      label.padding = grid::unit(rep(0, 1), 'pt'),
      color = WHITISH_FOREGROUND_COLOR,
      family = FONT,
      hjust = 0,
      vjust = -0.1,
      size = 14 / ggplot2::.pt,
      ggplot2::aes(
        x = x,
        y = y,
        label = label
      )
    ),
    ggplot2::labs(
      title = "Calibration of pre-match win probabilities",
      subtitle = SUBTITLE_LABEL,
      y = 'Actual match win rate',
      x = 'Pre-match win probablity',
      caption = CAPTION_LABEL,
      tag = TAG_LABEL
    )
  )
}

PLOT_RESOLUTION <- 300
WHITISH_FOREGROUND_COLOR <- 'white'
COMPLEMENTARY_FOREGROUND_COLOR <- '#999999'
BLACKISH_BACKGROUND_COLOR <- '#1c1c1c'
COMPLEMENTARY_BACKGROUND_COLOR <- '#4d4d4d'
FONT <- 'Titillium Web'
sysfonts::font_add_google(FONT, FONT)
## https://github.com/tashapiro/tanya-data-viz/blob/main/chatgpt-lensa/chatgpt-lensa.R for twitter logo
sysfonts::font_add('fb', 'Font Awesome 6 Brands-Regular-400.otf')
showtext::showtext_auto()
showtext::showtext_opts(dpi = PLOT_RESOLUTION)

ggplot2::theme_set(ggplot2::theme_minimal())
ggplot2::theme_update(
  text = ggplot2::element_text(family = FONT),
  title = ggplot2::element_text(size = 20, color = WHITISH_FOREGROUND_COLOR),
  plot.title = ggtext::element_markdown(face = 'bold', size = 20, color = WHITISH_FOREGROUND_COLOR),
  plot.title.position = 'plot',
  plot.subtitle = ggtext::element_markdown(size = 16, color = COMPLEMENTARY_FOREGROUND_COLOR),
  axis.text = ggplot2::element_text(color = WHITISH_FOREGROUND_COLOR, size = 14),
  axis.title.x = ggtext::element_markdown(size = 14, color = WHITISH_FOREGROUND_COLOR, face = 'bold', hjust = 0.99),
  axis.title.y = ggtext::element_markdown(size = 14, color = WHITISH_FOREGROUND_COLOR, face = 'bold', hjust = 0.99),
  axis.line = ggplot2::element_blank(),
  strip.text = ggplot2::element_text(size = 14, color = WHITISH_FOREGROUND_COLOR, face = 'bold', hjust = 0),
  legend.position = 'top',
  legend.text = ggplot2::element_text(size = 12, color = WHITISH_FOREGROUND_COLOR, face = 'plain'),
  legend.title = ggplot2::element_text(size = 12, color = WHITISH_FOREGROUND_COLOR, face = 'bold'),
  panel.grid.major = ggplot2::element_line(color = COMPLEMENTARY_BACKGROUND_COLOR),
  panel.grid.minor = ggplot2::element_line(color = COMPLEMENTARY_BACKGROUND_COLOR),
  panel.grid.minor.x = ggplot2::element_blank(),
  panel.grid.minor.y = ggplot2::element_blank(),
  plot.margin = ggplot2::margin(10, 20, 10, 20),
  plot.background = ggplot2::element_rect(fill = BLACKISH_BACKGROUND_COLOR, color = BLACKISH_BACKGROUND_COLOR),
  plot.caption = ggtext::element_markdown(color = WHITISH_FOREGROUND_COLOR, hjust = 0, size = 10, face = 'plain'),
  plot.caption.position = 'plot',
  plot.tag = ggtext::element_markdown(size = 12, color = WHITISH_FOREGROUND_COLOR, hjust = 1),
  plot.tag.position = c(0.99, 0.01),
  panel.spacing.x = grid::unit(2, 'lines'),
  panel.background = ggplot2::element_rect(fill = BLACKISH_BACKGROUND_COLOR, color = BLACKISH_BACKGROUND_COLOR)
)
ggplot2::update_geom_defaults('text', list(color = WHITISH_FOREGROUND_COLOR, size = 12 / .pt))
ggplot2::update_geom_defaults('point', list(color = WHITISH_FOREGROUND_COLOR))

CAL_COLORS <- c(
  'Un-calibrated' = '#009ffd', 
  'Calibrated' = '#ffa400'
)
point_to_annotate <- dplyr::filter(
  raw_calibration,
  round(predicted_midpoint, 2) == 0.15
)
raw_calibration_plot <- raw_calibration |> 
  ggplot2::ggplot() +
  ggplot2::aes(
    x = predicted_midpoint, 
    y = event_rate
  ) +
  ggplot2::geom_ribbon(
    fill = CAL_COLORS['Un-calibrated'],
    alpha = 0.25,
    ggplot2::aes(
      ymin = lower, 
      ymax = upper
    )
  ) +
  ggplot2::geom_line(
    color = CAL_COLORS['Un-calibrated']
  ) +
  ggplot2::geom_point(
    color = CAL_COLORS['Un-calibrated'],
    ggplot2::aes(size = total)
  ) +
  common_calibration_plot_layers() +
  ggplot2::labs(
    caption = paste0(CAPTION_LABEL, '<br/>Ribbon illustrates 90% confidence interval.')
  ) +
  ggtext::geom_richtext(
    data = data.frame(
      x = point_to_annotate$predicted_midpoint + 0.05,
      y = point_to_annotate$event_rate - 0.03,
      label = as.character(glue::glue('<i>On average, model predicts<span style="color:{BLACKISH_BACKGROUND_COLOR}">.</span> {scales::percent(point_to_annotate$predicted_midpoint, accuracy = 0.1)}<br/>when observed win rate is just<span style="color:{BLACKISH_BACKGROUND_COLOR}">.</span> {scales::percent(point_to_annotate$event_rate, accuracy = 0.1)}<i>.</i>'))
    ),
    fill = NA, label.color = NA,
    label.padding = grid::unit(rep(0, 1), 'pt'),
    color = WHITISH_FOREGROUND_COLOR,
    family = FONT,
    hjust = 0,
    vjust = 0.5,
    size = 10 / ggplot2::.pt,
    ggplot2::aes(
      x = x,
      y = y,
      label = label
    )
  ) +
  ggplot2::geom_curve(
    data = data.frame(
      x = point_to_annotate$predicted_midpoint + 0.05,
      xend = point_to_annotate$predicted_midpoint + 0.01,
      y = point_to_annotate$event_rate - 0.03,
      yend = point_to_annotate$event_rate - 0.01
    ),
    arrow = grid::arrow(length = grid::unit(3, 'pt'), type = 'closed'),
    color = WHITISH_FOREGROUND_COLOR,
    linewidth = 0.5,
    curvature = -0.25,
    ggplot2::aes(
      x = x,
      xend = xend,
      y = y,
      yend = yend
    )
  )
raw_calibration_plot

ggplot2::ggsave(
  raw_calibration_plot,
  filename = file.path(PROJ_DIR, 'raw-calibration.png'),
  width = 7,
  height = 8
)
```

![](raw-calibration.png)

Indeed, it looks like there is some room for improvement with the match probabilities. It seems that the [FiveThirtyEight](https://fivethirtyeight.com/) model over-predicts when the actual win rate is low, broadly below 20%; and, likewise, it tends to under-predict when the true win rate is really greater than 60%

### Remediation

Now we move on to the ["remedation"](https://www.tidymodels.org/learn/models/calibration/#remediation) step. That is, we fit a model, a "calibrator", with the binary outcome as the target variable and the probability estimate as the lone input feature. `{probably}` offers several options with [the `cal_estimate_*()` set of functions](https://probably.tidymodels.org/reference/index.html#calibration-validation).

I've opted for [Beta calibration](https://probably.tidymodels.org/reference/cal_estimate_beta.html), although [logistic calibration](https://probably.tidymodels.org/reference/cal_estimate_logistic.html) would work fine here as well. Beta calibration is a little more flexible, and, consequently, [can provide superior probability estimates](https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-11/issue-2/Beyond-sigmoids--How-to-obtain-well-calibrated-probabilities-from/10.1214/17-EJS1338SI.full), especially when the target distribution is skewed.[^3]

[^3]: We don't have a skew problem in this context, but it's good to note when a Beta calibration might provide meaningful benefits over other calibration methods.

```{r}
#| label: cal_estimate_beta
#| code-fold: false
matches |> 
  probably::cal_estimate_beta(
    truth = target,
    estimate = dplyr::starts_with('.pred'),
    event_level = 'second'
  )
#>  ── Probability Calibration 
#>  Method: Beta calibration
#>  Type: Binary
#>  Source class: Data Frame
#>  Data points: 1,358
#>  Truth variable: `target`
#>  Estimate variables:
#>  `.pred_no` ==> no
#>  .pred_yes` ==> yes
```

Under the hood, `cal_estimate_beta()` is doing something like this.

```{r}
#| label: beta_calibration
#| code-fold: false
library(betacal)

betacal::beta_calibration(
  p = matches$.pred_yes,
  y = matches$target == 'yes',
  parameters = 'abm'
)
```

It's almost shocking how simple the implementation is.

## Results

With the calibrator model in hand, let's make a scatter plot of all the points in the data set, viewing how the model has adjusted the original probabilities.

```{r}
#| label: calibration_scatter_plot
#| include: false
calibrator <- matches |> 
  probably::cal_estimate_beta(
    truth = target,
    estimate = dplyr::starts_with('.pred'),
    event_level = 'second'
  )

calibrated <- dplyr::bind_cols(
  matches |> 
    dplyr::rename(
      .raw_pred_yes = .pred_yes,
      .raw_pred_no = .pred_no
    ),
  probably::cal_apply(
    matches,
    calibrator
  ) |> 
    dplyr::select(.pred_yes, .pred_no)
)

calibration_scatter_plot <- calibrated |> 
  ggplot2::ggplot() +
  ggplot2::aes(
    y = .raw_pred_yes, 
    x = .pred_yes
  ) +
  ggplot2::geom_abline(
    color = WHITISH_FOREGROUND_COLOR, 
    linetype = 2) +
  ggplot2::geom_point(
    color = WHITISH_FOREGROUND_COLOR,
    shape = 21
  ) +
  ggplot2::scale_x_continuous(
    limits = c(0, 1),
    expand = c(0.01, 0.01)
  ) +
  ggplot2::scale_y_continuous(
    limits = c(0, 1),
    expand = c(0.01, 0.01)
  ) +
  ggplot2::theme(
    ## to fit in the plot width
    plot.title = ggtext::element_markdown(size = 19)
  ) +
  ggplot2::labs(
    title = glue::glue('<span style=color:{CAL_COLORS["Calibrated"]}>Calibrated</span> vs. <span style=color:{CAL_COLORS["Un-calibrated"]}>Un-calibrated</span> pre-match win probabilities'),
    subtitle = SUBTITLE_LABEL,
    y = glue::glue('<span style=color:{CAL_COLORS["Un-calibrated"]}>Uncalibrated</span> pre-match win probability'),
    x = glue::glue('<span style=color:{CAL_COLORS["Calibrated"]}>Calibrated</span> pre-match win probability'),
    caption = '<br/>',
    tag = TAG_LABEL
  )

ggplot2::ggsave(
  calibration_scatter_plot,
  filename = file.path(PROJ_DIR, 'calibration-scatter.png'),
  width = 7,
  height = 7
)
```

![](calibration-scatter.png)

We observe that the calibrator has increased point estimates on the lower end of the spectrum and decreased estimates on the upper end of the spectrum. The calibration has seemingly fine-tuned under-predicting and over-predicting behavior from the original model.

To see the change that calibrator has made, we can re-make our calibration curve plot, adding the "calibrated" curve alongside the original "un-calibrated" curve.

```{r}
#| label: compared_calibration_plot
#| include: false
calibrated_calibration <- calibrated |> 
  probably::cal_plot_breaks(
    truth = target,
    estimate = .pred_yes,
    event_level = 'second'
  ) |> 
  purrr::pluck('data')

compared_calibration <- dplyr::bind_rows(
  dplyr::mutate(raw_calibration, source = 'Un-calibrated'),
  dplyr::mutate(calibrated_calibration, source = 'Calibrated')
)

compared_calibration_plot <- compared_calibration |> 
  ggplot2::ggplot() +
  ggplot2::aes(
    x = predicted_midpoint, 
    y = event_rate,
    color = source
  ) +
  ggplot2::geom_line() +
  ggplot2::geom_point(
    ggplot2::aes(size = total)
  ) +
  ggplot2::scale_color_manual(
    values = CAL_COLORS
  ) +
  ggplot2::guides(
    color = ggplot2::guide_legend(
      title = '',
      override.aes = list(linewidth = 3)
    )
  ) +
  common_calibration_plot_layers()
compared_calibration_plot

ggplot2::ggsave(
  compared_calibration_plot,
  filename = file.path(PROJ_DIR, 'compared-calibration.png'),
  width = 7,
  height = 7
)
```

![](compared-calibration.png)

Visually, it's evident that the remediation has improved the probability estimates. The calibrated curve more closely "hugs" the ideal 45 degree slope across the whole probability spectrum.

### Validation

To quantitatively describe the difference in calibration between the two models, we can compare the [Brier Skill Score (BSS)](https://en.wikipedia.org/wiki/Brier_score#Brier_Skill_Score_(BSS)) of the un-calibrated and calibrated models.[^4] Keep in mind that a higher BSS indicates a more calibrated model. (1 is ideal. 0 indicates that the model is no better or worse than a reference model[^5].)

[^4]: I've discussed BSS in [a prior post on expected goals model calibration](/posts/opta-xg-model-calibration) and [another post on expected goals match-implied win probabilities](/posts/epl-xpts-simulation-1).

[^5]: In this case, I choose to use the observed match win rate as the reference model.

```{r}
#| label: compared_brier_skill_scores
#| code-fold: true
#| code-summary: Brier Skill Score (BSS) calculation
library(yardstick)
library(rlang)

brier_skill_score <- function(data, ...) {
  UseMethod('brier_skill_score')
}

brier_skill_score <- yardstick::new_prob_metric(
  brier_skill_score, 
  direction = 'maximize'
)

bss <- function(
    truth, 
    estimate, 
    ref_estimate, 
    event_level,
    case_weights,
    ...
) {
    
  if (length(estimate) == 1) {
    estimate <- rep(estimate, length(truth))
  }
  
  if (length(ref_estimate) == 1) {
    ref_estimate <- rep(ref_estimate, length(truth))
  }
  
  estimate_brier_score <- brier_class_vec(
    truth = truth,
    estimate = estimate,
    event_level = event_level,
    case_weights = case_weights,
    ...
  )
  
  ref_brier_score <- brier_class_vec(
    truth = truth,
    estimate = ref_estimate,
    event_level = event_level,
    case_weights = case_weights,
    ...
  )
  
  1 - (estimate_brier_score / ref_brier_score)
}

brier_skill_score_estimator_impl <- function(
    truth, 
    estimate, 
    ref_estimate, 
    event_level,
    case_weights
) {
  bss(
    truth = truth,
    estimate = estimate,
    ref_estimate = ref_estimate,
    event_level = event_level,
    case_weights = case_weights
  )
}


brier_skill_score_vec <- function(
    truth, 
    estimate, 
    ref_estimate, 
    na_rm = TRUE, 
    event_level = yardstick:::yardstick_event_level(),
    case_weights = NULL, 
    ...
) {
  
  yardstick:::abort_if_class_pred(truth)
  
  estimator <- yardstick::finalize_estimator(
    truth, 
    metric_class = 'brier_skill_score'
  )
  
  yardstick::check_prob_metric(truth, estimate, case_weights, estimator)
  
  if (na_rm) {
    result <- yardstick::yardstick_remove_missing(truth, estimate, case_weights)
    
    truth <- result$truth
    estimate <- result$estimate
    case_weights <- result$case_weights
  } else if (yardstick::yardstick_any_missing(truth, estimate, case_weights)) {
    return(NA_real_)
  }

  brier_skill_score_estimator_impl(
    truth = truth,
    estimate = estimate,
    ref_estimate = ref_estimate,
    event_level = event_level,
    case_weights = case_weights
  )
}

brier_skill_score.data.frame <- function(
    data, 
    truth, 
    ...,
    na_rm = TRUE,
    event_level = yardstick:::yardstick_event_level(),
    case_weights = NULL
) {
  yardstick::prob_metric_summarizer(
    name = 'brier_skill_score',
    fn = brier_skill_score_vec,
    data = data,
    truth = !!rlang::enquo(truth),
    ...,
    na_rm = na_rm,
    event_level = event_level,
    case_weights = !!rlang::enquo(case_weights),
    fn_options = list(
      ref_estimate = ref_estimate
    )
  )
}

## The BSS is more intuitive to caluclate using the match non-win rate.
##   Accordingly, we'll use .pred_no for our probability.
REF_ESTIMATE <- matches |> 
  dplyr::count(target) |> 
  dplyr::mutate(prop = n / sum(n)) |> 
  dplyr::filter(target != 'yes') |> 
  dplyr::pull(prop)

raw_brier_skill_score <- brier_skill_score_vec(
  truth = calibrated$target,
  estimate = calibrated$.raw_pred_no,
  ref_estimate = REF_ESTIMATE
)

calibrated_brier_skill_score <- brier_skill_score_vec(
  truth = calibrated$target,
  estimate = calibrated$.pred_no,
  ref_estimate = REF_ESTIMATE
)

compared_brier_skill_scores <- round(
  c(
    'Un-calibrated' = raw_brier_skill_score,
    'Calibrated' = calibrated_brier_skill_score
  ),
  3
)
```

```{r}
#| label: compared_brier_skill_scores-save
#| include: false
qs::qsave(compared_brier_skill_scores, file.path(PROJ_DIR, 'compared_brier_skill_scores.qs'))
```

```{r}
#| label: compared_brier_skill_scores-show
#| code-fold: false
compared_brier_skill_scores
#> Un-calibrated    Calibrated 
#>         0.196         0.205
```

Indeed, we have (marginally) improved the original pre-match win probabilities. But this approach is arguably a little naive--we've only re-assessed the entire data set a single time without accounting for potential uncertainties.

Fortunately, the `{probably}` package provides [the `cal_validate_*()` family](https://probably.tidymodels.org/reference/index.html#calibration-validation) of functions. These functions allow for a more rigorous assessment of whether the calibration enhances the original probabilities. We can generate resamples from the original data and then compute the average and standard error for our chosen metrics. This lets us compare the calibrated and uncalibrated probabilities more effectively.

Let's do just that, using [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) with 10 folds and 10 repeats We'll again use BSS to evaluate the model probabilities.

```{r}
#| label: validation_metrics
#| code-fold: true
#| code-summary: Robustly evaluate=ing calibrator
set.seed(42)
sets <- rsample::vfold_cv(
  matches, 
  v = 10, 
  repeats = 10
)

## "fixed" in the sense that we're pre-defining the reference estimate.
##   I'm not sure there's another way of going about this when working in conjunction `yardstick::metric_set()` and `probably::cal_validate_*()`
##   with
fixed_brier_skill_score.data.frame <- function(...) {
  brier_skill_score(
    ref_estimate = REF_ESTIMATE,
    ...
  )
}

fixed_brier_skill_score <- function(data, ...) {
  UseMethod('fixed_brier_skill_score')
}

fixed_brier_skill_score <- yardstick::new_prob_metric(
  fixed_brier_skill_score,
  direction = 'maximize'
)

eval_metrics <- yardstick::metric_set(
  fixed_brier_skill_score
)

validation <- probably::cal_validate_beta(
  sets,
  truth = target,
  metrics = eval_metrics
)

validation_metrics <- validation |> 
  tune::collect_metrics() |> 
  ## i think the `.config` column is bugged (it just says "config" for all rows?)
  dplyr::select(-dplyr::any_of('.config'))
validation_metrics
```

```{r}
#| label: validation_metrics-save
#| include: false
qs::qsave(validation_metrics, file.path(PROJ_DIR, 'validation_metrics.qs'))
```

```{r}
#| label: validation_metrics-show
#| code-fold: false
validation_metrics
#> # A tibble: 2 × 6
#>   .metric           .type        .estimator  mean     n std_err
#>   <chr>             <chr>        <chr>      <dbl> <int>   <dbl>
#> 1 brier_skill_score uncalibrated binary     0.196   100 0.00494
#> 2 brier_skill_score calibrated   binary     0.202   100 0.00628
```

We find that the calibrator does indeed offer a "significant" improvement when assessed through this more statistically rigorous method. Specifically, the mean BSS of the validation set, minus one standard error, exceeds the mean of the uncalibrated BSS by more than one standard error.

```{r}
#| label: pivoted_validation_metrics
#| code-fold: false
pivoted_validation_metrics <- validation_metrics |> 
  dplyr::transmute(
    .type,
    mean,
    lower = mean - std_err,
    upper = mean + std_err
  )
pivoted_validation_metrics
#> # A tibble: 2 × 4
#>   .type         mean lower upper
#>   <chr>        <dbl> <dbl> <dbl>
#> 1 uncalibrated 0.196 0.191 0.201
#> 2 calibrated   0.202 0.196 0.209
```

## Conclusion

While I wouldn't say "calibration is all you need", it's certainly something nice to have in your toolkit when you're working on a modeling task. It can be a game-changer, especially when tweaking the original model isn't an option, whether due to access limitations or, let's be honest, sheer laziness.

Oh, and I failed to mention this earlier---calibration isn't just for binary classifiers. Multinomial classifiers and even regression models can benefit from this technique as well.

Happy modeling, folks.
