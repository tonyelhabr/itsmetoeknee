{
  "hash": "59ee782c110f668fa97bf22a1547c66a",
  "result": {
    "markdown": "---\ntitle: A Meta Analysis of R Weekly Posts\ndescription: With Tidy Text Techniques\ndate: 2018-03-05\ncategories:\n  - r\nimage: viz_unigrams_corrs-1.png\nexecute:\n  include: true\n  echo: true\n---\n\n\n## Introduction\n\nI'm always intrigued by data science \"meta\" analyses or programming/data-science. For example, [Matt Dancho's analysis of renown data scientist David Robinson](http://www.business-science.io/learning-r/2018/03/03/how_to_learn_R_pt1.html). [David Robinson](http://varianceexplained.org/) himself has done some good ones, such as [his blog posts for *Stack Overflow* highlighting the growth of \"incredible\" growth of python](https://stackoverflow.blog/2017/09/06/incredible-growth-python/), and [the \"impressive\" growth of R](https://stackoverflow.blog/2017/10/10/impressive-growth-r/) in modern times.\n\nWith that in mind, I thought it would try to identify if any interesting trends have risen/fallen *within* the R community in recent years. To do this, I scraped and analyzed the \"weekly roundup\" posts put together by [*R Weekly*](https://rweekly.org/), which was originated in May 2016. These posts consist of links and corresponding descriptions, grouped together by topic. It should go without saying that this content serves as a reasonable heuristic for the interests of the `R` community at any one point in time. (Of course, the posts of other aggregate R blogs such as [R Bloggers](https://www.r-bloggers.com/) or [Revolution Analytics](http://blog.revolutionanalytics.com/) might serve as better resources since they post more frequently and have been around for quite a bit longer than R Weekly.)\n\n## Scraping and Cleaning\n\nAs always, it's good to follow the best practice of importing all needed packages before beginning.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nlibrary(tidyverse)\nlibrary(gh)\nlibrary(viridisLite)\n```\n:::\n\n\nFor the scraping, I drew upon some the principles shown by [Maelle Salmon](http://www.masalmon.eu/) in [her write-up](https://itsalocke.com/blog/markdown-based-web-analytics-rectangle-your-blog/) detailing how she scraped and cleaned the blog posts of the [Locke Data blog](https://itsalocke.com/blog). [^1]\n\n[^1]: Actually, I downloaded the data locally so that I would not have to worry about GitHub API request limits. Thus, in addition to other custom processing steps that I added, my final code does not necessarily resemble hers.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\n# Reference: https://itsalocke.com/blog/markdown-based-web-analytics-rectangle-your-blog/\nposts <- gh::gh(\n  endpoint = \"/repos/:owner/:repo/contents/:path\",\n  owner = \"rweekly\",\n  repo = \"rweekly.org\",\n  path = \"_posts\"\n)\n\nposts_info <- dplyr::data_frame(\n  name = purrr::map_chr(posts, \"name\"),\n  path = purrr::map_chr(posts, \"path\")\n)\n```\n:::\n\n\nIn all, R Weekly has made 93 (at the time of writing).\n\nNext, before parsing the text of the posts, I add some \"meta-data\" (mostly for dates) that is helpful for subsequent exploration and analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nconvert_name_to_date <- function(x) {\n  x %>% \n    stringr::str_extract(\"[0-9]{4}-[0-9]+-[0-9]+\") %>% \n    strftime(\"%Y-%m-%d\") %>% \n    lubridate::ymd()\n}\n\nposts_info <- posts_info %>% \n  mutate(date = convert_name_to_date(name)) %>% \n  mutate(num_post = row_number(date)) %>% \n  mutate(\n    yyyy = lubridate::year(date) %>% as.integer(),\n    mm = lubridate::month(date, label = TRUE),\n    wd = lubridate::wday(date, label = TRUE)\n  ) %>% \n  select(date, yyyy, mm, wd, num_post, everything())\n\nposts_info <- posts_info %>% \n  mutate(date_min = min(date), date_max = max(date)) %>% \n  mutate(date_lag = date - date_min) %>% \n  mutate(date_lag30 = as.integer(round(date_lag / 30, 0)), \n         date_lag60 = as.integer(round(date_lag / 60, 0)), \n         date_ntile = ntile(date, 6)) %>% \n  select(-date_min, -date_max) %>% \n  select(date_lag, date_lag30, date_lag60, date_ntile, everything())\n```\n:::\n\n\nLet's quickly look at whether or not R Weekly has been consistent with its posting frequency since its inception. The number of posts across 30-day windows should be around 4 or 5.\n\n![](explore_time-1.png)\n\nNow, I'll do the dirty work of cleaning and parsing the text of each post.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nget_rweekly_post_data <- function(filepath) {\n\n  path_prefix <- \"data-raw\"\n  path <- file.path(path_prefix, path)\n  rgx_rmv <- \"Â|Å|â€|œ|\\u009d\"\n  rgx_detect_link <- \"^\\\\+\\\\s+\\\\[\"\n  rgx_detect_head <- \"^\\\\s*\\\\#\"\n  rgx_link_post <- \"(?<=\\\\+\\\\s\\\\[).*(?=\\\\])\"\n  rgx_link_img <- \"(?<=\\\\!\\\\[).*(?=\\\\])\"\n  rgx_url <- \"(?<=\\\\().*(?=\\\\))\"\n  rgx_head <- \"(?<=\\\\#\\\\s).*$\"\n  \n  lines <- readLines(path)\n  lines_proc <- lines %>%\n    # This would be necessary if downloading directly from the repo.\n    # base64enc::base64decode() %>%\n    # rawToChar() %>%\n    stringr::str_split(\"\\n\") %>%\n    purrr::flatten_chr() %>%\n    as_tibble() %>%\n    rename(text = value) %>%\n    transmute(line = row_number(), text) %>%\n    filter(text != \"\") %>%\n    mutate(text = stringr::str_replace_all(text, rgx_rmv, \"\")) %>%\n    mutate(text = stringr::str_replace_all(text, \"&\", \"and\")) %>% \n    mutate(\n      is_link = ifelse(stringr::str_detect(text, rgx_detect_link), TRUE, FALSE),\n      is_head = ifelse(stringr::str_detect(text, rgx_detect_head), TRUE, FALSE)\n    ) %>%\n    mutate(\n      link_post = stringr::str_extract(text, rgx_link_post),\n      link_img = stringr::str_extract(text, rgx_link_img),\n      url = stringr::str_extract(text, rgx_url),\n      head = \n        stringr::str_extract(text, rgx_head) %>% \n        stringr::str_to_lower() %>% \n        stringr::str_replace_all(\"s$\", \"\") %>% \n        stringr::str_replace_all(\" the\", \"\") %>% \n        stringr::str_trim()\n    ) %>%\n    mutate(\n      is_head = ifelse(line == 1, TRUE, is_head),\n      head = ifelse(line == 1, \"yaml and intro\", head)\n    )\n  \n  # Couldn't seem to get `zoo::na.locf()` to work properly.\n  lines_head <- lines_proc %>%\n    mutate(line_head = ifelse(is_head, line, 0)) %>%\n    mutate(line_head = cumsum(line_head))\n  \n  lines_head %>%\n    select(-head) %>%\n    inner_join(\n      lines_head %>%\n        filter(is_head == TRUE) %>%\n        select(head, line_head),\n      by = c(\"line_head\")\n    ) %>% \n    select(-line_head)\n}\n\ndata <- posts_info %>% \n  tidyr::nest(path, .key = \"path\") %>% \n  mutate(data = purrr::map(path, get_rweekly_post_data)) %>% \n  select(-path) %>% \n  tidyr::unnest(data)\n```\n:::\n\n\n## Analyzing\n\n### Lines and Links\n\nNow, with the data in a workable format, we can explore some of the content.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nmetrics_bypost <- data %>% \n  group_by(name, date) %>% \n  summarize(\n    num_lines = max(line),\n    num_links = sum(!is.na(is_link)),\n    num_links_post = sum(!is.na(link_post)),\n    num_links_img = sum(!is.na(link_img))\n  ) %>% \n  ungroup() %>% \n  arrange(desc(num_lines))\n```\n:::\n\n\nHave the number of links per post increased over time?\n\n![](viz_metrics_cnt_bypost-1.png)\n\nIt looks like there has been a correlated increase in the overall length of the posts (as determined by non-empty lines) and the number of links in each post.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\ncorrr::correlate(metrics_bypost %>% select(num_lines, num_links))\n#> # A tibble: 2 x 3\n#>   rowname   num_lines num_links\n#>   <chr>         <dbl>     <dbl>\n#> 1 num_lines    NA         0.970\n#> 2 num_links     0.970    NA\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nbroom::tidy(lm(num_lines ~ num_links, data = metrics_bypost))\n#>          term  estimate  std.error statistic      p.value\n#> 1 (Intercept) 12.317353 4.93345168  2.496701 1.433479e-02\n#> 2   num_links  1.796912 0.04754462 37.794219 2.016525e-57\n```\n:::\n\n\nLet's break down the increase of the number of links over time. Are there more links simply due to an increased use of images?\n\n![](viz_links_cnt_bypost-1.png)\n\nIt is evident that the increase in the number of links is not the result of increased image usage, but, instead, to increased linkage to non-trivial content.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\ncorrr::correlate(metrics_bypost %>% select(num_links, num_links_img, num_links_post))\n#> # A tibble: 3 x 4\n#>   rowname        num_links num_links_img num_links_post\n#>   <chr>              <dbl>         <dbl>          <dbl>\n#> 1 num_links         NA             0.324          0.865\n#> 2 num_links_img      0.324        NA              0.264\n#> 3 num_links_post     0.865         0.264         NA\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nbroom::tidy(lm(num_links ~ num_links_img + num_links_post, data = metrics_bypost))\n#>             term  estimate std.error statistic      p.value\n#> 1    (Intercept) 29.094312 4.7262724  6.155869 2.040398e-08\n#> 2  num_links_img  1.008073 0.5275685  1.910790 5.921483e-02\n#> 3 num_links_post  1.168952 0.0749660 15.593093 2.586469e-27\n```\n:::\n\n\nR Weeklyuses a fairly consistent set of \"topics\" (corresponding to the `head` variable in the scraped data) across all of their posts.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nhead_rmv <- \"yaml and intro\"\ndata %>%\n  distinct(head, name) %>%\n  filter(!(head %in% head_rmv)) %>% \n  count(head, sort = TRUE)\n#> # A tibble: 44 x 2\n#>    head                   n\n#>    <chr>              <int>\n#>  1 r in real world       92\n#>  2 tutorial              92\n#>  3 upcoming event        92\n#>  4 highlight             89\n#>  5 r project update      89\n#>  6 r in organization     80\n#>  7 resource              71\n#>  8 quotes of week        63\n#>  9 insight               55\n#> 10 videos and podcast    55\n#> # ... with 34 more rows\n```\n:::\n\n\nIs there a certain topic (or topics) in the RWeekly posts that are causing the increased length of posts?\n\n![](viz_lines_cnt_bypost_byhead-1.png)\n\nThe steady increase in the length of the `tutorial` section stands out. (I suppose the `R` community really enjoys code-walkthroughs (like this one).) Also, the introduction of the `new package` header about a year after the first RWeekly post suggests that R developers really care about what their fellow community members are working on.\n\n## Words\n\nThe words used in the short descriptions that accompany each link to external content should provide a more focused perspective on what specifically is of interest in the `R` community. What are the most frequently used words in these short descriptions?\n\n![](viz_unigrams_cnts-1.png)\n\nSome unsurprising words appear at the top of this list, such as `data` and `analysis`. Some words that one would probably not see among the top of an analogous list for another programming community are `rstudio`, `shiny`, `ggplot2`, and `tidy`. It's interesting that `shiny` actually appears as the top individual package--this could indicate that bloggers like to share their content through interactive apps (presumably because it is a great way to captivate and engage an audience).\n\nIt's one thing to look at individual words, but it is perhaps more interesting to look at word relationships.\n\n![](viz_unigrams_corrs-1.png)\n\nThis visual highlights a lot of the pairwise word correlations that we might expect in the data science realm: `data` and `science`, `time` and `series`, `machine` and `learning`, etc. Nonetheless, there are some that are certainly unique to the `R` community: `purrr` with `mapping`; `community` with `building`; `shiny` with `interactive` and `learning`; and `rstudio` with (`microsoft`) `server`.\n\nThe numerical values driving this correlation network not only is useful for quantifying the visual relationships, but, in this case, it actually highlights some relationships that get a bit lost in the graph (simply due to clustering). In particular, the prominence of the words `tutorial`, `conf`, `user`, and `interactive` stand out.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nunigram_corrs <- unigrams %>%\n  tetext::compute_corrs_at(\n    word = \"word\",\n    feature = \"name\",\n    num_top_ngrams = 100,\n    num_top_corrs = 100\n  )\nunigram_corrs %>% head(20)\n#> # A tibble: 20 x 4\n#>    item1       item2     correlation  rank\n#>    <chr>       <chr>           <dbl> <int>\n#>  1 tutorials   html            0.966     1\n#>  2 user2016    tutorials       0.955     2\n#>  3 user2016    html            0.950     3\n#>  4 machine     learning        0.726     4\n#>  5 user        user2016        0.708     5\n#>  6 slides      html            0.698     6\n#>  7 time        series          0.695     7\n#>  8 slides      tutorials       0.695     8\n#>  9 rstudio     conf            0.691     9\n#> 10 user        tutorials       0.690    10\n#> 11 user        html            0.687    11\n#> 12 user2016    slides          0.687    12\n#> 13 interactive html            0.668    13\n#> 14 text        mining          0.659    14\n#> 15 interactive user            0.658    15\n#> 16 interactive user2016        0.653    16\n#> 17 interactive tutorials       0.650    17\n#> 18 earl        london          0.594    18\n#> 19 network     building        0.582    19\n#> 20 interactive slides          0.550    20\n```\n:::\n\n\n### Most Unique Words\n\nLet's try to identify words that have risen and fallen in popularity. While there are many ways of doing, let's try segmenting the [R Weekly](https://rweekly.org/) posts into intervals of 60 days and computing the [term-frequency, inverse-document-frequency](https://www.tidytextmining.com/tfidf) (TF-IDF) of words across these intervals. (I apologize if the resolution is sub-par.)\n\n![](unigrams_tfidf-1.png)\n\nA couple of things stand out:\n\n-   Posts were heavily influenced by [`user2016` conference](http://user2016.r-project.org/) content in the early days of R Weekly (light blue and blue).\n-   There was clearly a `20` theme in the 60 days between 2017-02-20 and 2017-04-10 (red).\n-   The [\"tabs vs. spaces\"](https://softwareengineering.stackexchange.com/questions/57/tabs-versus-spaces-what-is-the-proper-indentation-character-for-everything-in-e) debate rose to prominence during the late summer days of 2017 (orange), presumably after [David Robinson's *Stack Overflow* post on the topic](https://stackoverflow.blog/2017/06/15/developers-use-spaces-make-money-use-tabs/).\n-   R's ongoing global influence is apparent with the appearance of `euro` with the [`user2016` conference](http://user2016.r-project.org/) (light blue and blue); `poland` and `satrdays` (presumably due to the [Cape Town R conference of the namesake](https://capetown2018.satrdays.org/) in late 2016 (green), and several Spanish words in January 2018 (yellow).\n\nI tried some different methods, but did not find much interesting regarding change in word frequency over time (aside from the TF-IDF approach). When using the method discussed in the [*Tidy Text Mining* book for identifying change in word usage](https://www.tidytextmining.com/twitter.html#changes-in-word-use) across 60-day intervals, I found only two non-trivial \"significant\" changes among the top 5% of most frequently used words, which are for `user` and `tutorials`. `user` has dropped off a bit since the `useR2016` conference, and `tutorials` has grown in usage, which is evident with the increasing length of the `tutorial` section in posts.\n\nThat's all I got for this subject. As I mentioned at the top, there are many of other great \"meta\" analyses like this one that are worth looking at, so definitely check them out!\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}