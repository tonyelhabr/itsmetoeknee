[
  {
    "path": "post/epl-xpts-simulation-2/",
    "title": "What exactly is an \"expected point\"? (part 2)",
    "description": "Evaluating how we can use match outcome probabilites for season-long insights",
    "author": [
      {
        "name": "Tony ElHabr",
        "url": "https://twitter.com/TonyElHabr"
      }
    ],
    "date": "2022-09-05",
    "categories": [
      "r",
      "soccer"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nAnalysis\r\n1. Predicting season-long\r\npoints\r\nWith season-long xPts and\r\nxGD\r\nWith match-level outcome\r\nprobabilities\r\n\r\n2. Identifying un-expected\r\nplacings\r\nComparison with a simpler\r\napproach\r\n\r\n\r\nConclusion\r\n\r\nIntroduction\r\nI‚Äôll be picking up where I left off in my last\r\npost, so stop everything that you‚Äôre doing and go read that if you\r\nhaven‚Äôt already. In this post we‚Äôll do two things:\r\nWe‚Äôll compare how well season-level expected goal difference\r\n(xGD), season-level xPts, and aggregated match-level xPts predict\r\nseason-long points for a given team.\r\nWe‚Äôll use the match-level probabilites to answer the questions\r\n‚ÄúWhich teams had the most unlikely placings in the table given the\r\nquality of all their shots across the season?‚Äù and ‚ÄúHow unlikely were\r\nsuch placings?‚Äù\r\nAnalysis\r\n1. Predicting season-long\r\npoints\r\nWith season-long xPts and\r\nxGD\r\nWe start with the all_raw_understat_xpts_by_match\r\nvariable from the prior post, adding the opponent‚Äôs expected goals to\r\ncreate a column for expected goal difference (xgd).\r\n\r\n\r\nall_raw_understat_xpts_by_match_with_opponent <- all_raw_understat_xpts_by_match |> \r\n  inner_join(\r\n    all_understat_shots |> distinct(match_id, season, date, team, opponent),\r\n    by = c('season', 'date', 'team')\r\n  )\r\n\r\n## we've already determined that the raw_xpts (provided directly by understat) \r\n##   is close to our calculated xpts, so we'll just use the raw_xpts.\r\nall_raw_understat_xpts_xgd_by_match <- all_raw_understat_xpts_by_match_with_opponent |> \r\n  select(match_id, season, date, team, opponent, pts, xpts = raw_xpts, xg) |> \r\n  inner_join(\r\n    all_raw_understat_xpts_by_match_with_opponent |> \r\n      select(match_id, opponent = team, opponent_xg = xg),\r\n    by = c('match_id', 'opponent')\r\n  ) |> \r\n  mutate(xgd = xg - opponent_xg)\r\n\r\n\r\n\r\nNext, we aggregate up to the season-level.\r\n\r\n\r\nall_raw_understat_xpts_xgd_by_season <- all_raw_understat_xpts_xgd_by_match |> \r\n  group_by(season, team) |> \r\n  summarize(across(c(pts, xpts, xgd), sum)) |> \r\n  ungroup() |> \r\n  group_by(season) |> \r\n  mutate(xrank = row_number(desc(xpts))) |> \r\n  ungroup() |> \r\n  arrange(season, desc(pts), team)\r\n\r\n\r\n\r\nFinally, we compute RMSE and R squared, like we did in the last\r\npost.\r\n\r\n\r\ndiagnose_season_feature <- function(df, col) {\r\n  fit <- lm(df$pts ~ df[[col]])\r\n  tibble(\r\n    rmse = compute_rmse(df$pts, predict(fit)),\r\n    r2 = summary(fit)$r.squared\r\n  )\r\n}\r\n\r\nc('xgd', 'xpts') |> \r\n  set_names() |> \r\n  map_dfr(\r\n    ~diagnose_season_feature(all_raw_understat_xpts_xgd_by_season, .x), \r\n    .id = 'feature'\r\n  )\r\n#> # A tibble: 2 √ó 3\r\n#>   feature  rmse    r2\r\n#>   <chr>   <dbl> <dbl>\r\n#> 1 xgd      7.31 0.831\r\n#> 2 xpts     7.19 0.837\r\n\r\n\r\n\r\nAs we should expect, a model using season-long xPts to predict final\r\npoints outperforms one using season-long xGD as a feature, although\r\nmaybe the difference between the two is smaller than we might have\r\nexpected.\r\nWith match-level outcome\r\nprobabilities\r\nFirst, we use the full understat shot data set and the custom\r\nfunctions from the prior post to calculate xPts by match.\r\n\r\n\r\nall_understat_xpts_by_match <- all_understat_shots |> \r\n  calculate_permuted_xg() |> \r\n  summarize_permuted_xg_by_match()\r\n\r\n\r\n\r\nThen, before we move on, we need to handle an edge case: some teams\r\ndo not have any shots in some matches.1 We\r\nchoose to specify that that a probability of 0.9 to losing and 0.1 to\r\ndrawing for such teams.2\r\n\r\n\r\ninit_all_understat_probs_by_match <- all_understat_xpts_by_match |> \r\n  select(match_id, season, team, opponent, is_home, starts_with('prob')) |> \r\n  rename_with(~str_remove(.x, '^prob_'), starts_with('prob')) |> \r\n  pivot_longer(\r\n    c(win, lose, draw),\r\n    names_to = 'result',\r\n    values_to = 'prob'\r\n  )\r\n\r\nunderstat_guaranteed_wins <- init_all_understat_probs_by_match |> \r\n  group_by(team, match_id) |> \r\n  filter(all(prob == 1)) |> \r\n  ungroup()\r\n\r\nunderstat_guaranteed_losses <- init_all_understat_probs_by_match |> \r\n  group_by(team, match_id) |> \r\n  filter(all(prob == 0)) |> \r\n  ungroup()\r\n\r\nunderstat_probs_by_match <- init_all_understat_probs_by_match |> \r\n  anti_join(\r\n    understat_guaranteed_losses |> select(team, match_id, result),\r\n    by = c('team', 'match_id', 'result')\r\n  ) |> \r\n  bind_rows(\r\n    understat_guaranteed_wins |> \r\n      mutate(\r\n        prob = case_when(\r\n          result == 'win' ~ 0.9,\r\n          result == 'lose' ~ 0,\r\n          result == 'draw' ~ 0.1\r\n        )\r\n      )\r\n  ) |> \r\n  bind_rows(\r\n    understat_guaranteed_losses |> \r\n      mutate(\r\n        prob = case_when(\r\n          result == 'win' ~ 0,\r\n          result == 'lose' ~ 0.9,\r\n          result == 'draw' ~ 0.1\r\n        )\r\n      )\r\n  ) |> \r\n  arrange(season, team, match_id)\r\n\r\n\r\n\r\nNext, the fun part: simulating match outcomes using the xG-implied\r\nmatch outcome probabilities. This is computationally intense, so we\r\nparallelize the calculation.\r\n\r\n\r\nlibrary(parallel)\r\nlibrary(future)\r\nlibrary(furrr)\r\n\r\nsimulate_season_xpts <- function(...) {\r\n  sim_home_pts_by_match <- understat_probs_by_match |> \r\n    filter(is_home) |> \r\n    group_by(team, season, match_id) |> \r\n    slice_sample(n = 1, weight_by = prob) |> \r\n    ungroup() |>\r\n    mutate(\r\n      pts = case_when(\r\n        result == 'win' ~ 3L,\r\n        result == 'lose' ~ 0L,\r\n        TRUE ~ 1L\r\n      )\r\n    )\r\n  \r\n  sim_pts_by_match <- bind_rows(\r\n    sim_home_pts_by_match |> select(match_id, season, team, pts),\r\n    sim_home_pts_by_match |> \r\n      transmute(\r\n        match_id,\r\n        season,\r\n        team = opponent,\r\n        pts = case_when(\r\n          result == 'win' ~ 0L,\r\n          result == 'lose' ~ 3L,\r\n          TRUE ~ 1L\r\n        )\r\n      )\r\n  ) |> \r\n    group_by(season, team) |> \r\n    summarize(across(pts, sum)) |> \r\n    ungroup()\r\n  \r\n  sim_pts_by_match |> \r\n    group_by(season, team) |> \r\n    summarize(across(pts, sum)) |> \r\n    ungroup() |> \r\n    group_by(season) |> \r\n    mutate(rank = row_number(desc(pts))) |> \r\n    ungroup() |> \r\n    arrange(season, rank)\r\n}\r\n\r\nn_cores <- detectCores()\r\ncores_for_parallel <- ceiling(n_cores * 0.5)\r\nplan(\r\n  multisession,\r\n  workers = cores_for_parallel\r\n)\r\n\r\nset.seed(42)\r\nn_sims <- 10000\r\nunderstat_sim_pts_by_season <- set_names(1:n_sims) |> \r\n  future_map_dfr(\r\n    simulate_season_xpts, \r\n    .id = 'sim_idx', \r\n    .options = furrr_options(seed = 42)\r\n  )\r\n\r\n## back to normal processing\r\nplan(sequential)\r\n\r\n\r\n\r\nNext, we aggregate the season-long points across simulations,\r\ncalculating the relative proportion of simulations in which a given team\r\nends up at a given rank.\r\n\r\n\r\nunderstat_sim_placings <- understat_sim_pts_by_season |> \r\n  group_by(season, team, xrank = rank) |> \r\n  summarize(n = n(), xpts = mean(pts)) |> \r\n  ungroup() |> \r\n  group_by(season, team) |> \r\n  mutate(prop = n / sum(n)) |> \r\n  ungroup()\r\n\r\n\r\n\r\nFinally, we calculate the weighted average of expected points that a\r\nteam ends up with, and run the same regression that we ran earlier with\r\nseason-long xPts and xGD.\r\n\r\n\r\nunderstat_sim_placings_agg <- understat_sim_placings |> \r\n  group_by(season, team) |> \r\n  summarize(xpts = sum(xpts * prop)) |> \r\n  ungroup() |>\r\n  select(season, team, xpts) |>\r\n  inner_join(\r\n    all_raw_understat_xpts_xgd_by_season |> select(season, team, pts),\r\n    by = c('season', 'team')\r\n  ) |> \r\n  arrange(season, desc(xpts))\r\n\r\ndiagnose_season_feature(understat_sim_placings_agg, 'xpts')\r\n#> # A tibble: 1 √ó 2\r\n#>    rmse    r2\r\n#>   <dbl> <dbl>\r\n#> 1  7.14 0.839\r\n\r\n\r\n\r\nInterestingly, the RMSE and R squared values are almost identical to\r\nthose for the season-long xPts. Perhaps this is not too\r\nsurprising‚Äîmatch-level outcome probabilities simulated and averaged to\r\narrive at a singular estimate of season-long xPts should give us\r\nsomething very close to just computing season-long xPts directly.\r\nWhile the null result may be discouraging, the simulations are useful\r\nin and of themselves. They can be used to understand the distribution of\r\noutcomes for team in a given season, as seen in the table below.3\r\n\r\nWe can see that Leicester, Wolves, and Newcastle all placed at the\r\nuppper end of their simulated placings, indicating that they\r\nover-achieved relative to expectation; on the other hand, Crystal\r\nPalace, Brentford, and Leeds placed on the lower end of the distribution\r\nof placings, indicating that they under-achieved.\r\nIn fact, we can go beyond simple observational judgement of whether\r\nteams over- and under-achieved‚Äîwe can use the relative proportion of\r\nsimulations where a team ends up at a given placing (or ‚Äúrank‚Äù) in the\r\nstandings to quantify just how unexpected actual end-of-season placings\r\nwere.\r\n2. Identifying un-expected\r\nplacings\r\nFirst, we need to step back and retrieve data on the actual placings.\r\nWe could theoretically calculate this from the shot data we already\r\nhave. However, the logic for handling own goals is a little complicated.\r\nWe‚Äôre probably better off using\r\nworldfootballR::understat_league_match_results()‚Äîwhich\r\nreturns goals at the match-level‚Äîto calculate the table. 4\r\n\r\n\r\nmatch_results <- 2014:2021 |> \r\n  map_dfr(~understat_league_match_results('EPL', .x)) |> \r\n  as_tibble()\r\n\r\ninit_table <- match_results |> \r\n  transmute(\r\n    match_id,\r\n    across(season, ~str_replace(.x, '\\\\/20', '/')),\r\n    date = strptime(datetime, '%Y-%m-%d %H:%M:%S', tz = 'UTC') |> date(),\r\n    home_team,\r\n    home_goals,\r\n    away_team,\r\n    away_goals\r\n  )\r\n\r\ntable <- bind_rows(\r\n  init_table |>\r\n    mutate(is_home = TRUE) |> \r\n    rename_home_away_teams() |> \r\n    select(season, team, opponent, goals = home_goals, opponent_goals = away_goals),\r\n  init_table |> \r\n    mutate(is_home = FALSE) |> \r\n    rename_home_away_teams() |> \r\n    select(season, team, opponent, goals = away_goals,opponent_goals = home_goals)\r\n) |> \r\n  mutate(\r\n    pts = case_when(\r\n      goals > opponent_goals ~ 3L,\r\n      goals < opponent_goals ~ 0L,\r\n      TRUE ~ 1L\r\n    )\r\n  ) |> \r\n  group_by(season, team) |> \r\n  summarize(across(c(goals, opponent_goals, pts), sum)) |> \r\n  ungroup() |> \r\n  mutate(gd = goals - opponent_goals) |> \r\n  arrange(season, desc(pts), desc(gd)) |> \r\n  group_by(season) |> \r\n  mutate(rank = row_number(desc(pts))) |> \r\n  ungroup() |> \r\n  arrange(season, rank)\r\n\r\n\r\n\r\nNext, we join the table of end-of-season placements\r\n(actual_rank) to the simulation results that describe the\r\nfrequency with which a given team places at a given rank\r\n(xrank).\r\n\r\n\r\nunderstat_sim_placings_with_actual_ranks <- understat_sim_placings |> \r\n  inner_join(\r\n    table |> select(season, team, actual_pts = pts, actual_rank = rank),\r\n    by = c('season', 'team')\r\n  ) |> \r\n  inner_join(\r\n    all_raw_understat_xpts_xgd_by_season |> \r\n      select(season, team, actual_xpts = xpts, xgd),\r\n    by = c('season', 'team')\r\n  )\r\n\r\n\r\n\r\nFinally, to identify over-achieving teams, we find the teams that had\r\nthe lowest cumulative probability of placing at their actual placing or\r\nbetter; and to identify under-achieving teams, we find the teams with\r\nthe lowest cumulative probability of placing at their actual placing or\r\nworse.\r\n\r\nThis table certainly passes the eye test. Brighton‚Äôs sixteenth place\r\nfinish in the 2020/21 season was discussed ad nauseum in the analytics\r\nsphere. Brighton under-performed historically given their massively\r\npositive xGD.\r\nOn the other end of the spectrum, it‚Äôs not hyperbole to say that\r\nManchester United‚Äôs second place finish in the 2017/18 season was an\r\nover-achievement. Although they ended up with the third best goal\r\ndifferential that season, they were closely followed by several teams.\r\nAnd their xGD was sixth in the league that season.\r\nComparison with a simpler\r\napproach\r\nNotably, we could get somewhat similar results by simply looking at\r\nthe largest residuals of a model that regresses the actual final table\r\nplacing on just xGD, which is how most people tend to think of\r\n‚Äúunexpected placings‚Äù.\r\n\r\n\r\ntable_with_xgd <- table |> \r\n  select(season, team, actual_pts = pts, actual_rank = rank) |> \r\n  inner_join(\r\n    all_raw_understat_xpts_xgd_by_season |> select(season, team, xgd),\r\n    by = c('season', 'team')\r\n  )\r\n\r\nxgd_rank_fit <- lm(actual_rank ~ xgd, table_with_xgd)\r\n\r\n\r\n\r\nThe table below shows the top five over- and under-achieving teams\r\naccording to our regression explaining season-ending placing with\r\nseason-ending xGD. Three of the top five over- and under-performing\r\nteams appear in the respective top fives according to the ranks from the\r\nsimulations of match probabilities shown before.\r\n\r\nWe can also look at this from the opposite perspective. Where do the\r\ntop five over- and under-achievers according to our simulations with\r\nmatch outcome probabilities fall among the season-ending xGD ranks for\r\nunlikelihood?\r\n\r\nOutside of the the three team-season pairs appearing in the both of\r\nthe top five over- and under-achievers that we already saw before, one\r\nteam-season pair is not too far off from the top five‚Äîthe 2017/18\r\nManchester United squad ranked as the sixth biggest over-performers by\r\nthe season-long xGD regression.5 However, the other\r\nover-perfomer, 2019/20 Newcastle, and the two remaining\r\nunder-performers, 2021/22 Crystal Palace and 2021/22 Brentford, have\r\nsomewhat large ranking discrepancies. So yes, the two methods can lead\r\nto somewhat similar results in some cases, but there is some\r\nobservational evidence to suggest that there are non-trivial differences\r\nin other cases.\r\nIn fact, there are some big differences between the two methods once\r\nwe look outside the top five or so biggest over- and under- achievers.\r\nFor example, in terms of over-achieving, Arsenal‚Äôs fifth place finish\r\n2018/19 season is given just a 8.9% chance of occurring given their\r\nseason-long xGD, ranking them as the 16th biggest over-performer (among\r\n80 team-season pairs considered to have over-achieved from 2014/15).6 On the other hand, the match-level\r\nsimulation approach marks the likelihood of them finishing fourth as\r\n31.9% (38th of 80). The season-long xGD model essentially sees that they\r\nfinished with an xGD of 7.5‚Äîless than any other fifth place finisher\r\nfrom 2014/15 - 2021/22‚Äîand penalizes them, while the match-level\r\napproach contextualizes them among their competition more robustly.\r\nAs another example, Manchester United‚Äôs under-achieving sixth place\r\nfinish in the 2016/17 is given a fairly reasonable 37.3% chance (66th of\r\n80 under-achieving teams) of occurring given their season-long 25.9 xGD,\r\nmost for any sixth place team in the data set. On the other hand, the\r\nmatch-level simulation approach sees their sixth place finish as more\r\nunlikely, at just a 15.4% probability (15th of 80). While one might have\r\ntheir own opinion regarding which approach seems more ‚Äúcorrect‚Äù, I‚Äôd say\r\nthat likelihoods from the simulation approach seem more appropriate for\r\nextreme examples such as this one and the 2018/19 Arsenal example\r\nOverall, both approaches seem reasonable to use to answer the\r\nquestion ‚ÄúWhich teams had the most unlikely placings in the table given\r\nthe quality of all their shots across the season?‚Äù But the approach\r\nbased on simulations using match probabilities seems more appropriate to\r\nuse to quantify exactly how unlikely a team‚Äôs final placing was. While\r\nthe simpler regression approach can also be used to quantify likelihood,\r\nit is more brittle, dependent on statistical assumptions. Additionally,\r\nwhile it contextualizes a team‚Äôs xGD with historical xGD, it does not\r\ncontextualize a team‚Äôs xGD among the other teams in the league, meaning\r\nthat it does not do a good job with capturing likelihood when there are\r\nstrong xGD over- and under-performances among a set of teams in a given\r\nseason.\r\nConclusion\r\nWhile aggregating match-level outcome probabilities to try to predict\r\nseason-ending points does no better than more direct approaches with\r\nseason-long xGD or xPts, simulating seasons using match-level outcome\r\nprobabilities can be used in a perhaps more interesting way‚Äîto quantify\r\njust how unlikely a team‚Äôs placement in the table is, given xG for all\r\nof their shots across the season.\r\n\r\nTheoretically, there is an even more\r\nextreme case: when both teams do not have any shots in a match. This\r\ndoes not occur in the data set, so we do not need to correct for this.‚Ü©Ô∏é\r\nThese choices are purely based on\r\nintuition. We could be more rigorous about this, but I think these\r\ndefaults should be fine. It‚Äôs wrong to assign a probability of 1 for\r\nlosing, and a naive split of 0.5 and 0.5 for losing and drawing is\r\nsurely too optimistic in favor of drawing.‚Ü©Ô∏é\r\nTable code is not shown since it‚Äôs\r\nnot particularly instructive.‚Ü©Ô∏é\r\nMaybe even more simply, we could use\r\nworldfootballR::fb_season_team_stats(..., stat_type = 'league_table'),\r\nalthough we‚Äôd need to add a column for team names for FBref to our team mapping to\r\ncorroborate team names.‚Ü©Ô∏é\r\nNote that, given the 160 team-season\r\npairs in the data set, we should expect that 80 teams each are ranked as\r\nover-performing and under-performing. No team will be labeled as\r\nperforming exactly as expected, unless they happen to have only one\r\npossible placing in the simulation approach, or a residual of exactly 0\r\nin the regression approach.‚Ü©Ô∏é\r\nTo quantify the likelihood of a\r\nteam‚Äôs placing with the simple regression, we convert the standard\r\nresidual of the regression to a tail probability.‚Ü©Ô∏é\r\n",
    "preview": "post/epl-xpts-simulation-2/unexpected.png",
    "last_modified": "2022-09-08T17:45:37-05:00",
    "input_file": "epl-xpts-simulation-2.knit.md",
    "preview_width": 988,
    "preview_height": 979
  },
  {
    "path": "post/epl-xpts-simulation-1/",
    "title": "What exactly is an \"expected point\"? (part 1)",
    "description": "Calculating and comparing expected points from different expected goals sources",
    "author": [
      {
        "name": "Tony ElHabr",
        "url": "https://twitter.com/TonyElHabr"
      }
    ],
    "date": "2022-09-04",
    "categories": [
      "r",
      "soccer"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nObjectives\r\n\r\nAnalysis\r\n1. Calculating xPts from xG\r\n2. Match predictive\r\nperformance\r\nPredicting\r\nmatch outcomes with binary logistic regression\r\nPredicting points with\r\nlinear regression\r\nPredicting\r\nmatch outcomes with multinomial logistic regression\r\n\r\n3. Season predictive\r\nperformance\r\n\r\nConclusion\r\n\r\nIntroduction\r\nExpected\r\ngoals (xG) in soccer have gone mainstream and are no longer cool to\r\ntalk about.\r\n\r\n\r\nWhat exactly is an ‚Äù expected goal ‚Äú? Who decides the criteria ? Is\r\nthere a list of‚Äù expected goal scorers ‚Äù ? Or even ‚Äù unexpected ones ‚Äù\r\n?\r\n\r\n‚Äî Ian Darke (@IanDarke)\r\nDecember\r\n24, 2020\r\n\r\nSo let‚Äôs talk about expected\r\npoints (xPts). The one sentence explainer for xPts: it‚Äôs a number\r\nbetween 0 and 3 assigned to each team in a match that we estimate from\r\nthe xG of each shot in the match. Teams that accumulate more xG than\r\ntheir opponents in the match are more likely to have xPts closer to 3,\r\ni.e.¬†the points awarded for a win, and those that accumulate less than\r\ntheir opponents are more likely to earn xPts closer to 0. xPts is\r\nconvenient for translating a team‚Äôs xG (relative to it‚Äôs opponents) to\r\nthe team‚Äôs expected placement in the standings.\r\nWhile several\r\noutlets\r\nhave described computing expected points with simulation1, simulation is actually not\r\nnecessary if you have the xG for every shot taken in a match.2 For example, let‚Äôs say team A shoots\r\nsix times with an xG of 0.1 for each shot, and team B shoots three shots\r\nwith xG‚Äôs of 0.1, 0.2, and 0.3 respectively. Given these goal\r\nprobabilities, we can analytically compute xPts as follows.\r\nFirst, we find the probability of scoring 0, 1, 2, etc. goals (up to\r\nthe number of shots taken).3\r\n\r\n\r\nlibrary(poibin)\r\nxg_a <- rep(0.1, 6)\r\nxg_b <- c(0.1, 0.2, 0.3)\r\n\r\nprobs_a <- dpoibin(seq.int(0, length(xg_a)), xg_a)\r\nround(probs_a, 2)\r\n#> [1] 0.53 0.35 0.10 0.01 0.00 0.00 0.00\r\nprobs_b <- dpoibin(seq.int(0, length(xg_b)), xg_b)\r\nround(probs_b, 2)\r\n#> [1] 0.50 0.40 0.09 0.01\r\n\r\n\r\n\r\n\r\nSecond, we convert the goal probabilities to singular probabilities\r\nfor each team winning the match, as well as the probability of a draw.4\r\n\r\n\r\nlibrary(gdata)\r\nouter_prod <- outer(probs_a, probs_b)\r\np_a <- sum(upperTriangle(outer_prod))\r\np_b <- sum(lowerTriangle(outer_prod))\r\np_draw <- sum(diag(outer_prod))\r\nround(c(p_a, p_b, p_draw), 2)\r\n#> [1] 0.30 0.28 0.42\r\n\r\n\r\n\r\nFinally, given the match outcome probabilities, the xPts calculation\r\nis straightforward.\r\n\r\n\r\nxpts_a <- 3 * p_a + 1 * p_draw\r\nxpts_b <- 3 * p_b + 1 * p_draw\r\nround(c(xpts_a, xpts_b), 2)\r\n#> [1] 1.31 1.27\r\n\r\n\r\n\r\nFor this example, we arrive at the interesting result that, despite\r\nthe two teams total xG being equal (=0.6), team A has a slightly higher\r\nprobability of winning. There have been plenty of explanations\r\non this ‚Äúquality vs.¬†quantity‚Äù phenomenon, so I won‚Äôt go into it in\r\ndetail. Nonetheless, this simple example illustrates why it can be\r\nuseful to translate xG into another form‚Äîdoing so can provide a better\r\nperspective on match results and, consequently, team placement in the\r\nstandings.\r\nObjectives\r\nSo we‚Äôve gone over what expected points are and why they‚Äôre\r\nimportant. Now we set out to do the following.\r\nCalculate xPts from shot xG for multiple seasons of\r\ndata. We‚Äôll limit the scope to the 2020/21 and 2021/22 seasons\r\nfor the English Premier League.5\r\nCompare the calibration of the understat and fotmob match\r\noutcome probabilities. {worldfootballR} makes it\r\neasy for us to get xG from both understat and fotmob, and it should be interesting\r\nto compare the the predictive performance of the two models.\r\nCompare predictions of actual season-long points using xPts\r\nthat we derive from understat and fotmob xG. In particular,\r\nwe‚Äôll be interested to see if our conclusions regarding the better\r\nsource for xG here matches the conclusions for (2).\r\nAnalysis\r\n1. Calculating xPts from xG\r\nLet‚Äôs start by using the load_understat_league_shots()\r\nfunction from {worldfootballR} to retrieve understat xG by\r\nshot.\r\n\r\n\r\nlibrary(readr)\r\nlibrary(dplyr)\r\nlibrary(tibble)\r\nlibrary(tidyr)\r\nlibrary(stringr)\r\nlibrary(lubridate)\r\nlibrary(worldfootballR) ## version: 0.5.12.5000\r\nlibrary(janitor)\r\n\r\nrename_home_away_teams <- function(df) {\r\n  df |> \r\n    mutate(\r\n      team = ifelse(is_home, home_team, away_team),\r\n      opponent = ifelse(is_home, away_team, home_team)\r\n    ) |> \r\n    select(-c(home_team, away_team)) \r\n}\r\n\r\nconvert_understat_year_to_season <- function(x) {\r\n  sprintf('%s/%s', x, str_sub(x + 1, 3, 4))\r\n}\r\n\r\n## we'll use all of the shots later when exporing understat data only\r\nall_understat_shots <- load_understat_league_shots('EPL') |> \r\n  as_tibble() |> \r\n  ## camelcase like \"xG\" is for Java scrubs\r\n  clean_names() |> \r\n  filter(season <= 2021) |> \r\n  ## transmute = select + mutate\r\n  transmute(\r\n    match_id,\r\n    ## \"2021/2022\" format so that we have a clear, consistent way to represent season\r\n    across(season, convert_understat_year_to_season),\r\n    ## to convert \"2020-09-12 11:30:00\" to a date (\"2020-09-12\")\r\n    across(date, lubridate::date),\r\n    home_team,\r\n    away_team,\r\n    is_home = h_a == 'h',\r\n    xg = x_g\r\n  ) |>\r\n  rename_home_away_teams() |> \r\n  arrange(season, date, team)\r\n\r\n## but when comparing understat with fotmob, we'll need to limit the seasons to just\r\n##   those that both sources have\r\nunderstat_shots <- all_understat_shots |> filter(season >= 2020)\r\n\r\n\r\n\r\nWe can use load_fotmob_match_details() to get fotmob‚Äôs\r\nshot xG in a similar fashion.6\r\n\r\n\r\n## manually created CSV with at least 2 columns: team_understat, team_fotmob.\r\n##   use the team_understat name to be consistent across sources.\r\nteam_mapping <- 'https://raw.githubusercontent.com/tonyelhabr/sports_viz/master/59-xg_xpoints/team_mapping.csv' |> \r\n  read_csv()\r\n\r\nrename_fotmob_teams <- function(df) {\r\n  df |> \r\n    left_join(\r\n      team_mapping |> select(team_understat, team_fotmob),\r\n      by = c('home_team' = 'team_fotmob')\r\n    ) |> \r\n    select(-home_team) |> \r\n    rename(home_team = team_understat) |> \r\n    left_join(\r\n      team_mapping |> select(team_understat, team_fotmob),\r\n      by = c('away_team' = 'team_fotmob')\r\n    ) |> \r\n    select(-away_team) |> \r\n    rename(away_team = team_understat)\r\n}\r\n\r\nfotmob_shots <- load_fotmob_match_details(\r\n  country = 'ENG',\r\n  league_name = 'Premier League'\r\n) |> \r\n  mutate(\r\n    ## to convert strings from 'Sat, Sep 12, 2020, 11:30 UTC' to a date\r\n    date = strptime(match_time_utc, '%a, %b %d, %Y, %H:%M UTC', tz = 'UTC') |> date(),\r\n    ## fotmob's parent_league_season always reflects the current season, so we need to manually\r\n    ##   define the season from the date. we would certainly want a more automated approach\r\n    ##   if working with more seasons and more leagues.\r\n    season = case_when(\r\n      date >= ymd('2020-09-12') & date <= ymd('2021-05-23') ~ '2020/21',\r\n      date >= ymd('2021-08-13') & date <= ymd('2022-05-22') ~ '2021/22',\r\n      TRUE ~ NA_character_\r\n    )\r\n  ) |> \r\n  ## the NAs are for 2022/2023 (incomplete as of writing) and the partial data for 2019/2020\r\n  drop_na(season) |> \r\n  transmute(\r\n    match_id,\r\n    season,\r\n    date,\r\n    home_team,\r\n    away_team,\r\n    is_home = team_id == home_team_id,\r\n    ## some shots with NAs for some reason\r\n    xg = coalesce(expected_goals, 0)\r\n  ) |>\r\n  rename_fotmob_teams() |> \r\n  rename_home_away_teams() |> \r\n  arrange(season, date, team)\r\n\r\n\r\n\r\nAlright, now the fun part. We functionalize the code from the example\r\nfor calculating the probability that xG will result in 0, 1, 2, etc.\r\ngoals.\r\n\r\n\r\nlibrary(purrr)\r\npermute_xg <- function(xg) {\r\n  n <- length(xg)\r\n  x <- seq.int(0, n)\r\n  dpoibin(x, xg)\r\n}\r\n\r\ncalculate_permuted_xg <- function(df) {\r\n  df |> \r\n    group_by(across(c(everything(), -xg))) |> \r\n    summarize(across(xg, ~list(.x))) |> \r\n    mutate(\r\n      prob = map(xg, ~permute_xg(.x))\r\n    ) |> \r\n    select(-c(xg)) |> \r\n    unnest(cols = c(prob)) |> \r\n    group_by(across(-c(prob))) |>\r\n    mutate(\r\n      g = row_number() - 1L\r\n    ) |>\r\n    ungroup() |> \r\n    arrange(match_id, is_home, g)\r\n}\r\n\r\nunderstat_permuted_xg <- understat_shots |> calculate_permuted_xg()\r\nfotmob_permuted_xg <- fotmob_shots |> calculate_permuted_xg()\r\n\r\n\r\n\r\nNext, we identify all possible goal combinations using xG as\r\n‚Äúweights‚Äù to compute the relative likelihood of each combination, and\r\nthen analytically calculate the probabilities of winning, losing, and\r\ndrawing.\r\n\r\n\r\nsummarize_pivoted_permuted_xg <- function(prob_away, prob_home) {\r\n  outer_prod <- outer(prob_away, prob_home)\r\n  p_draw <- sum(diag(outer_prod), na.rm = TRUE)\r\n  p_home <- sum(upperTriangle(outer_prod), na.rm = TRUE)\r\n  p_away <- sum(lowerTriangle(outer_prod), na.rm = TRUE)\r\n  list(\r\n    draw = p_draw,\r\n    home = p_home,\r\n    away = p_away\r\n  )\r\n}\r\n\r\nsummarize_permuted_xg_by_match <- function(df) {\r\n  pivoted <- df |>\r\n    transmute(\r\n      match_id,\r\n      season,\r\n      date,\r\n      g,\r\n      is_home = ifelse(is_home, 'home', 'away'),\r\n      prob\r\n    ) |>\r\n    pivot_wider(\r\n      names_from = is_home,\r\n      names_prefix = 'prob_',\r\n      values_from = prob,\r\n      values_fill = 0L\r\n    )\r\n  \r\n  pivoted |> \r\n    select(match_id, season, date, prob_away, prob_home) |>\r\n    group_by(match_id, season, date) |> \r\n    summarize(\r\n      across(starts_with('prob_'), ~list(.x))\r\n    ) |> \r\n    ungroup() |> \r\n    inner_join(\r\n      df |> distinct(match_id, team, opponent, is_home),\r\n      by = 'match_id'\r\n    ) |> \r\n    mutate(\r\n      prob = map2(prob_away, prob_home, summarize_pivoted_permuted_xg)\r\n    ) |> \r\n    select(-starts_with('prob_')) |> \r\n    unnest_wider(prob, names_sep = '_') |> \r\n    mutate(\r\n      prob_win = ifelse(is_home, prob_home, prob_away),\r\n      prob_lose = ifelse(is_home, prob_away, prob_home),\r\n      xpts = 3 * prob_win + 1 * prob_draw\r\n    ) |> \r\n    select(-c(prob_home, prob_away))\r\n}\r\n\r\nunderstat_xpts_by_match <- understat_permuted_xg |> summarize_permuted_xg_by_match()\r\nfotmob_xpts_by_match <- fotmob_permuted_xg |> summarize_permuted_xg_by_match()\r\n\r\n\r\n\r\nLet‚Äôs take a quick peak at the distributions of xG and xPts, both as\r\na sanity check and to enhance our understanding of the relationship\r\nbetween the two. When plotting xPts as a function xG, we should expect\r\nto see a monotonically increasing relationship where xPts bottoms out at\r\nzero and tops out at three.\r\n\r\nFurther, if there is any doubt about the expected points calculation,\r\nnote that understat offers xPts directly in their data. The mean\r\nabsolute error of our calculation of xPts with theirs is ~0.02.\r\n\r\n\r\nlibrary(understatr)\r\n\r\nall_raw_understat_xpts_by_match <- 2014:2021 |> \r\n  set_names() |> \r\n  map_dfr(\r\n    ~get_league_teams_stats('EPL', .x),\r\n    .id = 'season'\r\n  ) |> \r\n  transmute(\r\n    across(season, ~convert_understat_year_to_season(as.integer(.x))),\r\n    date,\r\n    team = team_name,\r\n    result,\r\n    pts,\r\n    raw_xpts = xpts,\r\n    xg = xG\r\n  )\r\n\r\nraw_understat_xpts_by_match <- all_raw_understat_xpts_by_match |> \r\n  inner_join(\r\n    understat_xpts_by_match |> select(season, date, team, xpts),\r\n    by = c('season', 'date', 'team')\r\n  ) |> \r\n  mutate(\r\n    xptsd = raw_xpts - xpts\r\n  ) |> \r\n  arrange(season, date, team)\r\n\r\n## mean absolute error\r\nround(mean(abs(raw_understat_xpts_by_match$xptsd)), 2)\r\n#> [1] 0.02\r\n\r\n\r\n\r\n2. Match predictive performance7\r\nAs one might guess, the match outcome probabilities implied by the xG\r\nfrom understat and fotmob are strongly correlated.\r\n\r\n\r\nrename_xpts_by_match <- function(df, src) {\r\n  df |> \r\n    select(season, date, team, starts_with('prob_'), xpts) |> \r\n    rename_with(\r\n      ~sprintf('%s_%s', .x, src), c(starts_with('prob_'), xpts)\r\n    )\r\n}\r\n\r\nxpts_by_match <- raw_understat_xpts_by_match |> \r\n  select(season, date, team, result, pts) |> \r\n  inner_join(\r\n    understat_xpts_by_match |> rename_xpts_by_match('understat'),\r\n    by = c('season', 'date', 'team')\r\n  ) |> \r\n  inner_join(\r\n    fotmob_xpts_by_match |> rename_xpts_by_match('fotmob'),\r\n    by = c('season', 'date', 'team')\r\n  )\r\n\r\ncor_draw <- cor(xpts_by_match$prob_draw_fotmob, xpts_by_match$prob_draw_understat)\r\ncor_win <- cor(xpts_by_match$prob_win_fotmob, xpts_by_match$prob_win_understat)\r\ncor_lose <- cor(xpts_by_match$prob_lose_fotmob, xpts_by_match$prob_lose_understat)\r\nround(c(cor_draw, cor_win, cor_lose), 3)\r\n#> [1] 0.906 0.958 0.958\r\n\r\n\r\n\r\nNote that the win and loss correlations are identical. This is due to\r\nthe symmetric nature of the data‚Äîwe have two records for each match, one\r\nfrom each team‚Äôs perspective.8\r\nPredicting\r\nmatch outcomes with binary logistic regression\r\nNow let‚Äôs compare how ‚Äúgood‚Äù the implied probabilities from the two\r\nsources are. To do this, we‚Äôll create binary logistic regression models\r\nto predict a given outcome and compute:\r\nthe mean\r\nsquared error (MSE);\r\nthe brier\r\nskill score (BSS), treating the empirical proportion of the\r\nspecified outcome as the reference.910\r\na calibration\r\nplot, grouping predictions into ‚Äúbuckets‚Äù at every 5%.\r\n\r\n\r\nresult_props <- xpts_by_match |> \r\n  count(result) |> \r\n  mutate(prop = n / sum(n))\r\n\r\ncompute_mse <- function(truth, estimate) {\r\n  mean((truth - estimate)^2)\r\n}\r\n\r\ndiagnose_prob_by_match <- function(src, result) {\r\n  \r\n  df <- xpts_by_match |> \r\n    mutate(\r\n      result = ifelse(result == !!result, 1L, 0L) |> factor()\r\n    )\r\n  \r\n  result_name <- switch(\r\n    result,\r\n    'w' = 'win',\r\n    'l' = 'lose',\r\n    'd' = 'draw'\r\n  )\r\n  col <- sprintf('prob_%s_%s', result_name, src)\r\n  \r\n  fit <- glm(\r\n    df$result ~ df[[col]],\r\n    family = 'binomial'\r\n  )\r\n  \r\n  probs <- tibble(\r\n    result_num = as.numeric(df$result) - 1,\r\n    .prob = unname(predict(fit, type = 'response'))\r\n  )\r\n  \r\n  n_buckets <- 20\r\n  alpha <- 0.05\r\n  calib <- probs |>\r\n    mutate(\r\n      across(.prob, ~round(.x * n_buckets) / n_buckets)\r\n    ) |>\r\n    group_by(.prob) |>\r\n    summarize(\r\n      ## Jeffreys' prior\r\n      ci_lower = qbeta(alpha / 2, sum(result_num) + 0.5, n() - sum(result_num) + 0.5),\r\n      ci_upper = qbeta(1 - alpha / 2, sum(result_num) + 0.5, n() - sum(result_num) + 0.5),\r\n      actual = sum(result_num) / n(),\r\n      n = n()\r\n    ) |> \r\n    ungroup()\r\n  \r\n  mse <- compute_mse(probs$result_num, probs$.prob)\r\n  \r\n  ref_prob <- result_props |> \r\n    filter(result == !!result) |> \r\n    pull(prop)\r\n  \r\n  ref_mse <- compute_mse(probs$result_num, ref_prob)\r\n  bss <- 1 - (mse / ref_mse)\r\n  \r\n  list(\r\n    calib = calib,\r\n    mse = mse,\r\n    bss = bss\r\n  )\r\n}\r\n\r\ndiagnostics <- crossing(\r\n  result = c('w', 'd'),\r\n  src = c('understat', 'fotmob')\r\n) |> \r\n  mutate(\r\n    diagnostics = map2(src, result, diagnose_prob_by_match)\r\n  ) |> \r\n  unnest_wider(diagnostics)\r\ndiagnostics |> select(-calib)\r\n#> # A tibble: 4 √ó 4\r\n#>   result src         mse    bss\r\n#>   <chr>  <chr>     <dbl>  <dbl>\r\n#> 1 d      fotmob    0.170 0.0268\r\n#> 2 d      understat 0.166 0.0466\r\n#> 3 w      fotmob    0.173 0.270 \r\n#> 4 w      understat 0.162 0.317\r\n\r\n\r\n\r\nThe MSE (where lower is ‚Äúbetter‚Äù) and the BSS (where higher is\r\n‚Äúbetter‚Äù) lead us to the same conclusion‚Äîthe models based on understat‚Äôs\r\nxG slightly outperform the one based on fotmob‚Äôs xG.\r\nMoreover, looking at the calibration plot, the understat model\r\npredictions seem to stick closer to the 45 degree slope representing\r\nperfect calibration.\r\n\r\nPredicting points with\r\nlinear regression\r\nAlternatively, we could regress points on expected points. For linear\r\nregression, we can use the root\r\nmean squared error (RMSE) (where lower is ‚Äúbetter‚Äù) and R\r\nsquared (where higher is ‚Äúbetter‚Äù) to compare the models.\r\n\r\n\r\ncompute_rmse <- function(truth, estimate) {\r\n  sqrt(mean((truth - estimate)^2))\r\n}\r\n\r\ndiagnose_xpts_by_match <- function(src) {\r\n  \r\n  col <- sprintf('xpts_%s', src)\r\n  fit <- lm(xpts_by_match$pts ~ xpts_by_match[[col]])\r\n  \r\n  pred <- predict(fit)\r\n  \r\n  tibble(\r\n    rmse = compute_rmse(xpts_by_match$pts, pred),\r\n    r2 = summary(fit)$r.squared\r\n  )\r\n}\r\n\r\nc('understat', 'fotmob') |> \r\n  set_names() |> \r\n  map_dfr(diagnose_xpts_by_match, .id = 'src')\r\n#> # A tibble: 2 √ó 3\r\n#>   src        rmse    r2\r\n#>   <chr>     <dbl> <dbl>\r\n#> 1 understat  1.06 0.374\r\n#> 2 fotmob     1.10 0.322\r\n\r\n\r\n\r\nThe understat model proves to be better by both metrics, having a\r\nlower RMSE and higher R squared than the fotmob model.\r\nPredicting\r\nmatch outcomes with multinomial logistic regression\r\nPersonally, I don‚Äôt like predicting points directly like this since\r\nit‚Äôs a discrete variable that can only take on three values (0, 1, and\r\n3). If we‚Äôre going to predict points instead of a probability, I think\r\nthe better approach is to run a multinomial logistic regression and to\r\nconvert the predicted probabilities to expected points.\r\n\r\n\r\nlibrary(nnet)\r\ndiagnose_implied_xpts_by_match <- function(src) {\r\n  \r\n  col_win <- sprintf('prob_win_%s', src)\r\n  col_draw <- sprintf('prob_draw_%s', src)\r\n  fit <- multinom(\r\n    xpts_by_match$result ~ xpts_by_match[[col_win]] + xpts_by_match[[col_draw]],\r\n    trace = FALSE\r\n  )\r\n  probs <- predict(fit, type = 'probs') |> as_tibble()\r\n  preds <- 3 * probs$w + 1 * probs$d\r\n  \r\n  tibble(\r\n    rmse = compute_rmse(xpts_by_match$pts, preds),\r\n    r2 = cor(xpts_by_match$pts, preds)^2\r\n  )\r\n}\r\n\r\nc('understat', 'fotmob') |> \r\n  set_names() |> \r\n  map_dfr(diagnose_implied_xpts_by_match, .id = 'src')\r\n#> # A tibble: 2 √ó 3\r\n#>   src        rmse    r2\r\n#>   <chr>     <dbl> <dbl>\r\n#> 1 understat  1.06 0.374\r\n#> 2 fotmob     1.10 0.321\r\n\r\n\r\n\r\nAgain, we see that understat has a lower RMSE and higher R squared.\r\nThe implication that understat performs slightly better than fotmob\r\nagrees with the results from the binary logistic regression approiach\r\nfor predicting match outcome probabilities and the linear regression\r\napproach for predicting points.\r\nOverall, we might say that understat seems to be the better of the\r\ntwo xG sources for explaining individual match results, although the\r\nmargin is small enough that I would hesitate to say that this is the\r\ntrue across all leagues and all seasons.\r\n3. Season predictive\r\nperformance\r\nHow do the understat and fotmob models fare if we aggregate up the\r\nexpected points to the season level and predict actual points?11\r\n\r\n\r\nxpts_by_season <- xpts_by_match |> \r\n  group_by(season, team) |> \r\n  summarize(\r\n    across(c(pts, starts_with('xpts')), sum)\r\n  ) |> \r\n  ungroup()\r\n\r\ndiagnose_xpts_by_season <- function(src) {\r\n  \r\n  col <- sprintf('xpts_%s', src)\r\n  fit <- lm(xpts_by_season$pts ~ xpts_by_season[[col]])\r\n  \r\n  preds <- predict(fit)\r\n  \r\n  tibble(\r\n    rmse = compute_rmse(xpts_by_match$pts, preds),\r\n    r2 = summary(fit)$r.squared\r\n  )\r\n}\r\n\r\nc('understat', 'fotmob') |> \r\n  set_names() |> \r\n  map_dfr(diagnose_xpts_by_season, .id = 'src')\r\n#> # A tibble: 2 √ó 3\r\n#>   src        rmse    r2\r\n#>   <chr>     <dbl> <dbl>\r\n#> 1 understat  53.9 0.845\r\n#> 2 fotmob     53.8 0.825\r\n\r\n\r\n\r\nThe results are closer than those at the match-level. In fact, fotmob\r\njust barely edges out understat in terms of RMSE xPts, although\r\nunderstat outperforms fotmob according to R squared by a relatively\r\ncomfortable 0.02. It‚Äôs harder to make a general statement regarding\r\nwhich data source provides better xG for explaining season-long expected\r\npoints, although we might lean in favor of understat again.\r\nConclusion\r\nOverall, we find that understat‚Äôs xG model seems to very slightly\r\noutperform fotmob‚Äôs in terms of explaining match results and season-long\r\npoint totals.\r\nIn a follow up post, we‚Äôll go more in depth regarding how we can\r\nleverage the match outcome probabilities to simulate season-ending\r\npoints in a more rigorous fashion that done in the last section\r\nabove.\r\n\r\nDanny Page‚Äôs interactive\r\nweb app also uses simulation.‚Ü©Ô∏é\r\nNow, if you desire the statistical\r\nproperties that simulation offers, such as an estimation of error,\r\nthat‚Äôs understandable; however, in write-ups that I‚Äôve seen, such is not\r\nmentioned explicitly. Additionally, if one chooses to go down the\r\nsimulation route because they believe that it helps to suppress flaws\r\nwith the xG model, that‚Äôs also understandable. On the other hand, the\r\nanalytical approach I present should present nearly identical results to\r\nthat which one would find with simulation, and it offers the advantage\r\nof being much faster.‚Ü©Ô∏é\r\nPlotting code is omitted throughout\r\nthe post since it‚Äôs not particularly instructive.‚Ü©Ô∏é\r\nHow does this work? Under the\r\nassumption that xG comes from a Poisson\r\nbinomial distribution, we look at all combinations of makes and\r\nmisses of the shots and compare the relative proportion of instances in\r\nwhich one team‚Äôs number of success, i.e.¬†goals, is greater than, equal\r\nto, or less than their opponent‚Äôs.‚Ü©Ô∏é\r\nWe‚Äôve limited the scope for several\r\nreasons: (1) fotmob only has complete xG data for the 2020/21 and\r\n2021/22 seasons as of writing, (2) I didn‚Äôt want to have to map team\r\nnames across the two data sources for a ton of teams; and (3) of all\r\nleague, I‚Äôm most interested in the EPL üòÑ.‚Ü©Ô∏é\r\nNote that there are three additional\r\nshots in the fotmob data. There‚Äôs no simple solution to resolving this\r\ndata discrepancy since we don‚Äôt have matching shot identifiers in the\r\ntwo data sets ü§∑.‚Ü©Ô∏é\r\nUsing the adjective ‚Äúpredictive‚Äù is a\r\nlittle misleading, since we‚Äôre not actually making predictions\r\nout-of-sample. Rather, we‚Äôre using models based on xG to evaluate which\r\nxG data source better explains the observed results.‚Ü©Ô∏é\r\nHome field advantage is treated as a\r\nfeature instead of defined directly via columns,\r\ni.e.¬†home_team, home_score, etc., which is\r\ngood practice in general.‚Ü©Ô∏é\r\nDraws occur for 22.5% of matches in\r\nthe data set, and wins and losses occur in 38.8% of matches each.‚Ü©Ô∏é\r\nPersonally, I tend to rely on BSS\r\nwherever I can. Not only is it more interpretable‚Äîit‚Äôs a number between\r\n0 and 1, while MSE can take on any value, depending on the context‚ÄîI\r\nlike that it forces one to compare to a baseline, which is a good\r\nprinciple in general.‚Ü©Ô∏é\r\nNote that aggregating match-level\r\nprobabilities to the season-level is not a statistically valid way to\r\nuse the probabilities, which are intended to be treated independently.‚Ü©Ô∏é\r\n",
    "preview": "post/epl-xpts-simulation-1/calib.png",
    "last_modified": "2022-09-08T07:27:27-05:00",
    "input_file": {},
    "preview_width": 3000,
    "preview_height": 2250
  },
  {
    "path": "post/soccer-pass-network-max-cut/",
    "title": "Yet Another (Advanced?) Soccer Statistic",
    "description": "Quantifying soccer pass networks with weighted maximum cuts",
    "author": [
      {
        "name": "Tony ElHabr",
        "url": "https://twitter.com/TonyElHabr"
      }
    ],
    "date": "2022-01-31",
    "categories": [
      "r",
      "soccer"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nExamples\r\nSimple\r\nIn Soccer\r\n\r\nBut is Max Cut Useful?\r\nSetup\r\nCorrelations\r\nAccounting for Confounders\r\nWhy Haven‚Äôt People Tried this Before?\r\n\r\nConclusion\r\n\r\nIntroduction\r\nPass networks are a common visualization form used to summarize a team‚Äôs behavior in a soccer match. Nodes represent average player position on passes that they are involved with, and edges represent passes between players. Most pass networks also weight node size and edge width by the total number of passes.\r\n\r\nWhile pass networks provide a nice visual tool for providing insight that can (and should) be supplemented by more detailed analysis, they‚Äôre often just that‚Äîpurely a visual tool. In order to gain meaning beyond just anecdotal insight (‚ÄúLook at how far the wingbacks were up the field!‚Äù), practitioners may leverage graph theory concepts such as centrality to quantify relationships.1\r\nInspired by the findings of Eliakim et al.¬†in ‚ÄúThe development of metrics for measuring the level of symmetry in team formation and ball movement flow, and their association with performance‚Äù, I wanted to evaluate a graph theory concept that has not been explored in relation to soccer pass networks (except for by Eliakim et al.): maximum cuts.\r\nTo do so, I‚Äôll be using data from the 2017/18 - 2020/21 Premier League seasons, along with the games up through Boxing Day of the 2021/22 season. Passes and other events are only considered up through the first substitution or red card of each match.\r\nExamples\r\nSimple\r\nWhat is a maximum cut? Visually, it‚Äôs an arbitrary line that you can draw through the edges of a network that maximizes the sum of the edge weights.\r\n\r\nFor this example, 15 is actually a weighted max cut, since edges are treated differently, according to their assigned value. (An unweighted max cut would assign each edge a value of 1.)\r\nOn the other side of things, the min cut would be 2+4=6 for this example.\r\nIn Soccer\r\nA 4-node, 5-edge network is nice for illustration, but how does this bear out in soccer?\r\nTo give a soccer example, Here‚Äôs the pass network and weighted max cut numbers for the match between Liverpool and Manchester City on October 3, 2021. 2\r\n\r\nZooming out from a single example to all games in our data set, the distribution of weighted max cuts per 90 minutes looks relatively normal, perhaps log-normal. Note: It‚Äôs important to adjust for time since not all games have the same number of minutes played due to variance in the occurrence of the first formation change.\r\n\r\nBut is Max Cut Useful?\r\nSetup\r\nTo quantify the impact of weighted max cuts, we‚Äôll look at two measures of quality of play.\r\nexpected goals (xG): xG tells the story of shot quality and quantity, which is massively important in a low-scoring game like soccer.\r\nexpected threat (xT): xT quantifies scoring opportunities more generally, looking beyond shots.\r\nI‚Äôd argue that xT is more informative for our purposes since max cut is related to passes and xT accounts for passes; xG is so tied up in shots that their relationship to passes leading up to those shots may be lost. Nonetheless, we‚Äôll be considering both since both are commonly used for judging overall ‚Äúvalue‚Äù in soccer.\r\nWe‚Äôll be transforming these xG and xT in two manners.\r\nVolume-adjusting, i.e.¬†taking each value per 90 minutes. The justification for adjusting max cut for time also applies here.\r\nOpponent-adjusting, or ‚Äúdifferencing‚Äù, i.e.¬†subtracting one side‚Äôs value from the other‚Äôs value. Sure, having a lot of touches in the opponent‚Äôs half and taking a lot of shots means scoring is more likely, but if you‚Äôre also giving up a ton of shots, then that effort is essentially negated.\r\nGiven that we‚Äôll be making our two quality-of-play stats‚ÄîxG and xT‚Äîrelative to the opponent, I prefer to opponent-adjust weighted max cut in the same manner. I‚Äôd argue that weighted max cut differential is more informative than just the weighted max cut of one side or the other. Suppressing your opponent‚Äôs weighted max cut is reflective of limiting their pass volume, and, consequently, makes it more likely that your weighted max cut is higher.\r\nThe relationship between weighted max cut and weighted max cut differential is very linear, so, ultimately, it shouldn‚Äôt matter too much if we look at opponent-adjust weighted max cut versus just raw weighted max cut.\r\n\r\nDifferencing has the added benefit of making our distributions look more ‚Äúnormal‚Äù, by making them symmetric about 0. This generally is beneficial for regression analysis, which we go on to conduct.\r\nCorrelations\r\nA first step in looking at the relationship between weighted max cut with xG and xT is a correlation.\r\n\r\nWeighted max cut compares favorably to other network stats for summarizing (pass) networks. Perhaps this isn‚Äôt too surprising; Eliakim et al.¬†argue that, in relation to soccer, maximum cut surmises what is captured separately by various measures of centrality (betweenness, indegree and outdegree, etc.).\r\nWeighted max cut has a similar correlation to traditional pass metrics such as relative percentage of passes, but not as strong as counting stats for shots. We really shouldn‚Äôt expect any metric to have as strong a relation with xG and xT (especially xG) as shot-based metrics since these are derived from shots and their outcomes.\r\nOverall, the game-level correlations are not super strong, indicating that we can‚Äôt read too much into them for individual games. The correlations are much stronger at the season-level, showing the same ordinality in magnitude of correlation.\r\n\r\nObserving the difference in the game-level and season-level correlations should be a good reminder that single-game stats should not be scrutinized too heavily when evaluating a team‚Äôs performance over the course of a season. The same is true for max cuts!\r\nAccounting for Confounders\r\nThe correlation approach for quantifying the descriptive role of max cuts in quality of play is a bit naive. Single-variable regressions, i.e.¬†correlations, overstate the impact of the ‚Äútreatment‚Äù variables.\r\nIf we regress max cut on xG and xT with z-score normalized counts of shots and passes as confounders, we see that the influence of max cut is negated.\r\n\r\nIn the case of xG, the coefficient estimate for weighted max cut is offset by the coefficient for the passing term. This is due to their collinearity (over 90%) and their lack of explanatory value in the presence of shot counts, which directly informs xG. For xT, the weighted max cut coefficient is completely suppressed, likely due to collinearity with passing.\r\nOf course, we could be a little more sophisticated here, drawing out directed acyclic graphs (DAG) and running a more formal causal analysis. But my intuition is that we would come to the same general conclusion: in the face of more traditional metrics like shot and pass counts, possibly the most robust pass-network-derived statistic‚Äîweighted max cut‚Äîprovides minimal additional descriptive power for quantifying quality of play.\r\nWhy Haven‚Äôt People Tried this Before?\r\nI can think of a couple of reasons why weighted max cut isn‚Äôt commonly seen in soccer literature:\r\nThe calculation requires an un-directed network. In our context, this requires treating passes between players as equal, regardless of who is the passer and who is the receiver. This can distort the role of a striker, who may receive much more than pass, or a keeper, who may pass much more than receive.\r\nIt‚Äôs difficult to visualize beyond just reporting a single number, and, thus, may not resonate with an audience.\r\nIt‚Äôs not super easy to calculate! In fact, {igraph}‚Äîthe most popular R framework for network analysis‚Äîdoesn‚Äôt have a function for it!\r\nFor what it‚Äôs worth, the code that I wrote to calculate the weighted maximum cut for a pass network looks something like this.3 (This is the calculation for the 4-node example network shown before.)\r\n\r\n\r\nlibrary(tibble)\r\nlibrary(dplyr)\r\nlibrary(tidyr)\r\nlibrary(sdpt3r) ## Semi-Definite Quadratic Linear Programming Solver\r\n\r\ndf <- tibble(\r\n  from = c('a', 'a', 'a', 'b', 'b', 'b', 'c', 'c', 'c', 'd', 'd', 'd'),\r\n  to   = c('b', 'c', 'd', 'a', 'c', 'd', 'a', 'b', 'd', 'a', 'b', 'c'),\r\n  n    = c( 1L,  0L,  3L,  1L,  1L,  1L,  0L,  2L,  1L,  1L,  5L,  4L)\r\n)\r\n\r\nwide_df <- df %>% \r\n  pivot_wider(\r\n    names_from = to,\r\n    values_from = n,\r\n    values_fill = 0L\r\n  ) %>% \r\n  select(from, a, b, c, d) %>% \r\n  arrange(from) %>% \r\n  select(-from)\r\nwide_df\r\n## # A tibble: 4 x 4\r\n##       a     b     c     d\r\n##   <int> <int> <int> <int>\r\n## 1     0     1     0     3\r\n## 2     1     0     1     1\r\n## 3     0     2     0     1\r\n## 4     1     5     4     0\r\n\r\nm <- as.matrix(wide_df)\r\nsymmetric_m <- m + t(m) ## must be symmetric\r\nmc <- maxcut(symmetric_m)\r\nmax_cut <- -round(mc$pobj, 0)\r\nmax_cut\r\n## [1] 15\r\n\r\n\r\n\r\nOr, perhaps, people have actually evaluated max cut for pass networks, but have found the same non-significant result that I have found, and have simply opted not to write about it. ü§∑\r\nConclusion\r\nWeighted max cut can be a very informative metric for summarizing pass volume and pass network structure, as seen in a correlation analysis. It‚Äôs merit surpasses that of other summary network stats and is nearly equivalent to traditional pass-derived stats for explaining xG and xT. However, I don‚Äôt think it should supersede more traditional stats like shots and passes if purely evaluating attacking quality.\r\n\r\nThere are other things researchers have done, such as subsetting the passes that make up the network based on game situation, or supplement the visual with meaningful color‚Ü©Ô∏é\r\nUnfortunately, max cuts can be difficult to visualize for graphs with lots of edges and/or nodes, so reporting just the quantity is often the best thing we can do. ü§∑‚Ü©Ô∏é\r\nIf we wanted to do an un-weighted calculation, we would use 1 for any n>0 in the example here.‚Ü©Ô∏é\r\n",
    "preview": "post/soccer-pass-network-max-cut/network-game_id=1549604-max_cut_weighted_norm_w_logo.png",
    "last_modified": "2022-04-16T19:26:30-05:00",
    "input_file": {},
    "preview_width": 3108,
    "preview_height": 2400
  },
  {
    "path": "post/dimensionality-reduction-and-clustering/",
    "title": "Tired: PCA + kmeans, Wired: UMAP + GMM",
    "description": "An Alternative to the Classic Approach to Dimension Reduction + Clustering",
    "author": [
      {
        "name": "Tony ElHabr",
        "url": "https://twitter.com/TonyElHabr"
      }
    ],
    "date": "2021-06-30",
    "categories": [
      "r",
      "soccer"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nUnsupervised Evaluation\r\n‚ÄúSupervised‚Äù Evaluation\r\nAside: Re-coding Clusters\r\n\r\nCase Study: PCA vs.¬†UMAP\r\nCase\r\nStudy: UMAP + GMM\r\nConclusion\r\n\r\nIntroduction\r\nCombining principal\r\ncomponent analysis (PCA) and kmeans\r\nclustering seems to be a pretty popular 1-2 punch in data science.\r\nWhile there is some debate about whether combining dimensionality\r\nreduction and clustering is\r\nsomething we should ever do1, I‚Äôm not here to debate\r\nthat. I‚Äôm here to illustrate the potential advantages of upgrading your\r\nPCA + kmeans workflow to Uniform Manifold\r\nApproximation and Projection (UMAP) + Gaussian Mixture\r\nModel (GMM), as noted in my\r\nreply here.\r\n\r\n\r\ntired: PCA + kmeanswired: UMAP + GMM\r\n\r\n‚Äî Tony (@TonyElHabr)\r\nJune\r\n2, 2021\r\n\r\nFor this demonstration, I‚Äôll be using this\r\ndata set pointed out here,\r\nincluding over 100 stats for players from soccer‚Äôs\r\n‚ÄúBig 5‚Äù leagues.\r\n\r\n\r\nlibrary(tidyverse)\r\nraw_df <- 'FBRef 2020-21 T5 League Data.xlsx' %>% \r\n  readxl::read_excel() %>% \r\n  janitor::clean_names() %>% \r\n  mutate(across(where(is.double), ~replace_na(.x, 0)))\r\n\r\n# Let's only use players with a 10 matches' worth of minutes.\r\ndf <- raw_df %>% filter(min > (10 * 90))\r\n\r\n\r\n\r\n\r\n\r\ndim(df)\r\n\r\n\r\n\r\n\r\n\r\n## [1] 1626  128\r\n\r\n\r\n\r\nTrying to infer something from the correlation matrix doesn‚Äôt get you\r\nvery far, so one can see why dimensionality reduction will be\r\nuseful.\r\n\r\nAlso, we don‚Äôt really have ‚Äúlabels‚Äù here (more on this later), so\r\nclustering can be useful for learning something from our data.\r\nUnsupervised Evaluation\r\nWe‚Äôll be feeding in the results from the dimensionality\r\nreduction‚Äîeither PCA or UMAP‚Äîto a clustering method‚Äîeither kmeans or\r\nGMM. So, since clustering comes last, all we need to do is figure out\r\nhow to judge the clustering; this will tell us something about how\r\n‚Äúgood‚Äù the combination of dimensionality reduction and clustering is\r\noverall.\r\nI‚Äôll save you from google-ing and just tell you that within-cluster\r\nsum of squares (WSS) is typically used for kmeans, and Bayesian\r\nInformation Criteria (BIC) is the go-to metric for GMM. WSS and BIC\r\nare not on the same scale, so we can‚Äôt directly compare kmeans and GMM\r\nat this point. Nonetheless, we can experiment with different numbers of\r\ncomponents‚Äîthe one major ‚Äúhyperparameter‚Äù\r\nfor dimensionality reduction‚Äîprior to the clustering to identify if more\r\nor less components is ‚Äúbetter‚Äù, given the clustering method. Oh, and why\r\nnot also vary the number of clusters‚Äîthe one notable hyperparameter for\r\nclustering‚Äîwhile we‚Äôre at it?\r\n\r\nFor kmeans, we see that WSS decreases with increasing number of\r\nclusters, which is typically what we see in ‚Äúelbow‚Äù\r\nplots like this. Additionally, we see that WSS decreases with\r\nincreasing number of components. This makes sense‚Äîadditional components\r\nmeans more data is accounted for.2 There is definitely a\r\npoint of ‚Äúdiminishing returns‚Äù, somewhere around 3 clusters, after which\r\nWSS barely improves.3 Overall, we observe that the kmeans\r\nmodels using UMAP pre-processing do better, compared to those using\r\nPCA.\r\nMoving on to GMM, we observe that BIC generally increases with the\r\nnumber of clusters as well. (Note that, due to the way the\r\n{mclust} package defines its objective function, higher BIC\r\nis ‚Äúbetter‚Äù.)\r\n\r\nRegarding number of components, we see that the GMM models using more\r\nUMAP components do better, as we should have expected. On the other\r\nhand, we observe that GMM models using less PCA components do better\r\nthan those with more components. This is a bit of an odd finding that I\r\ndon‚Äôt have a great explanation for. (Someone please math-splain to me.)\r\nNonetheless, we see that UMAP does better than PCA overall, as we\r\nobserved with kmeans.\r\nFor those interested in the code, I map-ed a function\r\nacross a grid of parameters to generate the data for these plots.4\r\n\r\n\r\ndo_dimr_clust <- function(df, n, k, dimr, clust, ...) {\r\n  step_f <- switch(dimr, 'pca' = recipes::step_pca, 'umap' = embed::step_umap)\r\n  fit_f <- switch(clust, 'kmeans' = stats::kmeans, 'gmm' = mclust::Mclust)\r\n  \r\n  d <- recipes::recipe(formula( ~ .), data = df) %>%\r\n    recipes::step_normalize(recipes::all_numeric_predictors()) %>%\r\n    step_f(recipes::all_numeric_predictors(), num_comp = n) %>% \r\n    recipes::prep() %>% \r\n    recipes::juice() %>% \r\n    select(where(is.numeric))\r\n  fit <- fit_f(d, k, ...)\r\n  broom::glance(fit)\r\n}\r\n\r\nmetrics <- crossing(\r\n  n = seq.int(2, 8),\r\n  k = seq.int(2, 8),\r\n  f_dimr = c('pca', 'umap'),\r\n  f_clust = c('kmeans', 'mclust')\r\n) %>%\r\n  mutate(metrics = pmap(\r\n    list(n, k, f_dimr, f_clust),\r\n    ~ do_dimr_clust(\r\n      df = df,\r\n      n = ..1,\r\n      k = ..2,\r\n      f_dimr = ..3,\r\n      f_clust = ..4\r\n    )\r\n  ))\r\nmetrics\r\n#> # A tibble: 196 x 5\r\n#>        n     k f     g      metrics         \r\n#>    <int> <int> <chr> <chr>  <list>          \r\n#>  1     2     2 pca   kmeans <tibble [1 x 4]>\r\n#>  2     2     2 pca   gmm    <tibble [1 x 7]>\r\n#>  3     2     2 umap  kmeans <tibble [1 x 4]>\r\n#>  4     2     2 umap  gmm    <tibble [1 x 7]>\r\n#>  5     2     3 pca   kmeans <tibble [1 x 4]>\r\n#>  6     2     3 pca   gmm    <tibble [1 x 7]>\r\n#>  7     2     3 umap  kmeans <tibble [1 x 4]>\r\n#>  8     2     3 umap  gmm    <tibble [1 x 7]>\r\n#>  9     2     4 pca   kmeans <tibble [1 x 4]>\r\n#> 10     2     4 pca   gmm    <tibble [1 x 7]>\r\n#> # ... with 186 more rows\r\n\r\n\r\n\r\n‚ÄúSupervised‚Äù Evaluation\r\nWe actually do have something that we can use to help us identify\r\nclusters‚Äîplayer position (pos). Let‚Äôs treat these position\r\ngroups as pseudo-labels with which we can gauge the effectiveness of the\r\nclustering.\r\n\r\n\r\ndf <- df %>% \r\n  mutate(\r\n    across(\r\n      pos,\r\n      ~case_when(\r\n        .x %in% c('DF,MF', 'MF,DF') ~ 'DM',\r\n        .x %in% c('DF,FW', 'FW,DF') ~ 'M',\r\n        .x %in% c('MF,FW', 'FW,MF') ~ 'AM',\r\n        .x == 'DF' ~ 'D',\r\n        .x == 'MF' ~ 'M',\r\n        .x == 'FW' ~ 'F',\r\n        .x == 'GK' ~ 'G',\r\n        .x == 'GK,MF' ~ 'G',\r\n        TRUE ~ .x\r\n      )\r\n    )\r\n  )\r\ndf %>% count(pos, sort = TRUE)\r\n#> # A tibble: 6 x 2\r\n#>   pos       n\r\n#>   <chr> <int>\r\n#> 1 D       595\r\n#> 2 M       364\r\n#> 3 AM      273\r\n#> 4 F       196\r\n#> 5 G       113\r\n#> 6 DM       85\r\n\r\n\r\n\r\nTypically we don‚Äôt have labels for clustering tasks; if we do, we‚Äôre\r\nusually doing some kind of supervised multi-label classification. But\r\nour labels aren‚Äôt ‚Äútrue‚Äù labels in this case, both because:\r\na player‚Äôs nominal position often doesn‚Äôt completely describe their\r\nstyle of play, and\r\nthe grouping I did to reduce the number of positions from 11 to 6\r\nwas perhaps not optimal.\r\nSo now let‚Äôs do the same as before‚Äîevaluate different combinations of\r\nPCA and UMAP with kmeans and GMM. But now we can use some supervised\r\nevaluation metrics: (1) accuracy\r\nand (2) mean log\r\nloss. While the former is based on the ‚Äúhard‚Äù predictions, the\r\nlatter is based on probabilities for each class. kmeans returns just\r\nhard cluster assignments, so computing accuracy is straightforward;\r\nsince it doesn‚Äôt return probabilities, we‚Äôll treat the hard assignments\r\nas having a probability of 1 to compute log loss.5\r\nWe can compare the two clustering methods more directly now using\r\nthese two metrics. Since we know that there are 6 position groups, we‚Äôll\r\nkeep the number of clusters constant at 6. (Note that number of\r\nclusters was shown on the x-axis before; but since we\r\nhave fixed number of components at 6, now we show the number of\r\ncomponents on the x-axis.)\r\nLooking at accuracy first, we see that the best combo depends on our\r\nchoice for number of components. Overall, we might say that the UMAP\r\ncombos are better.\r\n\r\nNext, looking at average log loss, we see that the GMM clustering\r\nmethods seem to do better overall (although this may be due to the fact\r\nthat log loss is not typically used for supervised kmeans). The PCA +\r\nGMM does the best across all number of components, with the exception of\r\n7. Note that we get a mean log loss around 28 when we predict the\r\nmajority class (defender) with a probability of 1 for all observations.\r\n(This is a good ‚Äúbaseline‚Äù to contextualize our numbers.)\r\n\r\nUMAP shines relative to PCA according to accuracy, and GMM beats out\r\nkmeans in terms of log loss. Despite these conclusions, we still don‚Äôt\r\nhave clear evidence that UMAP + GMM is the best 1-2 combo; nonetheless,\r\nwe can at least feel good about its general strength.\r\nAside: Re-coding Clusters\r\nI won‚Äôt bother to show all the code to generate the above plots since\r\nit‚Äôs mostly just broom::augmment() and\r\n{ggplot2}. But, if you have ever worked with supervised\r\nstuff like this (if we can call it that), you‚Äôll know that figuring out\r\nwhich of your clusters correspond to your known groups can be difficult.\r\nIn this case, I started from a variable holding the predicted\r\n.class and the true class (pos).\r\n\r\n\r\nassignments\r\n#> # A tibble: 1,626 x 2\r\n#>    .class pos  \r\n#>     <int> <chr>\r\n#>  1      1 D    \r\n#>  2      2 D    \r\n#>  3      3 M    \r\n#>  4      3 M    \r\n#>  5      4 AM   \r\n#>  6      2 D    \r\n#>  7      2 D    \r\n#>  8      4 F    \r\n#>  9      2 D    \r\n#> 10      1 D    \r\n#> # ... with 1,616 more rows\r\n\r\n\r\n\r\nI generated a correlation matrix for these two columns, ready to pass\r\ninto a matching procedure.\r\n\r\n\r\ncors <- assignments %>% \r\n  fastDummies::dummy_cols(c('.class', 'pos'), remove_selected_columns = TRUE) %>% \r\n  corrr::correlate(method = 'spearman', quiet = TRUE) %>% \r\n  filter(term %>% str_detect('pos')) %>% \r\n  select(term, matches('^[.]class'))\r\ncors\r\n#> # A tibble: 6 x 7\r\n#>   term   .class_1 .class_2 .class_3 .class_4 .class_5 .class_6\r\n#>   <chr>     <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>\r\n#> 1 pos_AM  -0.208   -0.241   -0.178    0.0251   0.625   -0.123 \r\n#> 2 pos_D    0.499    0.615   -0.335   -0.264   -0.428   -0.208 \r\n#> 3 pos_DM   0.0797   0.0330   0.0548  -0.0829  -0.0519  -0.0642\r\n#> 4 pos_F   -0.171   -0.199   -0.168    0.724    0.0232  -0.101 \r\n#> 5 pos_G   -0.127   -0.147   -0.124   -0.0964  -0.157    1     \r\n#> 6 pos_M   -0.222   -0.267    0.724   -0.180    0.0395  -0.147\r\n\r\n\r\n\r\nThen I used clue::solve_LSAP() to do the bipartite\r\nmatching magic. The rest is just pre- and post-processing.\r\n\r\n\r\nk <- 6 # number of clusters\r\ncols_idx <- 2:(k+1)\r\ncors_mat <- as.matrix(cors[,cols_idx]) + 1 # all values have to be positive\r\nrownames(cors_mat) <- cors$term\r\ncols <- names(cors)[cols_idx]\r\ncolnames(cors_mat) <- cols\r\ncols_idx_min <- clue::solve_LSAP(cors_mat, maximum = TRUE)\r\ncols_min <- cols[cols_idx_min]\r\npairs <-\r\n  tibble::tibble(\r\n    .class = cols_min %>% str_remove('^[.]class_') %>% as.integer(),\r\n    pos = cors$term %>% str_remove('pos_')\r\n  )\r\npairs\r\n#> # A tibble: 6 x 2\r\n#>   .class pos  \r\n#>    <int> <chr>\r\n#> 1      5 AM   \r\n#> 2      2 D    \r\n#> 3      1 DM   \r\n#> 4      4 F    \r\n#> 5      6 G    \r\n#> 6      3 M \r\n\r\n\r\n\r\nThis pairs variable can be used to re-code the\r\n.class column in our assignments from\r\nbefore.\r\nCase Study: PCA vs.¬†UMAP\r\nLet‚Äôs step back from the clustering techniques and focus on\r\ndimensionality reduction for a moment. One of the ways that\r\ndimensionality reduction can be leveraged in sports like soccer is for\r\nplayer similarity metrics.6 Let‚Äôs take a look at how\r\nthis can be done, comparing the PCA and UMAP results while we‚Äôre at\r\nit.\r\nDirect comparison of the similarity ‚Äúscores‚Äù we‚Äôll compute‚Äîbased on\r\nEuclidean distance between a chosen player‚Äôs components and other\r\nplayers‚Äô components‚Äîis not wise given the different ranges of our PCA\r\nand UMAP components, so we‚Äôll rely on rankings based on these scores.7 Additionally, fbref provides a ‚Äúbaseline‚Äù that we can\r\nuse to judge our similarity rankings.8\r\nWe‚Äôll start with Jadon\r\nSancho, a highly discussed player at the moment (as a potential\r\ntransfer).\r\n\r\nWe first need to set up our data into the following format. (This is\r\nfor 2-component, 6-cluster UMAP + GMM.)\r\n\r\n\r\nsims_int\r\n#> # A tibble: 1,664 x 6\r\n#>    player_1     player_2           comp_1 comp_2 value_1 value_2\r\n#>    <chr>        <chr>               <int>  <int>   <dbl>   <dbl>\r\n#>  1 Jadon Sancho Aaron Leya Iseka        1      1  -4.18  -5.14  \r\n#>  2 Jadon Sancho Aaron Leya Iseka        2      2  -0.678  2.49  \r\n#>  3 Jadon Sancho Aaron Ramsey            1      1  -4.18  -3.25  \r\n#>  4 Jadon Sancho Aaron Ramsey            2      2  -0.678 -0.738 \r\n#>  5 Jadon Sancho Abdoul Kader Bamba      1      1  -4.18  -3.40  \r\n#>  6 Jadon Sancho Abdoul Kader Bamba      2      2  -0.678  0.0929\r\n#>  7 Jadon Sancho Abdoulaye Doucour√©      1      1  -4.18  -1.36  \r\n#>  8 Jadon Sancho Abdoulaye Doucour√©      2      2  -0.678 -2.66  \r\n#>  9 Jadon Sancho Abdoulaye Tour√©         1      1  -4.18  -1.36  \r\n#> 10 Jadon Sancho Abdoulaye Tour√©         2      2  -0.678 -2.89  \r\n#> # ... with 1,654 more rows\r\n\r\n\r\n\r\nThen the Euclidean distance calculation is fairly\r\nstraightforward.\r\n\r\n\r\nsims <- sims_init %>% \r\n  group_by(player_1, player_2) %>% \r\n  summarize(\r\n    d = sqrt(sum((value_1 - value_2)^2))\r\n  ) %>% \r\n  ungroup() %>% \r\n  mutate(score = 1 - ((d - 0) / (max(d) - 0))) %>% \r\n  mutate(rnk = row_number(desc(score))) %>% \r\n  arrange(rnk) %>% \r\n  select(player = player_2, d, score, rnk)\r\nsims\r\n#> # A tibble: 830 x 4\r\n#>    player                  d score   rnk\r\n#>    <chr>               <dbl> <dbl> <int>\r\n#>  1 Alexis S√°nchez     0.0581 0.994     1\r\n#>  2 Riyad Mahrez       0.120  0.988     2\r\n#>  3 Serge Gnabry       0.132  0.986     3\r\n#>  4 Jack Grealish      0.137  0.986     4\r\n#>  5 Pablo Sarabia      0.171  0.983     5\r\n#>  6 Thomas M√ºller      0.214  0.978     6\r\n#>  7 Leroy San√©         0.223  0.977     7\r\n#>  8 Callum Hudson-Odoi 0.226  0.977     8\r\n#>  9 Jesse Lingard      0.260  0.973     9\r\n#> 10 Ousmane Demb√©l√©    0.263  0.973    10\r\n#> # ... with 820 more rows\r\n\r\n\r\n\r\nDoing the same for PCA and combining all results, we get the\r\nfollowing set of rankings.\r\n\r\nWe see that the UMAP rankings are ‚Äúcloser‚Äù overall to the fbref\r\nrankings. Of course, there are some caveats:\r\nThis is just one player.\r\nThis is with a specific number of components and clusters.\r\nWe are comparing to similarity rankings based on a separate\r\nmethodology.\r\nOur observation here (that UMAP > PCA) shouldn‚Äôt be taken out of\r\ncontext to conclude that UMAP > PCA in all contexts. Nonetheless, I\r\nthink this is an interesting use case for dimensionality reduction,\r\nwhere one can justify PCA, UMAP, or any other similar technique,\r\ndepending on how intuitive the results are.\r\nCase Study: UMAP + GMM\r\nFinally, let‚Äôs bring clustering back into the conversation. We‚Äôre\r\ngoing to focus on how the heralded UMAP + GMM combo can be visualized to\r\nprovide insight that supports (or debunks) our prior understanding.\r\nWith a 2-component UMAP + 6-cluster GMM, we can see how the 6\r\nposition groups can be identified in a 2-D space.\r\n\r\nFor those curious, using PCA instead of UMAP also leads to an\r\nidentifiable set of clusters. However, uncertainties are generally\r\nhigher across the board (larger point sizes, more overlap between\r\ncovariance ellipsoids).\r\n\r\nIf we exclude keepers (G) and defenders (D) to focus on the other 4\r\npositions with our UMAP + GMM approach, we can better see how some\r\nindividual points ‚Äîat the edges or outside of covariance ellipsoids‚Äîare\r\nclassified with a higher degree of uncertainty.9\r\n\r\nNow, highlighting incorrect classifications, we can see how the\r\ndefensive midfielder (DM) position group (upper left) seems to be a\r\nblind spot in our approach.\r\n\r\nA more traditional confusion matrix10\r\nalso illustrates the inaccuracy with classifying DMs. (Note the lack of\r\ndark grey fill in the DM column.)\r\n\r\nDMs are often classified as defenders instead. I think this poor\r\nresult has is more so due to my lazy grouping of players with\r\n\"MF,DF\" or \"DF,MF\" positions in the original\r\ndata set than a fault in our approach.\r\nConclusion\r\nSo, should our overall conclusion be that we should never use PCA or\r\nkmeans? No, not necessarily. They can both be much faster to compute\r\nthan UMAP and GMMs respectively, which can be a huge positive if\r\ncomputation is a concern. PCA is linear while UMAP is not, so you may\r\nwant to choose PCA to make it easier to explain to your friends.\r\nRegarding clustering, kmeans is technically a specific form of a GMM, so\r\nif you want to sound cool to your friends and tell them that you use\r\nGMMs, you can do that.\r\nAnyways, I hope I‚Äôve shown why you should try out UMAP and GMM the\r\nnext time you think about using PCA and kmeans.\r\n\r\nIn some contexts you may want to do\r\nfeature selection and/or manual grouping of data.‚Ü©Ô∏é\r\nWhile this whole thing is more about\r\ncomparing techniques, I should make a note about WSS. We don‚Äôt want to\r\nincrease the number of components for the sake of minimizing WSS. We\r\nlose some degree of interpretation with increasing components.\r\nAdditionally, we could be overfitting the\r\nmodel by increasing the number of components. Although we don‚Äôt have the\r\nintention of classifying new observations in this demo, it‚Äôs still good\r\nto keep overfitting in mind.‚Ü©Ô∏é\r\nThis demo isn‚Äôt really intended to be\r\na study in how to choose the best number of clusters, but I figured I‚Äôd\r\npoint this out.‚Ü©Ô∏é\r\nI‚Äôd suggest this blog post\r\nfrom Julia Silge for a better explanation of clustering with R and\r\n{tidymodels}.‚Ü©Ô∏é\r\nPerhaps this is against best\r\npractice, but we‚Äôll do it here for the sake of comparison.‚Ü©Ô∏é\r\nNormalization perhaps doesn‚Äôt help\r\nmuch here given the clustered nature of the reduced data.‚Ü©Ô∏é\r\nNormalization perhaps doesn‚Äôt help\r\nmuch here given the clustered nature of the reduced data.‚Ü©Ô∏é\r\nfbref uses a different methodology,\r\nso perhaps it‚Äôs unwise to compare to them.‚Ü©Ô∏é\r\nSure, one can argue that a player\r\nlike Diogo Jota should have been classified as an attacking midfielder\r\n(AM) to begin with, in which case he might not have been misclassified\r\nhere.‚Ü©Ô∏é\r\nBy the way, the\r\nautoplot() function for yardstick::conf_mat()\r\nresults is awesome if you haven‚Äôt ever used it.‚Ü©Ô∏é\r\n",
    "preview": "post/dimensionality-reduction-and-clustering/featured.png",
    "last_modified": "2022-09-08T08:16:11-05:00",
    "input_file": {},
    "preview_width": 2400,
    "preview_height": 2400
  },
  {
    "path": "post/soccer-league-strength/",
    "title": "Quantifying Relative Soccer League Strength",
    "description": "With Atomic VAEP",
    "author": [
      {
        "name": "Tony ElHabr",
        "url": "https://twitter.com/TonyElHabr"
      }
    ],
    "date": "2021-06-26",
    "categories": [
      "r",
      "soccer"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nMethodology\r\nImprovements & Further Work\r\nAncillary Take-away\r\n\r\nIntroduction\r\nArguing about domestic league strength is something that soccer fans seems to never tire of. (‚ÄúCould Messi do it on a cold rainy night in Stoke?‚Äù) Many of these conversations are anecdotal, leading to ‚Äúhot takes‚Äù that are unfalsifiable. While we‚Äôll probably never move away from these kinds of discussions, we can at least try to inform them with a quantitative approach.\r\nPerhaps the obvious way to do so is to take match results from international tournaments (e.g.¬†Champions League, Europa). But such an approach can be flawed‚Äîthere‚Äôs not a large sample, and match results may not be reflective of ‚Äútrue‚Äù team strength (e.g.¬†one team may win on xG by a large margin, but lose the game.)\r\nMethodology\r\nBut what if we used an approach rooted in player performance? I asked myself that very question and came up with the following approach. (Thanks to Cahnzhi Ye for the data.)\r\nIdentify players who played in more than one league within the same season or across consecutive seasons. Calculate the difference in each player‚Äôs atomic VAEP1 per 90 minutes (VAEP/90) after changing leagues.\r\n\r\n\r\n## # A tibble: 2,462 x 7\r\n##    Season Player        `League A`        `League B`       `VAEP/90 A` `VAEP/90 B`   Diff.\r\n##     <dbl> <chr>         <chr>             <chr>                  <dbl>       <dbl>   <dbl>\r\n##  1   2020 Timo Werner   Bundesliga 1 (Ge~ Premier League ~       1.25        0.638  0.610 \r\n##  2   2020 Alexander S√∏~ Super Lig (Turke~ Bundesliga 1 (G~       1.07        0.773  0.296 \r\n##  3   2020 Hakim Ziyech  Eredivisie (Neth~ Premier League ~       0.958       0.365  0.593 \r\n##  4   2020 Nicol√°s Gonz~ Bundesliga 2 (Ge~ Bundesliga 1 (G~       0.917       0.943 -0.0256\r\n##  5   2020 Fabian Klos   Bundesliga 2 (Ge~ Bundesliga 1 (G~       0.904       0.547  0.358 \r\n##  6   2020 Victor Osimh~ Ligue 1 (France)  Serie A (Italy)        0.889       1.01  -0.120 \r\n##  7   2020 Eldor Shomur~ Premier League (~ Serie A (Italy)        0.882       0.755  0.126 \r\n##  8   2020 Callum Robin~ Championship (En~ Premier League ~       0.880       0.682  0.198 \r\n##  9   2020 Jarrod Bowen  Championship (En~ Premier League ~       0.871       0.448  0.423 \r\n## 10   2020 Aleksandar M~ Championship (En~ Premier League ~       0.866       0.499  0.367 \r\n## # ... with 2,452 more rows\r\n\r\n\r\n\r\nWhy VAEP? Theoretically it should capture more about in-game actions (including defense) than other stats such as xG, which is biased in favor of attacking players. VAEP is not perfect by any means (e.g.¬†it does not capture off-ball actions), but, in theory, it should be a better measure of overall performance. 2\r\nNotably, we give up a little in interpretability in using VAEP, since it‚Äôs not directly translatable to goals. 3 The following table of top season-long xG totals since 2012 to contextualize the magnitudes of xG and VAEP.\r\n\r\n\r\n## # A tibble: 36,857 x 5\r\n##    Season Player             Minutes    xG  VAEP\r\n##     <dbl> <chr>                <dbl> <dbl> <dbl>\r\n##  1   2015 Lionel Messi          3529  38.1  69.0\r\n##  2   2016 Luis Su√°rez           3294  35.6  55.7\r\n##  3   2018 Robert Lewandowski    2259  35.1  40.2\r\n##  4   2012 Lionel Messi          3425  32.4  76.2\r\n##  5   2013 Lionel Messi          2776  31.6  60.0\r\n##  6   2015 Cristiano Ronaldo     3236  30.8  58.7\r\n##  7   2018 Mohamed Salah         3080  30.4  50.5\r\n##  8   2012 Cristiano Ronaldo     3504  30.1  62.7\r\n##  9   2017 Edin Dzeko            3216  29.8  49.7\r\n## 10   2020 Robert Lewandowski    2902  29.7  49.2\r\n## # ... with 36,847 more rows\r\n\r\n\r\n\r\nAnd a scatter plot, because who doesn‚Äôt love a graph.\r\n\r\nConvert the player-level VAEP/90 differences to z-scores by position and age group.\r\n\r\n\r\n## # A tibble: 2,462 x 7\r\n##    Season Player      Position `Age Group` `League A`      `League B`    `VAEP/90 Diff. Z`\r\n##     <dbl> <chr>       <chr>    <chr>       <chr>           <chr>                     <dbl>\r\n##  1   2020 Timo Werner AM       24<=x<27    Bundesliga 1 (~ Premier Leagu~            2.28 \r\n##  2   2020 Alexander ~ FW       24<=x<27    Super Lig (Tur~ Bundesliga 1 ~            1.10 \r\n##  3   2020 Hakim Ziye~ M        27<=x<30    Eredivisie (Ne~ Premier Leagu~            3.20 \r\n##  4   2020 Nicol√°s Go~ M        18<=x<24    Bundesliga 2 (~ Bundesliga 1 ~           -0.148\r\n##  5   2020 Fabian Klos FW       30<=x<36    Bundesliga 2 (~ Bundesliga 1 ~            1.20 \r\n##  6   2020 Victor Osi~ FW       18<=x<24    Ligue 1 (Franc~ Serie A (Ital~           -0.439\r\n##  7   2020 Eldor Shom~ FW       24<=x<27    Premier League~ Serie A (Ital~            0.471\r\n##  8   2020 Callum Rob~ AM       24<=x<27    Championship (~ Premier Leagu~            0.740\r\n##  9   2020 Jarrod Bow~ AM       18<=x<24    Championship (~ Premier Leagu~            1.89 \r\n## 10   2020 Aleksandar~ FW       24<=x<27    Championship (~ Premier Leagu~            1.37 \r\n## # ... with 2,452 more rows\r\n\r\n\r\n\r\nWhy grouping? This is intended to account for the fact that attacking players and ‚Äúpeaking‚Äù players (usually age 24-30) tend to have higher VAEP/90, so their league-to-league differences have larger variation. The choice to normalize is perhaps more questionable. The mean of differences is ~0 for all groups already, but the dispersion is smaller without normalization (i.e.¬†standard deviations are closer to 0). So, in this case, normalization should help the linear model capture variation.\r\n\r\n\r\n## # A tibble: 20 x 5\r\n##    Position Age Group N    Mean   SD\r\n##    <chr>    <chr>    <int> <dbl>  <dbl>\r\n##  1 AM       18<=x<24   138     0 0.224 \r\n##  2 AM       24<=x<27   128     0 0.268 \r\n##  3 AM       27<=x<30   118     0 0.248 \r\n##  4 AM       30<=x<36    68     0 0.248 \r\n##  5 D        18<=x<24   203     0 0.112 \r\n##  6 D        24<=x<27   268     0 0.101 \r\n##  7 D        27<=x<30   295     0 0.0930\r\n##  8 D        30<=x<36   316     0 0.0939\r\n##  9 DM       18<=x<24    30     0 0.102 \r\n## 10 DM       24<=x<27    48     0 0.0913\r\n## 11 DM       27<=x<30    20     0 0.105 \r\n## 12 DM       30<=x<36    13     0 0.0719\r\n## 13 FW       18<=x<24    26     0 0.274 \r\n## 14 FW       24<=x<27    67     0 0.268 \r\n## 15 FW       27<=x<30    50     0 0.263 \r\n## 16 FW       30<=x<36    66     0 0.297 \r\n## 17 M        18<=x<24   113     0 0.173 \r\n## 18 M        24<=x<27   130     0 0.147 \r\n## 19 M        27<=x<30   189     0 0.185 \r\n## 20 M        30<=x<36   186     0 0.171  \r\n\r\n\r\n\r\nRun a single regression where the response variable is the z-transformed VAEP/90 difference, and the features are indicators for leagues, where -1 indicates player departure, a +1 indicates player arrival, and all other values are 0.4 5\r\nFor those familiar with basketball and hockey, this is similar to the set-up for an adjusted plus-minus (APM) calculation. Here, each feature column is a league (instead of a player), each row represents a player (instead of a ‚Äústint‚Äù), and the response is transformed VAEP/90 (instead of net points per possession).\r\n\r\n\r\n## tibble [2,472 x 16] (S3: tbl_df/tbl/data.frame)\r\n##  $ VAEP/90 Diff Z-Trans     : num [1:2472] -0.0825 0.3285 -0.0143 0.1137 0.1526 ...\r\n##  $ Serie A (Italy)          : int [1:2472] -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 ...\r\n##  $ Bundesliga 1 (Germany)   : int [1:2472] 1 1 0 0 0 0 0 0 0 0 ...\r\n##  $ La Liga (Spain)          : int [1:2472] 0 0 0 0 0 0 0 0 0 0 ...\r\n##  $ Serie A (Brazil)         : int [1:2472] 0 0 0 0 0 1 0 0 0 0 ...\r\n##  $ Super Lig (Turkey)       : int [1:2472] 0 0 1 0 0 0 0 1 0 0 ...\r\n##  $ Premier League (England) : int [1:2472] 0 0 0 0 0 0 0 0 0 0 ...\r\n##  $ Super League (China)     : int [1:2472] 0 0 0 0 0 0 0 0 0 1 ...\r\n##  $ Major League Soccer (USA): int [1:2472] 0 0 0 0 1 0 1 0 0 0 ...\r\n##  $ Primeira Liga (Portugal) : int [1:2472] 0 0 0 0 0 0 0 0 0 0 ...\r\n##  $ Ligue 1 (France)         : int [1:2472] 0 0 0 1 0 0 0 0 0 0 ...\r\n##  $ Bundesliga 2 (Germany)   : int [1:2472] 0 0 0 0 0 0 0 0 0 0 ...\r\n##  $ Championship (England)   : int [1:2472] 0 0 0 0 0 0 0 0 0 0 ...\r\n##  $ Premier League (Russia)  : int [1:2472] 0 0 0 0 0 0 0 0 0 0 ...\r\n##  $ Superliga (Argentina)    : int [1:2472] 0 0 0 0 0 0 0 0 1 0 ...\r\n##  $ Eredivisie (Netherlands) : int [1:2472] 0 0 0 0 0 0 0 0 0 0 ...\r\n\r\n\r\n\r\nThe result is a set of coefficient estimates corresponding to each league. Notably, these are all positive (even if subtracting the intercept), and the Netherlands coefficient is NA due to multi-collinearity in the data. 6\r\n\r\n\r\n## # A tibble: 16 x 2\r\n##    League                    Estimate\r\n##    <chr>                        <dbl>\r\n##  1 Premier League (England)    0.975 \r\n##  2 La Liga (Spain)             0.869 \r\n##  3 Ligue 1 (France)            0.786 \r\n##  4 Serie A (Italy)             0.738 \r\n##  5 Serie A (Brazil)            0.724 \r\n##  6 Primeira Liga (Portugal)    0.675 \r\n##  7 Bundesliga 1 (Germany)      0.649 \r\n##  8 Championship (England)      0.641 \r\n##  9 Super Lig (Turkey)          0.617 \r\n## 10 Premier League (Russia)     0.485 \r\n## 11 Superliga (Argentina)       0.463 \r\n## 12 Super League (China)        0.360 \r\n## 13 Major League Soccer (USA)   0.281 \r\n## 14 Bundesliga 2 (Germany)      0.197 \r\n## 15 (Intercept)                 0.0747\r\n## 16 Eredivisie (Netherlands)   NA     \r\n\r\n\r\n\r\nFor hockey/basketball APM, we would say the coefficient estimate represents how much a player contributes relative to an ‚Äúaverage‚Äù player. We might be tempted to try to interpret these coefficients directly as well. Yes, we can infer the league ‚Äúpower rankings‚Äù from just this singular coefficient list (Premier League as the strongest and Bundesliga 2 as the weakest), but there are some issues.\r\nWe first need to ‚Äúun-transform‚Äù this back to the VAEP/90 scale. (See next step.)\r\nNote that this is not a zero-sum situation (even after un-transforming). There is no notion of a matchup between one league and another like there is in hockey/basketball with players on the ice/court. Instead, our data is more analogous to a player playing against themselves (not a set of players versus another set of players).\r\nEven if this were a zero-sum type of problem and the model returned some negative coefficient estimates, it‚Äôs unclear what the intercept (or 0) even means. Does it mean ‚Äúaverage‚Äù? If so, what is an ‚Äúaverage‚Äù league?\r\nWe accounted for minutes played‚Äîthe ‚Äúper 90‚Äù denominator‚Äîprior to subtracting rates (difference in VAEP/90), which is different than how APM works. In APM, the minutes played is directly accounted for in the response variable (net points, divided by possessions).\r\nThe take-away here is that we can only interpret the model coefficients on a relative basis.\r\n\r\n\r\n## # A tibble: 256 x 5\r\n##    `League A`               `League B`               `Estimate A` `Estimate B` Diff.\r\n##    <fct>                    <fct>                           <dbl>        <dbl> <dbl>\r\n##  1 Premier League (England) Premier League (England)        0.975        0.975 0    \r\n##  2 Premier League (England) La Liga (Spain)                 0.975        0.869 0.107\r\n##  3 Premier League (England) Ligue 1 (France)                0.975        0.786 0.189\r\n##  4 Premier League (England) Serie A (Italy)                 0.975        0.738 0.238\r\n##  5 Premier League (England) Serie A (Brazil)                0.975        0.724 0.252\r\n##  6 Premier League (England) Primeira Liga (Portugal)        0.975        0.675 0.300\r\n##  7 Premier League (England) Bundesliga 1 (Germany)          0.975        0.649 0.326\r\n##  8 Premier League (England) Championship (England)          0.975        0.641 0.334\r\n##  9 Premier League (England) Super Lig (Turkey)              0.975        0.617 0.358\r\n## 10 Premier League (England) Premier League (Russia)         0.975        0.485 0.491\r\n## # ... with 246 more rows\r\n\r\n\r\n\r\n‚ÄúUn-transform‚Äù the coefficients of the regression using a ‚Äúweighted-average‚Äù standard deviation and mean from the z-transformations of groups. 7\r\nInterpretation after this transformation can be a little tricky. The differences between a specified pair of these post-transformed coefficients represents the expected change in an ‚Äúaverage‚Äù player‚Äôs VAEP/90 (Diff. (VAEP/90)) when moving between the specified leagues.\r\n\r\n\r\n## # A tibble: 256 x 4\r\n##    `League A`               `League B`               Diff. `Diff. (VAEP/90)`\r\n##    <fct>                    <fct>                    <dbl>             <dbl>\r\n##  1 Premier League (England) Premier League (England) 0                0     \r\n##  2 Premier League (England) La Liga (Spain)          0.107            0.0166\r\n##  3 Premier League (England) Ligue 1 (France)         0.189            0.0295\r\n##  4 Premier League (England) Serie A (Italy)          0.238            0.0371\r\n##  5 Premier League (England) Serie A (Brazil)         0.252            0.0393\r\n##  6 Premier League (England) Primeira Liga (Portugal) 0.300            0.0469\r\n##  7 Premier League (England) Bundesliga 1 (Germany)   0.326            0.0509\r\n##  8 Premier League (England) Championship (England)   0.334            0.0521\r\n##  9 Premier League (England) Super Lig (Turkey)       0.358            0.0559\r\n## 10 Premier League (England) Premier League (Russia)  0.491            0.0766\r\n## # ... with 246 more rows\r\n\r\n\r\n\r\nTo interpret these differences as a percentage (so that we can ‚Äúscale‚Äù the properly for a player with a VAEP/90 of 1.5, compared to a player with a lower VAEP/90 rate), we use the median VAEP/90 across all leagues as a ‚Äúbaseline‚Äù. For example, for Bundesliga -> Premier League, since the overall median VAEP/90 is 0.305 and the Diff. (VAEP/90) between league A and league B is 0.0509, the % Difference is 0.0509/0.305 = 17%.\r\n\r\n\r\n## # A tibble: 256 x 5\r\n##    `League A`               `League B`               Diff. `Diff. (VAEP/90)` `% Diff.`\r\n##    <fct>                    <fct>                    <dbl>             <dbl> <chr>    \r\n##  1 Premier League (England) Premier League (England) 0                0      0%       \r\n##  2 Premier League (England) La Liga (Spain)          0.107            0.0166 5%       \r\n##  3 Premier League (England) Ligue 1 (France)         0.189            0.0295 10%      \r\n##  4 Premier League (England) Serie A (Italy)          0.238            0.0371 12%      \r\n##  5 Premier League (England) Serie A (Brazil)         0.252            0.0393 13%      \r\n##  6 Premier League (England) Primeira Liga (Portugal) 0.300            0.0469 15%      \r\n##  7 Premier League (England) Bundesliga 1 (Germany)   0.326            0.0509 17%      \r\n##  8 Premier League (England) Championship (England)   0.334            0.0521 17%      \r\n##  9 Premier League (England) Super Lig (Turkey)       0.358            0.0559 18%      \r\n## 10 Premier League (England) Premier League (Russia)  0.491            0.0766 25%      \r\n## # ... with 246 more rows\r\n\r\n\r\n\r\n\r\nImprovements & Further Work\r\nAlthough my approach does eventually get back to the original units (VAEP/90), it does feel a little convoluted. Aditya Kothari proposed re-defining the target variable in the regression to be the ratio of VAEP/minute (instead of a z-transformed difference in VAEP/90) between the leagues that a player moves to and from. (See his full post.) In my eyes, the main advantage of such an approach is that it is more direct. A player-level ratio embeds information about position and age‚Äîa forward will tend to have higher VAEP/minute than a defender, and will continue to have higher VAEP/minute than a defender after transferring‚Äîso normalizing for age and position is not necessarily justified. Additionally, the model‚Äôs league coefficients can be directly interpreted, unlike my approach. Perhaps the main disadvantage is sensitivity to low minutes played. 8\r\nAnother weakness in my approach is the assumption that relative league strengths are the same every year, which is most certainly not true. One could apply a decaying weight to past seasons to account for varying league strength.\r\nI would be hesitant to use my results to directly infer how a specific player will translate going from one league to another. My approach focuses on leagues and is more about the ‚Äúaverage‚Äù player. One aught to include additional features about play style (e.g.¬†touches, progressive passes, team role) if interested in predicting individual player performance with a high degree of accuracy.\r\nOne can swap out the response variable with other reasonable metrics of player performance, such as xG (which is more readily available than atomic VAEP). In fact, I did this myself and came up with the result below (showing in units of xG/90 instead of as a percentage, since most fans are accustomed to seeing xG and are used to its relative magnitude).\r\n\r\nOne could stay in the realm of just purely ‚Äúpower rankings‚Äù and focus more on the estimates and error. For example, in an earlier iteration of this methodology, I used a Bradley-Terry approach to come up with a distribution of estimates for each league.9 Here, the x-axis could be loosely interpreted as the log odds of one league winning in a match versus another league, although it‚Äôs not clear exactly what that means. (An ‚Äúaverage‚Äù team from both leagues? A matchup of teams composed of ‚Äúaverage‚Äù players from any team in each league?)\r\n\r\nNotably, I‚Äôm not using match results at all! Certainly a model could learn something from international and tournament matches. However, using match-level data would require a whole new approach. Also, most would agree that tournament data can be biased by atypical lineups. For example, a manager on one side may opt to rest their best players, saving them for domestic league games, while the other manager may play their side at full strength.10\r\nSample size is an issue on two levels: (1) the number of transfers (more data would be better) and (2) minutes played.\r\nRegarding (1), one could expand the data set by including all seasons played by a player that has played in more than one league, taking all combinations of seasons in different leagues (i.e.¬†relaxing the the same-season or subsequent-season criteria). I actually did attempt this and found that overall the results were somewhat similar, but there were more questionable results overall. (Brazil‚Äôs Serie A was found to be the second strongest league overall with this approach.).\r\nRegarding (2), one has to make a choice to drop players with low minutes played to prevent outliers affecting the results of the model. However, in some cases, a loaned player coming in at the end of the season and making a huge impact can tell us a lot about the difference in strength of two leagues, so we may not want to drop some of the records after all. An empirical bayes adjustment to VAEP/90, not unlike the one described here by David Robinson, can help overcome this. Below shows how such an adjustment slightly ‚Äúshrinks‚Äù VAEP/90 numbers, especially for those who played less.\r\n\r\nOn the topic of ‚Äúshrinking‚Äù, we could have used ridge regression (regression with some penalty) to get more robust league estimates overall. However, there is a downside to ridge regression‚Äîwe give up some level of interpretability.11 Nonetheless, the relative ranking of leagues would be more reliable with ridge regression.\r\nAncillary Take-away\r\nOne final thing I‚Äôd like to point out here: I think this whole approach really showcases the inference made possible by player stats (xG, possession value metrics like atomic VAEP, etc.) aggregated over long periods of time. While such stats are often used to evaluate player performance in single games or even for singular in-game actions, they are most effective in providing insight when employed in higher-level analyses.\r\n\r\nValuing Actions by Estimating Probabilities (VAEP) based on the atomic SPADL format.‚Ü©Ô∏é\r\nI had a Twitter thread in May 2021 describing how one could use VAEP ratings.‚Ü©Ô∏é\r\nIt‚Äôs closer to xG+xA, although the authors might disagree with that as well. It‚Äôs really best treated separately, which perhaps explains why the authors often using ‚Äúrating‚Äù and ‚Äúcontribution‚Äù when referring to VAEP.‚Ü©Ô∏é\r\nEach row represents one player. Each row only has one +1 and one -1, and 0s for other features.‚Ü©Ô∏é\r\nWe‚Äôre including all positions and ages in this regression, even though these groupings have varying standard deviations for transformation of the response variable. (All have 0 mean, as one might expect with a feature representing the difference between values with the same distribution.)‚Ü©Ô∏é\r\nThis NA occurs even when setting the intercept to 0, which is typically the way to get around this kind of issue with lm in R. When changing the order of columns in the regression and forcing the Netherlands coefficient to be non-NA, its estimate is lower than that of Bundesliga 2 (and a different league‚Äôs estimate is NA).‚Ü©Ô∏é\r\n‚ÄúWeighted-average‚Äù: Diff. (VAEP/90) = (Diff. * sum(SD * (N * sum(N)))) - sum(Mean * (N * sum(N)))‚Ü©Ô∏é\r\nThis is also a weakness of my approach, but arguably ratios exacerbate this.‚Ü©Ô∏é\r\nHere I was treating the Champions League and Europa as their own ‚Äúleagues‚Äù, purely out of curiosity.‚Ü©Ô∏é\r\nArguably you‚Äôll have this kind of issue no matter what, due to injuries.‚Ü©Ô∏é\r\n‚ÄúUn-transformation‚Äù becomes more difficult since we have to account for the ridge penalty.‚Ü©Ô∏é\r\n",
    "preview": "post/soccer-league-strength/featured.png",
    "last_modified": "2021-07-18T20:36:22-05:00",
    "input_file": {},
    "preview_width": 4800,
    "preview_height": 2400
  },
  {
    "path": "post/fantasy-football-schedule-problem/",
    "title": "Fantasy Football and the Classical Scheduling Problem",
    "description": "Brute Force Programming Go Brrr",
    "author": [
      {
        "name": "Tony ElHabr",
        "url": "https://twitter.com/TonyElHabr"
      }
    ],
    "date": "2021-01-11",
    "categories": [
      "r",
      "python",
      "football (american)"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nProblem\r\nConstraint Programming\r\nAlternative: Exhaustive Search\r\nApplication\r\nTake-away\r\n\r\nIntroduction\r\nEvery year I play in several fantasy football (American) leagues. For those who are unaware, it‚Äôs a game that occurs every year in sync with the National Football League (NFL) where participants play in weekly head-to-head games as general managers of virtual football teams. (Yes, it‚Äôs very silly.) The winner at the end of the season is often not the player with the team that scores the most points; often a fortunate sequence of matchups dictates who comes out on top.\r\nI didn‚Äôt fare so well this year in one of my leagues, but my disappointing placement was not due to my team struggling to score points; rather, I was extremely unlucky. I finished the season in 7th place despite scoring the most points!\r\nThis inspired me to quantify just how unlikely I was. The most common way to calculate the likelihood of a given team‚Äôs ranking in a league with is with a Monte Carlo simulation based on some parameterized model of scoring to generate probabilities for the final standings. FiveThirtyEight uses such a model for their soccer models, for example. For a setting in which team scores are independent of one another, such as fantasy football, another approach is to simply calculate what each team‚Äôs record would be if they had played every other team each week. (So, if your league has 10 teams and each plays each other once, each team would have a hypothetical count of 90 games played.) However, I was particularly interested in answering the question: ‚ÄúIn how many different schedules would I have finished where I did?‚Äù\r\nProblem\r\nFiguring out how unlucky I was to finish 7th requires me to first figure out how many possible schedules there are. Formally, the problem can be put as follows1:\r\n\r\nLet \\(T={t_1, ., t_n}\\) be a set of an even \\(n\\) teams. Let \\(R\\) denote a round consisting of a set of pairs \\((t_i, t_j)\\) (denoting a match), such that \\(0 < i <j ??? n\\), and such that each team in \\(T\\) is participates exactly once in \\(R\\). Let \\(S\\) be a schedule consisting of a tuple of \\(n???1\\) valid rounds \\((R_1, ., R_{n???1})\\), such that all rounds in \\(S\\) are pair-wise disjoint (no round shares a match). How many valid constructions of \\(S\\) are there for \\(n\\) input teams?\r\n\r\nFor a small number of teams, it‚Äôs fairly simple to write out all possible combinations of matchups. For example, for a two-team league (where each team plays each other once), there is only one possible schedule (solution)‚ÄîTeam 1 vs.¬†Team 2. For a four-team league, there are six possible schedules. Two are shown below.\r\n\r\nsolution\r\nround\r\nteam1\r\nteam2\r\n1\r\n1\r\n1\r\n2\r\n\r\n\r\n3\r\n4\r\n\r\n2\r\n1\r\n3\r\n\r\n\r\n2\r\n4\r\n\r\n3\r\n1\r\n4\r\n\r\n\r\n2\r\n3\r\n2\r\n1\r\n1\r\n3\r\n\r\n\r\n2\r\n4\r\n\r\n2\r\n1\r\n2\r\n\r\n\r\n3\r\n4\r\n\r\n3\r\n1\r\n4\r\n\r\n\r\n2\r\n3\r\n\r\nNote that there is no concept of ‚Äúhome advantage‚Äù in fantasy football, so the order of teams in a given matchup does not matter. Also, note that if our restriction (‚Äúconstraint‚Äù) that each team must play each other once and only once, implies that the number of teams has to be an even number.\r\nConstraint Programming\r\nTo truly answer this question, we can turn to constraint programming. If you‚Äôre familiar with constraint programming, then you‚Äôll notice that this set-up is similar to the canonical nurse scheduling problem and is a specific form of the tournament problem.\r\nBelow is some python code that is able to identify the number feasible solutions for four teams. I print out the first solution for illustrative purposes.\r\n\r\nfrom ortools.sat.python import cp_model\r\n\r\nclass SolutionPrinter(cp_model.CpSolverSolutionCallback):\r\n    def __init__(self, games, n_team, n_show=None):\r\n        cp_model.CpSolverSolutionCallback.__init__(self)\r\n        self._games = games\r\n        self._n_show = n_show\r\n        self._n_team = n_team\r\n        self._n_sol = 0\r\n\r\n    def on_solution_callback(self):\r\n        self._n_sol += 1\r\n        print()\r\n        if self._n_show is None or self._n_sol <= self._n_show:\r\n            print(f'Solution {self._n_sol}.')\r\n            for team1 in range(self._n_team):\r\n                for team2 in range(self._n_team):\r\n                    if team1 != team2:\r\n                        print(\r\n                            f'Team {team1 + 1} vs. Team {team2 + 1} in Round {self.Value(self._games[(team1, team2)])}'\r\n                        )\r\n        else:\r\n            print(f'Found solution {self._n_sol}.')\r\n\r\n    def get_n_sol(self):\r\n        return self._n_sol\r\n\r\nn_team = 4\r\nn_w = n_team - 1\r\nmodel = cp_model.CpModel()\r\ngames = {}\r\nfor team1 in range(n_team):\r\n    for team2 in range(n_team):\r\n        if team1 != team2:\r\n            games[(team1, team2)] = model.NewIntVar(1, n_w, f'{team1:02}_{team2:02}')\r\n\r\nfor team1 in range(n_team):\r\n    for team2 in range(n_team):\r\n        if team1 != team2:\r\n            model.Add(games[(team1, team2)] == games[(team2, team1)])\r\n\r\n\r\n# Each team can only play in 1 game each week\r\nfor t in range(n_team):\r\n    model.AddAllDifferent(\r\n        [games[(t, team2)] for team2 in range(n_team) if t != team2]\r\n    )\r\n\r\nsolver = cp_model.CpSolver()\r\nsolution_printer = SolutionPrinter(games, n_team=n_team, n_show=2)\r\nstatus = solver.SearchForAllSolutions(model, solution_printer)\r\n\r\nprint()\r\nprint(f'Solve status: {solver.StatusName(status)}')\r\nprint(f'Solutions found: {solution_printer.get_n_sol()}')\r\n\r\n\r\n# Solution 1.\r\n# Team 1 vs. Team 2 in Round 3\r\n# Team 1 vs. Team 3 in Round 2\r\n# Team 1 vs. Team 4 in Round 1\r\n# Team 2 vs. Team 1 in Round 3\r\n# Team 2 vs. Team 3 in Round 1\r\n# Team 2 vs. Team 4 in Round 2\r\n# Team 3 vs. Team 1 in Round 2\r\n# Team 3 vs. Team 2 in Round 1\r\n# Team 3 vs. Team 4 in Round 3\r\n# Team 4 vs. Team 1 in Round 1\r\n# Team 4 vs. Team 2 in Round 2\r\n# Team 4 vs. Team 3 in Round 3\r\n# \r\n# Found solution 2.\r\n# \r\n# Found solution 3.\r\n# \r\n# Found solution 4.\r\n# \r\n# Found solution 5.\r\n# \r\n# Found solution 6.\r\n# \r\n# Solve status: OPTIMAL\r\n# Solutions found: 6\r\n\r\nEasy enough to run for 10 teams and get an answer, right? WRONG. Turns out this the number of feasible solutions (schedules) starts to blow up really quickly. In fact, I believe the number of solutions for this particular problem is only known up to 14 teams. (I‚Äôve intentionally left the numbers un-rounded to emphasize just how much the number of solutions increases as a function of the number of teams.)\r\n\r\nn\r\nsolutions\r\n2\r\n1\r\n4\r\n6\r\n6\r\n720\r\n8\r\n31,449,600\r\n10\r\n444,733,651,353,600\r\n12\r\n10,070,314,878,246,925,803,220,024\r\n14\r\n614,972,203,951,464,579,840,082,248,206,026,604,282\r\n\r\nUnless you happen to be an expert in graph theory and combinatorics, you probably wouldn‚Äôt be able to figure this out by hand; for us non-experts out there, we can refer to a known sequence of 1-factorizations of a complete graph \\(K_{2n}\\) and use our brain to figure out permutations in a given round. (Don‚Äôt worry if that makes no sense.)\r\nWhy do I bring this up? Well, I realized that generating all possible schedules for a 10-team league (such as my aforementioned league) is just not reasonable for anyone without a supercomputer and a lot of time. I enhanced the above python code a bit and tried it out for a 10-team league and was only able to generate a couple of million solutions after 3 hours.\r\nAlternative: Exhaustive Search\r\nThe failure to generate all solutions made me reconsider things a bit. If I can‚Äôt reasonably ‚Äúhave it all‚Äù, I should simplify things a bit. By ‚Äúsimplify‚Äù, I mean perform an ‚Äúexhaustive‚Äù (or \"brute-force) search that stops after a specified number of solutions. And, by re-writing things in R, I can eliminate dependencies on Google‚Äôs ortools package and python. (Both are great, but, nonetheless, they are potential obstacles for R users.)\r\nWriting a script to perform an exhaustive search is not so easy itself, and, in this case, requires a completely different approach to the problem. My steps are as follows:\r\nSet up an \\(n\\) x \\(n-1\\) matrix, where the \\(n\\) rows designate teams and the \\(n-1\\) columns designate rounds.\r\n\r\n\r\nleague_size = 4\r\nrounds <- league_size - 1\r\nmat <- matrix(nrow = league_size, ncol = rounds)\r\nmat\r\n\r\n\r\n\r\n\r\n\r\n#      [,1] [,2] [,3]\r\n# [1,]   NA   NA   NA\r\n# [2,]   NA   NA   NA\r\n# [3,]   NA   NA   NA\r\n# [4,]   NA   NA   NA\r\n\r\n\r\n\r\nRandomly select the opponent of team 1 in round 1.\r\n\r\n\r\nteam_i <- 1\r\nround_i <- 1\r\nretry_i <- 1\r\nidx_team <- 1:league_size\r\nset.seed(1)\r\n\r\nteam_1_round_1 <- sample(2:league_size, 1, replace = FALSE)\r\nmat[team_i, round_i] <- team_1_round_1\r\nmat\r\n\r\n\r\n\r\n\r\n\r\n#      [,1] [,2] [,3]\r\n# [1,]    2   NA   NA\r\n# [2,]   NA   NA   NA\r\n# [3,]   NA   NA   NA\r\n# [4,]   NA   NA   NA\r\n\r\n\r\n\r\nFind a unique set of opponents for teams 2 through \\(n\\) to fill the rest of the cells in column 1.\r\n\r\n\r\nwhile(team_i <= league_size) {\r\n  if(team_i %in% teams_already_matched) {\r\n    team_i_round_i <- which(team_i == teams_already_matched)\r\n    mat[team_i, round_i] <- team_i_round_i\r\n    team_i <- team_i + 1\r\n  } else {\r\n    teams_cant_match <- unique(c(teams_already_indexed, teams_already_matched))\r\n    teams_unmatched <- setdiff(teams_possible, teams_cant_match)\r\n    n_matched <- length(teams_unmatched)\r\n    if(n_matched == 0) {\r\n      mat[2:league_size, round_i] <- NA\r\n      team_i <- 2\r\n    } else {\r\n      team_i_round_i <- if(n_matched == 1) {\r\n        teams_unmatched\r\n      } else {\r\n        sample(teams_unmatched, 1)\r\n      }\r\n\r\n      mat[team_i, round_i] <- team_i_round_i\r\n      team_i <- team_i + 1\r\n    }\r\n  }\r\n}\r\n\r\n\r\n\r\n\r\n\r\n#      [,1] [,2] [,3]\r\n# [1,]    2   NA   NA\r\n# [2,]    1   NA   NA\r\n# [3,]    4   NA   NA\r\n# [4,]    3   NA   NA\r\n\r\n\r\n\r\nIdentify a unique set of opponents for team 1 for all other rounds (rounds 2 through \\(n-1\\)).\r\n\r\n\r\nteams_possible <- setdiff(idx_team, c(1, team_1_round_1))\r\nteam1_all_rounds <- sample(teams_possible, size = length(teams_possible))\r\nmat[1, 2:rounds] <- team1_all_rounds\r\nmat\r\n\r\n\r\n\r\n\r\n\r\n#      [,1] [,2] [,3]\r\n# [1,]    2    3    4\r\n# [2,]    1   NA   NA\r\n# [3,]    4   NA   NA\r\n# [4,]    3   NA   NA\r\n\r\n\r\n\r\nRepeat step 3 for rounds 2 through \\(n-2\\) (penultimate round).\r\n\r\n\r\nwhile(round_i < rounds) {\r\n  team_i <- 2\r\n  while(team_i <= league_size) {\r\n    teams_possible <- setdiff(idx_team, team_i)\r\n    teams_already_indexed <- 1:(team_i - 1)\r\n    teams_already_matched <- mat[teams_already_indexed, round_i]\r\n    teams_already_played <- mat[team_i, 1:(round_i - 1)]\r\n    reset <- FALSE\r\n    if(team_i %in% teams_already_matched) {\r\n      team_i_round_i <- which(team_i == teams_already_matched)\r\n      if(any(team_i_round_i == teams_already_played)) {\r\n        reset <- TRUE\r\n      }\r\n    } else {\r\n      teams_cant_match <-\r\n        unique(c(teams_already_indexed, teams_already_matched, teams_already_played))\r\n      teams_unmatched <- setdiff(teams_possible, teams_cant_match)\r\n      n_matched <- length(teams_unmatched)\r\n      if (n_matched == 0) {\r\n        reset <- TRUE\r\n      } else {\r\n        team_i_round_i <- if(n_matched == 1) {\r\n          teams_unmatched\r\n        } else {\r\n          sample(teams_unmatched, 1)\r\n        }\r\n      }\r\n    }\r\n    \r\n    if(reset) {\r\n      mat[2:league_size, round_i] <- NA\r\n      team_i <- 2\r\n      retry_i <- retry_i + 1\r\n    } else {\r\n      mat[team_i, round_i] <- team_i_round_i\r\n      team_i <- team_i + 1\r\n    }\r\n  }\r\n  round_i <- round_i + 1\r\n}\r\nmat\r\n\r\n\r\n\r\n\r\n\r\n#      [,1] [,2] [,3]\r\n# [1,]    2    3    4\r\n# [2,]    1    4   NA\r\n# [3,]    4    1   NA\r\n# [4,]    3    2   NA\r\n\r\n\r\n\r\nIdentify the only valid set of matchups for the last round \\(n-1\\).\r\n\r\n\r\nidx_not1 <- 2:league_size\r\ntotal <- Reduce(sum, idx_team) - idx_not1\r\nrs <- rowSums(mat[idx_not1, 1:(rounds - 1)])\r\nteams_last <- total - rs\r\nmat[idx_not1, rounds] <- teams_last\r\nmat\r\n\r\n\r\n\r\n\r\n\r\n#      [,1] [,2] [,3]\r\n# [1,]    2    3    4\r\n# [2,]    1    4    3\r\n# [3,]    4    1    2\r\n# [4,]    3    2    1\r\n\r\n\r\n\r\nThat is the core of the solution. The rest of the work2 involves repeating the steps for however many times you want, always checking for duplicates of previous solutions, i.e.¬†sampling without replacement. (Or, if you don‚Äôt care about schedules being unique, i.e.¬†sampling with replacement, it‚Äôs even easier.)\r\nApplication\r\nSince generating unique schedules is something I‚Äôd like to be able to do every year for my fantasy football leagues, I wrote a package for it, called {ffsched}. The package includes functionality to retrieve your league‚Äôs fantasy scores from ESPN, which you can combine with the simulated schedules to generate a plot such as the following.\r\n\r\nIt‚Äôs immediately evident how un-lucky I (‚ÄúTony El Tigre‚Äù) was. In the 100,000 simulations, I never finished below 7th, and I only finished 7th 1.1% of the time!\r\nIn the previous year I scored the most points and finished first. ‚ÄúThe Juggernaut‚Äù got the short end of the stick in 2019, finishing 7th. He only finished 7th or lower in 6.6% of schedules.\r\n\r\nTake-away\r\nAn exhaustive search as a work-around for true constraint programming isn‚Äôt always elegant and can be difficult to implement, but if you‚Äôre motivated enough to do it‚Äîas I was to prove my extreme lack of fortune‚Äîit can generate what you need to make a compelling point. My use case (for generating unique fantasy generating football schedules) is inconsequential, but such techniques are often immensely important in real world contexts.\r\n\r\nThis is almost directly taken from https://math.stackexchange.com/questions/284416/how-many-possible-arrangements-for-a-round-robin-tournament.‚Ü©Ô∏é\r\nIn fantasy football, teams often play each other more than once in a year (depending on your league size), so I‚Äôve somewhat simplified the problem for the purpose of this post. More work could be done to figure out the number of possibilities when more than one game has to be scheduled for each pair of teams.‚Ü©Ô∏é\r\n",
    "preview": "post/fantasy-football-schedule-problem/viz_standings_tile_2020.png",
    "last_modified": "2021-07-01T22:13:54-05:00",
    "input_file": {},
    "preview_width": 3600,
    "preview_height": 2400
  },
  {
    "path": "post/decomposition-smoothing-soccer/",
    "title": "Decomposition and Smoothing",
    "description": "With data.table, reticulate, and spatstat",
    "author": [
      {
        "name": "Tony ElHabr",
        "url": "https://twitter.com/TonyElHabr"
      }
    ],
    "date": "2020-10-14",
    "categories": [
      "r",
      "soccer"
    ],
    "contents": "\r\n\r\nContents\r\nData\r\nNon-Equi Joining with {data.table}\r\nNon-Negative Matrix Factorization (NNMF) with {reticulate} and sklearn\r\nGaussian Smoothing with {spatstat}\r\nConclusion\r\n\r\nWhile reading up on modern soccer analytics (I‚Äôve had an itch for soccer and tracking data recently, I stumbled upon an excellent set of tutorials written by Devin Pleuler. In particular, his notebook on non-negative matrix factorization (NNMF) caught my eye. I hadn‚Äôt really heard of the concept before, but it turned out to be much less daunting once I realized that it is just another type of matrix decomposition. Singular value decomposition (SVD), which I‚Äôm much more familiar with, belongs to the same family of calculations (although NNMF and SVD are quite different). In an effort to really gain a better understanding of NNMF, I set out to emulate his notebook.\r\nIn the process of converting his python code to R, I encountered three challenges with resolutions worth documenting.\r\nBefore the NNMF calculation, I needed to perform non-equi join with a fairly size-able data set. Unfortunately, {dplyr}1 does not have built-in support for such a join. I tossed aside any kind of personal implicit bias against {data.table}‚Äîwhich is certainly the go-to option in the R ecosystem for non-equi joins‚Äîand used it for this process.\r\nFor the NNMF calculation, the only R implementation (that I could find) comes with the {NMF} package2, which requires the installation of the Bioconductor-exclusive {BiocManager} package. I‚Äôm relatively unfamiliar with Bioconductor, so this was not very appealing (although I did end up downloading {NMF} and trying it out). Instead, I ended up using {reticulate} to call the skearn.decomposition.NMF() function directly (as is done in the python code). This is a perfect example of using {reticulate} for a non-trivial reason (i.e.¬†for an algorithm).\r\nAfter the NNMF computation, I needed to perform 2-D Gaussian smoothing, which is helpful for making the output of the NNMF output more interpretable. The {spatstat} package had just the function for the job (spatstat::blur()), and it all it took for me was to write some a tidy wrapper function to integrate it nicely into my workflow.\r\nI‚Äôve always considered myself a ‚Äúwhatever gets the job done‚Äù kind of person, not insistent on ignoring solutions that use ‚Äúbase‚Äù R, {data.table}, python, etc. Nonetheless, replicating Devin‚Äôs notebook really underscored the importance of being comfortable outside of a {tidyverse}-centric workflow.\r\nAnyways, this post outlines the code and my thought process in porting Devin‚Äôs code to R. I‚Äôll skip some of the details, emphasizing the things that are most interesting.\r\nData\r\nWe‚Äôll be working with the open-sourced StatsBomb data for the 2018 Men‚Äôs World Cup, which I‚Äôve called events below. 3\r\nThis is a relatively large data set with lots of columns (and rows). However, we only need three columns for what we‚Äôre going to do: (1) a unique identifier for each player, player_id, along with their (2) x and (3) y coordinates.\r\n\r\n\r\nlibrary(tidyverse)\r\n\r\n\r\n\r\nA quick summary of the data shows that there are 603 unique players, and that the x and y coordinates range from 1 to 120 (yards) and 1 to 80 respectively.\r\n\r\n\r\nevents %>% \r\n  summarize(\r\n    n = n(),\r\n    n_player = n_distinct(player_id),\r\n    across(c(x, y), list(min = min, max = max, mean = mean))\r\n  )\r\n\r\n\r\n\r\n\r\n\r\n## # A tibble: 1 x 8\r\n##       n n_player x_min x_max x_mean y_min y_max y_mean\r\n##   <int>    <int> <dbl> <dbl>  <dbl> <dbl> <dbl>  <dbl>\r\n## 1 224018      603     1   120  60.05     1    80  40.37\r\n\r\n\r\n\r\nNon-Equi Joining with {data.table}\r\nOur first challenge is to convert the following chunk of python.\r\n\r\nimport numpy as np\r\n\r\nx_scale, y_scale = 30, 20\r\n\r\nx_bins = np.linspace(0, 120, x_scale)\r\ny_bins = np.linspace(0, 80, y_scale)\r\n\r\nplayers = {}\r\n\r\nfor e in events:\r\n    if 'player' in e.keys():\r\n        player_id = e['player']['id']\r\n        if player_id not in players.keys():\r\n            players[player_id] = np.zeros((x_scale, y_scale))\r\n        try:\r\n            x_bin = int(np.digitize(e['location'][0], x_bins[1:], right=True))\r\n            y_bin = int(np.digitize(e['location'][1], y_bins[1:], right=True))\r\n            players[player_id][x_bin][y_bin] += 1\r\n        except:\r\n            pass\r\n\r\nThis code creates a nested dict, where the keys are player id‚Äôs and the values are 20x30 matrices. Each element in the matrix is an integer that represents the count of times that the player was recorded being at a certain position on the pitch. (These counts range from 0 to 94 for this data set.)\r\nSome technical details:\r\nThe python events is actually a pretty heavily nested list4, hence the non-rectangular operations such as e['player']['id'].\r\nObservations with missing coordinates are ignored with the try-except block.\r\nx and y values (elements of the 'location' sub-list) are mapped to ‚Äúbins‚Äù using numpy‚Äôs digitize() function, which is analogous to base::cut().\r\nHow can we do this same data manipulation in an idiomatic R fashion? We could certainly create a named list element and use base::cut() to closely match the python approach. However, I prefer to stick with data frames and SQL-ish operations since I think these are much more ‚Äúnatural‚Äù for R users.5\r\nSo, going forward with data frames and joins, it‚Äôs quickly apparent that we‚Äôll have to do some non-equi joining. {fuzzyjoin} and {sqldf} offer functionality for such an approach, but {data.table} is really the best option. The only minor inconvenience here is that we have to explicitly coerce our events data frame to a data.table.\r\nWe‚Äôll also need a helper, grid-like data frame to assist with the binning. The 600-row grid_xy_yards data frame (30 x bins * 20 y bins) below is essentially a tidy definition of the cells of the grid upon which we are binning the events data. (One can use whatever flavor of crossing(), expand.grid(), seq(), etc. that you prefer to create a data frame like this.)\r\nVisually, this grid looks like this.\r\n\r\nAnd if you prefer numbers instead of a chart, see the first 10 rows below.\r\n\r\n\r\ngrid_xy_yards\r\n\r\n\r\n\r\n\r\n\r\n## # A tibble: 600 x 5\r\n##     idx     x      y next_y next_x\r\n##   <int> <dbl>  <dbl>  <dbl>  <dbl>\r\n## 1     1     0  0      4.211  4.138\r\n## 2     2     0  4.211  8.421  4.138\r\n## 3     3     0  8.421 12.63   4.138\r\n## 4     4     0 12.63  16.84   4.138\r\n## 5     5     0 16.84  21.05   4.138\r\n## 6     6     0 21.05  25.26   4.138\r\n## 7     7     0 25.26  29.47   4.138\r\n## 8     8     0 29.47  33.68   4.138\r\n## 9     9     0 33.68  37.89   4.138\r\n## 10    10    0 37.89  42.11   4.138\r\n## # ... with 590 more rows\r\n\r\n\r\n\r\nTwo things to note about this supplementary data frame:\r\nCells aren‚Äôt evenly spaced integers, i.e.¬†x cells are defined at 0, 4.138, 8.276, ‚Ä¶, 80 instead of something like 0, 4, 8, ‚Ä¶, 80, and y cells are defined at 0, 4.211, 8.421, ‚Ä¶, 120 instead of something like 0, 4, 8, ‚Ä¶, 120). That‚Äôs simply due to using 30 and 20 instead of 31 and 21 to split up the x and y ranges respectively. I point this out because this SQL-ish approach would have been much easier if these numbers were just integers! We could have done an inner join on an integer grid instead of non-equi-joining upon a grid of floating point numbers. Unfortunately, joining on floating point numbers as keys leads to inconsistent results, simply due to the nature of floating points.6\r\nThe index idx is important! This will come back into play when we do the NNMF procedure, at which point we‚Äôll ‚Äúflatten‚Äù out our x-y pairs into a 1-d format.\r\nOk, on to the actual data joining.\r\n\r\n\r\nevents_dt <- events %>% drop_na() %>% data.table::as.data.table()\r\ngrid_xy_yards_dt <- grid_xy_yards %>% data.table::as.data.table()\r\n\r\n# We don't even have to load `{data.table}` for this to work!\r\nevents_binned <-\r\n  events_dt[grid_xy_yards_dt, on=.(x > x, x <= next_x, y >= y, y < next_y)] %>% \r\n  as_tibble() %>% \r\n  select(player_id, idx, x, y)\r\nevents_binned\r\n\r\n\r\n\r\n\r\n\r\n## # A tibble: 224,038 x 4\r\n##    player_id   idx     x     y\r\n##        <int> <int> <dbl> <dbl>\r\n##  1      5462     1     0     0\r\n##  2      5467     1     0     0\r\n##  3      5488     1     0     0\r\n##  4      3632     1     0     0\r\n##  5      5576     1     0     0\r\n##  6      5595     1     0     0\r\n##  7      5263     1     0     0\r\n##  8      4063     1     0     0\r\n##  9      5231     1     0     0\r\n## 10      5231     1     0     0\r\n## # ... with 224,028 more rows\r\n\r\n\r\n\r\nIn retrospect, this join was pretty straightforward!\r\nThe rest of the code below is just doing the actual tallying.\r\nFirst, we make an intermediate data set grid_players, which is the Cartesian product of all possible cells in the grid and all players in events.\r\nSecond, we ‚Äúadd back‚Äù missing cells to events_binned using the intermediate data set grid_players.\r\nIn the end, we end up with a players data frame with 603 player_ids * 30 x bins * 20 y bins = 361,800 rows.\r\n\r\n\r\n# This `dummy` column approach is an easy way to do a Cartesian join when the two data frames don't share any column names.\r\ngrid_players <-\r\n  grid_xy_yards %>% \r\n  mutate(dummy = 0L) %>% \r\n  # Cartesian join of all possible cells in the grid and all players in `events`.\r\n  full_join(\r\n    events %>% \r\n      drop_na() %>% \r\n      distinct(player_id) %>% \r\n      mutate(dummy = 0L),\r\n    by = 'dummy'\r\n  )\r\n\r\nplayers <-\r\n  events_binned %>% \r\n  group_by(player_id, x, y, idx) %>% \r\n  summarize(n = n()) %>% \r\n  ungroup() %>% \r\n  # Rejoin back on the grid to 'add back' cells with empty counts (i.e. `n = 0`).\r\n  full_join(grid_players, by = c('player_id', 'x', 'y', 'idx')) %>% \r\n  select(-dummy, -next_x, -next_y) %>% \r\n  replace_na(list(n = 0L)) %>% \r\n  arrange(player_id, x, y)\r\nplayers\r\n\r\n\r\n\r\nTo make this a little bit more tangible, let‚Äôs plot Messi‚Äôs heatmap. (Is this really a blog post about soccer if it doesn‚Äôt mention Messi üòÜ?)\r\n\r\nNon-Negative Matrix Factorization (NNMF) with {reticulate} and sklearn\r\nNext up is the actual NNMF calculation. I don‚Äôt care if you‚Äôre the biggest R stan in the world‚Äîyou have to admit that the python code to perform the NNMF is quite simple and (dare I say) elegant. The comps=30 here means\r\n\r\nfrom sklearn.decomposition import NMF\r\n\r\n# Flatten individual player matrices into shape=(600,) which is the product of the original shape components (30 by 20)\r\nunraveled = [np.matrix.flatten(v) for k, v in players.items()]\r\ncomps = 30\r\nmodel = NMF(n_components=comps, init='random', random_state=0)\r\nW = model.fit_transform(unraveled)\r\n\r\nMy understanding is that comps=30 is telling the algorithm to reduce our original data (with 603 players) to a lower dimensional space with 30 player ‚Äúarchetypes‚Äù that best represent the commonalities among the 603 players.7 Per Devin, the choice of 30 here is somewhat arbitrary. In practice, one might perform some cross validation to identify what number minimizes some loss function, but that‚Äôs beyond the scope of what we‚Äôre doing here.\r\nAfter re-formatting our players data into a wide format‚Äîequivalent to the numpy.matrix.flatten() call in the python code‚Äîwe could use the {NMF} package for an R replication.\r\n\r\n\r\n# Convert from tidy format to wide format (603 rows x 600 columns)\r\nplayers_mat <-\r\n  players %>% \r\n  drop_na() %>% \r\n  select(player_id, idx, n) %>% \r\n  pivot_wider(names_from = idx, values_from = n) %>% \r\n  select(-player_id) %>% \r\n  as.matrix()\r\n\r\ncomps <- 30L\r\nW <- NMF::nmf(NMF::rmatrix(players_mat), rank = comps, seed = 0, method = 'Frobenius')\r\n\r\n\r\n\r\nHowever, I found that the results weren‚Äôt all that comparable to the python results. (Perhaps I needed to define the arguments in a different manner.) So why not use {reticulate} and call the sklearn.decomposition.NMF() function to make sure that we exactly emulate the python decomposition?\r\n\r\n\r\nsklearn <- reticulate::import('sklearn')\r\n# Won't work if `n_components` aren't explicitly defined as integers!\r\nmodel <- sklearn$decomposition$NMF(n_components = comps, init = 'random', random_state = 0L)\r\nW <- model$fit_transform(players_mat)\r\n\r\n\r\n\r\nThe result includes 30 20x30 matrices‚Äîone 30x20 x-y matrix for each of the 30 components (comps). We have some wrangling left to do to gain anything meaningful from this NNMF procedure, but we have something to work with!\r\nGaussian Smoothing with {spatstat}\r\nThe last thing to do is to post-process the NNMF results and, of course, make pretty plots. The python plotting is pretty standard matplotlib, with the exception of the Gaussian smoothing performed on each component‚Äôs matrix model.component_ in the loop to make sub-plots.\r\n\r\nfrom scipy.ndimage import gaussian_filter\r\n\r\nfor i in range(9):\r\n    # ... Excerpted\r\n    z = np.rot90(gaussian_filter(model.components_[i].reshape(x_scale, y_scale), sigma=1.5), 1)\r\n    # ... Excerpted\r\n\r\nThe first 9 smoothed component matrices come out looking like this. 8\r\n\r\nThere‚Äôs a couple of steps involved to do the same thing in R.\r\nFirst, we‚Äôll convert the components matrices to a tidy format, decomp_tidy\r\nSecond, we‚Äôll join our tidied components matrices with our tidy grid of cells, grid_xy_yards, and convert our x and y bins to integers in preparation of the matrix operation performed in the subsequent step.\r\nLastly, we‚Äôll perform the Gaussian smoothing on nested data frames with a custom function, smoothen_dimension, that wraps spatstat::blur(). This function also maps idx back to field positions (in meters instead of yards) using the supplementary grid_xy_rev_m9 data frame (which is a lot like grid_xy_yards)\r\n\r\n\r\n## 1\r\ndecomp_tidy <-\r\n  model$components_ %>% \r\n  as_tibble() %>% \r\n  # \"Un-tidy\" tibble with 30 rows (one for each dimension) and 600 columns (one for every `idx`)\r\n  mutate(dimension = row_number()) %>% \r\n  # Convert to a tidy tibble with dimensions * x * y rows (30 * 30 * 20 = 18,000)\r\n  pivot_longer(-dimension, names_to = 'idx', values_to = 'value') %>% \r\n  # The columns from the matrix are named `V1`, `V2`, ... `V600` by default, so convert them to an integer that can be joined on.\r\n  mutate(across(idx, ~str_remove(.x, '^V') %>% as.integer()))\r\n\r\n## 2\r\ndecomp <-\r\n  decomp_tidy %>% \r\n  # Join on our grid of x-y pairs.\r\n  inner_join(\r\n    # Using `dense_rank` because we need indexes here (i.e. 1, 2, ..., 30 instead of 0, 4.1, 8.2, ..., 120 for `x`).\r\n    grid_xy_yards %>% \r\n      select(idx, x, y) %>% \r\n      mutate(across(c(x, y), dense_rank))\r\n  )\r\n\r\n## 3\r\nsmoothen_component <- function(.data, ...) {\r\n  mat <-\r\n    .data %>% \r\n    select(x, y, value) %>% \r\n    pivot_wider(names_from = x, values_from = value) %>% \r\n    select(-y) %>% \r\n    as.matrix()\r\n  \r\n  mat_smoothed <-\r\n    mat %>% \r\n    spatstat::as.im() %>% \r\n    # Pass `sigma` in here.\r\n    spatstat::blur(...) %>% \r\n    # Could use `spatstat::as.data.frame.im()`, but it converts directly to x,y,value triplet of columns, which is not the format I want.\r\n    pluck('v')\r\n  \r\n  res <-\r\n    mat_smoothed %>% \r\n    # Convert 20x30 y-x matrix to tidy format with 20*30 rows.\r\n    as_tibble() %>% \r\n    mutate(y = row_number()) %>% \r\n    pivot_longer(-y, names_to = 'x', values_to = 'value') %>% \r\n      # The columns from the matrix are named `V1`, `V2`, ... `V30` by default, so convert them to an integer that can be joined on.\r\n    mutate(across(x, ~str_remove(.x, '^V') %>% as.integer())) %>% \r\n    arrange(x, y) %>% \r\n    # \"Re-index\" rows with `idx`, ranging from 1 to 600.\r\n    mutate(idx = row_number()) %>% \r\n    select(-x, -y) %>% \r\n    # Convert `x` and `y` indexes (i.e. 1, 2, 3, ..., to meters and flip the y-axis).\r\n    inner_join(grid_xy_rev_m) %>% \r\n    # Re-scale smoothed values to 0-1 range.\r\n    mutate(frac = (value - min(value)) / (max(value) - min(value))) %>% \r\n    ungroup()\r\n  res\r\n}\r\n\r\ndecomp_smooth <-\r\n  decomp %>% \r\n  nest(data = -c(dimension)) %>% \r\n  # `sigma` passed into `...` of `smoothen_component()`. (`data` passed as first argument.)\r\n  mutate(data = map(data, smoothen_component, sigma = 1.5)) %>% \r\n  unnest(data)\r\ndecomp_smooth\r\n\r\n\r\n\r\n\r\n\r\n## # A tibble: 18,000 x 8\r\n##    dimension    value   idx     x     y next_y next_x     frac\r\n##        <int>    <dbl> <int> <dbl> <dbl>  <dbl>  <dbl>    <dbl>\r\n##  1         1 0.002191     1     0 68     4.211  4.138 0.004569\r\n##  2         1 0.004843     2     0 64.42  8.421  4.138 0.01064 \r\n##  3         1 0.008334     3     0 60.84 12.63   4.138 0.01863 \r\n##  4         1 0.01130      4     0 57.26 16.84   4.138 0.02541 \r\n##  5         1 0.01258      5     0 53.68 21.05   4.138 0.02834 \r\n##  6         1 0.01208      6     0 50.11 25.26   4.138 0.02719 \r\n##  7         1 0.01033      7     0 46.53 29.47   4.138 0.02319 \r\n##  8         1 0.008165     8     0 42.95 33.68   4.138 0.01824 \r\n##  9         1 0.006156     9     0 39.37 37.89   4.138 0.01364 \r\n## 10         1 0.004425    10     0 35.79 42.11   4.138 0.009680\r\n## # ... with 17,990 more rows\r\n\r\n\r\n\r\nWith the data in the proper format, the plotting is pretty straightforward {ggplot2} code (so it‚Äôs excerpted).\r\n\r\nViola! I would say that our R version of the python plot is very comparable (just by visual inspection). Note that we could achieve a similar visual profile without the smoothing‚Äîsee below‚Äîbut the smoothing undoubtedly makes pattern detection a little less ambiguous.\r\n\r\nFrom the smoothed contours, we can discern several different player profiles (in terms of positioning).\r\nComponents 1, 5, 9: left back\r\nComponents 2: right midfielder\r\nComponent 3: attacking right midfielder\r\nComponent 4: wide left midfielder\r\nComponent 6: central left midfielder\r\nComponents 7, 8: goalkeeper\r\nThe redundancy with left back and goalkeeper is not ideal. That‚Äôs certainly something we could fine tune with more experimentation with components. Anyways, the point of this post wasn‚Äôt so much about the insights that could be gained (although that‚Äôs ultimately what stakeholders would be interested in if this were a ‚Äúreal‚Äù analysis).\r\nConclusion\r\nTranslating python code can be challenging, throwing us off from our typical workflow (for me, being {tidyverse}-centric). But hopefully one can see the value in ‚Äúdoing whatever it takes‚Äù, even if it means using ‚Äúnon-tidy‚Äù R functions (e.g.¬†{data.table}, matrices, etc.) or a different language altogether.\r\n\r\nthe go-to package for data manipulation and all SQL-ish things‚Ü©Ô∏é\r\nNon-negative matrix factorization may also be abbreviated just as NMF, hence the package name.‚Ü©Ô∏é\r\nThere‚Äôs nothing too interesting about the data retrieval‚ÄîI‚Äôve essentially just called StatsBombR::FreeCompetitions(), StatsBombR::FreeMatches(),StatsBombR::FreeEvents(), and StatsBombR::allclean() in succession for competition_id = 43.‚Ü©Ô∏é\r\nminimally converted from the original JSON format‚Ü©Ô∏é\r\ncompared to dict and lists or python users‚Ü©Ô∏é\r\nA potential solution would be to round the floating point numbers before joining and ‚Äúrestore‚Äù them after the join, but that‚Äôs just kluge-y and inelegant.‚Ü©Ô∏é\r\nI believe the number of components is analogous to the number of components that one would define in performing principal components analysis (PCA).‚Ü©Ô∏é\r\nThere is nothing stopping us from plotting all 30 components‚Äîand, in fact, Devin does in his notebook‚Äîbut I think it‚Äôs easier to digest a fraction of the components (for pedagogical purposes).‚Ü©Ô∏é\r\nStatsBomb data treats the origin as the top-left corner of the pitch, which I find inconvenient for plotting since I prefer the origin to be the bottom left. Thus, this grid also flip the y-axis of the grid, hence the _rev part of the variable name.‚Ü©Ô∏é\r\n",
    "preview": "post/decomposition-smoothing-soccer/viz_nnmf_dimensions_1to9_r_smooth.png",
    "last_modified": "2021-07-18T20:29:26-05:00",
    "input_file": {},
    "preview_width": 3000,
    "preview_height": 2000
  },
  {
    "path": "post/soccer-pitch-control-r/",
    "title": "Creating a Soccer Pitch Control Model",
    "description": "With S3 Classes and vctrs",
    "author": [
      {
        "name": "Tony ElHabr",
        "url": "https://twitter.com/TonyElHabr"
      }
    ],
    "date": "2020-09-23",
    "categories": [
      "r",
      "soccer"
    ],
    "contents": "\r\n\r\nContents\r\nIntro\r\nConstructor\r\nValidator\r\nHelper\r\nAside\r\nPrinting\r\nBasic Usage\r\nPseudo-Encapsulation\r\nAdvanced Usage\r\nConclusion\r\n\r\nNote: This post was updated on 2020-09-24 to correct field dimension translations that were previously distorting the pitch control contours. The R equivalents now match up much more closely with the python versions after the updates.\r\nIntro\r\nThere‚Äôs never been a better time to be involved in sports analytics. There is a wealth of open-sourced data and code (not to mention well-researched and public analysis) to digest and use. Both people working for teams and people just doing at as a hobby are publishing new and interesting analyses every day.\r\nIn particular, the FriendsOfTracking (FOT) group, co-led by Professor and author David Sumpter1 have put together an awesome series of videos on YouTube discussing modern soccer analytics, along with a collection of repositories on GitHub sharing the code shown in videos.\r\nLaurie Shaw has shared code that implements the pitch control model described in William Spearman‚Äôs paper ‚ÄúBeyond Expected Goals‚Äù is interesting to me. The model is different than the one that I used to create some animations on Twitter. Those were based on the pitch control model described by Javier Fernandez and Luke Bornn in their paper ‚ÄúWide Open Spaces‚Äù (code courtesy of Rob Hickman). (Apologies for the onslaught of links!)\r\nNow, I am not one for language wars‚Äîand, in fact, I use python often‚Äîbut I thought it would be awesome to be able to plot Spearman‚Äôs pitch control model directly with {ggplot2} and friends. Thus, I set out to convert Laurie‚Äôs code to R, attempting to give it a ‚Äúnative‚Äù R feel while I was at it.\r\nMost of the process of translating python to R was relatively straightforward (slicing and dicing data frames and arrays/vectors is just part of data cleaning), so I won‚Äôt detail them here. However, there was one part that was particularly interesting‚Äîthe conversion of a python class object. This was actually the key (and most challenging part) of the conversion process.\r\nThere are some great resources for describing how to implement object-orientated programming (OOP) in R, including a couple of chapter‚Äôs from Hadley Wickham‚Äôs Advanced R book and a very practical write-up from Earo Wang. Every object-oriented task has its unique aspects, so hopefully my discussion here has something to add to what has already been written on the subject matter.\r\nFor demonstration purposes, I‚Äôm going to walk through my steps for converting the python class object as if I were doing it for the first time.\r\nConstructor\r\nBelow is a stripped down version of Laurie‚Äôs code, showing the ‚Äúessence‚Äù of what we need to replicate.2\r\n\r\nclass player(object):\r\n    def __init__(self,player_id,frame):\r\n        self.id = player_id\r\n        self.get_position(frame)\r\n        self.get_velocity(frame)\r\n        \r\n    def get_position(self,frame):\r\n        self.position = np.array(frame[self.player_id + 'x', self.player_id + 'y'])\r\n        \r\n    def get_velocity(self,frame):\r\n        self.velocity = np.array(frame[self.player_id + 'x_v', self.player_id + 'y_v'])\r\n    \r\n    def tti(self,final_position):\r\n        reaction_time = 0.7 # in s\r\n        vmax = 5 # in m/s\r\n        reaction_position = self.position + self.velocity * reaction_time\r\n        self.tti = reaction_time + np.linalg.norm(final_positon - reaction_position)/vmax\r\n\r\n    def p_intercept(self,t):\r\n        tti_sigma = 0.45\r\n        den = 1 + np.exp(-np.pi/np.sqrt(3.0)/tti_sigma * (t-self.tti)))\r\n        return 1 / den\r\n\r\nLet‚Äôs make some notes and come back to these as we develop our R class.\r\nWe need a unique identifier: player_id. This is just a ‚Äúbest practice‚Äù thing for object-oriented programming and makes sense given our context. For a sport like soccer, a unique identifier could just be the player‚Äôs name, a combination of the team name and the player jersey number, a league unique identifier, etc.\r\nA single-row data frame frame is passed to several of the methods, including the constructor __init__. This single row data frame is sourced from a much larger tracking data frame, with rows for every 0.04 second time interval (25 frames per second, or one frame per 0.04 seconds) in the game.\r\nThe python code stores both the player‚Äôs position and velocity as 2x1 arrays. This works well with the unpacking that is done in other places in Laurie‚Äôs code.\r\ntti, short for ‚Äútime to intercept (a target location)‚Äù, uses the player‚Äôs position and velocity to define the attribute tti (not to be confused with the method itself). This implies that position and velocity should be defined before tti() is ever called, as they are in __init__. tti needs the position_final 2x1 array to calculate tti which is not known upon instantiation; rather, tti can only be properly defined when called to do a specific calculation relating the player‚Äôs position and velocity (both defined implicitly in the class, without needing user-specification) with a user-supplied position_final pair of x and y values.\r\np_intercept, short for ‚Äúprobability to intercept (a target location)‚Äù depends on tti and an additional parameter t, a user-specified value representing how much time is allotted to reach the ball. Like tti, p_intercept is only ‚Äúproperly‚Äù defined when actually doing a calculation on the player‚Äôs attributes. Unlike tti, there is no attribute in the player instance that stores this probability; it‚Äôs value must be saved in a variable external to the player class if the user wants to use it for something other than an ephemeral calculation.3\r\nTime to intercept a ‚Äútarget‚Äù location (tti) may not be intuitive to comprehend immediately. The plot4 below annotates the tti of a ‚Äútarget‚Äù location on the pitch (which does not have to be where the ball actually is). tti assumes that the player continues moving at their current speed (annotated by the arrows) for reaction_time seconds before running at vmax (full speed) to the target position. tti for each player is independent of the tti of all other players, which is a relatively reasonable assumption. 5\r\n\r\nThe probability of reaching the ‚Äútarget‚Äù location (p_intercept) is directly related to the player‚Äôs tti. Uncertainty about how long it will take the player to reach the target location is quantified by the constant tti_sigma in the calculation. (tti is the mean and tti_sigma is the standard deviation of the distribution for a player‚Äôs time to arrive at the target location.)\r\n\r\nNotably, this probability is independent of all other players‚Äô probabilities (which explains how it is possible that both players are shown to have probabilities greater than 50% when t = 6 above). When adjusting for all players‚Äô probabilities (by dividing by the sum of all probabilities), the numbers change. This probability adjustment is key when we calculate pitch control.\r\n\r\nOk, on to the R code. We‚Äôll be using S3 and the {vctrs} package to help create our player class. (As with the python class, I‚Äôve simplified the actual implementation for demonstration purposes.)\r\nFirst, we start with the constructor new_player(). Note that there is no direct __init__ equivalent in R. Here we will make a function that is prefixed with new_ and ends with the name of our class (player).\r\n\r\n\r\nnew_player <-\r\n  function(player_id = integer(),\r\n           x = double(),\r\n           y = double(),\r\n           x_v = double(),\r\n           y_v = double()) {\r\n    vctrs::new_rcrd(\r\n      list(\r\n        player_id = player_id,\r\n        x = x,\r\n        y = y,\r\n        x_v = x_v,\r\n        y_v = y_v,\r\n        tti = -1 # dummy value\r\n      ),\r\n      class = 'player'\r\n    )\r\n  }\r\n\r\n\r\n\r\nNow let‚Äôs reflect upon our prior notes.\r\nWe have the player_id in this constructor.\r\nWe don‚Äôt pass the data frame tracking here. We‚Äôll do it in our helper function. We might say that our constructor is ‚Äúlow-level‚Äù, not intended for the user to call directly.\r\nWe split the position and velocity vectors into their individual x and y components, resulting in four total variables instead of two. I don‚Äôt think a vector (unnamed or named), list, or matrix are particularly compelling data types to use for an x-y pair of values in R. None natively support unpacking (although R vectors do have some form of ‚Äúbroadcasting‚Äù with their recycling behavior).\r\nWe assign a ‚Äúdummy‚Äù value (-1) to tti when initializing the class instance. We will have a method to update tti based on x and y components.\r\nLike tti, we will need a separate p_intercept method to be used to calculate the probabililty of intercepting a ball given a player‚Äôs position, speed, and the final position of the ball (all fed as inputs to tti), as well as the additional user-specified t, representing how much time is allotted to reach the ball.\r\nValidator\r\nLet‚Äôs proceed by creating a validator function to, you guessed it, validate fields in the player class. It is good practice to check the values used to construct the class. The python code did not have any validation like this, but I don‚Äôt think it was ever expected to be extremely robust to any user input.\r\n\r\n\r\nvalidate_player <- function(player) {\r\n  vctrs::vec_assert(vctrs::field(player, 'player_id'), integer())\r\n  vctrs::vec_assert(vctrs::field(player, 'x'), double())\r\n  vctrs::vec_assert(vctrs::field(player, 'y'), double())\r\n  vctrs::vec_assert(vctrs::field(player, 'tti'), double())\r\n  player\r\n}\r\n\r\n\r\n\r\nNote that we could have simply done this validation in the constructor function, but I think it makes sense to put the validation in its own function so that the constructor is more direct (especially if the validation checks are complex).\r\nHelper\r\nFinally, we‚Äôll create a helper player() function, which is our ‚Äúuser-facing‚Äù function that we expect/want users to use to instantiate objects.\r\n\r\n\r\nplayer <- \r\n  function(player_id, frame, tracking) {\r\n    \r\n    player_id <- as.integer(player_id)\r\n    frame <- as.integer(frame)\r\n\r\n    assertthat::assert_that(is.data.frame(tracking))\r\n    nms_req <- c('player_id', 'frame', 'x', 'y', 'x_v', 'y_v')\r\n    assertthat::assert_that(all(nms_req %in% names(tracking)))\r\n    \r\n    # `!!` to make sure that we filter using the integer values, not the column itself.\r\n    tracking_filt <- tracking %>% filter(player_id == !!player_id, frame == !!frame)\r\n    assertthat::assert_that(nrow(tracking_filt) == 1L)\r\n    \r\n    player <-\r\n      new_player(\r\n        player_id = player_id,\r\n        x = tracking_filt[['x']],\r\n        y = tracking_filt[['y']],\r\n        x_v = tracking_filt[['x_v']],\r\n        y_v = tracking_filt[['y_v']]\r\n      )\r\n    validate_player(player)\r\n  }\r\n\r\n\r\n\r\nNote the following:\r\nWe coerce player_id and frame to integers instead of doubles (particularly since they are expected to be integers in the constructor). This ensures that the new player is instantiated properly by the constructor and passes our validation.\r\nWe pass in our entire tracking data frame (that has rows for every 0.04 second interval in the game), as well as the frame to slice out of it. (player_id is also used to filter tracking.) This makes it convenient for user to instantiate new player objects when operating on the tracking data frame. There is no need to extract the singular initial position and velocity components ‚Äúmanually‚Äù; instead, the helper function does it for the user.\r\nAside\r\nR‚Äôs S3 framework is not a formal OOP framework (not even close really). Note that it does not have a reserved keyword to represent the instance of the class like self in python. Also, it is not actually necessary for most of what is done above (with the constructor, validator, and helper).\r\nFor example, we don‚Äôt actually have to create a formal-ish constructor prefixed with new_. We don‚Äôt even need a constructor function at all in S3. We could do something like class(var) <- 'player' to create a a player object. Of course, this is prone to errors down the line, so we don‚Äôt do that. Likewise with the validator and helper functions. The point of these constructs is to add clarity to our class code. They aren‚Äôt strictly necessary.\r\nPrinting\r\nLet‚Äôs do one more thing for our player class‚Äîcreate a custom print method. (Writing a custom print method is not required whatsoever, but it can be very helpful for debugging.) If we weren‚Äôt using {vctrs} and just S3, we would do this by writing a print.player function. However, {vctrs} provides a ‚Äúpretty‚Äù header for us auto-magically (that looks like <player[1]>) if we use it to write our print method.\r\nTo take advantage of the pretty-printing functionality offered by {vctrs}, we write a format.player() method that will be called by a subclass of the generic vctrs::obj_print_data method6, which itself is called whenever we print out an object (whether explicitly with print or just by typing the name of the variable representing our player instance). We‚Äôll add the player‚Äôs position and velocity components to the print out.\r\n\r\n\r\nformat.player <- function(player, ...) {\r\n  if(vctrs::field(player, 'in_frame')) {\r\n    suffix <- \r\n      sprintf(\r\n        'with `position = (%.2f, %.2f)` and `velocity = <%.1f, %.1f>`', \r\n        vctrs::field(player, 'player_id'), \r\n        vctrs::field(player, 'y'), \r\n        vctrs::field(player, 'x_v'),\r\n        vctrs::field(player, 'y_v')\r\n      )\r\n  } else {\r\n    suffix <- 'is not on the pitch'\r\n  }\r\n  prefix <- sprintf('`player_id = %s` ', vctrs::field(player, 'player_id'))\r\n  msg <- sprintf('%s%s', prefix, suffix)\r\n  paste(msg, sep = '\\n')\r\n}\r\n\r\nobj_print_data.player <- function(player) {\r\n  cat(format(player), sep = '\\n')\r\n}\r\n\r\n\r\n\r\nBasic Usage\r\nOk, so that is all fine and dandy, but how would we go about instantiating players in a normal workflow?\r\nLet‚Äôs say that we want to calculate the pitch control for a single frame in the tracking data (called tracking_start below).7\r\n\r\n\r\ntracking_start\r\n\r\n\r\n\r\n\r\n\r\n##  # A tibble: 26 x 9\r\n##     frame ball_x ball_y side  player_id     x     y   x_v    y_v\r\n##     <int>  <dbl>  <dbl> <chr>     <int> <dbl> <dbl> <dbl>  <dbl>\r\n##   1 53027  93.71  24.56 home          1 90.72 39.37 5.906 -3.985\r\n##   2 53027  93.71  24.56 home          2 95.10 27.14 1.5   -2.023\r\n##   3 53027  93.71  24.56 home          3 96.01 23.32 1.418  2.395\r\n##   4 53027  93.71  24.56 home          4 92.39 15.64 1.005  3.473\r\n##   5 53027  93.71  24.56 home          5 83.96 24.69 4.238  1.2  \r\n##   6 53027  93.71  24.56 home          6 82.19 35.63 3.893 -0.619\r\n##   7 53027  93.71  24.56 home          7 85.79 17.34 1.703  1.523\r\n##   8 53027  93.71  24.56 home          8 76.06 50.16 2.018 -0.493\r\n##   9 53027  93.71  24.56 home          9 61.22 25.35 0.863 -0.77 \r\n##  10 53027  93.71  24.56 home         10 59.69 35.10 0.9   -0.573\r\n##  # ... with 16 more rows\r\n\r\n\r\n\r\nLet‚Äôs convert players with id‚Äôs 10 through 12 (on the home team) to player instances and see how they look when printed out.\r\n\r\n\r\n10L:12L %>% map(~player(player_id = .x, frame = 53027L, tracking = tracking_start))\r\n\r\n\r\n\r\n\r\n\r\n##  [[1]]\r\n##  <player[1]>\r\n##  `player_id = 10` with `position = (10.00, 35.09)` and `velocity = <0.9, -0.6>`\r\n##  \r\n##  [[2]]\r\n##  <player[1]>\r\n##  `player_id = 11` with `position = (11.00, 32.28)` and `velocity = <-0.3, 0.6>`\r\n##  \r\n##  [[3]]\r\n##  <player[1]>\r\n##  `player_id = 12` is not on the pitch\r\n\r\n\r\n\r\nPseudo-Encapsulation\r\nWe still need to implement analogues for the tti and p_intercept methods in the python player class. Starting with tti, let‚Äôs use some pseudo-encapsulation (with getters and setters) for a player‚Äôs tti value.\r\n\r\n\r\n# Frobenious norm\r\neuclidean_norm <- function(x1, x2, y1, y2) {\r\n  m <- matrix(c(x1, y1)) - matrix(c(x2, y2))\r\n  sqrt(sum(m^2))\r\n}\r\n\r\n.get_tti.player <- function(player, x2, y2) {\r\n  ri <- 0.7 # in s\r\n  vmax <- 5 # in m/s\r\n  x1 <- vctrs::field(player, 'x') + vctrs::field(player, 'x_v') * ri\r\n  y1 <- vctrs::field(player, 'y') + vctrs::field(player, 'y_v') * ri\r\n  ri + euclidean_norm(x1, x2, y1, y2) / vmax\r\n}\r\n\r\n.msg_cls_err <- function(player, f) {\r\n  cls <- class(player)[1]\r\n  sprintf('`%s()` doesn\\'t know how to handle class `%s`!', f, cls) \r\n}\r\n\r\n.get_tti.default <- function(player, ...) {\r\n  stop(.msg_cls_err(player, '.get_tti'), call. = FALSE)\r\n}\r\n\r\n.get_tti <- function(player, ...) {\r\n  UseMethod('.get_tti')\r\n}\r\n\r\n`.set_tti<-.player` <- function(player, value) {\r\n  vctrs::field(player, 'tti') <- value\r\n  player\r\n}\r\n\r\n`.set_tti<-.default` <- function(player, ...) {\r\n  stop(.msg_cls_err(player, '.set_tti'), call. = FALSE)\r\n}\r\n\r\n`.set_tti<-` <- function(player, ...) {\r\n  UseMethod('.set_tti<-')\r\n}\r\n\r\n\r\n\r\nThere‚Äôs a couple of things going on here:\r\nThe .get_tti and .set_tti functions that call UseMethod are true S3 generics that perform method dispatch, i.e.¬†find the correct method for the object passed to the generic (based on the class of the object). The .get_tti.player and .set_tti.player with the .player ‚Äúsuffix‚Äù so that they only work in their defined manners when passed in a player instance. (They won‚Äôt be called with an object that is not of the player class.)\r\nThe ellipses (...) in the S3 generic function signatures may be a bit mysterious since they aren‚Äôt passed explicitly to UseMethod. Any non-player arguments are captured in these ellipses and passed to whatever method that is called from the generic (e.g.¬†.get_tti.player method called from the .get_tti generic). For .get_tti, the ellipses is intended to capture x2 and y2, and for .set_tti, it captures value.\r\nWe must use the ‚Äústrange‚Äù syntax .set_tti<-.player (instead of just .set_tti.player, which may seem more ‚Äúnatural‚Äù) in order to update an attribute in an already instantiated class. 8\r\nWe define the function euclidean_norm() outside of .get_tti.player simply because it is not something that is specific to the time to intercept calculation for a player; it can work with any two pairs of x and y coordinates.9\r\nri and vmax, representing a player‚Äôs reaction time and a player‚Äôs maximum velocity respectively, are constants defined in the Spearman paper. We could change these if we wanted to, or even make them dynamic (i.e.¬†configurable via other function parameters, or even at instantiation time).\r\nTo really complete our getter and setter methods for tti, we should write methods to handle the case when a non-player object is passed to them. The generic .get_tti and .set_tti methods will dispatch to these functions if the object passed to them (the first argument named player) doesn‚Äôt actually inherit from the player class.\r\n\r\n\r\n.get_tti.default <- function(player, ...) {\r\n  stop(.msg_cls_err(player, '.get_tti'), call. = FALSE)\r\n}\r\n\r\n.set_tti.default <- function(player, ...) {\r\n  stop(.msg_cls_err(player, '.get_tti'), call. = FALSE)\r\n}\r\n\r\n\r\n\r\nLet‚Äôs see how our pseudo-encapsulation looks in action.\r\n\r\n\r\nplayers <- 8L:10L %>% map(~player(player_id = .x, frame = 53027L, tracking = tracking_start))\r\nmap(players, ~vctrs::field(.x, 'tti'))\r\n\r\n\r\n\r\n\r\n\r\n## [[1]]\r\n\r\n## [1] -1\r\n\r\n## \r\n\r\n## [[2]]\r\n\r\n## [1] -1\r\n\r\n## \r\n\r\n## [[3]]\r\n\r\n## [1] -1\r\n\r\n\r\n\r\n\r\n\r\ntarget_x <- 94\r\ntarget_y <- 63\r\nfor(i in seq_along(players)) {\r\n  value <- .get_tti(players[[i]], x2 = target_x, y2 = target_y)\r\n  .set_tti(players[[i]]) <- value\r\n}\r\nmap(players, ~vctrs::field(.x, 'tti'))\r\n\r\n\r\n\r\n\r\n\r\n## [[1]]\r\n\r\n## [1] 4.92839\r\n\r\n## \r\n\r\n## [[2]]\r\n\r\n## [1] 10.6878\r\n\r\n## \r\n\r\n## [[3]]\r\n\r\n## [1] 9.49904\r\n\r\n\r\n\r\nNote how the player tti values changed after we defined them for a specified target_x and target_y.\r\nOur approach to p_intercept is very similar to that for tti, so I don‚Äôt show most of it here. As before, we define getters and setters, as well as generics for the class (the intended target of method dispatch), as well as a default class to handle unexpected inputs. Probably the only interesting part is the calculation itself, as shown below. If you compare it to the p_intercept method in the python object definition, you‚Äôll see it‚Äôs basically identical.\r\n\r\n\r\n.get_p_intercept.player <- function(player, t) {\r\n  tti_sigma <- 0.45\r\n  den <- 1 + exp((-base::pi / sqrt(3) / tti_sigma) * (t - vctrs::field(player, 'tti')))\r\n  1 / den\r\n}\r\n\r\n\r\n\r\nThere is certainly more to show, especially for what is needed to calculate pitch control. (We need to integrate probabilities across all players over time, and do it for the entire pitch.) Nonetheless, the player class and the pseudo-encapsulation that we‚Äôve implemented with S3 and {vctrs} is really the key component underlying the whole pitch control calculation.\r\nAdvanced Usage\r\nTo really motivate the reader, let‚Äôs see what this implementation allows us to do.\r\nFirst, let‚Äôs emulate the pitch control plot of event 823, which is a pass by the away (blue) team in the home (red) team‚Äôs penalty area preceding a successful shot.\r\n\r\nCompare this to the python version.\r\n\r\nIt‚Äôs not a perfect replication, but I think it‚Äôs very close overall.\r\nSecond, let‚Äôs replicate the expected possession value (EPV) plot of the same event, including the EPV added by the pass.\r\n\r\nAgain, we can compare this plot to the python equivalent.\r\n\r\nCool, my R version seems to be very close to the python original. We do have a small discrepancy in the EPV added calculation. (This EPV is actually an ‚Äúexpected‚Äù EPV calculation that uses pitch control to weight the pre-learned EPV grid). I believe this is probably due to discrepancies in the integration done in the pitch control calculation and not due to a significant code issue.\r\nThe code to prepare the data for these plots gets more complex, which is why I have excluded it here.10 However, none of it is unreasonably difficult to understand or implement once we have a properly defined player object.\r\nConclusion\r\nThus, we see that there is a huge payoff to creating a sound and robust player object‚Äîwe can calculate pitch control and EPV, and feed them into pretty visualizations that can provide insight. I believe that the code here could be easily adapted to fit whatever one would like to study. For example, the valuation grid used here could be easily swapped out with expected threat (xT), which is a more modern and probably a better valuation framework than the one used here.11 Furthermore, one could calculate EPV across the entire game. The possibilities for analyses really open up.\r\nauthor of one of my favorite books Soccermatics‚Ü©Ô∏é\r\nIf you actually browse Laurie‚Äôs code, you‚Äôll notice that I‚Äôve changed some of the names of these functions, e.g.¬†tti() here instead of simple_time_to_intercept() there, tracking_df instead of team there. Don‚Äôt worry about that. I just wanted to make things as comparable as possible for the diligent reader, and I tried to make variable names that were either (or both) more succinct or more clear.‚Ü©Ô∏é\r\nOne might argue that we should be consistent with tti and p_intercept and store them in the same way‚Äîeither both as attributes or neither as attributes‚Äîgiven that both are dependent on some user-specified values (final_position for tti and t for p_intercept). I‚Äôm just showing how it is done in Laurie‚Äôs code. I think it is simple enough as is and there is no compelling functional reason why we should change the implementation.‚Ü©Ô∏é\r\nData for this plot and all that follow from post-processed Metrica Sports sample game 2 data.‚Ü©Ô∏é\r\nauthor of one of my favorite books Soccermatics‚Ü©Ô∏é\r\nvctrs::obj_print_data that can also handle Unicode easily, although we are not using any Unicode characters here. Also, it is able to handle extra NULLs and [1] that may be printed out if we just use cat or print directly.‚Ü©Ô∏é\r\nThis is the same data used to generate the first handful of plots.‚Ü©Ô∏é\r\nR6 is probably a better OOP system to use for this whole use case. The capability to update instance attributes is more native to that framework.‚Ü©Ô∏é\r\nIt‚Äôs best to separate our logic in functions like this where it makes sense to do so. It ultimately makes re-factoring and debugging a lot easier.‚Ü©Ô∏é\r\nFeel free to check out the source the code used in the full implementation, as well as the code to generate the plots.‚Ü©Ô∏é\r\nHere is a CSV with the grid for xT.\r\n\r\n‚Ü©Ô∏é\r\n",
    "preview": "post/soccer-pitch-control-r/viz_pc_823_combined.png",
    "last_modified": "2021-07-19T07:32:52-05:00",
    "input_file": {},
    "preview_width": 1536,
    "preview_height": 642
  },
  {
    "path": "post/variable-importance-compare/",
    "title": "Comparing Variable Importance Functions (For Modeling)",
    "description": "Permuation, SHAP, and More",
    "author": [
      {
        "name": "Tony ElHabr",
        "url": "https://twitter.com/TonyElHabr"
      }
    ],
    "date": "2020-07-13",
    "categories": [
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nSetup\r\nResults\r\nConclusion\r\n\r\nI‚Äôve been doing some machine learning recently, and one thing that keeps popping up is the need to explain the models and their components. There are a variety of ways to go about explaining model features, but probably the most common approach is to use variable (or feature) importance scores. Unfortunately, computing variable importance scores isn‚Äôt as straightforward as one might hope‚Äîthere are a variety of methodologies! Upon implementation, I came to the question ‚ÄúHow similar are the variable importance scores calculated using different methodologies?‚Äù 1 I think it‚Äôs important to know if the different methods will lead to drastically different results. If so, then the choice of method is a source of bias in model interpretation, which is not ideal.\r\nThis post isn‚Äôt intended to be a deep-dive into model interpretability or variable importance, but some concepts should be highlighted before attempting to answer this question. Generally, variable importance can be categorized as either being ‚Äúmodel-specific‚Äù or ‚Äúmodel-agnostic‚Äù. Both depend upon some kind of loss function, e.g.¬†root mean squared error (RMSE), classification error, etc. The loss function for a model-specific approach will generally be ‚Äúfixed‚Äù by the software and package that are used2, while model-agnostic approaches tend to give the user flexibility in choosing a loss function. Finally, within model-agnostic approaches, there are different methods, e.g.¬†permutation and SHAP (Shapley Additive Explanations).\r\nSo, to summarize, variable importance ‚Äúmethodologies‚Äù can be broken down in several ways:\r\nmodel-specific vs.¬†model-agnostic approach\r\nloss function 3 . model agnostic method (given a model agnostic approach)\r\nI‚Äôm going to attempt to address (1) and (3) above. I‚Äôm leaving (2) out because (a) I think the results won‚Äôt differ too much when using different loss functions (although I haven‚Äôt verified this assumption) and (b) for the sake of simplicity, I don‚Äôt want to be too exhaustive in this analysis. 3\r\nI also want to evaluate how variable importance scores differ across more than one of each of the following:\r\nmodel type (e.g.¬†linear regression, decision trees, etc.)\r\ntype of target variables (continuous or discrete )\r\ndata set\r\nWhile evaluating the sensitivity of variable importance score to different methodologies is the focus of this analysis, I think it‚Äôs important to test how the findings hold up when (1) varying model types, (2) varying target variables, and (3) varying the data itself. This should help us highlight any kind of bias in the results due to choice of model type and type of target variable. Put another way, it should help us quantify the robustness the conclusions that are drawn. If we find that the scores are similar under variation, then we can be more confident that the findings can be generalized.\r\nAdditionally, I‚Äôm going to use more than one package for computing variable importance scores. As with varying model types, outcome variables, and data, the purpose is to highlight and quantify possible bias due to choices in this analysis‚Äîin this case, the choice of package. Are the results of a permutation-based variable importance calculation the same when using different packages (holding all else equal)?\r\nSpecifically, I‚Äôll be using the {vip} and {DALEX} packages. The {vip} package is my favorite package to compute variable importance scores using Ris because it is capable of doing both types of calculations (model-specific and model-agnostic) for a variety of model types. But other packages are also great. {DALEX} package specializes in model-agnostic model interpretability and can do a lot more than just variable importance calculations.\r\nSetup\r\nFor data, I‚Äôm going to be using two data sets from {ggplot2}. 4\r\nincrease computation time. (b) I‚Äôve excluded two of the categorical features‚Äîclarity and color, both of which are categorical with a handful of levels. I‚Äôve done this in order to reduce the number of variables involved and, consequently, to speed up computation. (This is just an example after all!) (c) To test how variable importance scores differ for a continuous target variable, I‚Äôll be defining models that predict price as a function of all other variables. (d) For discrete predictions, the target is a binary variable grp that I‚Äôve added. It is equal to '1. Good' when cut %in% c('Idea', 'Premium') and 2. Bad' otherwise. It just so happens that grp is relatively evenly balanced between the two levels, so there should not be any bias in the results due to class imbalance.\r\nI made modifications to both, so see the footnotes and/or code if you‚Äôre interested in the detailI made modifications to both, so see the footnotes and/or code if you‚Äôre interested in the details.\r\n\r\nFor model types, I‚Äôm going to trial the following:\r\ngeneralized linear model (linear and logistic regression) with stats::lm() and stats::glm() respective ly\r\ngeneralized linear model with regularization using the {glmnet} package\r\nbagged tree (random forest) using the {ranger} package\r\nboosted tree (extreme gradient boosting) using the {xgboost} package\r\nWith glmnet::glmnet(), I‚Äôm actually not going to use a penalty, so (I think) it should return the same results as lm()/glm(). 5 For {ranger} and {xgboost}, I‚Äôm going to be using defaults for all parameters. 6\r\n{vip}‚Äôs model-specific scores with (vip::vip(method = 'mod l'))\r\n{vip}‚Äôs permutation-based scores (with vip::vip(method = 'permute'))\r\n{vip}‚Äôs SHAP-based values (with vip::vip(method = 'shap'))\r\n{DALEX}‚Äôs permutation-based scores (with DALEX::variable_importance())\r\nNote that the model-specific vs.¬†model-agnostic concern is addressed in comparing method (1) vs.¬†methods (2)-(4). I‚Äôll be consistent with the loss function in variable importance computations for the model-agnostic methods‚Äìminimization of RMSE for a continuous target variable and sum of squared errors (SSE) for a discrete target variable. 7\r\nResults\r\nThe following handful of plots illustrate normalized variable importance scores and ranks derived from the scores by data set and type of target variable.\r\nFirst up is the results for the diamonds data set with a continuous target variable.\r\nme: the model-specific scores differ relatively strongly from the rest of the scores given a specific model type. (See the numbers in the parentheses in the first column in each facet labeled vip_model compared to those in the other columns of each facet. 8 For example, the model-specific variable importance score for the carat feature for the {glm} model type is 49%, while the same score for the SHAP variable importance method (vip_shap) is 35%. To be honest, this is not too surprising. The model-specific methods are exactly that‚Äîspecific to the model type‚Äîwhich suggests that they may strongly dissimilar to the model-agnostic approaches. Nonetheless, despite the scores themselves having some notable variance, the rankings derived from the scores are relatively similar across a given model type (and, arguably, across all model types).\r\nAs a second observation, there is some disagreement between the {glm} and {glmnet} model types and the {ranger} and {xgboost} model types about which feature is the most important: the former two identify carat has being the most important, while the latter two prioritize y.\r\nThirdly‚Äìand lastly for this plot‚Äîit‚Äôs nice to see that the vip_permute and dalex methods produce nearly identical results for each model type, with the exception of {glmnet}. (Hypothetically, these should have nearly identical results since they are both permutation based methods.) Notably, I implemented the explain() function for {glmnet} myself since the {DALEX} package does not export one, so that is probably the reason for the discrepancy.\r\nNow let‚Äôs look at the the results when predicting a discrete target variable with the same data set.\r\n\r\nCompared to the results for a continuous target variable, we see greater variation across the model types‚Äîthe rankings from {glm} and {glmnet} are nearly identical, but they are different from those of {xgboost}, and all are different from those of {ranger}. {ranger} has an additional level of variation‚Äîlack of agreement among the methodologies.\r\nAdditionally, we observe that the scores for our two permutation implementations‚Äî vip_permute and dalex‚Äîare very different. I think this might have to do with how I‚Äôve chosen to normalize scores (i.e.¬†using absolute value to convert negative scores to positive ones prior to 0-1 normalization) or something I‚Äôve over-looked that is specific to classification settings. If something that can be attributed to me (and not the underlying methods) is really the source of discrepancies, then we should be less concerned with the variation in scores and ranks since it seems most strongly associated with the vip_permute-dalex differences.\r\nBefore we can begin to generalize any deductions (possibly biased by our single data set), let‚Äôs take a look at the results for the second data set, mpg. First is the results for the continuous target variable.\r\n\r\nThere is consensus on what the most important variable is‚Äîcyl‚Äîbut beyond that, the results are somewhat varied across the board. One might argue that there is going to be lack of agreement among methods (and model types), it‚Äôs preferable that the discrepancies occur among lower ranks, as seen here. On the other hand, we‚Äôd surely like to see more consensus among variables ranked among the top half or so.\r\nAnd now for the results when ranking with models targeting a discrete variable.\r\n\r\nThere is some pretty strong variation in the {ranger} results. Also, there are discrepancies between the two permutation methods (vip_permute and dalex), which we also noted in the discrete results for diamonds as well. This makes me think again that the issue is due to something I‚Äôve done and not something that could be attributed to the underlying methods. Aside from these, I would say that the results within each model type are pretty coherent (more so than those with the continuous outcome.)\r\nEven without performing any kind of similarity evaluation, we can argue that, in general, the rankings computed by the different methods are relatively similar across the two data sets (diamonds and mpg) and the two types of target variables (continuous and discrete). But why stop there? After all, we can quantify the similarities between ranks.\r\n\r\nThe plot above shows the pairwise correlations among the variable importance ranks computed for each package-function combo, averaged over the two data sets and over the models for the two types of target variables‚Äîcontinuous and discrete. 9 While nothing immediately jumps out from this plot, I think the most notable thing is that the {ranger} scores seem to vary the most across the different variable importance methodologies, bottoming out at 74% for the correlation between the SHAP (vip_shap) and model-specific (vip_model) methodologies. On the other hand, {xgboost} seems to have the most ‚Äúagreement‚Äù and least variance in its scores.\r\nConclusion\r\nOverall, we might say that rankings of variable importance based on normalized variable importance scores in this analysis showed that differences will arise when evaluating different methodologies, but the differences may not be strong enough to change any deductions that one might draw. Of course, this will depend on the context. A small differences could make a huge difference in a field like medicine!\r\nI wouldn‚Äôt go so far as to say that these insights can be generalized‚Äîamong other things, I think I would need to evaluate a much larger variety of data sets‚Äîbut I think it‚Äôs good to be conscious how much the results can vary. It‚Äôs ultimately up to the user whether the differences are significant.\r\n\r\nAfter all, I want to make sure my results aren‚Äôt sensitive to some kind of bias (unintentional in this case).‚Ü©Ô∏é\r\nThis isn‚Äôt an academic paper after all!‚Ü©Ô∏é\r\nThis isn‚Äôt an academic paper after all!‚Ü©Ô∏é\r\nModifications include the following: (a) I‚Äôve excluded manufacturer, model, trans, and class. (b) For continuous predictions, I‚Äôll predict displ as a function of all other variables. (c) For discrete predictions, I‚Äôve created a binary variable grp based on class.‚Ü©Ô∏é\r\n(I haven‚Äôt actually checked the source for {glmnet} and compared it to that of lm()/glm(). Differences may arise due to underlying differences in the algorithm for least squares.)‚Ü©Ô∏é\r\nI should say that I‚Äôm using the {tidymodels} package to assist with all of this. It really shows off its flexibility here, allowing me to switch between models only having to change-out one line of code!Finally, for variable importance scores (which is really the focus), I‚Äôm going to use the following packages and functi ons.‚Ü©Ô∏é\r\nYes, SSE is certainly not the best measure of loss for classification. Nonetheless, when dealing with a binary outcome variable, as is done here, it can arguably be cceptable.‚Ü©Ô∏é\r\nDon‚Äôt be deceived by the fill contours, which are based on the rankings‚Äìthe number in front of the parentheses.‚Ü©Ô∏é\r\nI could have split (or ‚Äúfacetted‚Äù) in a different way‚Äìe.g.¬†by type of target variable instead of by package-function combo‚Äîbut I think splitting in this way makes the most sense because the type of model‚Äî{glm}, {ranger}, etc.‚Äîis likely the biggest source of variation.‚Ü©Ô∏é\r\n",
    "preview": "post/variable-importance-compare/viz_diamonds_c_rnks.png",
    "last_modified": "2021-07-01T22:07:16-05:00",
    "input_file": {},
    "preview_width": 2400,
    "preview_height": 2400
  },
  {
    "path": "post/bayesian-statistics-english-premier-league/",
    "title": "Ranking English Premier League Teams",
    "description": "With a Bayesian Approach",
    "author": [
      {
        "name": "Tony ElHabr",
        "url": "https://twitter.com/TonyElHabr"
      }
    ],
    "date": "2019-12-13",
    "categories": [
      "r",
      "soccer"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nData Collection\r\n\r\nModeling\r\nInterpretation & Discussion\r\nPredictions\r\n\r\nConclusion\r\nFuture Work\r\n\r\n\r\nI haven‚Äôt had as much time (since summer of 2018) to write due to taking classes in pursuit of a degree from Georgia Tech‚Äôs Online Master of Science in Analytics (OMSA) program. On the other hand, the classes have given me some ideas for future content. And, in the case of the Bayesian Statistics class that I took this past fall, there‚Äôs content that translates well to a blog post directly. What follows is a lightly edited version of the report that I submitted at the end of the semester for this class.\r\nIntroduction\r\nI model and predict English Premier League (EPL) game outcomes using Bayesian methods. Specifically, I estimated goals scored by each team in a given game as independent Poisson processes, taking the difference of the estimated points scored on each side to determine game winners. More broadly, one may call this a hierarchical Bayesian Poisson model.\r\nWhy model goals scored using a Poisson distribution? By definition, it ‚Äúis a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time with a known constant rate.‚Äù In the context of soccer, the fixed interval of time is the 90 minutes of a game (disregarding injury time and over time), and the known constant rate is the expected number of goals scored per minute. Importantly, I must make the assumption that the rate of scored goals is the same across all minutes of a game. 1 Additionally, when computing the difference between Poisson distributions, I must assume that the two distributions are independent of one another. 2\r\nUsing Poisson distributions to model soccer scores is certainly not a novel concept. 3 4 In particular, I would like to acknowledge the work of Rasmus Baath‚Äôs, whose series of blog posts exemplifying the use of R and JAGS to model scores in La Liga games between the 2008-09 to 2012-13 season served as a guide for the analysis that I conduct here. 5 6\r\nData Collection\r\nFor this project I retrieved game scores and outcomes for the previous three seasons of EPL games (i.e.¬†from the 2016-17 season through the 2018-2019 season).\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nModeling\r\nMy model is formally defined as follows.\r\n\\[\r\n\\begin{array}{c}\r\ng_h \\sim \\mathcal{Pois}(\\lambda_{h,i,j}) \\\\\r\ng_a \\sim \\mathcal{Pois}(\\lambda_{a,i,j}) \\\\\r\n\\log(\\lambda_{h,i,j}) = \\text{baseline}_h + (z_i - z_j) \\\\\r\n\\log(\\lambda_{a,i,j}) = \\text{baseline}_a + (z_j - z_i). \\\\\r\n\\end{array}\r\n\\]\r\nThis model estimates the goals scored by the home team, \\(g_h\\), and the goals scored by the away team, \\(g_a\\), in a given game between home team, \\(\\text{tm}_h\\), and away team, \\(\\text{tm}_a\\), as random variables coming from independent Poisson processes, \\(\\mathcal{Pois}(\\lambda_{h,i,j})\\) and \\(\\mathcal{Pois}(\\lambda_{a,i,j})\\). The log of the rate of goals scored by the home team, \\(\\lambda_{h,i,j}\\), in a game between \\(\\text{tm}_i\\) and \\(\\text{tm}_j\\) is modeled as the sum of a ‚Äúbaseline‚Äù average of goals scored by any given team playing at home, \\(\\text{baseline}_h\\), and the difference between the team ‚Äústrength‚Äù \\(z\\) of teams \\(i\\) and \\(j\\) in a given game. I define the log of the goal rate by the away team, \\(\\lambda_{a,i,j}\\), in a similar fashion. 7 It is important to distinguish the baseline levels for home and away so as to account for ‚Äúhome field advantage‚Äù. (One should expect to find that \\(\\text{baseline}_h > \\text{baseline}_a\\) in the posterior estimates.)\r\nSince I am employing a Bayesian approach, I need to model priors as well. I define them as follows.\r\n\\[\r\n\\begin{array}{c}\r\n\\text{baseline}_h \\sim \\mathcal{N}(0, 2^2) \\\\\r\n\\text{baseline}_a \\sim \\mathcal{N}(0, 2^2) \\\\\r\nz_{i} \\sim \\mathcal{N}(z_{\\text{all}} , \\sigma^2_{\\text{all}}) \\quad \\text{tm}_i > 1 \\\\\r\nz_{\\text{all}} \\sim \\mathcal{N}(0, 2^2) \\\\\r\n\\sigma_{\\text{all}} \\sim \\mathcal{U}(0, 2).\r\n\\end{array}\r\n\\]\r\nThere are a couple of things to note about these priors. First, I must ‚Äúzero‚Äù-anchor the strength estimate \\(z\\) of one team. (This is manifested by \\(\\text{tm}_i > 1\\).) Here, I choose the first team alphabetically‚ÄîArsenal. Second, the priors are intentionally defined to be relatively vague (although not too vauge) so as to allow the posterior estimates to be heavily defined by the data rather than the priors. Note that the standard deviation of the overall team strength parameter \\(z_{\\text{all}}\\), defined as \\(2\\) on a log scale, corresponds to an interval of \\(\\left[e^{-2}, e^2\\right] = \\left[0.13, 7.40\\right]\\) on an unstransformed scale, i.e.¬†goals scored per game.\r\nI leverage the {R2OpenBUGs} package to create this model with R on the ‚Äúfrontend‚Äù and generate the results using the OpenBUGS engine on the ‚Äúbackend‚Äù. Regarding the implementation itself, note that I run 100,000 simulations (n.iter), minus 1,000 ‚Äúburn-in‚Äù runs (n.burn).\r\nThe raw results are as follows. (As a quick ‚Äúvalidation‚Äù of these results, note that \\(\\text{baseline}_h > \\text{baseline}_a\\), as hypothesized.)\r\nInterpretation & Discussion\r\nNext, I correspond the strength estimates \\(z\\) to teams. Notably, I must ‚Äúre-add‚Äù the zero-anchored team‚ÄîArsenal (whose \\(z\\) is assigned a dummy value of 1). To do this, I impute its credible set quantiles using the values of the overall strength term \\(z_{\\text{all}}\\).\r\n\r\nIt‚Äôs not surprising to see that the strength (\\(z\\)) corresponding to all but three teams‚ÄîLiverpool, Man City, and Tottenham‚Äîis negative. These three teams, followed closely by Arsenal have been regarded as the best teams for the past two or three EPL seasons. So, relative to Arsenal, all other teams (aside from the top three) are viewed as ‚Äúworse‚Äù by the model.\r\nNote that the \\(z\\) estimates above should not be interpreted as goals scored by the teams because they are relative to the strength of Arsenal. To facilitate such an interpretation, I need to translate \\(z\\) to goals scored per game. To do this, for each \\(z\\), I (1) subtract the average value of all \\(z\\)‚Äôs, (2) add the posterior mean of \\(\\text{baseline}_{h}\\), and (3) exponentiate.\r\nThe plot below shows the results of this transformation.\r\n\r\nPredictions\r\nI can make predictions of game results for the historical data, given the model. Specifically, I simulate the score for both teams in each matchup (1,140 in all) 1,000 times, choosing the result inferred by the mode of each side‚Äôs simulated score. (For example, if the mode of the 1,000 simulated scores for the away team is 1 and that of the home team is 2, then the predicted outcome is a win for the home team.) A breakdown of the predicted and actual outcomes is shown below.\r\n\r\nI make a couple of observations:\r\nThe most common outcome is an actual win by the home team and a predicted win by the home team.\r\nThe model never predicts a tie. (This may seem ‚Äúunreasonable‚Äù, but B√É¬•√É¬•th also found this to be true for his final model.)\r\nThe model predicts the outcome correctly in 447 + 216 = 663 of 1,140 games (i.e., 58%).\r\nThe next couple of visuals provide more details regarding the simulated outcomes.\r\n\r\nFrom the above graph of the mode of goals scored by both sides, it‚Äôs apparent that a 2-1 scores in favor of the home side is the most common outcome.\r\n\r\nThe above histogram illustrating the mean (instead of the mode) of the simulated goals provides a bit more nuance to our understanding of modes shown before.\r\n\r\nFinally, the above visual shows the predicted outcomes (inferred from the prior graph of predicted modes).\r\nTo better understand how the model works on a team-level basis, let‚Äôs look at how well it predicts for each team.\r\n\r\n\r\n\r\nTeam\r\n# of Wins\r\nWin %\r\nArsenal\r\n46\r\n80.70%\r\nMan City\r\n45\r\n78.95%\r\nTottenham\r\n43\r\n75.44%\r\nLiverpool\r\n41\r\n71.93%\r\nChelsea\r\n39\r\n68.42%\r\nCardiff\r\n12\r\n63.16%\r\nEverton\r\n36\r\n63.16%\r\nFulham\r\n12\r\n63.16%\r\nMan United\r\n35\r\n61.40%\r\nHuddersfield\r\n23\r\n60.53%\r\nBurnley\r\n34\r\n59.65%\r\nStoke\r\n22\r\n57.89%\r\nBournemouth\r\n30\r\n52.63%\r\nCrystal Palace\r\n30\r\n52.63%\r\nSunderland\r\n10\r\n52.63%\r\nSwansea\r\n20\r\n52.63%\r\nWest Ham\r\n30\r\n52.63%\r\nWatford\r\n29\r\n50.88%\r\nNewcastle\r\n19\r\n50.00%\r\nLeicester\r\n28\r\n49.12%\r\nMiddlesbrough\r\n9\r\n47.37%\r\nWest Brom\r\n18\r\n47.37%\r\nWolves\r\n9\r\n47.37%\r\nBrighton\r\n15\r\n39.47%\r\nSouthampton\r\n22\r\n38.60%\r\nHull\r\n6\r\n31.58%\r\nIn most cases, the model predicts the outcome correctly (see is_correct) with greater than 50% accuracy, although there are also teams for which its accuracy is less than 50%.\r\nConclusion\r\nIn summary, I have created a hierarchical Poisson model to predict scores‚Äîand, consequently, game outcomes‚Äîfor EPL games for the three seasons starting in 2016 and ending in 2018. The model has an training set prediction accuracy of 66.3%. Baath, whose work inspired mine, achieved an accuracy of 56% with his final model.\r\nFuture Work\r\nMy model can certainly be improved. One major flaw of the model is that it does not account for temporal effects, i.e.¬†differences in team strength across seasons. 8 The consequences of this flaw are compounded by the fact that the pool of teams in each EPL season changes. At the end of each season, the three ‚Äúworst‚Äù EPL teams (by win-loss-tie record) are ‚Äúrelegated‚Äù to a secondary league, and, in turn, three secondary league teams are ‚Äúpromoted‚Äù to the EPL in their place. 9 Consequently, one might say that the estimates of the teams that do not appear in all seasons are exaggerated.\r\n\r\nThis is arguably a ‚Äúbad‚Äù assumption. Research has shown that goal rate per minute increases in the last 15 minutes of a game.‚Ü©Ô∏é\r\nThis may also be perceived to be a questionable assumption. One may argue that a matchup of ‚Äústyles‚Äù‚Äîe.g.¬†an aggressive team against another aggressive team‚Äîmay distort the results from what would otherwise be expected.‚Ü©Ô∏é\r\nThis approach is arguably too ‚Äúsimplistic‚Äù, but it is certainly a valid approach.‚Ü©Ô∏é\r\nSee this Pinnacle blog post for a discussion of the topic. (Undoubtedly there are many more articles and papers that explore a similar notion.)‚Ü©Ô∏é\r\nThere are several notable differences with my work compared to that of B√É¬•√É¬•th: (1) I use the OpenBUGS software (and the {R2OpenBUGS} package) instead of JAGS; (2) I evaluate EPL teams instead of La Liga teams, and over a different time period; (3) I use a ‚Äútidy‚Äù approach (in terms of packages, plotting, coding style, etc.) instead of a more traditional ‚Äúbase R‚Äù approach; (4) I implement a modified version of the second of Baath‚Äôs three proposed models (notably, using different priors).‚Ü©Ô∏é\r\nBaath‚Äôs work is licensed under the Creative Commons license, which allows for others to adapt the work of another.‚Ü©Ô∏é\r\nNote that I substitute the baseline home average goal rate with a baseline for away teams, \\(\\text{baseline}_a\\), and I swap the order of the \\(z_j\\) and \\(z_i\\) teams since the relationship is not bi-directional. Also, note that I am careful to distinguish between subscript pair \\(_h\\) and \\(_a\\) for home and away and pair \\(_i\\) and \\(_j\\) for team \\(i\\) and team \\(j\\). The latter pair is independent of the notion of home or away.‚Ü©Ô∏é\r\nThere are certainly also changes in team strength within seasons, which are even more difficult to model.‚Ü©Ô∏é\r\nThis explains why there are more than 20 teams in thee data set even though there are only 20 teams in the EPL in a given season.‚Ü©Ô∏é\r\n",
    "preview": "post/bayesian-statistics-english-premier-league/viz_summ_1_z_adj.png",
    "last_modified": "2021-07-01T22:12:34-05:00",
    "input_file": {},
    "preview_width": 2100,
    "preview_height": 1500
  },
  {
    "path": "post/cheat-sheet-rmarkdown/",
    "title": "Making a Cheat Sheet",
    "description": "With Rmarkdown",
    "author": [
      {
        "name": "Tony ElHabr",
        "url": "https://twitter.com/TonyElHabr"
      }
    ],
    "date": "2019-07-07",
    "categories": [
      "r",
      "latex"
    ],
    "contents": "\r\n\r\nContents\r\nThe Template\r\nUsage\r\nWhy This Way?\r\nWhat Works for Me May Not Work For You\r\n\r\nUnfortunately, I haven‚Äôt had as much time to make blog posts in the past year or so. I started taking classes as part of Georgia Tech‚Äôs Online Master of Science in Analytics (OMSA) program last summer (2018) while continuing to work full-time, so extra time to code and write hasn‚Äôt been abundant for me.\r\nAnyways, I figured I would share one neat thing I learned as a consequence of taking classes‚Äîwriting compact ‚Äúcheat sheets‚Äù with {rmarkdown}. 1\r\nWriting with {rmarkdown} is fairly straightforward‚Äîmostly thanks to an abundance of freely available learning resources, like the R Markdown: The Definitive Guide‚Äîand using CSS to customize your Rmarkdown output to your liking is not too difficult either. (By the way, huge shout-out to Yihui Xie and everyone else who has contributed to the development of the {rmarkdown} package.) My objective was to make an extremely compact PDF that minimizes all white space 2. Despite my knowledge of CSS, I had a hard time getting an output that I liked purely from CSS, so I looked online to see if I could find some good LaTex templates. (After all, I would be knitting the Rmarkdown document to PDF, and LaTex would be incorporated via the equations on the cheat sheet.) Some templates I found worked fine but weren‚Äôt completely to my liking. 3\r\nIn my search for an ‚Äúideal‚Äù template, I stumbled upon a small tidbit in the very last portion of the PDF chapter of the R Markdown book stating ‚ÄúYou can also replace the underlying pandoc template using the template option‚Äù.\r\nAt first, I was a bit intimidated by the idea of writing my own template. (‚ÄúI have to write my own template from scratch using a framework (LaTeX) that I‚Äôve hardly even touched before now!‚Äù) But alas, the task became less intimidating when I realized that I could use the tried-and-true method of copying-pasting-modifying from Stack Overflow!\r\nThe Template\r\nUsing the template from this Stack Overflow post 4 as a basis, I ended up creating a relatively minimal template. For the curious reader, see this GitHub repo, for the latest version of my template. It also includes an example cheat sheet.\r\n\r\nThe ‚Äúgist‚Äù of my template is shown below.\r\n\r\n% Packages and preamble\r\n% ...\r\n\r\n\\begin{document}\r\n\r\n\\begin{multicols*}{4}\r\n\r\n$body$\r\n\r\n\\end{multicols*}\r\n\r\n\\end{document}\r\n\r\nThe key for me was to understand how pandoc variables like $body$ are used as placeholders for user-supplied content. (I know I haven‚Äôt mentioned pandoc up to this point, but suffice it to say that it‚Äîalong with the R package {knitr}‚Äîare what power the {rmarkdown} package.)\r\nThe multicols command shown in the snippet above is also noteworthy. This LaTex command provides the functionality for I wanted most for my cheat sheet‚Äîmultiple columns of content! I should point out that there are in_header, before_body, and after_body YAML options for customizing PDF output with {rmarkdown}. 5\r\nThese options are probably sufficient for most people‚Äôs customization needs (so using a custom template would not be necessary). But for me personally, the appeal of having ‚Äúcomplete‚Äù control of my output by using a template convinced me to forego these options. 6\r\nUsage\r\nSo, exactly how do you use a custom template with {rmarkdown}? It‚Äôs as simple as specifying the path to your template file with the template option in the YAML header of your Rmarkdown document. 7\r\nWhy This Way?\r\nBefore I was using Rstudio and {rmarkdown} to write my cheat sheets, I tried out a couple of LaTex editors 8. First, I tried the very popular Overleaf. It is well known and commonly used because it is web-based, allows the user to collaborate in real-time, and provides real-time previewing 9. However, there was just something that felt ‚Äúclunky‚Äù about the editor, and the ambiguity over package versions and usage was bothersome to me. 10 The other editor I tried for some time was TeXworks (with the pdftex distribution) Using the ‚ÄúTypset‚Äù command to generate my PDF output on an ad-hoc basis seemed to me to be a satisfactory workflow, but, among other things, I felt limited by the customization offered by TeXworks. 11\r\nAnd so I turned to Rstudio and {rmarkdown} and didn‚Äôt look back. While learning how to create a custom template was a (minor) inconvenience, it has paid off in a number of ways:\r\nI can use a familiar editor‚ÄîRstudio.\r\nI can use a familiar workflow‚Äîwriting in an Rmarkdown document and knitting to create my desired output.\r\nBecause I‚Äôm using {rmarkdown}, I can use {rmarkdown} functionality that is not available when solely writing in LaTex.\r\nThis last point is huge. The whole world of markdown syntax is valid! For example, I can add emphasis to text with markdown‚Äôs ** and __ tokens (instead of LaTex‚Äôs more ‚Äúverbose‚Äù syntax); I can use # to define section headers (which I just think is super ‚Äúelegant‚Äù); and I can use HTML comments to comments out multiple lines of text. (Note that native LaTex only has a single-line comment token‚Äî%. 12) Additionally, beyond just the markdown functionality, I can include R code thanks to added layer of functionality offered by {rmarkdown}.\r\nThe one big thing that I feel like I ‚Äúsacrificed‚Äù by moving to Rstudio and {rmarkdown} is the live preview feature that comes with Overleaf (and can be emulated with some configuration in other LaTex editors). Nonetheless, I feel like I get a reasonable facsimile of this feature with Rstudio‚Äôs functionality for inline previews of equations. 13 Below are examples of the preview capabilities for both single- and multi-line equations.\r\nA single-line equation previewA multi-line equation previewWhat Works for Me May Not Work For You\r\nAlthough what I‚Äôve described in this post has been working well for me‚Äîand I‚Äôd encourage others to try it out‚ÄîI don‚Äôt claim it to be the ‚Äúbest‚Äù solution for all of your cheat sheet needs. 14 If you‚Äôve got a workflow that works for you, that‚Äôs great! Keep using it! Be pragmatic.\r\nFor those unfamiliar with the concept of a cheat sheet, there‚Äôs no malice in it, despite what the moniker implies. From my experience, it is relatively common for teachers to let students use self-created note sheets (i.e.¬†cheat sheets) for aid with taking exams.‚Ü©Ô∏é\r\nin order to maximize the amount of space used for content, of course‚Ü©Ô∏é\r\nOne of the ones that I really liked was this one. However, it‚Äôs a bit more complex than I wanted. (This one implements a ‚Äústructure‚Äù in which one ‚Äúmain‚Äù tex file references several others with the \\input Latex command.)‚Ü©Ô∏é\r\nwhich was super helpful for a LaTex noob like me because it has comments explaining what specific lines/sections are doing‚Ü©Ô∏é\r\nSee the PDF chapter of the R Markdown book for some guidance with these.‚Ü©Ô∏é\r\nI‚Äôm sure I could create a perfectly fine cheat sheet using just these options, or, even re-create the output that I have achieved with my template.‚Ü©Ô∏é\r\nYou can specify other options as well, such as keep_latex: true for an alternative LaTex engine with latex_engine.‚Ü©Ô∏é\r\nand there are lots of them out there‚Ü©Ô∏é\r\nThe live preview feature is probably my favorite of all.‚Ü©Ô∏é\r\nOthers may view the hands-off approach to package management as an advantage of using Overleaf.‚Ü©Ô∏é\r\nPerhaps this is the fault of my own. Perhaps all the customization that I would like exists and I just have not discovered how to enable it.‚Ü©Ô∏é\r\nI realize that you can define custom commands or use a package to create multi-line comments in LaTex, but that ruins the point that I‚Äôm trying to make.‚Ü©Ô∏é\r\nSee the ‚ÄúShow equation and image previews‚Äù option in Tools > Global Options‚Ä¶ > R Markdown.‚Ü©Ô∏é\r\nI wouldn‚Äôt be surprised if I find a better workflow for myself in the future.\r\n\r\n‚Ü©Ô∏é\r\n",
    "preview": "post/cheat-sheet-rmarkdown/cheat-sheet-example.png",
    "last_modified": "2021-07-01T22:07:46-05:00",
    "input_file": {},
    "preview_width": 654,
    "preview_height": 925
  },
  {
    "path": "post/text-parsing-analysis-periodic-report/",
    "title": "Text Parsing and Analysis of a Periodic Report",
    "description": "With tidytext and Friends",
    "author": [
      {
        "name": "Tony ElHabr",
        "url": "https://twitter.com/TonyElHabr"
      }
    ],
    "date": "2019-06-29",
    "categories": [
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nA Brief Outline of the ‚ÄúProcess‚Äù\r\nThe Analysis\r\nTable of Contents (TOC)\r\nSentences\r\nWords\r\n\r\nThe End\r\n\r\nIntroduction\r\nThose of you non-academia folk who work in industry are probably conscious of any/all periodic reports that an independent entity publishes for your company‚Äôs industry. For example, in the insurance industry in the United States, the Federal Insurance Office of the U.S. Department of the Treasury publishes several reports on an annual basis discussing the industry at large, like this past year‚Äôs Annual Report on the Insurance Industry. (Admittedly, these kinds of reports are not always the most interesting things to read, but they are (usually) very informative.)\r\nThe same goes for the electricity grid and markets operated by the Electric Reliability Council of Texas (ERCOT) in Texas. Potomac Economics publishes an annual report ‚Äúproviding an independent assessment of the competitive performance and operational efficiency of the market‚Äù operated by ERCOT. This report tends to be pretty long‚Äîover 160 pages in every reports since 2016. A relatively large amount of the content is replication of language, figures, and tables from the prior years‚Äô report, substituting and updating numbers from the past year. 1 As an annual reader of said report, I wish it were easier to ‚Äúparse out‚Äù the most important things from a given year‚Äôs report. 2\r\nWith that in mind, I thought I would take a shot at using text analysis to compare Potomac Economics‚Äô annual reports on ERCOT for three years‚Äî2016, 2017, and 2018‚Äîto, among other things, identify the most ‚Äúunique‚Äù things in each report. This kind of analysis can enhance one‚Äôs understanding of current grid and market trends, as well as make one aware of things to look out for in the future.\r\nA Brief Outline of the ‚ÄúProcess‚Äù\r\nWhile I initially planned on doing some kind of code tutorial with this project (primarily to demonstrate techniques for parsing and cleaning PDF text with R), I found that the code became more complex than I would have liked 3. (Let‚Äôs just say that regular expressions were my best friend.) Instead, I decided to focus this post the visuals I created, providing some brief commentary where needed.\r\nNonetheless, I think a high-level description of my ‚Äúprocess‚Äù may be useful to the reader, as I think it is probably generalizable to any project of this likeness (i.e.¬†text analysis of similarly structured documents). (For those who are interested in the code, it can all be found in this GitHub repo.) My approach (using R, of course can be simplified to the following set of steps.\r\nDownload the reports to a local directory and create a data frame with the year and file location of the report.\r\nImport the raw text for each report to create a singular data frame with nested data frames storing the text for each year‚Äôs report.\r\n‚ÄúUnnest‚Äù the singular data frame to create a ‚Äúlong‚Äù data frame where each row represents a page in the reports.\r\nSplit this long data frame (with pages as observations) into two data frames, one for the table of contents (TOC) for each report and another for the body of each report.\r\nClean the two data frames (for TOC and body content) separately.\r\nCreate ‚Äúchild‚Äù data frames as needed for analysis, and make plots for key insights from this analysis.\r\nFrom this breakdown of the steps, it is evident that the effort that goes into the data pre-processing (i.e.¬†steps 1 through 5) makes up most of the work! What most people perceive to be ‚Äúdata analysis‚Äù is all in step 6. In this case, the saying about ‚Äú80% of data science is really data cleaning‚Äù could not be any truer.\r\nThe Analysis\r\nSo, what exactly does this report look like? Well, you could always open up one of the reports yourself and peruse the 160+ pages 4, but maybe the following screenshot of the 2018 report can provide some visual context. (If for nothing else, I thought this would be fun to look at.) I borrowed some code that data visualization phenom Nathan Yau demonstrated in a short write-up at his awesome website FlowingData.\r\n\r\nYou can really get an idea of the vast amount of charts and tables that make up each one of these reports just from this one figure!\r\nTable of Contents (TOC)\r\nNext, let‚Äôs begin to shape out this initial holistic perspective of the reports with some analysis of the table of contents (TOC). This may seem trivial, but I assure you that it‚Äôs not! With a large document like this, we really ought to have a solid understanding of the content that the document discusses.\r\nEach document‚Äôs TOC is broken down into a couple of sections, including:\r\nAn outline for the executive summary (on the first page of the TOC);\r\nAn outline of the first- and second-level sections of the body of the document (also starting on the first page of the TOC);\r\nA list of figures (starting on the second page of the TOC);\r\nA list of tables (on the fourth page).\r\nScreenshots of parts of the TOC of the 2018 report are shown below.\r\n\r\n\r\n\r\nA first question we might ask is ‚ÄúHow do (counts of) the lists of figures and tables correspond to the sections of the text?‚Äù (For the sake of readability, I‚Äôve truncated some of the section labels, e.g.¬†‚ÄúReview of Real-Time Market Outcomes‚Äù was truncated to just ‚ÄúRTM‚Äù, which is an abbreviation for Real-Time Market.)\r\n\r\nSo we see that the Real-Time Market (RTM) and Day-Ahead Market (DAM) sections seem to make a larger-than-equal share of the reports in terms of simple counts of figures and tables. We might hypothesize that these things are arguably the ‚Äúeasiest‚Äù things to track in a graphical or quantitative manner among the many aspects of electricity markets and grid operations. Conversely, we see that the Analysis section (truncated from ‚ÄúAnalysis of Competitive Performance‚Äù) leverages plots and tables the least. We might say that this supports our hypothesis that, in a few words, may be stated as ‚Äúeasy-to-quantify topics have more figures and tables‚Äù. The section labels ‚ÄúAnalysis‚Äù (shortend from ‚ÄúAnalysis of Competitive Performance‚Äù for visualization purposes) suggests that its content is ‚Äúdeeper‚Äù in nature, and that its may not be quite as easy to illustrate via figures and tables.\r\nOur initial hypothesis‚Äîthat easy-to-quantify topics have more figures and tables‚Äîseems reasonable enough, but is it just a direct consequence of the sections having more pages? We can plot the number of pages per section against the count of sections to help us answer this question.\r\n\r\nSo it seems that those sections having more pages do NOT necessarily have more figures and tables. So our hypothesis still seems reasonable.\r\nYou might have noticed that the first plot (the ‚Äútreemap‚Äù) only showed data for the 2018 report. I didn‚Äôt deem it necessary/helpful to make the same plot for each of the three reports from 2016 through 2018 because, as it turns out, the TOC of the three reports are nearly identical in composition! (Really, this is probably unsurprising.) That is, they have identical‚Äîor near identical‚Äînames and indexes for sections, figures, and tables. From an analysis point of view, this is good‚Äìthe structure of the three reports facilitates direct comparison.\r\nBut note that I say that the TOCs as nearly identical, not exactly identical. What exactly are the differences between the three? More specifically, we might be curious to know which figures and tables were only in one of the three reports.\r\n\r\nAha! We see that the 2016 and 2018 reports had more than a handful of figures and tables that were unique to those reports. The table below lists exactly which figures and tables those are.\r\nYear\r\nSection\r\nType\r\nLabel\r\n2016\r\nRTM\r\nfigure\r\nimplied heat rate duration curve top 2 percent of hours\r\n2016\r\nRTM\r\nfigure\r\naggregated peak hour generation offer stack\r\n2016\r\nRTM\r\nfigure\r\nload, reserves and prices in august\r\n2016\r\nSupply/Demand\r\nfigure\r\nload duration curve top five percent of hours\r\n2016\r\nSupply/Demand\r\nfigure\r\nhistoric coal generation and capacity factor\r\n2016\r\nSupply/Demand\r\nfigure\r\ntop and bottom ten percent of net load\r\n2016\r\nReliability\r\nfigure\r\nfrequency of reliability unit commitments\r\n2016\r\nReliability\r\nfigure\r\naverage on-line summer reserves\r\n2016\r\nReliability\r\nfigure\r\npotential for combined cycle capacity available to ruc in houston\r\n2016\r\nAnalysis\r\nfigure\r\nsurplus capacity\r\n2016\r\nRTM\r\ntable\r\n15-minute price changes as a percentage of annual average prices\r\n2016\r\nCongestion\r\ntable\r\nirresolvable elements\r\n2017\r\nSupply/Demand\r\nfigure\r\nload duration curve top five percent of hours with highest load\r\n2017\r\nSupply/Demand\r\nfigure\r\nenergy transacted across dc ties in august\r\n2018\r\nRTM\r\nfigure\r\nercot rena analysis\r\n2018\r\nRTM\r\nfigure\r\naverage real-time energy price spikes\r\n2018\r\nRTM\r\nfigure\r\nmonthly load exposure\r\n2018\r\nDAM\r\nfigure\r\ndaily collateral\r\n2018\r\nDAM\r\nfigure\r\naverage costs of procured sasm ancillary services\r\n2018\r\nDAM\r\nfigure\r\nercot-wide net ancillary service shortages\r\n2018\r\nDAM\r\nfigure\r\nqse-portfolio net ancillary service shortages\r\n2018\r\nCongestion\r\nfigure\r\n[year] crr auction revenue\r\n2018\r\nSupply/Demand\r\nfigure\r\nload duration curve top 5% of hours with highest load\r\n2018\r\nSupply/Demand\r\nfigure\r\nannual energy transacted across dc ties\r\n2018\r\nReliability\r\nfigure\r\ncapacity commitment timing july and august hour ending 17\r\n2018\r\nReliability\r\nfigure\r\nreal-time to cop comparisons for renewable capacity\r\n2018\r\nReliability\r\nfigure\r\nreal-time to cop comparisons for thermal capacity\r\n2018\r\nReliability\r\nfigure\r\nstandard deviations of real-time to cop capacity differences\r\n2018\r\nResource Adequacy\r\nfigure\r\nwest zone net revenues\r\n2018\r\nResource Adequacy\r\nfigure\r\nnet revenues by generation resource type\r\n2018\r\nAnalysis\r\nfigure\r\nderating, planned outages and forced outages\r\n2018\r\nReliability\r\ntable\r\nruc settlement\r\nOk, enough about the TOC. Let‚Äôs see what kind of things we can learn about the text.\r\nSentences\r\nWe saw that some sections have many more figures and tables than others and that this does not necessarily correlate with the number of pages in the section. Is there some kind of correlation with the number of sentences of text in each section?\r\n\r\nIt may (or may not) be surprising to find a lack of a relationship here. Given our previous finding that the number of figures and tables and the number of pages in a given section are not really related. 5\r\nNext, I think it is interesting to look at how the counts of sentences per section has changed over time.\r\n\r\nThe first thing we might notice from the plot is that the number of sentences has increased across all sections from their totals in 2016. Ok, so maybe that‚Äôs not so interesting‚ÄîI think it‚Äôs reasonable to assume that an annual report like this incrementally adds on its ‚Äúfoundation‚Äù from the prior year(s).\r\nPerhaps the most interesting that we might observe from this graph is the ‚Äúleaps‚Äù in the sentence counts from 2017 to 2018 for the Reliability and Resource Adequacy sections. One might draw a connection between this and what we observed earlier when looking at the figures and tables that were unique to a single report. There were more than a handful of observations for Resource Adequacy and Reliability (2 and 5 respectively) exclusive to the 2018 report.\r\nA final takeaway that I have from this chart is the ‚Äúevenness‚Äù of the counts across the sections (which wasn‚Äôt quite as obvious in the previous plot depicting sentence counts). The difference between the maximum and the minimum number of sentences per section in a given year is always below 50. Personally, I might have expected greater variation. Either way, it doesn‚Äôt really say anything about the ‚Äúgoodness‚Äù of the content; this is just something‚Äîthe count of sentences in a section of a long document‚Äîfor which I don‚Äôt really have a strong prior knowledge. 6\r\nOk, so the prior plot wasn‚Äôt so complex, so let‚Äôs now look at something more complicated and which deserves some explanation.\r\nOne of the things that I really wanted to investigate when I started this analysis was ‚Äútext similarity‚Äù‚Äîjust how much of each report is ‚Äúboiler-plate, copy-paste‚Äù text? There are lots of ways of going about quantifying this. (Just do an Internet search for Natural Language Processing (NLP) and word embeddings and see what kind of rabbit hole that takes you down.) I decided to use cosine similarity 7 of sentences. 8\r\nTo provide a kind of ‚Äúbaseline‚Äù expectation of what these cosine similarity values might be, see the table below. It provides a handful of simple examples of the cosine similarity for singular strings of characters. 9\r\nCase\r\nDescription\r\nString 1\r\nString 2\r\nCosine Similarity\r\n1\r\nIdentical strings.\r\nabcde\r\nabcde\r\n1.00\r\n2\r\nOne different character (‚Äúz‚Äù in String 2 instead of ‚Äúa‚Äù).\r\nabcde\r\nzbcde\r\n0.80\r\n3\r\nTwo different characters (‚Äúz‚Äù and ‚Äúy‚Äù in String 2 instead of ‚Äúa‚Äù and ‚Äúb‚Äù).\r\nabcde\r\nzycde\r\n0.60\r\n4\r\nAll different characters.\r\nabcde\r\nfghij\r\n0.00\r\n5\r\nDifferent ordering of characters, but identical characters.\r\nabcde\r\nbaecd\r\n1.00\r\n6\r\nRepeated characters (‚Äúa‚Äù in String 2), one-character difference in string lengths.\r\nabcde\r\naabcde\r\n0.95\r\n7\r\nRepeated characters (‚Äúa‚Äù twice in String 2), two-character difference in string lengths.\r\nabcde\r\naaabcde\r\n0.87\r\n8\r\nSame characters, one additional character (‚Äúz‚Äù in String 2) in one string.\r\nabcde\r\nabcdef\r\n0.91\r\n9\r\nSame characters, one additional character (‚Äúe‚Äù in String 1) in one string.\r\nabcde\r\nabcd\r\n0.89\r\nCases 2 and 3 in the table above are probably the most illustrative of the cosine similarity calculation. In case 2, a one-character difference in two strings having five characters total results in a value of 0.8. (i.e.¬†Four of five characters matched.) In case 3, a two-character difference (given the same setup) results in a value of 0.6. (i.e.¬†Three of five characters matched.)\r\nNote that the cosine similarity calculation is identical for different types of text tokens (i.e.¬†sentences, paragraphs, etc.). In general, the calculated values are likely to be smaller for longer tokens.\r\nOk, so given the above examples and explanation, we now have some context for understanding (and appreciating) the high degree of sentence similarity demonstrated across the reports, as illustrated in the figure below.\r\n\r\nIf you have read the reports, you‚Äôll realize that there are lots of ‚Äúrepeated‚Äù sentences across the documents. For example, take the description of the first figure in 2018: ‚ÄúFigure 1 summarizes changes in energy prices and other market costs by showing the all-in price of electricity, which is a measure of the total cost of serving load in ERCOT for 2016 through 2018.‚Äù The ‚Äúsame‚Äù sentence in the 2017 report is ‚ÄúFigure 1 summarizes changes in energy prices and other market costs by showing the all-in price of electricity, which is a measure of the total cost of serving load in ERCOT for 2015 through 2017.‚Äù Just by inspection, you can tell that these two sentences would have a cosine similarity almost equal to 1. In fact, I did some extra text-processing‚Äîmost notably, replacing year and months with generic labels, e.g.¬†‚Äú[year]‚Äù and ‚Äú[month]‚Äù‚Äîthat would make these two sentences appear exactly identical (and, consequently, have a cosine similarity of exactly 1). This whole effort to quantify of sentence similarity was probably the most interesting thing to me out of this entire analysis. 10\r\nI think that‚Äôs enough about sentences. Let‚Äôs next explore what we might learn from just the words.\r\nWords\r\nOne of the best ways to gauge ‚Äúimportance‚Äù of words (or any kind of text token) across multiple documents is term-frequency, inverse-document-frequency (TF-IDF). 11\r\nFirst, let‚Äôs use TFIDF to identify which sections have the most ‚Äúunique‚Äù words (aggregating across the three reports).\r\n\r\nSo we see that the Analysis section appears to be the most unique from this perspective. This deduction can be interpreted as an extension of the hypothesis developed earlier‚Äîthat easy-to-quantify topics have more figures and tables. The extension of this notion can be stated explicitly as ‚Äúeasy-to-quantify topics‚Äù have less unique words.\r\nWhat exactly are some of the words that were identified as the most unique?\r\n\r\nThe ‚Äúchatter‚Äù plot 12 above depicts the most unique words identified by TFIDF (after applying common text processing techniques such as ‚Äústemming‚Äù and dropping ‚Äústop words‚Äù). Several of the words shown happen to be words associated with sources of high electric transmission congestion 13 in the Texas grid during the year for the which the report was made. This set of words could be potentially leveraged as a starting point for an ambitious reader who is curious to learn more about the Texas electricity grid.\r\nThe End\r\nI had a really fun time exploring this topic. I encourage any readers to do something similar with periodic or standardized reports that are relevant to you in some way. If they are boring to you (yet you must read them for whatever reason), then a text analysis like that demonstrated here can really spark some life into your consumption of the report!\r\n\r\nThis is not a knock on the report whatsoever‚Äîannual reports will inevitably have redundancy like this! In fact, I really do enjoy the report. The explanations of the various (and sometimes complex) aspects of the electric reliability and market operation are fantastic.‚Ü©Ô∏é\r\nThe executive summary section at the beginning of each report actually does a very good job of this already, but I was curious if we could identify ‚Äúhighlights‚Äù from a given year in a more quantitative/rigorous manner.‚Ü©Ô∏é\r\nNot to say that it is extremely complex, but it‚Äôs not exactly trivial.‚Ü©Ô∏é\r\nThe 2016 report has 160 pages total, and the 2017 and 2018 reports are even longer, so we can say 160 pages is a lower bound for our purposes. These totals include the title page, blank pages, and the table of contents, so its an overestimation of the actual content presented in the report.‚Ü©Ô∏é\r\nA similar plot could have been made for the counts of figures and tables, but, as I hinted at before, doing so was would have been trivial since those totals weren‚Äôt much different across the three reports. (That‚Äôs why it was more interesting to look at the differences in the figures and tables included in the reports.)‚Ü©Ô∏é\r\nThat‚Äôs the cool thing about plotting this kind of data‚Äîyou gain an understanding of something that you wouldn‚Äôt have known otherwise, which often invites further questions and insight. This is true of data visualization in general‚Ü©Ô∏é\r\nalso employing normalization and penalization‚Ü©Ô∏é\r\nI might have instead tried using Jaccard Similarity, word mover‚Äôs distance, or one of the several others described eloquently in this blog post; and, furthermore, I could have used other forms of text tokens, such as words (more granular) or paragraphs (less granular).‚Ü©Ô∏é\r\nThese calculations employ the {tidystringdist} package, written by the one-and-only Colin Fay.‚Ü©Ô∏é\r\nIf nothing else, this part of the analysis was a great opportunity to experiment with the wonderful {text2vec} package.‚Ü©Ô∏é\r\nIf you‚Äôre looking to do text analysis using R, then I highly recommend reading through the wonderful Tidy Text Mining with R book by David Robinson and Julia Silge‚Ü©Ô∏é\r\nwhich, in my opinion, is a better format for illustrating word importance compared to a word cloud‚Ü©Ô∏é\r\nThis concept is analogous to traffic congestion, but for electricity.‚Ü©Ô∏é\r\n",
    "preview": "post/text-parsing-analysis-periodic-report/viz_words_section_tfidf.PNG",
    "last_modified": "2021-07-01T22:09:48-05:00",
    "input_file": {},
    "preview_width": 3600,
    "preview_height": 2400
  },
  {
    "path": "post/rstudio-conf-2019-summary/",
    "title": "Summarizing rstudio conf 2019 Summaries",
    "description": "A Meta Analysis Using Tidy Text Techniques",
    "author": [
      {
        "name": "Tony ElHabr",
        "url": "https://twitter.com/TonyElHabr"
      }
    ],
    "date": "2019-01-27",
    "categories": [
      "r"
    ],
    "contents": "\r\nTo be honest, I planned on writing a review of this past weekend‚Äôs rstudio::conf 2019, but several other people have already done a great job of doing that‚Äîjust check out Karl Broman‚Äôs aggregation of reviews at the bottom of the page here! (More on this in a second.) In short, my thoughts on the whole experience are captured perfectly by Nick Strayer‚Äôs tweet the day after the conference ended.\r\n\r\nAnyways, I figured that this was the perfect opportunity to do some text analysis. Why not extract the text from the reviews of others‚Äîlinked in Karl‚Äôs repo‚Äîand make my own summary of the event? Plotting word frequencies and sentiments, while not exactly ‚Äúcutting edge‚Äù compared to robust natural language processing techniques, is perfect for getting a nice, simple overview of the conference. (I have applied some of the techniques described by David Robinson and Julia Silge in their Tidy Text Mining with R book\r\nMoreover, after reading Bob Rudis‚Äôs recent post and discovering his work-in-progress \" {curl} +{httr} + {rvest}\" package called {reapr}, I realized that the work of cleaning the HTML and text for each person‚Äôs blog post would not be so bad. In fact, it turned out to be as easy as reapr::reap_url() %>% reapr::mill() (with only a little bit of extra work :smile:).\r\nAfter trying a couple of different approaches (e.g.¬†bigrams, topic modeling, etc.) and experimenting with some different visualizations, I ended up making the two plots below. (To the interested reader, I‚Äôve included all of the code at the bottom of this post.) From the second plot‚Äîwhere positive sentiment heavily outweighs negative sentiment‚Äîone thing is clear: the R community is super supportive and positive, just as Nick alluded to in his tweet.\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "post/rstudio-conf-2019-summary/headliner.jpg",
    "last_modified": "2021-07-01T22:11:30-05:00",
    "input_file": {}
  },
  {
    "path": "post/nested-json-to-tidy-data-frame-r/",
    "title": "Converting nested JSON to a tidy data frame with R",
    "description": "Web Scraping Made Easier",
    "author": [
      {
        "name": "Tony ElHabr",
        "url": "https://twitter.com/TonyElHabr"
      }
    ],
    "date": "2018-10-20",
    "categories": [
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nGetting the data\r\nParsing the data\r\nPost-processing the data\r\nSign-off\r\n\r\nUPDATE: The data retrieval demonstrated in this post no longer seems to work due to a change in the ESPN‚ÄôS ‚Äúsecret‚Äù API. In any matter, the techniques for working with JSON data are still valid.\r\nIn this ‚Äúhow-to‚Äù post, I want to detail an approach that others may find useful for converting nested (nasty!) json to a tidy (nice!) data.frame/tibble that is should be much easier to work with. 1\r\nFor this demonstration, I‚Äôll start out by scraping National Football League (NFL) 2018 regular season week 1 score data from ESPN, which involves lots of nested data in its raw form. 2\r\nThen, I‚Äôll work towards getting the data in a workable format (a data.frame!). (This is the crux of what I want to show.) Finally, I‚Äôll filter and wrangle the data to generate a final, presentable format.\r\nEven if one does not care for sports and knows nothing about the NFL, I believe that the techniques that I demonstrate are generalizable to a broad set of JSON-related ‚Äúproblems‚Äù.\r\nGetting the data\r\nLet‚Äôs being with importing the package(s) that we‚Äôll need.\r\nNext, we‚Äôll create a variable for the url from which we will get the data. The url here will request the scores for week 1 of the 2018 NFL season from ESPN‚Äôs ‚Äúsecret‚Äù API. 3\r\n\r\n\r\nurl <- \"http://site.api.espn.com/apis/site/v2/sports/football/nfl/scoreboard?&dates=2018&seasontype=2&week=1\"\r\n\r\n\r\n\r\nAnd now, the actual HTTP GET request for the data (using the {httr} package‚Äôs appropriately named GET() function).\r\n\r\n\r\nresp <- httr::GET(url)\r\nresp\r\n\r\n\r\n\r\n\r\n\r\n## Response [http://site.api.espn.com/apis/site/v2/sports/football/nfl/scoreboard?&dates=2018&seasontype=2&week=1]\r\n##   Date: 2018-10-24 18:41\r\n##   Status: 200\r\n##   Content-Type: application/json;charset=UTF-8\r\n##   Size: 189 kB\r\n\r\n\r\n\r\nEverything seems to be going well. However, after using another handy {httr} function‚Äîcontent()‚Äîto extract the data, we see that the data is an nasty nested format! (I only print out some of the top-level elements to avoid cluttering the page.)\r\n\r\n\r\ncont_raw <- httr::content(resp)\r\nstr(cont_raw, max.level = 3, list.len = 4)\r\n\r\n\r\n\r\n\r\n\r\n## List of 4\r\n##  $ leagues:List of 1\r\n##   ..$ :List of 11\r\n##   .. ..$ id                 : chr \"28\"\r\n##   .. ..$ uid                : chr \"s:20~l:28\"\r\n##   .. ..$ name               : chr \"National Football League\"\r\n##   .. ..$ abbreviation       : chr \"NFL\"\r\n##   .. .. [list output truncated]\r\n##  $ season :List of 2\r\n##   ..$ type: int 2\r\n##   ..$ year: int 2018\r\n##  $ week   :List of 1\r\n##   ..$ number: int 1\r\n##  $ events :List of 16\r\n##   ..$ :List of 9\r\n##   .. ..$ id          : chr \"401030710\"\r\n##   .. ..$ uid         : chr \"s:20~l:28~e:401030710\"\r\n##   .. ..$ date        : chr \"2018-09-07T00:55Z\"\r\n##   .. ..$ name        : chr \"Atlanta Falcons at Philadelphia Eagles\"\r\n##   .. .. [list output truncated]\r\n##   ..$ :List of 9\r\n##   .. ..$ id          : chr \"401030718\"\r\n##   .. ..$ uid         : chr \"s:20~l:28~e:401030718\"\r\n##   .. ..$ date        : chr \"2018-09-09T17:00Z\"\r\n##   .. ..$ name        : chr \"Pittsburgh Steelers at Cleveland Browns\"\r\n##   .. .. [list output truncated]\r\n##   ..$ :List of 9\r\n##   .. ..$ id          : chr \"401030717\"\r\n##   .. ..$ uid         : chr \"s:20~l:28~e:401030717\"\r\n##   .. ..$ date        : chr \"2018-09-09T17:00Z\"\r\n##   .. ..$ name        : chr \"Cincinnati Bengals at Indianapolis Colts\"\r\n##   .. .. [list output truncated]\r\n##   ..$ :List of 9\r\n##   .. ..$ id          : chr \"401030716\"\r\n##   .. ..$ uid         : chr \"s:20~l:28~e:401030716\"\r\n##   .. ..$ date        : chr \"2018-09-09T17:00Z\"\r\n##   .. ..$ name        : chr \"Tennessee Titans at Miami Dolphins\"\r\n##   .. .. [list output truncated]\r\n##   .. [list output truncated]\r\n\r\n\r\n\r\nParsing the data\r\nGiven the nature of the data, we might hope that the {jsonlite} package will save us here. However, straightforward usage of it‚Äôs fromJSON() package only reduces the mess a bit.\r\n\r\n\r\ndf_raw_ugly <- jsonlite::fromJSON(rawToChar(resp$content))\r\nglimpse(df_raw_ugly, max.level = 3, list.len = 4)\r\n\r\n\r\n\r\n\r\n\r\n## List of 4\r\n##  $ leagues:'data.frame': 1 obs. of  11 variables:\r\n##   ..$ id                 : chr \"28\"\r\n##   ..$ uid                : chr \"s:20~l:28\"\r\n##   ..$ name               : chr \"National Football League\"\r\n##   ..$ abbreviation       : chr \"NFL\"\r\n##   .. [list output truncated]\r\n##  $ season :List of 2\r\n##   ..$ type: int 2\r\n##   ..$ year: int 2018\r\n##  $ week   :List of 1\r\n##   ..$ number: int 1\r\n##  $ events :'data.frame': 16 obs. of  9 variables:\r\n##   ..$ id          : chr [1:16] \"401030710\" \"401030718\" \"401030717\" \"401030716\" ...\r\n##   ..$ uid         : chr [1:16] \"s:20~l:28~e:401030710\" \"s:20~l:28~e:401030718\" \"s:20~l:28~e:401030717\" \"s:20~l:28~e:401030716\" ...\r\n##   ..$ date        : chr [1:16] \"2018-09-07T00:55Z\" \"2018-09-09T17:00Z\" \"2018-09-09T17:00Z\" \"2018-09-09T17:00Z\" ...\r\n##   ..$ name        : chr [1:16] \"Atlanta Falcons at Philadelphia Eagles\" \"Pittsburgh Steelers at Cleveland Browns\" \"Cincinnati Bengals at Indianapolis Colts\" \"Tennessee Titans at Miami Dolphins\" ...\r\n##   .. [list output truncated]\r\n\r\n\r\n\r\nOne could go on and try some other functions from the {jsonlite} package (or another JSON-related package), but, in my own attempts, I was unable to figure out a nice way of getting a data.frame(). (This is not to say that there is something wrong with the package‚ÄîI simply could not figure out how to use it to get the result that I wanted.)\r\nSo, what to do now? Well, after some struggling, I stumbled upon the following solution to put me on the right path.\r\n\r\n\r\ndf_raw <- enframe(unlist(cont_raw))\r\ndf_raw\r\n\r\n\r\n\r\n\r\n\r\n## # A tibble: 6,629 x 2\r\n##    name                     value                   \r\n##    <chr>                    <chr>                   \r\n##  1 leagues.id               28                      \r\n##  2 leagues.uid              s:20~l:28               \r\n##  3 leagues.name             National Football League\r\n##  4 leagues.abbreviation     NFL                     \r\n##  5 leagues.slug             nfl                     \r\n##  6 leagues.season.year      2018                    \r\n##  7 leagues.season.startDate 2018-08-02T07:00Z       \r\n##  8 leagues.season.endDate   2019-02-06T07:59Z       \r\n##  9 leagues.season.type.id   2                       \r\n## 10 leagues.season.type.type 2                       \r\n## # ... with 6,619 more rows\r\n\r\n\r\n\r\nCombining unlist() and tibble::enframe(), we are able to get a (very) long data.frame without any nested elements! Note that the would-have-been-nested elements are joined by ‚Äú.‚Äù in the ‚Äúname‚Äù column, and the values associated with these elements are in the ‚Äúvalue‚Äù column. (These are the default column names that tibble::enframe() assigns to the tibble that it creates from a list.)\r\nWhile this tibble is still not in a tidy format‚Äîthere are variables implicitly stored in the ‚Äúname‚Äù column rather than in their own columns‚Äî‚Äìit‚Äôs in a much more user-friendly format (in my opinion). (e.g The variable \"leagues.season.startDate\" implicitly encodes three variables‚Äî\"leagues\", \"season\", and \"startDate\"‚Äîeach deserving of their own column.)\r\nGiven the format of the implicit variable sin the ‚Äúname‚Äù column, We can use tidyr::separate() to create columns for each.\r\n\r\n\r\ndf_raw %>% separate(name, into = c(paste0(\"x\", 1:10)))\r\n\r\n\r\n\r\n\r\n\r\n## Warning: Expected 10 pieces. Missing pieces filled with `NA` in 6629\r\n## rows [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\r\n## 20, ...].\r\n\r\n## # A tibble: 6,629 x 11\r\n##    x1     x2      x3    x4    x5    x6    x7    x8    x9    x10   value   \r\n##    <chr>  <chr>   <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr>   \r\n##  1 leagu~ id      <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  28      \r\n##  2 leagu~ uid     <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  s:20~l:~\r\n##  3 leagu~ name    <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  Nationa~\r\n##  4 leagu~ abbrev~ <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  NFL     \r\n##  5 leagu~ slug    <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  nfl     \r\n##  6 leagu~ season  year  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  2018    \r\n##  7 leagu~ season  star~ <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  2018-08~\r\n##  8 leagu~ season  endD~ <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  2019-02~\r\n##  9 leagu~ season  type  id    <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  2       \r\n## 10 leagu~ season  type  type  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  2       \r\n## # ... with 6,619 more rows\r\n\r\n\r\n\r\nWe get a warning indicating when using separate() because we have ‚Äúover-estimated‚Äù how many columns we will need to create. Note that, with my specification of (dummy) column names with the into argument, I guessed that there we would need 10 columns. Why 10? Because I expected that 10 would be more than I needed, and it‚Äôs better to over-estimate and remove the extra columns in a subsequent step than to under-estimate and lose data because there are not enough columns to put the ‚Äúseparated‚Äù data in.\r\nWe can get rid of the warning by providing an appropriate value for separate()‚Äôs fill argument. (Note that \"warn\" is the default value of the fill argument.)\r\n\r\n\r\ndf_raw %>% separate(name, into = c(paste0(\"x\", 1:10)), fill = \"right\")\r\n\r\n\r\n\r\n\r\n\r\n## # A tibble: 6,629 x 11\r\n##    x1     x2      x3    x4    x5    x6    x7    x8    x9    x10   value   \r\n##    <chr>  <chr>   <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr>   \r\n##  1 leagu~ id      <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  28      \r\n##  2 leagu~ uid     <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  s:20~l:~\r\n##  3 leagu~ name    <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  Nationa~\r\n##  4 leagu~ abbrev~ <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  NFL     \r\n##  5 leagu~ slug    <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  nfl     \r\n##  6 leagu~ season  year  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  2018    \r\n##  7 leagu~ season  star~ <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  2018-08~\r\n##  8 leagu~ season  endD~ <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  2019-02~\r\n##  9 leagu~ season  type  id    <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  2       \r\n## 10 leagu~ season  type  type  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  2       \r\n## # ... with 6,619 more rows\r\n\r\n\r\n\r\nHowever, while this action gets rid of the warning, it does not actually resolve the underlying issue‚Äîspecifying the correct number of columns to create with separate(). We can do that by identifying the name with the most number of ‚Äúdots‚Äù (i.e.¬†.s).\r\n\r\n\r\nrgx_split <- \"\\\\.\"\r\nn_cols_max <-\r\n  df_raw %>%\r\n  pull(name) %>% \r\n  str_split(rgx_split) %>% \r\n  map_dbl(~length(.)) %>% \r\n  max()\r\nn_cols_max\r\n\r\n\r\n\r\n\r\n\r\n## [1] 7\r\n\r\n\r\n\r\nWith this number (7) identified, we can now choose the ‚Äúcorrect‚Äù number of columns to create with separate(). Note that we‚Äôll still be left with lots of NA values (corresponding to rows that don‚Äôt have the maximum number of variables). This is expected.\r\n\r\n\r\nnms_sep <- paste0(\"name\", 1:n_cols_max)\r\ndf_sep <-\r\n  df_raw %>% \r\n  separate(name, into = nms_sep, sep = rgx_split, fill = \"right\")\r\ndf_sep\r\n\r\n\r\n\r\n\r\n\r\n## # A tibble: 6,629 x 8\r\n##    name1   name2      name3    name4 name5 name6 name7 value              \r\n##    <chr>   <chr>      <chr>    <chr> <chr> <chr> <chr> <chr>              \r\n##  1 leagues id         <NA>     <NA>  <NA>  <NA>  <NA>  28                 \r\n##  2 leagues uid        <NA>     <NA>  <NA>  <NA>  <NA>  s:20~l:28          \r\n##  3 leagues name       <NA>     <NA>  <NA>  <NA>  <NA>  National Football ~\r\n##  4 leagues abbreviat~ <NA>     <NA>  <NA>  <NA>  <NA>  NFL                \r\n##  5 leagues slug       <NA>     <NA>  <NA>  <NA>  <NA>  nfl                \r\n##  6 leagues season     year     <NA>  <NA>  <NA>  <NA>  2018               \r\n##  7 leagues season     startDa~ <NA>  <NA>  <NA>  <NA>  2018-08-02T07:00Z  \r\n##  8 leagues season     endDate  <NA>  <NA>  <NA>  <NA>  2019-02-06T07:59Z  \r\n##  9 leagues season     type     id    <NA>  <NA>  <NA>  2                  \r\n## 10 leagues season     type     type  <NA>  <NA>  <NA>  2                  \r\n## # ... with 6,619 more rows\r\n\r\n\r\n\r\nBy my interpretation, this df_sep variable is in tidy format. (Of course, there is still lots of cleaning to be done before it can actually be useful!)\r\nPost-processing the data\r\nGetting the raw data in the format that df_sep is what I primarily wanted to show. Nonetheless, there‚Äôs more to the story! (Reminder: We‚Äôre seeking to get the scores from the 16 games in week 1 of the NFL‚Äôs 2018 regular season.) How can we work with the NAs to get a final format that is actually presentable?\r\nWe continue by filter the tibble for only the rows that we will need.\r\n\r\n\r\ndf_filt <-\r\n  df_sep %>%\r\n  filter(\r\n    (\r\n      name1 == \"events\" &\r\n        name2 == \"shortName\"\r\n    ) |\r\n      (\r\n        name1 == \"events\" &\r\n          name2 == \"competitions\" &\r\n          name3 == \"date\"\r\n      ) | (\r\n        name1 == \"events\" &\r\n          name2 == \"competitions\" &\r\n          name3 == \"status\" &\r\n          name4 == \"type\" &\r\n          name5 == \"name\"\r\n      ) |\r\n      (\r\n        name1 == \"events\" &\r\n          name2 == \"competitions\" &\r\n          name3 == \"competitors\" &\r\n          name4 == \"score\"\r\n      )\r\n  )\r\ndf_filt\r\n\r\n\r\n\r\n\r\n\r\n## # A tibble: 80 x 8\r\n##    name1  name2        name3      name4 name5 name6 name7 value           \r\n##    <chr>  <chr>        <chr>      <chr> <chr> <chr> <chr> <chr>           \r\n##  1 events shortName    <NA>       <NA>  <NA>  <NA>  <NA>  ATL @ PHI       \r\n##  2 events competitions date       <NA>  <NA>  <NA>  <NA>  2018-09-07T00:5~\r\n##  3 events competitions competito~ score <NA>  <NA>  <NA>  18              \r\n##  4 events competitions competito~ score <NA>  <NA>  <NA>  12              \r\n##  5 events competitions status     type  name  <NA>  <NA>  STATUS_FINAL    \r\n##  6 events shortName    <NA>       <NA>  <NA>  <NA>  <NA>  PIT @ CLE       \r\n##  7 events competitions date       <NA>  <NA>  <NA>  <NA>  2018-09-09T17:0~\r\n##  8 events competitions competito~ score <NA>  <NA>  <NA>  21              \r\n##  9 events competitions competito~ score <NA>  <NA>  <NA>  21              \r\n## 10 events competitions status     type  name  <NA>  <NA>  STATUS_FINAL    \r\n## # ... with 70 more rows\r\n\r\n\r\n\r\nNext, we‚Äôll create appropriately named columns for the values that we filtered for in the step above. 4\r\n\r\n\r\ndf_clean1 <-\r\n  df_filt %>%\r\n  select(name3, name4, name5, value) %>%\r\n  mutate(status = if_else(name5 == \"name\", value, NA_character_)) %>%\r\n  mutate(isscore = if_else(name4 == \"score\", TRUE, FALSE)) %>%\r\n  mutate(datetime = if_else(\r\n    name3 == \"date\",\r\n    str_replace_all(value, \"\\\\s?T\\\\s?\", \" \") %>% str_replace(\"Z$\", \"\"),\r\n    NA_character_\r\n  )) %>%\r\n  mutate(gm = if_else(\r\n    is.na(isscore) &\r\n      is.na(datetime) & is.na(status),\r\n    value,\r\n    NA_character_\r\n  ))\r\ndf_clean1\r\n\r\n\r\n\r\n\r\n\r\n## # A tibble: 80 x 8\r\n##    name3    name4 name5 value       status    isscore datetime     gm     \r\n##    <chr>    <chr> <chr> <chr>       <chr>     <lgl>   <chr>        <chr>  \r\n##  1 <NA>     <NA>  <NA>  ATL @ PHI   <NA>      NA      <NA>         ATL @ ~\r\n##  2 date     <NA>  <NA>  2018-09-07~ <NA>      NA      2018-09-07 ~ <NA>   \r\n##  3 competi~ score <NA>  18          <NA>      TRUE    <NA>         <NA>   \r\n##  4 competi~ score <NA>  12          <NA>      TRUE    <NA>         <NA>   \r\n##  5 status   type  name  STATUS_FIN~ STATUS_F~ FALSE   <NA>         <NA>   \r\n##  6 <NA>     <NA>  <NA>  PIT @ CLE   <NA>      NA      <NA>         PIT @ ~\r\n##  7 date     <NA>  <NA>  2018-09-09~ <NA>      NA      2018-09-09 ~ <NA>   \r\n##  8 competi~ score <NA>  21          <NA>      TRUE    <NA>         <NA>   \r\n##  9 competi~ score <NA>  21          <NA>      TRUE    <NA>         <NA>   \r\n## 10 status   type  name  STATUS_FIN~ STATUS_F~ FALSE   <NA>         <NA>   \r\n## # ... with 70 more rows\r\n\r\n\r\n\r\nWith these columns created, we can use tidyr::fill() and dplyr::filter() in a strategic manner to get rid of all the NAs cluttering our tibble. Additionally, we can drop the dummy name columns that we created with the tidyr::separate() call before.\r\n\r\n\r\ndf_clean2 <-\r\n  df_clean1 %>% \r\n  fill(status, .direction = \"up\") %>%\r\n  filter(status == \"STATUS_FINAL\") %>%\r\n  fill(gm, .direction = \"down\") %>%\r\n  fill(datetime, .direction = \"down\") %>%\r\n  filter(name3 == \"competitors\") %>% \r\n  select(-matches(\"name[0-9]\"))\r\ndf_clean2\r\n\r\n\r\n\r\n\r\n\r\n## # A tibble: 32 x 5\r\n##    value status       isscore datetime         gm       \r\n##    <chr> <chr>        <lgl>   <chr>            <chr>    \r\n##  1 18    STATUS_FINAL TRUE    2018-09-07 00:55 ATL @ PHI\r\n##  2 12    STATUS_FINAL TRUE    2018-09-07 00:55 ATL @ PHI\r\n##  3 21    STATUS_FINAL TRUE    2018-09-09 17:00 PIT @ CLE\r\n##  4 21    STATUS_FINAL TRUE    2018-09-09 17:00 PIT @ CLE\r\n##  5 23    STATUS_FINAL TRUE    2018-09-09 17:00 CIN @ IND\r\n##  6 34    STATUS_FINAL TRUE    2018-09-09 17:00 CIN @ IND\r\n##  7 27    STATUS_FINAL TRUE    2018-09-09 17:00 TEN @ MIA\r\n##  8 20    STATUS_FINAL TRUE    2018-09-09 17:00 TEN @ MIA\r\n##  9 24    STATUS_FINAL TRUE    2018-09-09 17:00 SF @ MIN \r\n## 10 16    STATUS_FINAL TRUE    2018-09-09 17:00 SF @ MIN \r\n## # ... with 22 more rows\r\n\r\n\r\n\r\nFinally, we can use a chain of {dplyr} actions to get a pretty output. I should note that it is likely that everything up to this point would have an analogous action no matter what the data set is that you are working with. However, these final actions are unique to this specific data.\r\n\r\n\r\ndf_clean3 <-\r\n  df_clean2 %>% \r\n  group_by(gm) %>%\r\n  mutate(rn = row_number()) %>%\r\n  ungroup() %>%\r\n  mutate(tm_dir = if_else(rn == 1, \"pts_home\", \"pts_away\")) %>%\r\n  select(datetime, gm, tm_dir, value) %>%\r\n  spread(tm_dir, value) %>%\r\n  separate(gm, into = c(\"tm_away\", \"tm_home\"), sep = \"(\\\\s+\\\\@\\\\s+)|(\\\\s+vs.*\\\\s+)\") %>% \r\n  mutate_at(vars(matches(\"pts\")), funs(as.integer)) %>%\r\n  mutate(date = datetime %>% str_remove(\"\\\\s.*$\") %>% lubridate::ymd()) %>%\r\n  mutate(time = datetime %>% lubridate::ymd_hm()) %>%\r\n  select(date, time, tm_home, tm_away, pts_home, pts_away)\r\ndf_clean3\r\n\r\n\r\n\r\n\r\n\r\n## # A tibble: 16 x 6\r\n##    date       time                tm_home tm_away pts_home pts_away\r\n##    <date>     <dttm>              <chr>   <chr>      <int>    <int>\r\n##  1 2018-09-07 2018-09-07 00:55:00 PHI     ATL           18       12\r\n##  2 2018-09-09 2018-09-09 17:00:00 BAL     BUF           47        3\r\n##  3 2018-09-09 2018-09-09 17:00:00 IND     CIN           23       34\r\n##  4 2018-09-09 2018-09-09 17:00:00 NE      HOU           27       20\r\n##  5 2018-09-09 2018-09-09 17:00:00 NYG     JAX           15       20\r\n##  6 2018-09-09 2018-09-09 17:00:00 CLE     PIT           21       21\r\n##  7 2018-09-09 2018-09-09 17:00:00 MIN     SF            24       16\r\n##  8 2018-09-09 2018-09-09 17:00:00 NO      TB            40       48\r\n##  9 2018-09-09 2018-09-09 17:00:00 MIA     TEN           27       20\r\n## 10 2018-09-09 2018-09-09 20:05:00 LAC     KC            28       38\r\n## 11 2018-09-09 2018-09-09 20:25:00 CAR     DAL           16        8\r\n## 12 2018-09-09 2018-09-09 20:25:00 DEN     SEA           27       24\r\n## 13 2018-09-09 2018-09-09 20:25:00 ARI     WSH            6       24\r\n## 14 2018-09-10 2018-09-10 00:20:00 GB      CHI           24       23\r\n## 15 2018-09-10 2018-09-10 23:10:00 DET     NYJ           17       48\r\n## 16 2018-09-11 2018-09-11 02:20:00 OAK     LAR           13       33\r\n\r\n\r\n\r\nAnd there we have it! A nice, tidy tibble with the scores of the first week of regular season games in the 2018 NFL regular season.\r\nSign-off\r\nHopefully someone out there will find the technique(s) shown in this post to be useful for an endeavor of their own.\r\nPersonally, I find web scraping to be fascinating, so I doubt this will be the last time I write about something of this nature.\r\n\r\nI use data.frame and tibble interchangeably. See this chapter of the R for Data Science for more details about the differences/similarities between the two.‚Ü©Ô∏é\r\n(See the webpage here: http://www.espn.com/nfl/scoreboard/_/year/2018/seasontype/2/week/1. Note that we won‚Äôt be scraping the html, but, instead, the underlying JSON from which the html is generated.)‚Ü©Ô∏é\r\nI say that it‚Äôs a secret because it‚Äôs API documentation is out of date.‚Ü©Ô∏é\r\nI don‚Äôt recommend suffixing variable names with numbers as I do in the next couple of step (i.e.¬†variables suffixed with 1, 2, ‚Ä¶) (It‚Äôs ugly!) In practice, you might do this during your exploratory phase of data scraping/analysis, but you should come up with more informative names and combine actions in a logical manner for your final script/package (in my opinion).‚Ü©Ô∏é\r\n",
    "preview": "post/nested-json-to-tidy-data-frame-r/featured.jpg",
    "last_modified": "2021-07-01T22:12:02-05:00",
    "input_file": {}
  },
  {
    "path": "post/using-flexdashboard/",
    "title": "Thoughts on Using Flexdashboard",
    "description": "Examples and Alternatives",
    "author": [
      {
        "name": "Tony ElHabr",
        "url": "https://twitter.com/TonyElHabr"
      }
    ],
    "date": "2018-04-30",
    "categories": [
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nThe Alternatives\r\nSome Other Opinions\r\nConclusion\r\n\r\nI‚Äôve experimented with the {flexdashboard} package for a couple of things after first trying out not so long ago. In particular, I found the storyboard format to be my favorite. I used it to create the storyboard for tracking the activity of NBA team Twitter accounts.\r\n\r\nRecently, I‚Äôve been experimenting with the storyboard {flexdashboard} format for visualizing the weekly NFL picks that my brother and I do.\r\n\r\nThe Alternatives\r\nIn all, I have found {flexdashboard}s to be an extremely effective format. It functions as a good compromise among a number of different alternatives.\r\nA ‚Äústandard‚Äù document knitted from a .Rmd file (e.g.¬†HTML, Markdown, Word, or PDF). In my opinion, the main advantage of {flexdashboard} compared to the traditional .Rmd-knitr-Rmarkdown workflow is the ease with which shiny apps and other interactive formats (e.g.¬†htmlwidgets can be integrated. 1\r\nA presentation knitted from a .Rmd file (i.e.¬†a beamer_presentation, ioslides_presentation, slidy_presentation, or a revealjs::revealjs_presentation). Compared to these formats, I have found that modifying the default styling of the base CSS is much simpler with the themes in the {flexdashboard} package.\r\nA ‚Äústandard‚Äù R presentation (with the .Rpres extension). I must admit that I don‚Äôt have much experience with the .Rpres format‚ÄìI prefer Rmarkdown and the capability that it offers to create different kinds of output from a single ‚Äúbase‚Äù file.\r\nA {shinydashboard}. In comparison to shinydashboards, I like the ‚Äúlight-weight‚Äù framework offered by {flexdashboard}. While shinydashboard is certainly a better option for developing a complex dashboard (perhaps for a business purpose), this is not typically my use case.\r\nSome Other Opinions\r\nHere‚Äôs a couple of my other thoughts and tips for using {flexdashboard}:\r\nAdding JavaScript functionality is straightforward.\r\nFor example, I have used the following snippet of code to add functionality for a ‚Äútoggle‚Äù button to show/hide code.\r\n<script>\r\n  $(\".toggle\").click(function() {\r\n    $(this).toggleClass(\"open\");\r\n  });\r\n<\/script>\r\nCustomizing the colors and styles (with .css) is also simple.\r\nWhen modifying a theme‚Äôs CSS, the typical/recommended approach is to use the browser‚Äôs ‚ÄúInspect‚Äù tool (or some browser extension such as Selector Gadget to identify which specific styles to adjust.\r\nFor example, if the theme that is chosen for the {flexdashboard} (in particular, the storyboard format) uses the same styling for code and its output, the following CSS code can be used to distinguish the two.\r\npre.sourceCode.r {\r\n  background-color: #ffffff;\r\n}\r\nTo add to this technique, because the CSS files in the {flexdashboard} package, it‚Äôs not too difficult to simply pick out the file for the theme that is being used (see the resources/ folder in the package‚Äôs GitHub repo) and quickly identify what to modify.\r\nConclusion\r\nAs if I need to re-iterate it again, I would highly recommend trying out the {flexdashboard} package. If anything, you‚Äôll learn that you don‚Äôt like it.\r\n\r\nThese can also be be embedded into .html files knitted from a .Rmdfile, but I think that it‚Äôs more natural in the {flexdashboard} context. Also, I believe that the HTML format is the only standard .Rmd output format that has interactive capability, so other output formats cannot be used if opting for the knitr-Rmarkdown combo.‚Ü©Ô∏é\r\n",
    "preview": "post/using-flexdashboard/featured.jpg",
    "last_modified": "2021-07-01T22:08:54-05:00",
    "input_file": {}
  },
  {
    "path": "post/text-analysis-rweekly/",
    "title": "A Meta Analysis of R Weekly Posts",
    "description": "With Tidy Text Techniques",
    "author": [
      {
        "name": "Tony ElHabr",
        "url": "https://twitter.com/TonyElHabr"
      }
    ],
    "date": "2018-03-05",
    "categories": [
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nScraping and Cleaning\r\nAnalyzing\r\nLines and Links\r\nWords\r\nMost Unique Words\r\n\r\n\r\nIntroduction\r\nI‚Äôm always intrigued by data science ‚Äúmeta‚Äù analyses or programming/data-science. For example, Matt Dancho‚Äôs analysis of renown data scientist David Robinson. David Robinson himself has done some good ones, such as his blog posts for Stack Overflow highlighting the growth of ‚Äúincredible‚Äù growth of python, and the ‚Äúimpressive‚Äù growth of R in modern times.\r\nWith that in mind, I thought it would try to identify if any interesting trends have risen/fallen within the R community in recent years. To do this, I scraped and analyzed the ‚Äúweekly roundup‚Äù posts put together by R Weekly, which was originated in May 2016. These posts consist of links and corresponding descriptions, grouped together by topic. It should go without saying that this content serves as a reasonable heuristic for the interests of the R community at any one point in time. (Of course, the posts of other aggregate R blogs such as R Bloggers or Revolution Analytics might serve as better resources since they post more frequently and have been around for quite a bit longer than R Weekly.)\r\nScraping and Cleaning\r\nAs always, it‚Äôs good to follow the best practice of importing all needed packages before beginning.\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(gh)\r\nlibrary(viridisLite)\r\n\r\n\r\n\r\nFor the scraping, I drew upon some the principles shown by Maelle Salmon in her write-up detailing how she scraped and cleaned the blog posts of the Locke Data blog. 1\r\n\r\n\r\n# Reference: https://itsalocke.com/blog/markdown-based-web-analytics-rectangle-your-blog/\r\nposts <-\r\n  gh::gh(\r\n    endpoint = \"/repos/:owner/:repo/contents/:path\",\r\n    owner = \"rweekly\",\r\n    repo = \"rweekly.org\",\r\n    path = \"_posts\"\r\n  )\r\n\r\nposts_info <-\r\n  dplyr::data_frame(\r\n    name = purrr::map_chr(posts, \"name\"),\r\n    path = purrr::map_chr(posts, \"path\")\r\n  )\r\n\r\n\r\n\r\nIn all, R Weekly has made 93 (at the time of writing).\r\nNext, before parsing the text of the posts, I add some ‚Äúmeta-data‚Äù (mostly for dates) that is helpful for subsequent exploration and analysis.\r\n\r\n\r\nconvert_name_to_date <- function(x) {\r\n  x %>% \r\n    stringr::str_extract(\"[0-9]{4}-[0-9]+-[0-9]+\") %>% \r\n    strftime(\"%Y-%m-%d\") %>% \r\n    lubridate::ymd()\r\n}\r\n\r\nposts_info <-\r\n  posts_info %>% \r\n  mutate(date = convert_name_to_date(name)) %>% \r\n  mutate(num_post = row_number(date)) %>% \r\n    mutate(\r\n    yyyy = lubridate::year(date) %>% as.integer(),\r\n    mm = lubridate::month(date, label = TRUE),\r\n    wd = lubridate::wday(date, label = TRUE)\r\n  ) %>% \r\n  select(date, yyyy, mm, wd, num_post, everything())\r\n\r\nposts_info <-\r\n  posts_info %>% \r\n  mutate(date_min = min(date), date_max = max(date)) %>% \r\n  mutate(date_lag = date - date_min) %>% \r\n  mutate(date_lag30 = as.integer(round(date_lag / 30, 0)), \r\n         date_lag60 = as.integer(round(date_lag / 60, 0)), \r\n         date_ntile = ntile(date, 6)) %>% \r\n  select(-date_min, -date_max) %>% \r\n  select(date_lag, date_lag30, date_lag60, date_ntile, everything())\r\n\r\n\r\n\r\nLet‚Äôs quickly look at whether or not R Weekly has been consistent with its posting frequency since its inception. The number of posts across 30-day windows should be around 4 or 5.\r\n\r\nNow, I‚Äôll do the dirty work of cleaning and parsing the text of each post.\r\n\r\n\r\nget_rweekly_post_data <-\r\n  function(filepath) {\r\n    # This would be necessary if downloading directly from the repo.\r\n    # path <-\r\n    #   gh::gh(\r\n    #     \"/repos/:owner/:repo/contents/:path\",\r\n    #     owner = \"rweekly\",\r\n    #     repo = \"rweekly.org\",\r\n    #     path = path\r\n    #   )\r\n\r\n    path_prefix <- \"data-raw\"\r\n    path <- file.path(path_prefix, path)\r\n    rgx_rmv <- \"√Ç|√Ö|√¢‚Ç¨|≈ì|\\u009d\"\r\n    rgx_detect_link <- \"^\\\\+\\\\s+\\\\[\"\r\n    rgx_detect_head <- \"^\\\\s*\\\\#\"\r\n    rgx_link_post <- \"(?<=\\\\+\\\\s\\\\[).*(?=\\\\])\"\r\n    rgx_link_img <- \"(?<=\\\\!\\\\[).*(?=\\\\])\"\r\n    rgx_url <- \"(?<=\\\\().*(?=\\\\))\"\r\n    rgx_head <- \"(?<=\\\\#\\\\s).*$\"\r\n    \r\n    lines <- readLines(path)\r\n    lines_proc <-\r\n      lines %>%\r\n      # This would be necessary if downloading directly from the repo.\r\n      # base64enc::base64decode() %>%\r\n      # rawToChar() %>%\r\n      stringr::str_split(\"\\n\") %>%\r\n      purrr::flatten_chr() %>%\r\n      as_tibble() %>%\r\n      rename(text = value) %>%\r\n      transmute(line = row_number(), text) %>%\r\n      filter(text != \"\") %>%\r\n      mutate(text = stringr::str_replace_all(text, rgx_rmv, \"\")) %>%\r\n      mutate(text = stringr::str_replace_all(text, \"&\", \"and\")) %>% \r\n      mutate(\r\n        is_link = ifelse(stringr::str_detect(text, rgx_detect_link), TRUE, FALSE),\r\n        is_head = ifelse(stringr::str_detect(text, rgx_detect_head), TRUE, FALSE)\r\n      ) %>%\r\n      mutate(\r\n        link_post = stringr::str_extract(text, rgx_link_post),\r\n        link_img = stringr::str_extract(text, rgx_link_img),\r\n        url = stringr::str_extract(text, rgx_url),\r\n        head = \r\n          stringr::str_extract(text, rgx_head) %>% \r\n          stringr::str_to_lower() %>% \r\n          stringr::str_replace_all(\"s$\", \"\") %>% \r\n          stringr::str_replace_all(\" the\", \"\") %>% \r\n          stringr::str_trim()\r\n      ) %>%\r\n      mutate(\r\n        is_head = ifelse(line == 1, TRUE, is_head),\r\n        head = ifelse(line == 1, \"yaml and intro\", head)\r\n      )\r\n\r\n    # Couldn't seem to get `zoo::na.locf()` to work properly.\r\n    lines_head <-\r\n      lines_proc %>%\r\n      mutate(line_head = ifelse(is_head, line, 0)) %>%\r\n      mutate(line_head = cumsum(line_head))\r\n    \r\n    out <-\r\n      lines_head %>%\r\n      select(-head) %>%\r\n      inner_join(\r\n        lines_head %>%\r\n          filter(is_head == TRUE) %>%\r\n          select(head, line_head),\r\n        by = c(\"line_head\")\r\n      ) %>% \r\n      select(-line_head)\r\n    out\r\n  }\r\n\r\ndata <-\r\n  posts_info %>% \r\n  tidyr::nest(path, .key = \"path\") %>% \r\n  mutate(data = purrr::map(path, get_rweekly_post_data)) %>% \r\n  select(-path) %>% \r\n  tidyr::unnest(data)\r\ndata\r\n\r\n\r\n\r\nAnalyzing\r\nLines and Links\r\nNow, with the data in a workable format, we can explore some of the content.\r\n\r\n\r\nmetrics_bypost <-\r\n  data %>% \r\n  group_by(name, date) %>% \r\n  summarize(\r\n    num_lines = max(line),\r\n    num_links = sum(!is.na(is_link)),\r\n    num_links_post = sum(!is.na(link_post)),\r\n    num_links_img = sum(!is.na(link_img))\r\n  ) %>% \r\n  ungroup() %>% \r\n  arrange(desc(num_lines))\r\n\r\n\r\n\r\nHave the number of links per post increased over time?\r\n\r\nIt looks like there has been a correlated increase in the overall length of the posts (as determined by non-empty lines) and the number of links in each post.\r\n\r\n\r\ncorrr::correlate(metrics_bypost %>% select(num_lines, num_links))\r\n\r\n\r\n\r\n\r\n\r\n## # A tibble: 2 x 3\r\n##   rowname   num_lines num_links\r\n##   <chr>         <dbl>     <dbl>\r\n## 1 num_lines    NA         0.970\r\n## 2 num_links     0.970    NA\r\n\r\n\r\n\r\n\r\n\r\nbroom::tidy(lm(num_lines ~ num_links, data = metrics_bypost))\r\n\r\n\r\n\r\n\r\n\r\n##          term  estimate  std.error statistic      p.value\r\n## 1 (Intercept) 12.317353 4.93345168  2.496701 1.433479e-02\r\n## 2   num_links  1.796912 0.04754462 37.794219 2.016525e-57\r\n\r\n\r\n\r\nLet‚Äôs break down the increase of the number of links over time. Are there more links simply due to an increased use of images?\r\n\r\nIt is evident that the increase in the number of links is not the result of increased image usage, but, instead, to increased linkage to non-trivial content.\r\n\r\n\r\ncorrr::correlate(metrics_bypost %>% select(num_links, num_links_img, num_links_post))\r\n\r\n\r\n\r\n\r\n\r\n## # A tibble: 3 x 4\r\n##   rowname        num_links num_links_img num_links_post\r\n##   <chr>              <dbl>         <dbl>          <dbl>\r\n## 1 num_links         NA             0.324          0.865\r\n## 2 num_links_img      0.324        NA              0.264\r\n## 3 num_links_post     0.865         0.264         NA\r\n\r\n\r\n\r\n\r\n\r\nbroom::tidy(lm(num_links ~ num_links_img + num_links_post, data = metrics_bypost))\r\n\r\n\r\n\r\n\r\n\r\n##             term  estimate std.error statistic      p.value\r\n## 1    (Intercept) 29.094312 4.7262724  6.155869 2.040398e-08\r\n## 2  num_links_img  1.008073 0.5275685  1.910790 5.921483e-02\r\n## 3 num_links_post  1.168952 0.0749660 15.593093 2.586469e-27\r\n\r\n\r\n\r\nR Weeklyuses a fairly consistent set of ‚Äútopics‚Äù (corresponding to the head variable in the scraped data) across all of their posts.\r\n\r\n\r\nhead_rmv <- \"yaml and intro\"\r\ndata %>%\r\n  distinct(head, name) %>%\r\n  filter(!(head %in% head_rmv)) %>% \r\n  count(head, sort = TRUE)\r\n\r\n\r\n\r\n\r\n\r\n## # A tibble: 44 x 2\r\n##    head                   n\r\n##    <chr>              <int>\r\n##  1 r in real world       92\r\n##  2 tutorial              92\r\n##  3 upcoming event        92\r\n##  4 highlight             89\r\n##  5 r project update      89\r\n##  6 r in organization     80\r\n##  7 resource              71\r\n##  8 quotes of week        63\r\n##  9 insight               55\r\n## 10 videos and podcast    55\r\n## # ... with 34 more rows\r\n\r\n\r\n\r\nIs there a certain topic (or topics) in the RWeekly posts that are causing the increased length of posts?\r\n\r\nThe steady increase in the length of the tutorial section stands out. (I suppose the R community really enjoys code-walkthroughs (like this one).) Also, the introduction of the new package header about a year after the first RWeekly post suggests that R developers really care about what their fellow community members are working on.\r\nWords\r\nThe words used in the short descriptions that accompany each link to external content should provide a more focused perspective on what specifically is of interest in the R community. What are the most frequently used words in these short descriptions?\r\n\r\nSome unsurprising words appear at the top of this list, such as data and analysis. Some words that one would probably not see among the top of an analogous list for another programming community are rstudio, shiny, ggplot2, and tidy. It‚Äôs interesting that shiny actually appears as the top individual package‚Äìthis could indicate that bloggers like to share their content through interactive apps (presumably because it is a great way to captivate and engage an audience).\r\nIt‚Äôs one thing to look at individual words, but it is perhaps more interesting to look at word relationships.\r\n\r\nThis visual highlights a lot of the pairwise word correlations that we might expect in the data science realm: data and science, time and series, machine and learning, etc. Nonetheless, there are some that are certainly unique to the R community: purrr with mapping; community with building; shiny with interactive and learning; and rstudio with (microsoft) server.\r\nThe numerical values driving this correlation network not only is useful for quantifying the visual relationships, but, in this case, it actually highlights some relationships that get a bit lost in the graph (simply due to clustering). In particular, the prominence of the words tutorial, conf, user, and interactive stand out.\r\n\r\n\r\nunigram_corrs <-\r\n  unigrams %>%\r\n  tetext::compute_corrs_at(\r\n    word = \"word\",\r\n    feature = \"name\",\r\n    num_top_ngrams = 100,\r\n    num_top_corrs = 100\r\n  )\r\nunigram_corrs %>% head(20)\r\n\r\n\r\n\r\n\r\n\r\n## # A tibble: 20 x 4\r\n##    item1       item2     correlation  rank\r\n##    <chr>       <chr>           <dbl> <int>\r\n##  1 tutorials   html            0.966     1\r\n##  2 user2016    tutorials       0.955     2\r\n##  3 user2016    html            0.950     3\r\n##  4 machine     learning        0.726     4\r\n##  5 user        user2016        0.708     5\r\n##  6 slides      html            0.698     6\r\n##  7 time        series          0.695     7\r\n##  8 slides      tutorials       0.695     8\r\n##  9 rstudio     conf            0.691     9\r\n## 10 user        tutorials       0.690    10\r\n## 11 user        html            0.687    11\r\n## 12 user2016    slides          0.687    12\r\n## 13 interactive html            0.668    13\r\n## 14 text        mining          0.659    14\r\n## 15 interactive user            0.658    15\r\n## 16 interactive user2016        0.653    16\r\n## 17 interactive tutorials       0.650    17\r\n## 18 earl        london          0.594    18\r\n## 19 network     building        0.582    19\r\n## 20 interactive slides          0.550    20\r\n\r\n\r\n\r\nMost Unique Words\r\nLet‚Äôs try to identify words that have risen and fallen in popularity. While there are many ways of doing, let‚Äôs try segmenting the R Weekly posts into intervals of 60 days and computing the [term-frequency, inverse-document-frequency]((https://www.tidytextmining.com/tfidf) (TF-IDF) of words across these intervals. (I apologize if the resolution is sub-par.)\r\n\r\nA couple of things stand out:\r\nPosts were heavily influenced by user2016 conference content in the early days of R Weekly (light blue and blue).\r\nThere was clearly a 20 theme in the 60 days between 2017-02-20 and 2017-04-10 (red).\r\nThe ‚Äútabs vs.¬†spaces‚Äù debate rose to prominence during the late summer days of 2017 (orange), presumably after David Robinson‚Äôs Stack Overflow post on the topic.\r\nR‚Äôs ongoing global influence is apparent with the appearance of euro with the user2016 conference (light blue and blue); poland and satrdays (presumably due to the Cape Town R conference of the namesake in late 2016 (green), and several Spanish words in January 2018 (yellow).\r\nI tried some different methods, but did not find much interesting regarding change in word frequency over time (aside from the TF-IDF approach). When using the method discussed in the Tidy Text Mining book for identifying change in word usage across 60-day intervals, I found only two non-trivial ‚Äúsignificant‚Äù changes among the top 5% of most frequently used words, which are for user and tutorials. user has dropped off a bit since the useR2016 conference, and tutorials has grown in usage, which is evident with the increasing length of the tutorial section in posts.\r\nThat‚Äôs all I got for this subject. As I mentioned at the top, there are many of other great ‚Äúmeta‚Äù analyses like this one that are worth looking at, so definitely check them out!\r\n\r\nActually, I downloaded the data locally so that I would not have to worry about GitHub API request limits. Thus, in addition to other custom processing steps that I added, my final code does not necessarily resemble hers.‚Ü©Ô∏é\r\n",
    "preview": "post/text-analysis-rweekly/featured.jpg",
    "last_modified": "2021-07-01T21:54:24-05:00",
    "input_file": {}
  },
  {
    "path": "post/interval-data-nycflights13/",
    "title": "Dealing with Interval Data and the nycflights13 package",
    "description": "Dates, ANOVAs, and More",
    "author": [
      {
        "name": "Tony ElHabr",
        "url": "https://twitter.com/TonyElHabr"
      }
    ],
    "date": "2018-02-17",
    "categories": [
      "r"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nMinute-Level Data\r\nAn Example (With the nycflights13 Package)\r\n\r\nFinal Thoughts\r\n\r\nIntroduction\r\nIn my job, I often work with data sampled at regular intervals. Samples may range from 5-minute intervals to daily intervals, depending on the specific task. While working with this kind of data is straightforward when its in a database (and I can use SQL), I have been in a couple of situations where the data is spread across .csv files. In these cases, I lean on R to scrape and compile the data. Although some other languages might be superior for such a task, I often need to produce some kind of visualization or report at the end, so choosing to handle all of the data with R is a no-brainer for me‚ÄìI can easily transition to using {ggplot2} and {rmarkdown} to generate some pretty output.\r\nMinute-Level Data\r\nWhen working with data sampled at 5-minute intervals (resulting in 288 intervals in a day), I‚Äôve found that I‚Äôve used a common ‚Äúidiom‚Äù to generate a time-based ‚Äúdictionary‚Äù. For example, here‚Äôs how I might create such a date dictionary for the year 2013. 1\r\n\r\n\r\nlibrary(dplyr)\r\n\r\ndate_1 <- lubridate::ymd(\"2013-01-01\")\r\ndate_2 <- lubridate::ymd(\"2013-12-31\")\r\n\r\nymd_seq <- seq.Date(date_1, date_2, by = \"day\")\r\nymd_grid <-\r\n  data_frame(\r\n    ymd = lubridate::ymd(ymd_seq),\r\n    yyyy = lubridate::year(ymd_seq),\r\n    mm = lubridate::month(ymd_seq),\r\n    dd = lubridate::day(ymd_seq)\r\n  )\r\n\r\nhhmmss <-\r\n  expand.grid(\r\n    hh = seq(1L, 24L, 1L), \r\n    min = seq(5L, 60L, by = 5L), \r\n    sec = 0L) %>% \r\n  as_tibble()\r\n\r\ndates_dict <-\r\n  ymd_grid %>%\r\n  right_join(hhmmss %>% mutate(yyyy = lubridate::year(date_1), by = \"year\")) %>% \r\n  arrange(yyyy, mm, dd, hh, min)\r\ndates_dict\r\n\r\n\r\n\r\n\r\n\r\n## # A tibble: 105,120 x 8\r\n##    ymd         yyyy    mm    dd    hh   min   sec by   \r\n##    <date>     <dbl> <dbl> <int> <int> <int> <int> <chr>\r\n##  1 2013-01-01  2013     1     1     1     5     0 year \r\n##  2 2013-01-01  2013     1     1     1    10     0 year \r\n##  3 2013-01-01  2013     1     1     1    15     0 year \r\n##  4 2013-01-01  2013     1     1     1    20     0 year \r\n##  5 2013-01-01  2013     1     1     1    25     0 year \r\n##  6 2013-01-01  2013     1     1     1    30     0 year \r\n##  7 2013-01-01  2013     1     1     1    35     0 year \r\n##  8 2013-01-01  2013     1     1     1    40     0 year \r\n##  9 2013-01-01  2013     1     1     1    45     0 year \r\n## 10 2013-01-01  2013     1     1     1    50     0 year \r\n## # ... with 1.051e+05 more rows\r\n\r\n\r\n\r\nAnd, just to prove that there are 288 5-minute intervals for each day in the year, I can use dplyr::count() twice in succession.\r\n\r\n\r\ndates_dict %>% count(yyyy, mm, dd) %>% count()\r\n\r\n\r\n\r\n\r\n\r\n## # A tibble: 1 x 1\r\n##      nn\r\n##   <int>\r\n## 1   365\r\n\r\n\r\n\r\nI then extract data (from individual files) using the time-based dictionary as a ‚Äúhelper‚Äù for custom functions for creating file paths and processing the data after importing it.\r\nAfter the dirty work of is done, I can transition to the fun part‚Äìexploring and interpreting the data. This process often turns out to be a cycle of visualization, data transformation, and modeling.\r\n\r\nAn Example (With the nycflights13 Package)\r\nTo provide an example, I‚Äôll use the flights data set from the {nycflight13} package. This package includes information regarding all flights leaving from New York City airports in 2013, as well as information regarding weather, airlines, airports, and planes.\r\nLet‚Äôs say that I that I‚Äôm interested in the average flight departure delay time at the JFK airport. I might hypothesize that there is a relationship between departure delay time with different time periods, such as hour in the day and days in the week.\r\nFirst, I‚Äôll perform the necessary transformation to the flights data to investigate my hypothesis. Specifically, I need to create columns for hour and weekday. (For hour (hh), I simply use the scheduled departure time (sched_dep_time).)\r\n\r\n\r\nlibrary(nycflights13)\r\n\r\nflights_jfk <-\r\n  nycflights13::flights %>% \r\n  filter(origin == \"JFK\") %>% \r\n  mutate(hh = round(sched_dep_time / 100, 0) - 1) %>% \r\n  mutate(ymd = lubridate::ymd(sprintf(\"%04.0f-%02.0f-%02.0f\", year, month, day))) %>% \r\n  mutate(wd = lubridate::wday(ymd, label = TRUE))\r\n\r\n\r\n\r\nNext, I might create a heat map plotting hours against weekdays.\r\n\r\nTo investigate the patterns more ‚Äúscientifically‚Äù, I might perform a one-way Analysis of Variance (ANOVA) on different time variables. I would make sure to test time periods other than just weekday (wd) and mhour (hh), such as month and day.\r\n\r\n\r\nsummary(aov(dep_delay ~ month, data = flights_jfk))\r\n\r\n\r\n\r\n\r\n\r\n##                 Df    Sum Sq Mean Sq F value   Pr(>F)    \r\n## month            1     55213   55213   36.25 1.74e-09 ***\r\n## Residuals   109414 166664445    1523                     \r\n## ---\r\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n## 1863 observations deleted due to missingness\r\n\r\n\r\n\r\n\r\n\r\nsummary(aov(dep_delay ~ day, data = flights_jfk))\r\n\r\n\r\n\r\n\r\n\r\n##                 Df    Sum Sq Mean Sq F value Pr(>F)\r\n## day              1        40    40.2   0.026  0.871\r\n## Residuals   109414 166719617  1523.8               \r\n## 1863 observations deleted due to missingness\r\n\r\n\r\n\r\n\r\n\r\nsummary(aov(dep_delay ~ wd, data = flights_jfk))\r\n\r\n\r\n\r\n\r\n\r\n##                 Df    Sum Sq Mean Sq F value Pr(>F)    \r\n## wd               6    246401   41067   26.99 <2e-16 ***\r\n## Residuals   109409 166473256    1522                   \r\n## ---\r\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n## 1863 observations deleted due to missingness\r\n\r\n\r\n\r\n\r\n\r\nsummary(aov(dep_delay ~ hh, data = flights_jfk))\r\n\r\n\r\n\r\n\r\n\r\n##                 Df    Sum Sq Mean Sq F value Pr(>F)    \r\n## hh               1   5701754 5701754    3874 <2e-16 ***\r\n## Residuals   109414 161017904    1472                   \r\n## ---\r\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n## 1863 observations deleted due to missingness\r\n\r\n\r\n\r\nThe ‚Äústatistically significant‚Äù p values for the wd and hh variables provides incentive to investigate them more closely. I might try an ANOVA F-statistic test comparing linear regression models using each variable as a lone predictor with a linear model where both are used as predictors.\r\n\r\n\r\nlm_wd <- lm(dep_delay ~ wd, data = flights_jfk)\r\nlm_hh <- lm(dep_delay ~ hh, data = flights_jfk)\r\nlm_both <- lm(dep_delay ~ wd + hh, data = flights_jfk)\r\nanova(lm_both, lm_wd, test = \"F\")\r\n\r\n\r\n\r\n\r\n\r\n## Analysis of Variance Table\r\n## \r\n## Model 1: dep_delay ~ wd + hh\r\n## Model 2: dep_delay ~ wd\r\n##   Res.Df       RSS Df Sum of Sq      F    Pr(>F)    \r\n## 1 109408 160778456                                  \r\n## 2 109409 166473256 -1  -5694800 3875.2 < 2.2e-16 ***\r\n## ---\r\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n\r\n\r\n\r\n\r\nanova(lm_both, lm_hh, test = \"F\")\r\n\r\n\r\n\r\n\r\n\r\n## Analysis of Variance Table\r\n## \r\n## Model 1: dep_delay ~ wd + hh\r\n## Model 2: dep_delay ~ hh\r\n##   Res.Df       RSS Df Sum of Sq      F    Pr(>F)    \r\n## 1 109408 160778456                                  \r\n## 2 109414 161017904 -6   -239447 27.157 < 2.2e-16 ***\r\n## ---\r\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n\r\n\r\nOf course, this process would go on. For instance, I would certainly need to investigate if there is a relationship between departure delay and specific airlines.\r\nFinal Thoughts\r\nWorking with interval data was initially a challenge for me, but after working with it more and more often, I find that it‚Äôs not so bad after all. It gets more interesting when there is missing data or data samples at irregular intervals, but that‚Äôs a story for another day.\r\n\r\nThe technique that I show here would have to be adjusted slightly if working with more than one year at a time, but it wouldn‚Äôt be difficult to do so. I tend to only use this design pattern for one year at a time anyways.‚Ü©Ô∏é\r\n",
    "preview": "post/interval-data-nycflights13/featured.jpg",
    "last_modified": "2021-07-01T22:13:16-05:00",
    "input_file": {}
  },
  {
    "path": "post/visualizing-nba-team-schedule/",
    "title": "Visualizing an NBA Team's Schedule Using R",
    "description": "A Calendar Heatmap with ggplot2",
    "author": [
      {
        "name": "Tony ElHabr",
        "url": "https://twitter.com/TonyElHabr"
      }
    ],
    "date": "2017-11-26",
    "categories": [
      "r"
    ],
    "contents": "\r\nIf you‚Äôre not completely new to the data science community (specifically, the #rstats community), then you‚Äôve probably seen a version of the ‚Äúfamous‚Äù data science workflow diagram. 1\r\n\r\nIf one is fairly familiar with a certain topic, then one might not spend much time with the initial ‚Äúvisualize‚Äù step of the workflow. Such is the case with me and NBA data‚Äìas a relatively knowledgeable NBA follower, I don‚Äôt necessarily need to spend much of my time exploring raw NBA data prior to modeling.\r\nAnyways, as a break from experimenting with predictive models, I decided to make a visualization just for the sake of trying something I hadn‚Äôt done before. 2 In particular, I was inspired by the calendar heat map visualization that I saw in the Top 50 ggplot visualizations post on the https://r-statistics.co website.\r\n\r\nTo implement a plot of this nature, I decided to look at how my hometown team, the San Antonio Spurs, fared last season (2016) in terms of point differential. In case it‚Äôs not immediately obvious, lots of green is good. (This is not surprising to those of us who follow the NBA‚Äìthe Spurs have been consistently good since the end of the 1990s.)\r\n\r\n\r\nlibrary(dplyr)\r\nlibrary(ggplot2)\r\n\r\nresults_prepared <- 'game_results-prepared.csv' %>% read_csv()\r\n\r\ncolnames_base <- c('date', 'season', 'tm')\r\ncolnames_calc_dates <- c('yyyy', 'mm', 'dd', 'wd', 'mm_yyyy', 'mm_w')\r\n# Look at a couple of different metrics.\r\n# Specifically, look at games played to date (g_td) and point differential (pd).\r\ncolnames_viz <- c('g_td', 'pd')\r\n\r\nresults_calendar_tm <-\r\n  results_prepared %>%\r\n  filter(tm == 'SAS') %>%\r\n  mutate(\r\n    yyyy = year(date),\r\n    mm = lubridate::month(date),\r\n    dd = lubridate::day(date),\r\n    wd = lubridate::wday(date, label = TRUE, abbr = TRUE),\r\n    mm_yyyy = zoo::as.yearmon(date)\r\n  ) %>%\r\n  group_by(mm_yyyy) %>%\r\n  mutate(mm_w = ceiling(dd / 7)) %>%\r\n  ungroup() %>%\r\n  select(one_of(colnames_base, colnames_calc_dates, colnames_viz)) %>%\r\n  arrange(season, g_td, tm)\r\nresults_calendar_tm\r\n\r\n# Tidy up because I was experimenting with different metrics, not just point differential.\r\nresults_calendar_tm_tidy <-\r\n  results_calendar_tm %>%\r\n  tidyr::gather(metric, value, colnames_viz)\r\n\r\nseason <- 2016\r\nwd_labels <- levels(results_calendar_tm$wd)\r\nwd_labels[2:6] <- ''\r\ntitle <- str_c('San Antonio Spurs Point Differential in ', season, ' NBA Season')\r\n\r\nviz_pd_sas_2016 <-\r\n  results_calendar_tm_tidy %>%\r\n  filter(season %in% seasons) %>%\r\n  filter(metric == 'pd') %>%\r\n  ggplot() +\r\n  geom_tile(aes(x = wd, y = mm_w, fill = value), colour = 'white') +\r\n  scale_y_reverse() +\r\n  scale_x_discrete(labels = wd_labels) +\r\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5)) +\r\n  scale_fill_gradient2(low = 'red', mid = 'grey', high = 'green') +\r\n  theme(legend.position = 'bottom') +\r\n  labs(x = '', y = '', title = title) +\r\n  facet_wrap( ~ mm_yyyy, nrow = 2)\r\nviz_pd_sas_2016\r\n\r\n\r\n\r\n\r\nThere are an infinite number of ways to visualize data like this, but I thought this was interesting because of the temporal nature of the data.\r\n\r\nThe figure shown here comes from the introductory chapter of the R for Data Science book‚Ü©Ô∏é\r\nHere, I use the NBA data that I have already scraped and cleaned.‚Ü©Ô∏é\r\n",
    "preview": "post/visualizing-nba-team-schedule/featured.jpg",
    "last_modified": "2021-07-01T21:58:22-05:00",
    "input_file": {}
  }
]
