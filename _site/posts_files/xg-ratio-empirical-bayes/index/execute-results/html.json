{
  "hash": "3096b6b70212697179eed00ddc428083",
  "result": {
    "markdown": "---\ntitle: Measuring Shooting Overperformance in Soccer\ndescription: Using empirical Bayes and the Gamma-Poisson conjugate pair\ndate: 2023-08-28\ncategories:\n  - r\n  - soccer\nimage: beyond-replication.png\nexecute: \n  eval: false\n  include: true\n  echo: true\n  code-fold: false\n---\n\n\n## Introduction\n\nThis blog post is my attempt to replicate the results in Laurie Shaw's 2018 blog post [\"Exceeding Expected Goals\"](http://eightyfivepoints.blogspot.com/2018/09/exceeding-expected-goals.html). Specifically, I want to shed light on how to implement [Gamma-Poisson](https://en.wikipedia.org/wiki/Empirical_Bayes_method#Poisson%E2%80%93gamma_model) [empirical Bayes](https://en.wikipedia.org/wiki/Empirical_Bayes_method) (EB) estimation. If you don't care at all about the theory behind EB and its application to this context, then go ahead and skip ahead to the \"Implementation\" section.\n\n### What Is empirical Bayes (EB) estimation?\n\n**Empirical Bayes** (EB) estimation. Wow, just typing that out makes me feel smart. But what is it, really? In short, I'd describe it as a mix of [Bayesian](https://en.wikipedia.org/wiki/Bayesian_inference) and [Frequentist](https://en.wikipedia.org/wiki/Frequentist_inference) inference. We lean into the observed frequencies of the data (Frequentist) while simultaneously refining our initial data assumptions through Bayesian updating. In practice, one might use EB as a (relatively) simple alternative to a full Bayesian analysis, which can feel daunting.\n\nIn regular Bayesian analysis, you start with your initial \"guess\" (prior distribution) about something, and as you gather data, you tweak that \"guess\" using Bayes' theorem to get a final view (posterior distribution). We combine what we thought about the data beforehand with how likely the data matches (likelihood).\n\nEmpirical Bayes puts a twist on this. Instead of having a prior guess, you figure out that initial guess from the same data you're analyzing. This can make things simpler, especially when you're dealing with tons of guesses but not much initial info.\n\n### A canonical example of EB estimation (Beta-Binomial)\n\n[David Robinson](https://github.com/dgrtwo) wrote [a wonderful blog post](http://varianceexplained.org/r/empirical_bayes_baseball/) about empirical Bayes estimation for estimating [batting averages in baseball](https://en.wikipedia.org/wiki/Batting_average_(baseball)), notably \"shrinking\" the battering average sof those with relatively few at bats closer to some \"prior\" estimate derived from a choice of hyperparameters. For context, batting average, $BA$, is defined as a player's count of hits, $H$, divided by the count of their at-bats, $AB$.\n\n$$\nBA = H / AB\n$$ {#eq-ba}\n\n::: callout-note\nI'd David's post **must read** material prior to going through this blog post.\n:::\n\nIn his post, David uses [a Beta prior](https://en.wikipedia.org/wiki/Beta_distribution) and [a binomial posterior](https://en.wikipedia.org/wiki/Binomial_distribution) together, i.e. a [Beta-binomial Bayesian model](https://www.bayesrulesbook.com/chapter-3))[^1][^2], since this tandem is suitable for proportions and probabilities. The gist of his approach: we add some fixed number of hits, $\\alpha_0$, and a fixed number of at-bats, $\\beta_0$, to the numerator and denominator of the battering average equation as so.\n\n[^1]: This [conjugate distribution table](https://en.wikipedia.org/wiki/Conjugate_prior) might be handy for those curious to know which distributions are typically paired together for empirical Bayes estimation.\n\n[^2]: If you've seen my work, you might have noticed that I've used Beta-Binomial EB a few times for public projects in the past:\n\n    1.  [to estimate the proportion of direct free kick shots on target (soccer), grouped by league](https://twitter.com/TonyElHabr/status/1457377069957107715?s=20)\n    2.  [to adjust Whataburger Yelp reviews for small sample sizes](https://twitter.com/TonyElHabr/status/1429610210964955137?s=20)\n\n$$\n(H + \\alpha_0) / (AB + \\alpha_0 + \\beta_0)\n$$ {#eq-adj-ba}\n\nSpecifically, the \"prior\" estimate of batting average is found from isolating the $\\alpha_0$ and $\\beta_0$ elements:\n\n$$\n\\alpha_0 / (\\alpha_0 + \\beta_0)\n$$ {#eq-ba-prior}\n\nIf, for example, `alpha0 = 70` and `beta0 = 230`, then the prior estimate of batting average is effectively `70 / (70 + 230) = 0.3`. Note that `alpha0` and `beta0` are learned from the data using [maximum likelihood estimation (MLE)](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation), although other approaches, such as [\"method of moments\"](https://en.wikipedia.org/wiki/Method_of_moments_(statistics)) could be used. (Heck, you could even defensibly choose these \"hyperparameters\" yourself, without any fancy statistics, if you feel that you have enough knowledge of the data.)\n\n### Gamma-Poisson EB estimation\n\nNow, for my replication of Shaw's analysis, we're going to be focusing on the ratio of a player's goals, $G$, divided by their [expected goals](/posts/epl-xpts-simulation-1)), $xG$, summed up over a fixed time period. Shaw refers to this as \"overperformance\" $O$ for a player $p$:\n\n$$\nO_p = \\frac{G_p}{xG_p}\n$$ {#eq-o}\n\nWhile one might be tempted to use Beta-Binomial EB since this setup seems similar to batting average in @eq-ba, Shaw used a [Gamma-Poisson](https://www.bayesrulesbook.com/chapter-5#gamma-poisson-conjugate-family) EB adjustment, and justifiably so. Gamma-Poisson makes more sense when the underlying data consists of counts *and* what you're trying to estimate is a rate or ratio, not a proportion bounded between 0 and 1. Note that a $O_p$ ratio of 1 indicates that a player is scoring as many goals as expected; a ratio greater than 1 indicates underperformance; and a ratio less than 1 indicates overperformance. On, the other hand, batting average is bounded between 0 and 1.\n\nNow, despite the naming of conjugate prior pairs--e.g. \"Beta-Binomial\" and \"Gamma-Poisson\", where the prior distribution is represented by the first distribution and the likelihood distribution is indicated by the second--let's not forget that there are actually a third distribution to be noted: the posterior. In the case of the Gamma-Poisson model, the unnormalized posterior distribution of the \"kernel\" (i.e. the prior and likelihood pair) is a Gamma distribution. (This is always the case with Gamma-Prior kernels.)\n\nIn practice, this means that we'll be using the Gamma distribution for both estimating hyperparameters and posterior sampling. Perhaps surprising to the reader, you won't actually need any Poisson functions in the code implementation. Rather, the Poisson distribution is pertinent to implementation only to the extent that the Gamma distribution happens to be the most reasonable distribution to pair with it.\n\nI've woefully explained away a lot of details here, but hopefully this all makes sense to those with a basic understanding of the Gamma and Poisson distributions themselves.\n\n## Implementation\n\nOk, so with all of that context provided, now let's do the replication of Shaw's findings.\n\n### Data\n\nFirst, let's pull the data we'll need--2016/17 and 2017/18 [English Premier League](https://www.premierleague.com) goals and [expected goals (xG)](https://theanalyst.com/na/2023/08/what-is-expected-goals-xg/) by player. I'm using [understat](https://understat.com/) since it is a reliable source of data[^3] and is easy to retrieve data from via the [`{worldfootballR}` package](https://jaseziv.github.io/worldfootballR/).[^4]\n\n[^3]: Note that understat maintains its own xG model, so the xG on understat won't exactly match what you might get from [Opta](https://www.statsperform.com/opta/) or [StatsBomb](https://statsbomb.com/).\n\n[^4]: [FBRef](https://fbref.com/en/expected-goals-model-explained/) only provides expected goals dating back to the 2017/18 season, so unfortunately it's not viable for this analysis.\n\nNote that Shaw used data from a provider, [Stratagem](https://www.linkedin.com/company/stratagem-ltd/about/), that no long provides data, as far as I can tell. For at least this reason, I won't be able to exactly match his reason.[^5]\n\n[^5]: The other major reason why I may not be able to match his results is if I've implemented the Gamma-Poisson adjustment in a different (hopefully, not incorrect ðŸ˜…) manner.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n## data wrangling\nlibrary(worldfootballR)\nlibrary(dplyr)\nlibrary(tibble)\n\n## distribution fitting and wrangling\nlibrary(MASS, include.only = 'fitdistr') ## to avoid `select` name conflict with dplyr\nlibrary(withr)\nlibrary(purrr)\nlibrary(tidyr)\n\nraw_shots <- worldfootballR::load_understat_league_shots(league = 'EPL')\nshots <- raw_shots |> \n  tibble::as_tibble() |> \n  dplyr::filter(\n    season %in% c(2016L, 2017L), ## 2016/17 and 2017/18 seasons\n    situation != 'DirectFreeKick' ## \"excluding free-kicks\" in the blog post\n  ) |> \n  dplyr::arrange(id) |> \n  dplyr::transmute(\n    id,\n    player,\n    xg = x_g,\n    g = as.integer(result == 'Goal')\n  )\nshots\n#> # A tibble: 19,047 Ã— 4\n#>        id player               xg     g\n#>     <dbl> <chr>             <dbl> <int>\n#>  1 112088 Aaron Ramsey    0.0695      0\n#>  2 112089 Nathaniel Clyne 0.0293      0\n#>  3 112090 Aaron Ramsey    0.00734     0\n#>  4 112091 Roberto Firmino 0.0856      0\n#>  5 112092 Roberto Firmino 0.0441      0\n#>  6 112093 Sadio ManÃ©      0.0607      0\n#>  7 112094 Ragnar Klavan   0.0742      0\n#>  8 112095 Theo Walcott    0.761       0\n#>  9 112096 Theo Walcott    0.0721      1\n#> 10 112097 Roberto Firmino 0.0241      0\n#> # â„¹ 19,037 more rows\n```\n:::\n\n\nAbove was pulling in every record of shots, with 1 row per shot. Now we aggregate to the player-level, such that we have one row per player.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nshots_by_player <- shots |> \n  dplyr::group_by(player) |> \n  dplyr::summarize(\n    shots = dplyr::n(),\n    dplyr::across(c(g, xg), sum)\n  ) |> \n  dplyr::ungroup() |> \n  dplyr::mutate(raw_ratio = g / xg) |> \n  dplyr::arrange(dplyr::desc(shots))\nshots_by_player\n#> # A tibble: 588 Ã— 5\n#>    player            shots     g    xg raw_ratio\n#>    <chr>             <int> <int> <dbl>     <dbl>\n#>  1 Harry Kane          293    59  46.7     1.26 \n#>  2 Sergio AgÃ¼ero       234    41  41.2     0.994\n#>  3 Christian Eriksen   229    18  16.1     1.12 \n#>  4 Alexis SÃ¡nchez      217    33  29.1     1.13 \n#>  5 Romelu Lukaku       196    41  32.1     1.28 \n#>  6 Roberto Firmino     184    26  21.1     1.23 \n#>  7 Kevin De Bruyne     179    14  12.2     1.15 \n#>  8 SalomÃ³n RondÃ³n      171    15  16.2     0.924\n#>  9 Paul Pogba          168    11  14.3     0.768\n#> 10 Christian Benteke   164    18  28.5     0.631\n#> # â„¹ 578 more rows\n```\n:::\n\n\n### EB step 1: estimate prior hyperparameters\n\nNext, we estimate hyperparameters for our prior gamma distribution using MLE. (With R's `dgamma`, the hyperparameters are `shape` and `rate`[^6]). I subset the data down to players having taken at least 50 shots for estimating these hyperparameters, as this is what Shaw does. In general, you'd want to filter your data here to records that provide good \"signal\", and, therefore, will provide reliable estimates of your hyperparameters.[^7]\n\n[^6]: In [the wild](https://en.wikipedia.org/wiki/Gamma_distribution), you'll see `alpha` and `beta` used to describe the hyperparameters. `shape` and `rate` are different ways of framing these parameters.\n\n[^7]: Note that this process of selecting priors is the \"twist\" I mentioned in the introduction that really separates empirical Bayes estimation from a traditional, full Bayesian approach. In the latter, one chooses priors for an analysis without using the data to be included in the analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nprior_shots_by_player <- dplyr::filter(\n  shots_by_player, \n  shots >= 50,\n  g > 0 ## prevent error with fitting prior distribution\n)\n\nprior_distr <- MASS::fitdistr(\n  prior_shots_by_player$raw_ratio,\n  dgamma,\n  start = list(shape = 1, rate = 1)\n)\nprior_shape <- prior_distr$estimate[1]\nprior_rate <- prior_distr$estimate[2]\nlist(prior_shape = round(prior_shape, 2), prior_rate = round(prior_rate, 2))\n#> $prior_shape\n#> shape \n#>  9.39 \n#> \n#> $prior_rate\n#> rate \n#> 8.93\n```\n:::\n\n\n### EB step 2: Use prior distribution to sample the posterior\n\nNow we use our prior distribution's hyperparameters to update all players' $O$ ratio based on their individual volume of evidence, i.e. their goals and xG.\n\n\n::: {.cell data-label='adj_ratio_by_player'}\n\n```{.r .cell-code  code-fold=\"false\"}\nsimulate_gamma_posterior <- function(\n    successes, \n    trials, \n    prior_shape, \n    prior_rate, \n    n_sims = 10000,\n    seed = 42\n) {\n  posterior_shape <- prior_shape + successes\n  posterior_rate <- prior_rate + trials\n  withr::local_seed(seed)\n  posterior_sample <- rgamma(n = n_sims, shape = posterior_shape, rate = posterior_rate)\n  list(\n    mean = mean(posterior_sample),\n    sd = sd(posterior_sample)\n  )\n}\n\nshots_by_player$adj_ratio <- purrr::map2(\n  shots_by_player$g, shots_by_player$xg,\n  function(g, xg) {\n    simulate_gamma_posterior(\n      successes = g,\n      trials = xg,\n      prior_shape = prior_shape,\n      prior_rate = prior_rate\n    )\n  }\n)\n\nadj_ratio_by_player <- shots_by_player |> \n  tidyr::unnest_wider(\n    adj_ratio, \n    names_sep = '_'\n  ) |> \n  dplyr::arrange(dplyr::desc(adj_ratio_mean))\nadj_ratio_by_player\n#> # A tibble: 588 Ã— 7\n#>    player            shots     g    xg raw_ratio adj_ratio_mean adj_ratio_sd\n#>    <chr>             <int> <int> <dbl>     <dbl>          <dbl>        <dbl>\n#>  1 Fernando Llorente    57    16  9.19      1.74           1.40        0.281\n#>  2 Philippe Coutinho   160    20 12.5       1.60           1.37        0.256\n#>  3 Shkodran Mustafi     37     5  1.82      2.75           1.34        0.357\n#>  4 Pascal GroÃŸ          43     7  3.34      2.10           1.34        0.334\n#>  5 Ryan Fraser          55     8  4.09      1.95           1.34        0.324\n#>  6 Eden Hazard         148    28 19.2       1.45           1.33        0.219\n#>  7 James McArthur       53    10  5.93      1.69           1.31        0.299\n#>  8 Charlie Daniels      39     5  2.18      2.30           1.30        0.346\n#>  9 Xherdan Shaqiri     117    12  7.61      1.58           1.29        0.282\n#> 10 Andy Carroll         67    10  6.09      1.64           1.29        0.296\n#> # â„¹ 578 more rows\n```\n:::\n\n\nFinally, let's plot our results, plotting our adjusted mean estimates of $O_p$, Â±1 standard deviation about the adjusted mean. As noted earlier, we won't achieve exactly the same results as Shaw due to using a different set of xG values, but evidently we've achieved results reasonably close to his.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(ggplot2)\nlibrary(forcats)\nlibrary(ggh4x)\nlibrary(magick)\n\nshaw_players <- c(\n  'Eden Hazard' = 'E. Hazard',\n  'Mohamed Salah' = 'Mohamed Salah',\n  'Son Heung-Min' = 'Heung-Min Son',\n  'Joshua King' = 'J. King',\n  'Romelu Lukaku' = 'R. Lukaku',\n  'Harry Kane' = 'H. Kane',\n  'Sadio ManÃ©' = 'S. Mane',\n  'Dele Alli' = 'D. Ali',\n  'Riyad Mahrez' = 'R. Mahrez',\n  'Christian Eriksen' = 'C. Eriksen',\n  'Pedro' = 'Pedro',\n  'Alexis SÃ¡nchez' = 'A. Sanchez',\n  'Roberto Firmino' = 'Roberto Firmino',\n  'Jamie Vardy' = 'J. Vardy',\n  'Xherdan Shaqiri' = 'X. Shaqiri',\n  'Wilfried Zaha' = 'W. Zaha',\n  'Nathan Redmond' = 'N. Redmond',\n  'Gylfi Sigurdsson' = 'G. Sigurdsson',\n  'Kevin De Bruyne' = 'K. De Bruyne',\n  'Andros Townsend' = 'A. Townsend',\n  'Sergio AgÃ¼ero' = 'S. Aguero',\n  'Marcus Rashford' = 'M. Rashford',\n  'Jermain Defoe' = 'J. Defoe',\n  'Raheem Sterling' = 'R. Sterling',\n  'Marko Arnautovic' = 'M. Arnautovic',\n  'Paul Pogba' = 'P. Pogba',\n  'SalomÃ³n RondÃ³n' = 'S. Rondon',\n  'Christian Benteke' = 'C. Benteke'\n)\n\nordinal_adj_ratio_by_player <- adj_ratio_by_player |>\n  dplyr::filter(\n    player %in% names(shaw_players)\n  ) |> \n  dplyr::mutate(\n    player = forcats::fct_reorder(shaw_players[player], adj_ratio_mean)\n  )\n\nadj_ratio_plot <- ordinal_adj_ratio_by_player |>\n  ggplot2::ggplot() +\n  ggplot2::aes(y = player) +\n  ggplot2::geom_errorbarh(\n    aes(\n      xmin = adj_ratio_mean - adj_ratio_sd,\n      xmax = adj_ratio_mean + adj_ratio_sd\n    ),\n    color = 'blue',\n    linewidth = 0.1,\n    height = 0.3\n  ) +\n  ggplot2::geom_point(\n    ggplot2::aes(x = adj_ratio_mean),\n    shape = 23,\n    size = 0.75,\n    stroke = 0.15,\n    fill = 'red',\n    color = 'black'\n  ) +\n  ggplot2::geom_vline(\n    ggplot2::aes(xintercept = 1), \n    linewidth = 0.1, \n    linetype = 2\n  ) +\n  ## add duplicate axis for ticks: https://stackoverflow.com/questions/56247205/r-ggplot2-add-ticks-on-top-and-right-sides-of-all-facets\n  ggplot2::scale_x_continuous(sec.axis = ggplot2::dup_axis()) +\n  ## ggplot2 doesn't support duplicated and creatinga  second axis for discrete variables:\n  ##   https://github.com/tidyverse/ggplot2/issues/3171.\n  ##   using ggh4x is a workaround.\n  ggplot2::guides(\n    y.sec = ggh4x::guide_axis_manual(\n      breaks = ordinal_adj_ratio_by_player$player,\n      labels = ordinal_adj_ratio_by_player$player\n    )\n  ) +\n  ggplot2::theme_linedraw(base_family = 'DejaVu Sans', base_size = 4) +\n  ggplot2::theme(\n    plot.title = ggplot2::element_text(hjust = 0.5, size = 4.25, face = 'plain'),\n    axis.ticks.length = ggplot2::unit(-1, 'pt'),\n    axis.ticks = ggplot2::element_line(linewidth = 0.05),\n    panel.grid.major.y = ggplot2::element_blank(),\n    panel.grid.minor = ggplot2::element_blank(),\n    panel.grid.major.x = ggplot2::element_line(linetype = 2),\n    axis.text.x.top = ggplot2::element_blank(),\n    axis.text.y.right = ggplot2::element_blank(),\n    axis.title.x.top = ggplot2::element_blank(),\n    axis.title.y.right = ggplot2::element_blank()\n  ) +\n  ggplot2::labs(\n    title = 'Shots from 2016/17 & 2017/18 seasons',\n    y = NULL,\n    x = 'Outperformance (= G/xG)'\n  )\n\nproj_dir <- 'posts/xg-ratio-empirical-bayes'\nplot_path <- file.path(proj_dir, 'shaw-figure-1-replication.png')\nggplot2::ggsave(\n  adj_ratio_plot,\n  filename = plot_path,\n  units = 'px',\n  width = 549,\n  height = 640\n)\n\norig_image <- magick::image_read(file.path(proj_dir, 'shaw-figure-1.png'))\nreplicated_image_with_asa_logo <- magick::image_read(plot_with_asa_logo_path)\ncombined_image_with_tony_logo <- magick::image_append(\n  c(orig_image, replicated_image_with_tony_logo), \n  stack = TRUE\n)\n\nmagick::image_write(\n  combined_image_with_tony_logo, \n  path = file.path(proj_dir, 'shaw-figure-1-compared-w-tony-logo.png')\n)\n```\n:::\n\n\n![](shaw-figure-1-compared-w-tony-logo.png)\n\n### Beyond Replication\n\nFor the sake of having a pretty plot that's not just an attempt to replicate the original, let's run it all back, this time with EPL 2021/22 and 2022/23 data.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nraw_shots <- worldfootballR::load_understat_league_shots(league = 'EPL')\nshots <- raw_shots |> \n  tibble::as_tibble() |> \n  dplyr::filter(\n    season %in% c(2021L, 2022L),\n    situation != 'DirectFreeKick'\n  ) |> \n  dplyr::arrange(id) |> \n  dplyr::transmute(\n    id,\n    player,\n    ## since 2022/23, xG is filled out, not x_g\n    xg = dplyr::coalesce(x_g, xG),\n    g = as.integer(result == 'Goal')\n  )\n\nshots_by_player <- shots |> \n  dplyr::group_by(player) |> \n  dplyr::summarize(\n    shots = dplyr::n(),\n    dplyr::across(c(g, xg), sum)\n  ) |> \n  dplyr::ungroup() |> \n  dplyr::mutate(raw_ratio = g / xg) |> \n  dplyr::arrange(dplyr::desc(shots))\nshots_by_player\n\nshots_by_player$adj_ratio <- purrr::map2(\n  shots_by_player$g, shots_by_player$xg,\n  function(g, xg) {\n    simulate_gamma_posterior(\n      successes = g,\n      trials = xg,\n      prior_shape = prior_shape,\n      prior_rate = prior_rate\n    )\n  }\n)\n\nadj_ratio_by_player <- shots_by_player |> \n  tidyr::unnest_wider(\n    adj_ratio, \n    names_sep = '_'\n  ) |> \n  dplyr::arrange(dplyr::desc(adj_ratio_mean))\n\nordinal_adj_ratio_by_player <- adj_ratio_by_player |>\n  dplyr::filter(\n    player %in% names(shaw_players)\n  ) |> \n  dplyr::mutate(\n    player = forcats::fct_reorder(shaw_players[player], adj_ratio_mean)\n  )\n\nlibrary(htmltools)\nlibrary(sysfonts)\nlibrary(showtext)\n\nblackish_background <- '#1f1f1f'\nfont <- 'Titillium Web'\nsysfonts::font_add_google(font, font)\nsysfonts::font_add('fb', 'Font Awesome 6 Brands-Regular-400.otf')\nshowtext::showtext_auto()\nplot_resolution <- 300\nshowtext::showtext_opts(dpi = plot_resolution)\n## https://github.com/tashapiro/tanya-data-viz/blob/1dfad735bca1a7f335969f0eafc94cf971345075/nba-shot-chart/nba-shots.R#L64\n\ntag_lab <- htmltools::tagList(\n  htmltools::tags$span(htmltools::HTML(enc2utf8(\"&#xf099;\")), style='font-family:fb'),\n  htmltools::tags$span(\"@TonyElHabr\"),\n)\n\nbeyond_replication_adj_ratio_plot <- ordinal_adj_ratio_by_player |>\n  ggplot2::ggplot() +\n  ggplot2::aes(y = player) +\n  ggplot2::geom_vline(\n    ggplot2::aes(xintercept = 1), \n    linewidth = 1.5,\n    linetype = 2,\n    color = 'white'\n  ) +\n  ggplot2::geom_errorbarh(\n    ggplot2::aes(\n      xmin = adj_ratio_mean - adj_ratio_sd,\n      xmax = adj_ratio_mean + adj_ratio_sd\n    ),\n    color = 'white',\n    height = 0.5\n  ) +\n  ggplot2::geom_point(\n    ggplot2::aes(x = adj_ratio_mean, size = shots),\n    color = 'white'\n  ) +\n  ggplot2::theme_minimal() +\n  ggplot2::theme(\n    text = ggplot2::element_text(family = font, color = 'white'),\n    title = ggplot2::element_text(size = 14, color = 'white'),\n    plot.title = ggplot2::element_text(face = 'bold', size = 16, color = 'white', hjust = 0),\n    plot.title.position = 'plot',\n    plot.subtitle = ggplot2::element_text(size = 14, color = 'white', hjust = 0),\n    plot.margin = ggplot2::margin(10, 20, 10, 20),\n    plot.caption = ggtext::element_markdown(color = 'white', hjust = 0, size = 10, face = 'plain', lineheight = 1.1),\n    plot.caption.position = 'plot',\n    plot.tag = ggtext::element_markdown(size = 10, color = 'white', hjust = 1),\n    plot.tag.position = c(0.99, 0.01),\n    panel.grid.major.y = ggplot2::element_blank(),\n    panel.grid.minor.x = ggplot2::element_blank(),\n    panel.grid.major.x = ggplot2::element_line(linewidth = 0.1),\n    plot.background = ggplot2::element_rect(fill = blackish_background, color = blackish_background),\n    panel.background = ggplot2::element_rect(fill = blackish_background, color = blackish_background),\n    axis.title = ggplot2::element_text(color = 'white', size = 14, face = 'bold', hjust = 0.99),\n    axis.line = ggplot2::element_blank(),\n    axis.text = ggplot2::element_text(color = 'white', size = 12),\n    axis.text.y = ggtext::element_markdown(),\n    legend.text = ggplot2::element_text(color = 'white', size = 12),\n    legend.position = 'top'\n  ) +\n  ggplot2::labs(\n    title = 'Top 20 shooting overperformers in the EPL',\n    subtitle = 'EPL 2021/22 and 2022/23 seasons. ',\n    caption = 'Players sorted according to descending adjusted G/xG ratio. Minimum 100 shots.<br/>**Source**: understat.',\n    y = NULL,\n    x = 'Adjusted G/xG Ratio',\n    tag = tag_lab\n  )\n\nbeyond_replication_plot_path <- file.path(proj_dir, 'beyond-replication.png')\nggplot2::ggsave(\n  beyond_replication_adj_ratio_plot,\n  filename = beyond_replication_plot_path,\n  width = 7,\n  height = 7\n)\n```\n:::\n\n\n![](beyond-replication.png)\n\nFor those who follow the EPL, The usual suspects, like Heung-Min Son, show up among the best of the best HERE. The adjusted mean minus one standard deviation value exceeds zero for Son, so one might say that he was a significantly skilled shooter over the past two season.[^8]\n\n[^8]: A more strict and, arguably, correct measure is a 90 or 95% credible interval to ascertain \"significance\". Nonetheless, for the purpose of maintaining consistency with Shaw, I'm showing standard deviation.\n\n## Conclusion\n\nHaving not seen a very clear example of Gamma-Prior EB implementation on the internet--although there are several good explanations with toy examples such as [this e-book chapter from HyvÃ¶nen and Topias Tolonen](https://vioshyvo.github.io/Bayesian_inference/conjugate-distributions.html)--I hope I've perhaps un-muddied the waters for at least one interested reader.\n\nAs for the actual application of $G / xG$ for evaluating shooting performance, I have mixed feelings about it. Further analysis shows that it has basically zero year-over-year stability, i.e. one shouldn't use a player's raw or adjusted G/xG ratio in one season to try to predict whether their overperformance ratio will sustain in the next season. On the other hand, by simply making player estimates more comparable, the EB adjustment of $O_p$ certainly is an improvement over raw $G / xG$ itself.\n\nComparing hypothetical player A with 6 goals on 2 xG ($O_p = 3$) vs. player B with 120 goals on 100 xG directly ($O_p = 1.2$) is unfair; the former could be performing at an unsustainable rate, while the latter has demonstrated sustained overperformance over a lot more time. Indeed, applying the EB adjustment to these hypothetical numbers, player A's $O_p$ would be shrunken back towards 1, and player B's adjustment would be basically nothing, indicating that player B's shooting performance is stronger, on average.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}