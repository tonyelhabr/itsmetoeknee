{
  "hash": "7e9257a95645951b82023485f47ff983",
  "result": {
    "markdown": "---\ntitle: Should we account for team quality in an xG model?\ndescription: \"F**king around (with an xG model) and finding out\"\ndate: 2023-12-31\ncategories:\n  - r\n  - soccer\nimage: elo-pdp.png\nexecute: \n  code-fold: false\n  eval: false\n  include: true\n  echo: true\n---\n\n\n## Introduction\n\n\"Should we account for team quality in an [xG model](https://theanalyst.com/na/2021/07/what-are-expected-goals-xg/)?\" From a purely philosophical point of view, my opinion is \"no\"--I think that xG models should be a purely \"descriptive\", agnostic to player and team abilities, as well as other non-event factors such as game state.\n\nBut I thought it might be fun to entertain the question from a quantitative perspective. If we add features for team strength to an xG model, can we meaningfully improve the model's overall predictive performance and [calibration](https://tonyelhabr.rbind.io/posts/probability-calibration/) (with respect to team quality)?\n\n### Motivation\n\nThis write-up is inspired by [Ryan Brill](https://twitter.com/RyanBrill_)'s recent [presentation on fourth-down decision-making in the National Football League (NFL)](https://youtu.be/uS4XxQ0LVfE?si=BnmzeePnk3R5uiY3&t=361). He points out that [expected points (EP)](https://www.nfeloapp.com/analysis/expected-points-added-epa-nfl/) models in the NFL have a [selection bias](https://en.wikipedia.org/wiki/Selection_bias) problem--they tend to under-rate the probability of a positive outcome for \"good\" teams and over-rate such outcomes for \"bad\" teams.\n\nExpected goals (xG) in soccer also show signs of this phenomenon. [Lars Maurath](https://github.com/larsmaurath) has [a great deep-dive](https://www.thesignificantgame.com/portfolio/do-naive-xg-models-underestimate-expected-goals-for-top-teams/) looking into expected goals under-estimation for strong teams in the [Big 5 European leagues](https://fbref.com/en/comps/Big5/Big-5-European-Leagues-Stats). The plot below (copied shamelessly from Lars' post) shows that a naive xG model consistently under-predicts Barcelona's goals over the course of the season, for seasons from 2007/08 to 2018/19. Even [StatsBomb](https://statsbomb.com/)'s model--which is more sophisticated--tends to underestimate the true cumulative goal total!\n\n![](barcelona-cumulative-xg.png)\n\n## Analysis and Results\n\n### Data\n\nI'll be using event data that I've ingested with the [`{socceraction}` package](https://github.com/ML-KULeuven/socceraction) (which I've made available [here](https://github.com/tonyelhabr/socceraction-streamlined/releases)!) for the 2013/14 through 2022/23 [English Premier League (EPL)](https://www.premierleague.com/) seasons. I'll focus on just \"open-play\" shots, i.e. shots excluding penalties and those taken from set pieces.\n\nBut we also need some measure of team quality. To that end, I've scraped [Elo](https://en.wikipedia.org/wiki/Elo_rating_system) ratings from [ClubElo](http://clubelo.com/). I've chosen Elo because provides a intuitive, sport-agnostic measure of relative skill. Also, it is calculated independent of the events that take place in a game, so correlation with measures of shot volume, quality, etc. are only coincidental. (It was also fairly easy to retrieve!)\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Package imports and other setup\"}\n## Data retrieval\nlibrary(curl)\nlibrary(arrow)\nlibrary(qs) ## local dev\n\n## Data manipulation\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(lubridate)\n\n## Modeling\nlibrary(rsample)\nlibrary(recipes)\nlibrary(parsnip)\nlibrary(workflows)\nlibrary(hardhat)\n\n## Model tuning\nlibrary(tune)\nlibrary(dials)\nlibrary(workflowsets)\nlibrary(finetune)\n\n## Model diagnostics\nlibrary(rlang)\nlibrary(yardstick)\nlibrary(pdp)\nlibrary(vip)\n\n## Plotting\nlibrary(ggplot2)\nlibrary(sysfonts)\nlibrary(showtext)\nlibrary(ggtext)\nlibrary(htmltools)\nlibrary(scales)\n\nPROJ_DIR <- 'posts/xg-team-quality'\n\nTAG_LABEL <- htmltools::tagList(\n  htmltools::tags$span(htmltools::HTML(enc2utf8(\"&#xf099;\")), style = 'font-family:fb'),\n  htmltools::tags$span(\"@TonyElHabr\"),\n)\nSUBTITLE_LABEL <- 'English Premier League, 2012/13 - 2022/23'\nPLOT_RESOLUTION <- 300\nWHITISH_FOREGROUND_COLOR <- 'white'\nCOMPLEMENTARY_FOREGROUND_COLOR <- '#cbcbcb' # '#f1f1f1'\nBLACKISH_BACKGROUND_COLOR <- '#1c1c1c'\nCOMPLEMENTARY_BACKGROUND_COLOR <- '#4d4d4d'\nFONT <- 'Titillium Web'\nsysfonts::font_add_google(FONT, FONT)\n## https://github.com/tashapiro/tanya-data-viz/blob/main/chatgpt-lensa/chatgpt-lensa.R for twitter logo\nsysfonts::font_add('fb', 'Font Awesome 6 Brands-Regular-400.otf')\nshowtext::showtext_auto()\nshowtext::showtext_opts(dpi = PLOT_RESOLUTION)\n\nggplot2::theme_set(ggplot2::theme_minimal())\nggplot2::theme_update(\n  text = ggplot2::element_text(family = FONT),\n  title = ggplot2::element_text(size = 20, color = WHITISH_FOREGROUND_COLOR),\n  plot.title = ggtext::element_markdown(face = 'bold', size = 20, color = WHITISH_FOREGROUND_COLOR),\n  plot.title.position = 'plot',\n  plot.subtitle = ggtext::element_markdown(size = 16, color = COMPLEMENTARY_FOREGROUND_COLOR),\n  axis.text = ggplot2::element_text(color = WHITISH_FOREGROUND_COLOR, size = 14),\n  # axis.title = ggplot2::element_text(size = 14, color = WHITISH_FOREGROUND_COLOR, face = 'bold', hjust = 0.99),\n  axis.title.x = ggtext::element_markdown(size = 14, color = WHITISH_FOREGROUND_COLOR, face = 'bold', hjust = 0.99),\n  axis.title.y = ggtext::element_markdown(size = 14, color = WHITISH_FOREGROUND_COLOR, face = 'bold', hjust = 0.99),\n  axis.line = ggplot2::element_blank(),\n  strip.text = ggplot2::element_text(size = 14, color = WHITISH_FOREGROUND_COLOR, face = 'bold', hjust = 0),\n  legend.position = 'top',\n  legend.text = ggplot2::element_text(size = 12, color = WHITISH_FOREGROUND_COLOR, face = 'plain'),\n  legend.title = ggplot2::element_text(size = 12, color = WHITISH_FOREGROUND_COLOR, face = 'bold'),\n  panel.grid.major = ggplot2::element_line(color = COMPLEMENTARY_BACKGROUND_COLOR),\n  panel.grid.minor = ggplot2::element_line(color = COMPLEMENTARY_BACKGROUND_COLOR),\n  panel.grid.minor.x = ggplot2::element_blank(),\n  panel.grid.minor.y = ggplot2::element_blank(),\n  plot.margin = ggplot2::margin(10, 20, 10, 20),\n  plot.background = ggplot2::element_rect(fill = BLACKISH_BACKGROUND_COLOR, color = BLACKISH_BACKGROUND_COLOR),\n  plot.caption = ggtext::element_markdown(color = WHITISH_FOREGROUND_COLOR, hjust = 0, size = 10, face = 'plain'),\n  plot.caption.position = 'plot',\n  plot.tag = ggtext::element_markdown(size = 10, color = WHITISH_FOREGROUND_COLOR, hjust = 1),\n  plot.tag.position = c(0.99, 0.01),\n  panel.spacing.x = grid::unit(2, 'lines'),\n  panel.background = ggplot2::element_rect(fill = BLACKISH_BACKGROUND_COLOR, color = BLACKISH_BACKGROUND_COLOR)\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Retrieve and wrangle data\"}\nread_parquet_from_url <- function(url) {\n  load <- curl::curl_fetch_memory(url)\n  arrow::read_parquet(load$content)\n}\n\nREPO <- 'tonyelhabr/socceraction-streamlined'\nread_socceraction_parquet_release <- function(name, tag) {\n  url <- sprintf('https://github.com/%s/releases/download/%s/%s.parquet', REPO, tag, name)\n  read_parquet_from_url(url)\n}\n\nread_socceraction_parquet_releases <- function(name, tag = 'data-processed') {\n  purrr::map_dfr(\n    2013:2022,\n    \\(season_start_year) {\n      basename <- sprintf('8-%s-%s', season_start_year, name)\n      message(basename)\n      read_socceraction_parquet_release(basename, tag = tag)\n    }\n  )\n}\n\nread_socceraction_parquet <- function(name, branch = 'main') {\n  url <- sprintf('https://github.com/%s/raw/%s/%s.parquet', REPO, branch, name)\n  read_parquet_from_url(url)\n}\n\nx <- read_socceraction_parquet_releases('x')\ny <- read_socceraction_parquet_releases('y')\nactions <- read_socceraction_parquet_releases('actions')\ngames <- read_socceraction_parquet_releases('games') |> \n  dplyr::mutate(\n    date = lubridate::date(game_date)\n  )\nteam_elo <- read_socceraction_parquet('data/final/8/2013-2022/clubelo-ratings')\n\nopen_play_shots <- games |>\n  dplyr::transmute(\n    season_id,\n    game_id,\n    date,\n    home_team_id,\n    away_team_id\n  ) |> \n  dplyr::inner_join(\n    x |> \n      dplyr::filter(type_shot_a0 == 1) |> \n      dplyr::select(\n        game_id,\n        action_id,\n        \n        ## features\n        start_x_a0,\n        start_y_a0,\n        start_dist_to_goal_a0,\n        start_angle_to_goal_a0,\n        type_dribble_a1,\n        type_pass_a1,\n        type_cross_a1,\n        type_corner_crossed_a1,\n        type_shot_a1,\n        type_freekick_crossed_a1,\n        bodypart_foot_a0,\n        bodypart_head_a0,\n        bodypart_other_a0\n      ) |> \n      dplyr::mutate(\n        dplyr::across(-c(game_id, action_id), as.integer)\n      ),\n    by = dplyr::join_by(game_id),\n    relationship = 'many-to-many'\n  ) |> \n  dplyr::inner_join(\n    y |> \n      dplyr::transmute(\n        game_id, \n        action_id,\n        scores = ifelse(scores, 'yes', 'no') |> factor(levels = c('yes', 'no'))\n      ),\n    by = dplyr::join_by(game_id, action_id)\n  ) |> \n  dplyr::inner_join(\n    actions |> \n      dplyr::select(\n        game_id,\n        action_id,\n        team_id,\n        player_id\n      ),\n    by = dplyr::join_by(game_id, action_id)\n  ) |> \n  dplyr::left_join(\n    team_elo |> dplyr::select(date, home_team_id = team_id, home_elo = elo),\n    by = dplyr::join_by(date, home_team_id)\n  ) |> \n  dplyr::left_join(\n    team_elo |> dplyr::select(date, away_team_id = team_id, away_elo = elo),\n    by = dplyr::join_by(date, away_team_id)\n  ) |> \n  dplyr::transmute(\n    date,\n    season_id,\n    game_id,\n    team_id,\n    opponent_team_id = ifelse(team_id == home_team_id, away_team_id, home_team_id),\n    action_id,\n    \n    scores,\n    \n    elo = ifelse(team_id == home_team_id, home_elo, away_elo),\n    opponent_elo = ifelse(team_id == home_team_id, away_elo, home_elo),\n    elo_diff = elo - opponent_elo,\n    \n    start_dist_to_goal_a0,\n    start_angle_to_goal_a0,\n    type_dribble_a1,\n    type_pass_a1,\n    type_cross_a1,\n    type_corner_crossed_a1,\n    type_shot_a1,\n    type_freekick_crossed_a1,\n    bodypart_foot_a0,\n    bodypart_head_a0,\n    bodypart_other_a0\n  )\n```\n:::\n\n\n\n\n\n\nThe open play shot data looks like this.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ndplyr::glimpse(open_play_shots)\n#> Rows: 92,322\n#> Columns: 21\n#> $ date                     <date> 2012-08-18, 2012-08-18, 2012-08-18, 2012-…\n#> $ season_id                <int> 2013, 2013, 2013, 2013, 2013, 2013, 2013, …\n#> $ game_id                  <int> 614051, 614051, 614051, 614051, 614051, 61…\n#> $ team_id                  <int> 16, 13, 16, 13, 13, 13, 13, 13, 13, 13, 16…\n#> $ opponent_team_id         <dbl> 13, 16, 13, 16, 16, 16, 16, 16, 16, 16, 13…\n#> $ action_id                <int> 109, 123, 276, 436, 502, 517, 541, 573, 61…\n#> $ scores                   <fct> no, no, no, no, no, no, no, no, no, no, no…\n#> $ elo                      <dbl> 1669.172, 1827.481, 1669.172, 1827.481, 18…\n#> $ opponent_elo             <dbl> 1827.481, 1669.172, 1827.481, 1669.172, 16…\n#> $ elo_diff                 <dbl> -158.3093, 158.3093, -158.3093, 158.3093, …\n#> $ start_dist_to_goal_a0    <int> 12, 27, 21, 15, 8, 16, 29, 10, 26, 32, 30,…\n#> $ start_angle_to_goal_a0   <int> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, …\n#> $ type_dribble_a1          <int> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, …\n#> $ type_pass_a1             <int> 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, …\n#> $ type_cross_a1            <int> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, …\n#> $ type_corner_crossed_a1   <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#> $ type_shot_a1             <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#> $ type_freekick_crossed_a1 <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#> $ bodypart_foot_a0         <int> 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, …\n#> $ bodypart_head_a0         <int> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, …\n#> $ bodypart_other_a0        <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n```\n:::\n\n\nWe'll set aside the shots from the 7 seasons from 2013/14 to 2019/20 for our training set, and the 3 seasons from 2020/21 to 2022/23 for our test set.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Split the data for modeling\"}\nsplit <- rsample::make_splits(\n  open_play_shots |> dplyr::filter(season_id %in% c(2013L:2019L)),\n  open_play_shots |> dplyr::filter(season_id %in% c(2020L:2022L))\n)\n\ntrain <- rsample::training(split)\ntest <- rsample::testing(split)\n```\n:::\n\n\nIt's worth spotlighting Elo a bit more, since it's the novel feature here. Below is a look at the distribution of pre-match Elo over the course of the entire data set.\n\n\n\n\n\n![](elo-hist.png)\n\nTo make Elo values feel a bit more tangible, note that:\n\n-   Man City tends to sustain an Elo greater than 1850 (arguably the best team in the EPL for the past decade).\n-   Bottom-table teams tend to have Elos less than 1650.\n\n\n\n\n\nThe pre-match difference team Elos follows a normal-ish distribution, with most values falling within the ±300 range.\n\n![](elo-diff-hist.png)\n\n### Model Training\n\nThe feature set for our \"base\" xG model consists of the following:\n\n-   location of the shot (distance and angle of the shot to the center of the goal mouth)[^1].\n-   type of action leading to the shot.\n-   body part with which the shot was taken.\n\n[^1]: One might get a slightly more performant by adding the `x` and `y` coordinates of the shot--to implicitly account for right-footed bias, for example--but I actually prefer not to add those in the model. Such terms can result in slight [over-fitting](https://en.wikipedia.org/wiki/Overfitting), in the presence of other features that provide information about the location of the shot, such as distance and angle. (This is the classical [\"bias-variance\" trade-off](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff).)\n\nThese features are essentially what [all xG models](https://fbref.com/en/expected-goals-model-explained/) have in common, although the exact implementation differs. Data providers such as [Opta also account for information](https://theanalyst.com/na/2023/08/what-is-expected-goals-xg/) that is not captured in traditional event data, such as the position of the goalkeeper.\n\nFor the team-quality-adjusted (or \"Elo-augmented\") model, I'll add two additional features:\n\n-   `elo`: the Elo of the team of the shot-taker.\n-   `elo_diff`: the difference in the ELO of the shot-taking team and the opposing team.\n\nThe former is meant to capture the opponent-agnostic quality of a team, while the latter captures the quality of the team relative to their opponent.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Setting up the models\"}\nrec_elo <- recipes::recipe(\n  scores ~ \n    elo +\n    elo_diff +\n    start_dist_to_goal_a0 +\n    start_angle_to_goal_a0 +\n    type_dribble_a1 +\n    type_pass_a1 +\n    type_cross_a1 +\n    type_corner_crossed_a1 +\n    type_shot_a1 +\n    type_freekick_crossed_a1 +\n    bodypart_foot_a0 +\n    bodypart_head_a0 +\n    bodypart_other_a0,\n  data = train\n)\n\nrec_base <- rec_elo |> \n  recipes::step_rm(elo, elo_diff)\n```\n:::\n\n\nI'll be using [xgboost](https://xgboost.readthedocs.io/en/stable/) for our model, the state-of-the-art framework for tabular machine learning tasks. (It's also the type of model used by providers like Opta.) I'll choose [hyperparameters](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)) using [an efficient grid search](https://finetune.tidymodels.org/index.html), evaluating models with the [Brier skill score (BSS)](https://en.wikipedia.org/wiki/Brier_score#Brier_Skill_Score_(BSS)). For the reference Brier score for BSS, I'll use a dummy model that predicts 12% conversion for all shots.[^2]\n\n[^2]: 12% is approximately the observed shot conversion rate in the whole data set.\n\n::: {.callout-note collapse=\"true\"}\n## Brier skill score (BSS)\n\nIf this isn't the first blog post you've read of mine, you probably know that [I love to use BSS](https://tonyelhabr.rbind.io/posts/opta-xg-model-calibration) for classification tasks, especially for xG models.\n\nBut why BSS? Simply put, [Brier scores](https://en.wikipedia.org/wiki/Brier_score) are known as [the best evaluation metric](https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/) to use for classification tasks where you're purely interested in probabilities. And BSS goes one step beyond Brier scores, forcing one to contextualize the model evaluation with a reasonable baseline. In the context of xG, BSS helps us directly see whether our fitted model is better than a naive prediction, such as guessing that all shots convert at the observed shot conversion rate.\n\nKeep in mind that a higher BSS is ideal. A perfect model would have a BSS of 1; a model that is no better than a reference model would have a BSS of 0.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Functions for Brier skill score\"}\n## See also: probability-calibration\nbrier_skill_score <- function(data, ...) {\n  UseMethod('brier_skill_score')\n}\n\nbrier_skill_score <- yardstick::new_prob_metric(\n  brier_skill_score, \n  direction = 'maximize'\n)\n\nbss <- function(\n    truth, \n    estimate, \n    ref_estimate, \n    event_level,\n    case_weights,\n    ...\n) {\n  \n  if (length(estimate) == 1) {\n    estimate <- rep(estimate, length(truth))\n  }\n  \n  if (length(ref_estimate) == 1) {\n    ref_estimate <- rep(ref_estimate, length(truth))\n  }\n  \n  estimate_brier_score <- brier_class_vec(\n    truth = truth,\n    estimate = estimate,\n    event_level = event_level,\n    case_weights = case_weights,\n    ...\n  )\n  \n  ref_brier_score <- brier_class_vec(\n    truth = truth,\n    estimate = ref_estimate,\n    event_level = event_level,\n    case_weights = case_weights,\n    ...\n  )\n  \n  1 - (estimate_brier_score / ref_brier_score)\n}\n\nbrier_skill_score_estimator_impl <- function(\n    truth, \n    estimate, \n    ref_estimate, \n    event_level,\n    case_weights\n) {\n  bss(\n    truth = truth,\n    estimate = estimate,\n    ref_estimate = ref_estimate,\n    event_level = event_level,\n    case_weights = case_weights\n  )\n}\n\nbrier_skill_score_vec <- function(\n    truth, \n    estimate, \n    ref_estimate, \n    na_rm = TRUE, \n    event_level = yardstick:::yardstick_event_level(),\n    case_weights = NULL, \n    ...\n) {\n  \n  yardstick:::abort_if_class_pred(truth)\n  \n  estimator <- yardstick::finalize_estimator(\n    truth, \n    metric_class = 'brier_skill_score'\n  )\n  \n  yardstick::check_prob_metric(truth, estimate, case_weights, estimator)\n  \n  if (na_rm) {\n    result <- yardstick::yardstick_remove_missing(truth, estimate, case_weights)\n    \n    truth <- result$truth\n    estimate <- result$estimate\n    case_weights <- result$case_weights\n  } else if (yardstick::yardstick_any_missing(truth, estimate, case_weights)) {\n    return(NA_real_)\n  }\n  \n  brier_skill_score_estimator_impl(\n    truth = truth,\n    estimate = estimate,\n    ref_estimate = ref_estimate,\n    event_level = event_level,\n    case_weights = case_weights\n  )\n}\n\nbrier_skill_score.data.frame <- function(\n    data, \n    truth, \n    ...,\n    na_rm = TRUE,\n    event_level = yardstick:::yardstick_event_level(),\n    case_weights = NULL,\n    \n    ref_estimate = 0.5,\n    name = 'brier_skill_score'\n) {\n  yardstick::prob_metric_summarizer(\n    name = name,\n    fn = brier_skill_score_vec,\n    data = data,\n    truth = !!rlang::enquo(truth),\n    ...,\n    na_rm = na_rm,\n    event_level = event_level,\n    case_weights = !!rlang::enquo(case_weights),\n    fn_options = list(\n      ref_estimate = ref_estimate\n    )\n  )\n}\n\nxg_brier_skill_score <- function(data, ...) {\n  UseMethod('xg_brier_skill_score')\n}\n\n# REF_ESTIMATE <- open_play_shots |>\n#   dplyr::summarize(goal_rate = sum(scores == 'yes') / dplyr::n()) |>\n#   dplyr::pull(goal_rate)\nREF_ESTIMATE <- 0.12\n\nxg_brier_skill_score.data.frame <- function(...) {\n  brier_skill_score(\n    ref_estimate = REF_ESTIMATE,\n    name = 'xg_brier_skill_score',\n    ...\n  )\n}\n\nxg_brier_skill_score <- yardstick::new_prob_metric(\n  xg_brier_skill_score,\n  direction = 'maximize'\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Tuning the xG models\"}\n## Useful reference: https://jlaw.netlify.app/2022/01/24/predicting-when-kickers-get-iced-with-tidymodels/\nTREES <- 500\nLEARN_RATE <- 0.01\ndefine_xgboost_spec <- function(...) {\n  parsnip::boost_tree(\n    trees = !!TREES,\n    learn_rate = !!LEARN_RATE,\n    tree_depth = tune::tune(),\n    min_n = tune::tune(), \n    loss_reduction = tune::tune(),\n    sample_size = tune::tune(), \n    mtry = tune::tune(),\n    stop_iter = tune::tune()\n  ) |>\n    parsnip::set_engine('xgboost', ...) |> \n    parsnip::set_mode('classification')\n}\n\nspec_base <- define_xgboost_spec()\n\nelo_features <- rec_elo |> recipes::prep() |> recipes::juice() |> colnames()\nspec_elo <- define_xgboost_spec(\n  monotone_constraints = !!ifelse(elo_features %in% c('elo', 'elo_diff'), 1, 0)\n)\n\ngrid <- dials::grid_latin_hypercube(\n  dials::tree_depth(),\n  dials::min_n(),\n  dials::loss_reduction(),\n  sample_size = dials::sample_prop(),\n  dials::finalize(dials::mtry(), train),\n  dials::stop_iter(range = c(10L, 50L)),\n  size = 50\n)\n\nwf_sets <- workflowsets::workflow_set(\n  preproc = list(\n    base = rec_base, \n    elo = rec_elo\n  ),\n  ## Separate specs because we want monotonic constraints on the Elo model\n  models = list(\n    model = spec_base,\n    model = spec_elo\n  ),\n  cross = FALSE\n)\n\ncontrol <- finetune::control_race(\n  save_pred = TRUE,\n  # burn_in = 2,\n  parallel_over = 'everything',\n  save_workflow = TRUE,\n  verbose = TRUE\n)\n\nset.seed(42)\ntrain_folds <- rsample::vfold_cv(train, strata = scores, v = 5)\n\ntuned_results <- workflowsets::workflow_map(\n  wf_sets,\n  fn = 'tune_race_anova',\n  grid = grid,\n  control = control,\n  metrics = yardstick::metric_set(xg_brier_skill_score),\n  resamples = train_folds,\n  seed = 42\n)\n```\n:::\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Choosing best hyper-parameters\"}\nMODEL_TYPES <- c(\n  'base_model' = 'Base',\n  'elo_model' = 'Elo-augmented'\n)\nselect_best_model <- function(tuned_results, model_type) {\n  tuned_results |>\n    workflowsets::extract_workflow_set_result(model_type) |> \n    ## TODO\n    # tune::select_best(metric = 'xg_brier_skill_score') |> \n    tune::select_best(metric = 'f_meas') |> \n    dplyr::mutate(model_type = MODEL_TYPES[model_type])\n}\n\nbest_base_set <- select_best_model(tuned_results, 'base_model')\nbest_elo_set <- select_best_model(tuned_results, 'elo_model')\n\ndplyr::bind_rows(\n  best_base_set,\n  best_elo_set\n) |> \n  dplyr::transmute(\n    model_type,\n    mtry,\n    min_n,\n    tree_depth, \n    loss_reduction,\n    sample_size,\n    stop_iter\n  ) |> \n  knitr::kable()\n```\n:::\n\n\n::: callout-warning\n## Model tuning procedure\n\nThis post isn't meant to be so much about the why's and how's of model tuning and training, so I've spared commentary on the code.\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Model hyperparameters\n\nFor reproducibility, the chosen hyperparameters are as follows.\n\n| model_type | mtry | min_n | tree_depth | loss_reduction | sample_size | stop_iter |\n|:-----------|-----:|------:|-----------:|---------------:|------------:|----------:|\n| base       |   13 |    35 |          7 |      0.0010687 |   0.2217629 |        11 |\n| elo        |   14 |     9 |         14 |      0.0000274 |   0.8725705 |        14 |\n\nThe number of `trees` and `learning_rate` were pre-defined to be 500 and 0.01 respectively.\n:::\n\nFinally, we fit singular models on the entire training set. These are the models that we'll use to evaluate the effect of team quality on xG.[^3]\n\n[^3]: The hyperparameter search evaluated models fit on portions (or \"folds\") of the training data in a [cross-validation procedure](https://scikit-learn.org/stable/modules/cross_validation.html). We should fit one \"final\" model with all of the training data, per [best practice](https://tune.tidymodels.org/reference/last_fit.html).\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Fit models on entire training set after choosing best hyper-parameters\"}\nfinalize_tuned_results <- function(tuned_results, model_type) {\n  best_set <- select_best_model(tuned_results, model_type)\n  tuned_results |>\n    hardhat::extract_workflow(paste0(model_type, '_model')) |>\n    tune::finalize_workflow(best_base_set) |> \n    tune::last_fit(\n      split,\n      metrics = yardstick::metric_set(xg_brier_skill_score)\n    )\n}\nlast_base_fit <- finalize_tuned_results(tuned_results, 'base')\nlast_elo_fit <- finalize_tuned_results(tuned_results, 'elo')\n```\n:::\n\n\n\n\n\n\n### Feature Importance\n\nWe should look to see that the xG models are behaving is expected. One way of doing so is to look at the feature importance. In novel modeling cases, feature importance can be enlightening, as it tell us which features are contributing most to the predicted outcomes. In this case, I know that shot distance and angle should be two of the most important features (by far), as these are found to be the most important features in other similar \"basic\" public xG models, such as [this](https://github.com/AnshChoudhary/xGModel/blob/main/expected-goals-model-xg.ipynb) and [this](https://github.com/ML-KULeuven/soccer_xg/blob/master/notebooks/4-creating-custom-xg-pipelines.ipynb).\n\n\n::: {.cell}\n\n:::\n\n\n![](var-imp-base.png)\n\nIndeed, this is exactly what we see with the base xG model.\n\nNow, if we make the same plot for our team-quality-adjusted xG model, we see that Elo and Elo difference are the second and third most important features! That's pretty interesting.\n\nFurther, we can verify that the Elo-augmented model predicts xG in a manner that matches intuition using [partial dependence plots](https://christophm.github.io/interpretable-ml-book/pdp.html) (PDP). We should see that the augmented model predicts higher xG for teams with higher Elo.[^4]\n\n[^4]: Partial dependence plots aren't quite the gold standard for model interpretability, but I find them useful for getting a sense of the orientation and magnitude of a feature's effect, particularly for non-linear modeling techniques like gradient boosting.\n\n\n\n\n\n![](elo-pdp.png)\n\n::: callout-note\nKeep in mind that this shows just the average effect. On individual shots, the role of Elo could be stronger or weaker. And the range of the curve is constrained by the averaging--individual model outputs may be much lower than 12% or higher than 16% (i.e. the extremes of the y-axis).\n:::\n\nThe PDP for the Elo difference feature looks similar, so I've omitted it. Nonetheless, we can go one layer deeper and look at the average marginal effect of Elo and Elo difference simultaneously.[^5]\n\n[^5]: Note that the average marginal effect in the 2-D PDP below is shown via color instead of the y-axis.\n\n\n\n\n\n![](elo-pdp-2d.png)\n\nAs we should expect, predicted shot conversion is higher for strong teams and for teams that are stronger than their opponent.\n\n### Model Evaluation\n\nLet's take a look at whether the Elo-augmented model has a higher BSS than the \"base\" model.\n\n\n\n\n\n| model_type | brier_skill_score |\n|:-----------|------------------:|\n| base       |             0.104 |\n| elo        |             0.092 |\n\nSo, alas, it looks like there's effectively no difference between the models.[^6] In fact, the team quality adjusted model performs worse on the test set!\n\n[^6]: One could run a bootstrap analysis to prove this with more statistical rigor, but I'll leave that as an exercise for the eager reader.\n\nBut BSS is just one, wholistic measure. We also wanted to look at the calibration of an xG model that accounts for team quality might look compared to that of a traditional xG model. To put that into context, let's circle back to the premise that xG models under-estimate the shot conversion of strong teams.\n\n\n\n\n\nThe table above indeed provides evidence for the miscalibration phenomenon--the \"base\" xG model tends to predict higher xG compared to actual goals scores for teams with lower Elo compared to higher Elo.\n\n## Discussion\n\n## Conclusion\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}