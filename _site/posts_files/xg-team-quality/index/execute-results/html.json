{
  "hash": "2c3e2ede44af898ae39adee9496b5b65",
  "result": {
    "markdown": "---\ntitle: Should we account for team quality in an xG model?\ndescription: \"Finnicking around (with an xG model) and finding out\"\ndate: 2023-12-29\ncategories:\n  - r\n  - soccer\nimage: compared-calibration.png\nexecute: \n  code-fold: false\n  eval: false\n  include: true\n  echo: true\n---\n\n\n## Introduction\n\n\"Should we account for team quality in an [xG model](https://theanalyst.com/na/2021/07/what-are-expected-goals-xg/)?\" From a purely philosophical point of view, my opinion is \"no\"--I think that xG models should be agnostic to player and team abilities, even if we were to find that accounting for shot-taker or team identity improves the performance of the model.\n\nBut I thought it might be fun to entertain the question from a quantitative perspective. If we add features for team strength to an xG model, can we meaningfully improve the model's overall predictive performance and [calibration](https://tonyelhabr.rbind.io/posts/probability-calibration/)?\n\n### Motivation\n\nThis write-up is inspired by [Ryan Brill](https://twitter.com/RyanBrill_)'s recent [presentation on fourth-down decision-making in the National Football League (NFL)](https://youtu.be/uS4XxQ0LVfE?si=BnmzeePnk3R5uiY3&t=361). He points out that [expected points (EP)](https://www.nfeloapp.com/analysis/expected-points-added-epa-nfl/) models in the NFL have a [selection bias](https://en.wikipedia.org/wiki/Selection_bias) problem--they tend to under-rate the probability of a positive outcome for \"good\" teams and over-rate such outcomes for \"bad\" teams.\n\nExpected goals (xG) in soccer also suffer from this bias. [Lars Maurath](https://github.com/larsmaurath) has [a great deep-dive](https://www.thesignificantgame.com/portfolio/do-naive-xg-models-underestimate-expected-goals-for-top-teams/) looking into expected goals under-estimation for strong teams in the [Big 5 European leagues](https://fbref.com/en/comps/Big5/Big-5-European-Leagues-Stats). The plot below (copied shamelessly from Lars' post) shows that a naive xG model consistently under-predicts Barcelona's goals over the course of the season, for seasons from 2007/08 to 2018/19. Even [StatsBomb](https://statsbomb.com/)'s model--which is more sophisticated--tends to underestimate the true cumulative goal total!\n\n![](barcelona-cumulative-xg.png)\n\nThis is evident in the [English Premier League (EPL)](https://en.wikipedia.org/wiki/Premier_League) as well. The [end-of-season table for the 2022/23 season](https://fbref.com/en/comps/9/2022-2023/2022-2023-Premier-League-Stats) shows that xGD, i.e. goals (G) minus xG, is very positive for the top 6 teams and very negative for the bottom 6 teams.\n\n\n\n\n\n| Team            | Points |   G |   xG |   xGD |\n|:----------------|-------:|----:|-----:|------:|\n| Manchester City |     89 |  33 | 78.6 |  46.5 |\n| Arsenal         |     84 |  43 | 71.9 |  29.9 |\n| Manchester Utd  |     75 |  43 | 67.7 |  17.3 |\n| Newcastle Utd   |     71 |  33 | 72.0 |  32.4 |\n| Liverpool       |     67 |  47 | 72.6 |  21.7 |\n| Brighton        |     62 |  53 | 73.3 |  23.1 |\n| Aston Villa     |     61 |  46 | 50.2 |  -2.2 |\n| Tottenham       |     60 |  63 | 57.1 |   7.4 |\n| Brentford       |     59 |  46 | 56.8 |   6.8 |\n| Fulham          |     52 |  53 | 46.2 | -17.6 |\n| Crystal Palace  |     45 |  49 | 39.3 |  -8.8 |\n| Chelsea         |     44 |  47 | 49.5 |  -3.0 |\n| Wolves          |     41 |  58 | 36.8 | -23.1 |\n| West Ham        |     40 |  55 | 49.2 |  -3.9 |\n| Bournemouth     |     39 |  71 | 38.6 | -25.3 |\n| Nott'ham Forest |     38 |  68 | 39.3 | -24.9 |\n| Everton         |     36 |  57 | 45.2 | -20.5 |\n| Leicester City  |     34 |  68 | 50.6 | -12.8 |\n| Leeds United    |     31 |  78 | 47.4 | -19.8 |\n| Southampton     |     25 |  73 | 37.7 | -23.3 |\n\nLars does an \"ex-post\" analysis--evaluating the \"residual\" of expected goals, or xGD--to arrive at his conclusion that team quality does seem to explain large deviations between goals and expected goals for top teams.\n\n> \\[W\\]hen conditioning on... team quality (Elo ranking) I do not find this bias in either naive or sophisticated models\n\nI'm curious to see if we can arrive at a related conclusion in a more direct, \"ex-ante\" fashion. Can we reduce the underestimation of goals for strong teams by directly accounting for team quality in an xG model?\n\n## Analysis and Results\n\n### Data\n\nI'll be using event data that I've ingested with the [`{socceraction}` package](https://github.com/ML-KULeuven/socceraction) (which I've made available [here](https://github.com/tonyelhabr/socceraction-streamlined/releases)!) for the 2013/14 through 2022/23 EPL seasons. I'll focus on just \"open-play\" shots, i.e. shots excluding penalties and those taken from set pieces.\n\nAdditionally, I've scraped [Elo](https://en.wikipedia.org/wiki/Elo_rating_system) ratings from [ClubElo](http://clubelo.com/) for my measure of team quality. I've chosen Elo because it provides an intuitive, sport-agnostic measure of relative skill. Also, it is calculated independent of the events that take place in a game, so any correlation with measures of shot volume, quality, etc. are only coincidental. (It was also fairly easy to retrieve and is what Lars used to gauge \"top\" teams in a quantitative way.)\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Package imports and other setup\"}\n## Data retrieval\nlibrary(curl)\nlibrary(arrow)\nlibrary(qs) ## local dev\nlibrary(worldfootballR)\n\n## Data manipulation\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(lubridate)\nlibrary(forcats)\n\n## Modeling\nlibrary(rsample)\nlibrary(recipes)\nlibrary(parsnip)\nlibrary(workflows)\nlibrary(hardhat)\n\n## Model tuning\nlibrary(tune)\nlibrary(dials)\nlibrary(workflowsets)\nlibrary(finetune)\n\n## Model diagnostics\nlibrary(rlang)\nlibrary(yardstick)\nlibrary(SHAPforxgboost)\n\n## Plotting\nlibrary(ggplot2)\nlibrary(sysfonts)\nlibrary(showtext)\nlibrary(ggtext)\nlibrary(htmltools)\nlibrary(scales)\nlibrary(grid)\nlibrary(glue)\nlibrary(knitr)\n\nPROJ_DIR <- 'posts/xg-team-quality'\n\nTAG_LABEL <- htmltools::tagList(\n  htmltools::tags$span(htmltools::HTML(enc2utf8(\"&#xf099;\")), style = 'font-family:fb'),\n  htmltools::tags$span(\"@TonyElHabr\"),\n)\nSUBTITLE_LABEL <- 'English Premier League, 2012/13 - 2022/23'\nPLOT_RESOLUTION <- 300\nWHITISH_FOREGROUND_COLOR <- 'white'\nCOMPLEMENTARY_FOREGROUND_COLOR <- '#cbcbcb' # '#f1f1f1'\nBLACKISH_BACKGROUND_COLOR <- '#1c1c1c'\nCOMPLEMENTARY_BACKGROUND_COLOR <- '#4d4d4d'\nFONT <- 'Titillium Web'\nsysfonts::font_add_google(FONT, FONT)\n## https://github.com/tashapiro/tanya-data-viz/blob/main/chatgpt-lensa/chatgpt-lensa.R for twitter logo\nsysfonts::font_add('fb', 'Font Awesome 6 Brands-Regular-400.otf')\nshowtext::showtext_auto()\nshowtext::showtext_opts(dpi = PLOT_RESOLUTION)\n\nggplot2::theme_set(ggplot2::theme_minimal())\nggplot2::theme_update(\n  text = ggplot2::element_text(family = FONT),\n  title = ggplot2::element_text(size = 20, color = WHITISH_FOREGROUND_COLOR),\n  plot.title = ggtext::element_markdown(face = 'bold', size = 20, color = WHITISH_FOREGROUND_COLOR),\n  plot.title.position = 'plot',\n  plot.subtitle = ggtext::element_markdown(size = 16, color = COMPLEMENTARY_FOREGROUND_COLOR),\n  axis.text = ggplot2::element_text(color = WHITISH_FOREGROUND_COLOR, size = 14),\n  # axis.title = ggplot2::element_text(size = 14, color = WHITISH_FOREGROUND_COLOR, face = 'bold', hjust = 0.99),\n  axis.title.x = ggtext::element_markdown(size = 14, color = WHITISH_FOREGROUND_COLOR, face = 'bold', hjust = 0.99),\n  axis.title.y = ggtext::element_markdown(size = 14, color = WHITISH_FOREGROUND_COLOR, face = 'bold', hjust = 0.99),\n  axis.line = ggplot2::element_blank(),\n  strip.text = ggplot2::element_text(size = 14, color = WHITISH_FOREGROUND_COLOR, face = 'bold', hjust = 0),\n  legend.position = 'top',\n  legend.text = ggplot2::element_text(size = 12, color = WHITISH_FOREGROUND_COLOR, face = 'plain'),\n  legend.title = ggplot2::element_text(size = 12, color = WHITISH_FOREGROUND_COLOR, face = 'bold'),\n  panel.grid.major = ggplot2::element_line(color = COMPLEMENTARY_BACKGROUND_COLOR),\n  panel.grid.minor = ggplot2::element_line(color = COMPLEMENTARY_BACKGROUND_COLOR),\n  panel.grid.minor.x = ggplot2::element_blank(),\n  panel.grid.minor.y = ggplot2::element_blank(),\n  plot.margin = ggplot2::margin(10, 20, 10, 20),\n  plot.background = ggplot2::element_rect(fill = BLACKISH_BACKGROUND_COLOR, color = BLACKISH_BACKGROUND_COLOR),\n  plot.caption = ggtext::element_markdown(color = WHITISH_FOREGROUND_COLOR, hjust = 0, size = 10, face = 'plain'),\n  plot.caption.position = 'plot',\n  plot.tag = ggtext::element_markdown(size = 10, color = WHITISH_FOREGROUND_COLOR, hjust = 1),\n  plot.tag.position = c(0.99, 0.01),\n  panel.spacing.x = grid::unit(2, 'lines'),\n  panel.background = ggplot2::element_rect(fill = BLACKISH_BACKGROUND_COLOR, color = BLACKISH_BACKGROUND_COLOR)\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Retrieve and wrangle data\"}\nread_parquet_from_url <- function(url) {\n  load <- curl::curl_fetch_memory(url)\n  arrow::read_parquet(load$content)\n}\n\nREPO <- 'tonyelhabr/socceraction-streamlined'\nread_socceraction_parquet_release <- function(name, tag) {\n  url <- sprintf('https://github.com/%s/releases/download/%s/%s.parquet', REPO, tag, name)\n  read_parquet_from_url(url)\n}\n\nread_socceraction_parquet_releases <- function(name, tag = 'data-processed') {\n  purrr::map_dfr(\n    2013:2022,\n    \\(season_start_year) {\n      basename <- sprintf('8-%s-%s', season_start_year, name)\n      message(basename)\n      read_socceraction_parquet_release(basename, tag = tag)\n    }\n  )\n}\n\nread_socceraction_parquet <- function(name, branch = 'main') {\n  url <- sprintf('https://github.com/%s/raw/%s/%s.parquet', REPO, branch, name)\n  read_parquet_from_url(url)\n}\n\nx <- read_socceraction_parquet_releases('x')\ny <- read_socceraction_parquet_releases('y')\nactions <- read_socceraction_parquet_releases('actions')\ngames <- read_socceraction_parquet_releases('games') |> \n  dplyr::mutate(\n    date = lubridate::date(game_date)\n  )\nteam_elo <- read_socceraction_parquet('data/final/8/2013-2022/clubelo-ratings')\n\nopen_play_shots <- games |>\n  dplyr::transmute(\n    season_id,\n    game_id,\n    date,\n    home_team_id,\n    away_team_id\n  ) |> \n  dplyr::inner_join(\n    x |> \n      dplyr::filter(type_shot_a0 == 1) |> \n      dplyr::select(\n        game_id,\n        action_id,\n        \n        ## features\n        start_x_a0,\n        start_y_a0,\n        start_dist_to_goal_a0,\n        start_angle_to_goal_a0,\n        type_dribble_a1,\n        type_pass_a1,\n        type_cross_a1,\n        type_corner_crossed_a1,\n        type_shot_a1,\n        type_freekick_crossed_a1,\n        bodypart_foot_a0,\n        bodypart_head_a0,\n        bodypart_other_a0\n      ) |> \n      dplyr::mutate(\n        dplyr::across(-c(game_id, action_id), as.integer)\n      ),\n    by = dplyr::join_by(game_id),\n    relationship = 'many-to-many'\n  ) |> \n  dplyr::inner_join(\n    y |> \n      dplyr::transmute(\n        game_id, \n        action_id,\n        scores = ifelse(scores, 'yes', 'no') |> factor(levels = c('yes', 'no'))\n      ),\n    by = dplyr::join_by(game_id, action_id)\n  ) |> \n  dplyr::inner_join(\n    actions |> \n      dplyr::select(\n        game_id,\n        action_id,\n        team_id,\n        player_id\n      ),\n    by = dplyr::join_by(game_id, action_id)\n  ) |> \n  dplyr::left_join(\n    team_elo |> dplyr::select(date, home_team_id = team_id, home_elo = elo),\n    by = dplyr::join_by(date, home_team_id)\n  ) |> \n  dplyr::left_join(\n    team_elo |> dplyr::select(date, away_team_id = team_id, away_elo = elo),\n    by = dplyr::join_by(date, away_team_id)\n  ) |> \n  dplyr::transmute(\n    date,\n    season_id,\n    game_id,\n    team_id,\n    opponent_team_id = ifelse(team_id == home_team_id, away_team_id, home_team_id),\n    action_id,\n    \n    scores,\n    \n    elo = ifelse(team_id == home_team_id, home_elo, away_elo),\n    opponent_elo = ifelse(team_id == home_team_id, away_elo, home_elo),\n    elo_diff = elo - opponent_elo,\n    \n    start_dist_to_goal_a0,\n    start_angle_to_goal_a0,\n    type_dribble_a1,\n    type_pass_a1,\n    type_cross_a1,\n    type_corner_crossed_a1,\n    type_shot_a1,\n    type_freekick_crossed_a1,\n    bodypart_foot_a0,\n    bodypart_head_a0,\n    bodypart_other_a0\n  )\n```\n:::\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Split the data for modeling\"}\nsplit <- rsample::make_splits(\n  open_play_shots |> dplyr::filter(season_id %in% c(2013L:2019L)),\n  open_play_shots |> dplyr::filter(season_id %in% c(2020L:2022L))\n)\n\ntrain <- rsample::training(split)\ntest <- rsample::testing(split)\n```\n:::\n\n\nIt's worth spotlighting Elo a bit more since it's the novel feature here. Below is a look at the distribution of pre-match Elo over the course of the entire data set.\n\n\n\n\n\n![](elo-hist.png)\n\nTo make Elo values feel a bit more tangible, note that:\n\n-   Man City--arguably the best team in the EPL for the past decade--has sustained an Elo greater than 1850 for a majority of the past decade.\n-   Bottom-table teams often in the [relegation](https://en.wikipedia.org/wiki/Promotion_and_relegation) battle tend to have Elos less than 1650.\n\n\n\n\n\nThe pre-match difference team Elos follows a normal-ish distribution, with most values falling within the Â±300 range.\n\n![](elo-diff-hist.png)\n\n### Model Training\n\nThe feature set for our \"base\" xG model consists of the following:\n\n-   location of the shot (distance and angle of the shot to the center of the goal mouth)[^1].\n-   type of action leading to the shot.\n-   body part with which the shot was taken.\n\n[^1]: One might get a slightly more performant by adding the `x` and `y` coordinates of the shot--to implicitly account for right-footed bias, for example--but I actually prefer not to add those in the model. Such terms can result in slight [over-fitting](https://en.wikipedia.org/wiki/Overfitting), in the presence of other features that provide information about the location of the shot, such as distance and angle. (This is the classical [\"bias-variance\" trade-off](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff).)\n\nThese features are essentially what [all xG models](https://fbref.com/en/expected-goals-model-explained/) have in common, although the exact implementation differs. Data providers such as [Opta also account for information](https://theanalyst.com/na/2023/08/what-is-expected-goals-xg/) that is not captured in traditional event data, such as the position of the goalkeeper.\n\nFor the team-quality-adjusted (or \"Elo-augmented\") model, I'll add two additional features:\n\n-   `elo`: the Elo of the team of the shot-taker.\n-   `elo_diff`: the difference in the ELO of the shot-taking team and the opposing team.\n\nThe former is meant to capture the opponent-agnostic quality of a team, while the latter captures the quality of the team relative to their opponent.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Setting up the models\"}\nrec_elo <- recipes::recipe(\n  scores ~ \n    elo +\n    elo_diff +\n    start_dist_to_goal_a0 +\n    start_angle_to_goal_a0 +\n    type_dribble_a1 +\n    type_pass_a1 +\n    type_cross_a1 +\n    type_corner_crossed_a1 +\n    type_shot_a1 +\n    type_freekick_crossed_a1 +\n    bodypart_foot_a0 +\n    bodypart_head_a0 +\n    bodypart_other_a0,\n  data = train\n)\n\nrec_base <- rec_elo |> \n  recipes::step_rm(elo, elo_diff)\n```\n:::\n\n\nI'll be using [xgboost](https://xgboost.readthedocs.io/en/stable/) for my xG models. xgboost is the state-of-the-art framework for tabular machine learning tasks that's used by data and model providers like Opta. I'll choose [hyperparameters](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)) using [an efficient grid search](https://finetune.tidymodels.org/index.html), evaluating models with the [Brier skill score (BSS)](https://en.wikipedia.org/wiki/Brier_score#Brier_Skill_Score_(BSS)). For the reference Brier score for BSS, I'll use a dummy model that predicts 12% conversion for all shots.[^2]\n\n[^2]: 12% is approximately the observed shot conversion rate in the whole data set.\n\n::: {.callout-note collapse=\"true\"}\n## Brier skill score (BSS)\n\nIf this isn't the first blog post you've read of mine, you probably know that [I love to use BSS](https://tonyelhabr.rbind.io/posts/opta-xg-model-calibration) for classification tasks, especially for xG models.\n\nBut why BSS? Simply put, [Brier scores](https://en.wikipedia.org/wiki/Brier_score) are known as [the best evaluation metric](https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/) to use for classification tasks where you're purely interested in probabilities. And BSS goes one step beyond Brier scores, forcing one to contextualize the model evaluation with a reasonable baseline. In the context of xG, BSS helps us directly see whether our fitted model is better than a naive prediction, such as guessing that all shots convert at the observed shot conversion rate.\n\nKeep in mind that a higher BSS is ideal. A perfect model would have a BSS of 1; a model that is no better than a reference model would have a BSS of 0.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Functions for Brier skill score\"}\n## See also: probability-calibration\nbrier_skill_score <- function(data, ...) {\n  UseMethod('brier_skill_score')\n}\n\nbrier_skill_score <- yardstick::new_prob_metric(\n  brier_skill_score, \n  direction = 'maximize'\n)\n\nbss <- function(\n    truth, \n    estimate, \n    ref_estimate, \n    event_level,\n    case_weights,\n    ...\n) {\n  \n  if (length(estimate) == 1) {\n    estimate <- rep(estimate, length(truth))\n  }\n  \n  if (length(ref_estimate) == 1) {\n    ref_estimate <- rep(ref_estimate, length(truth))\n  }\n  \n  estimate_brier_score <- brier_class_vec(\n    truth = truth,\n    estimate = estimate,\n    event_level = event_level,\n    case_weights = case_weights,\n    ...\n  )\n  \n  ref_brier_score <- brier_class_vec(\n    truth = truth,\n    estimate = ref_estimate,\n    event_level = event_level,\n    case_weights = case_weights,\n    ...\n  )\n  \n  1 - (estimate_brier_score / ref_brier_score)\n}\n\nbrier_skill_score_estimator_impl <- function(\n    truth, \n    estimate, \n    ref_estimate, \n    event_level,\n    case_weights\n) {\n  bss(\n    truth = truth,\n    estimate = estimate,\n    ref_estimate = ref_estimate,\n    event_level = event_level,\n    case_weights = case_weights\n  )\n}\n\nbrier_skill_score_vec <- function(\n    truth, \n    estimate, \n    ref_estimate, \n    na_rm = TRUE, \n    event_level = yardstick:::yardstick_event_level(),\n    case_weights = NULL, \n    ...\n) {\n  \n  yardstick:::abort_if_class_pred(truth)\n  \n  estimator <- yardstick::finalize_estimator(\n    truth, \n    metric_class = 'brier_skill_score'\n  )\n  \n  yardstick::check_prob_metric(truth, estimate, case_weights, estimator)\n  \n  if (na_rm) {\n    result <- yardstick::yardstick_remove_missing(truth, estimate, case_weights)\n    \n    truth <- result$truth\n    estimate <- result$estimate\n    case_weights <- result$case_weights\n  } else if (yardstick::yardstick_any_missing(truth, estimate, case_weights)) {\n    return(NA_real_)\n  }\n  \n  brier_skill_score_estimator_impl(\n    truth = truth,\n    estimate = estimate,\n    ref_estimate = ref_estimate,\n    event_level = event_level,\n    case_weights = case_weights\n  )\n}\n\nbrier_skill_score.data.frame <- function(\n    data, \n    truth, \n    ...,\n    na_rm = TRUE,\n    event_level = yardstick:::yardstick_event_level(),\n    case_weights = NULL,\n    \n    ref_estimate = 0.5,\n    name = 'brier_skill_score'\n) {\n  yardstick::prob_metric_summarizer(\n    name = name,\n    fn = brier_skill_score_vec,\n    data = data,\n    truth = !!rlang::enquo(truth),\n    ...,\n    na_rm = na_rm,\n    event_level = event_level,\n    case_weights = !!rlang::enquo(case_weights),\n    fn_options = list(\n      ref_estimate = ref_estimate\n    )\n  )\n}\n\nxg_brier_skill_score <- function(data, ...) {\n  UseMethod('xg_brier_skill_score')\n}\n\n# REF_ESTIMATE <- open_play_shots |>\n#   dplyr::summarize(goal_rate = sum(scores == 'yes') / dplyr::n()) |>\n#   dplyr::pull(goal_rate)\nREF_ESTIMATE <- 0.12\n\nxg_brier_skill_score.data.frame <- function(...) {\n  brier_skill_score(\n    ref_estimate = REF_ESTIMATE,\n    name = 'xg_brier_skill_score',\n    ...\n  )\n}\n\nxg_brier_skill_score <- yardstick::new_prob_metric(\n  xg_brier_skill_score,\n  direction = 'maximize'\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Tuning the xG models\"}\n## Useful reference: https://jlaw.netlify.app/2022/01/24/predicting-when-kickers-get-iced-with-tidymodels/\nTREES <- 500\nLEARN_RATE <- 0.01\nspec <- parsnip::boost_tree(\n  trees = !!TREES,\n  learn_rate = !!LEARN_RATE,\n  tree_depth = tune::tune(),\n  min_n = tune::tune(), \n  loss_reduction = tune::tune(),\n  sample_size = tune::tune(), \n  mtry = tune::tune(),\n  stop_iter = tune::tune()\n) |>\n  parsnip::set_engine('xgboost') |> \n  parsnip::set_mode('classification')\n\ngrid <- dials::grid_latin_hypercube(\n  dials::tree_depth(),\n  dials::min_n(range = c(5L, 40L)),\n  dials::loss_reduction(),\n  sample_size = dials::sample_prop(),\n  dials::finalize(dials::mtry(), train),\n  dials::stop_iter(range = c(10L, 50L)),\n  size = 50\n)\n\nwf_sets <- workflowsets::workflow_set(\n  preproc = list(\n    base = rec_base, \n    elo = rec_elo\n  ),\n  models = list(\n    model = spec\n  ),\n  cross = FALSE\n)\n\ncontrol <- finetune::control_race(\n  save_pred = TRUE,\n  parallel_over = 'everything',\n  save_workflow = TRUE,\n  verbose = TRUE\n)\n\nset.seed(42)\ntrain_folds <- rsample::vfold_cv(train, strata = scores, v = 5)\n\ntuned_results <- workflowsets::workflow_map(\n  wf_sets,\n  fn = 'tune_race_anova',\n  grid = grid,\n  control = control,\n  metrics = yardstick::metric_set(xg_brier_skill_score),\n  resamples = train_folds,\n  seed = 42\n)\n```\n:::\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Choosing best hyper-parameters\"}\nMODEL_TYPES <- c(\n  'base_model' = 'Base',\n  'elo_model' = 'Elo-augmented'\n)\nselect_best_model <- function(tuned_results, model_type) {\n  tuned_results |>\n    workflowsets::extract_workflow_set_result(model_type) |> \n    tune::select_best(metric = 'xg_brier_skill_score') |>\n    dplyr::transmute(\n      model_type = MODEL_TYPES[model_type],\n      mtry,\n      min_n,\n      tree_depth, \n      loss_reduction,\n      sample_size,\n      stop_iter\n    )\n}\n\nbest_base_set <- select_best_model(tuned_results, 'base_model')\nbest_elo_set <- select_best_model(tuned_results, 'elo_model')\n\ndplyr::bind_rows(\n  best_base_set,\n  best_elo_set\n) |> \n  dplyr::transmute(\n    model_type,\n    mtry,\n    min_n,\n    tree_depth, \n    loss_reduction,\n    sample_size,\n    stop_iter\n  ) |> \n  knitr::kable()\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Model hyperparameters\n\nFor reproducibility, the chosen hyperparameters are as follows.[^3] By coincidence, the same hyper-parameters are chosen. (Only 50 combinations were evaluated.)\n\n| model_type    | mtry | min_n | tree_depth | loss_reduction | sample_size | stop_iter |\n|:--------------|-----:|------:|-----------:|---------------:|------------:|----------:|\n| Base          |    7 |    17 |          6 |              0 |   0.4641502 |        50 |\n| Elo-augmented |    7 |    17 |          6 |              0 |   0.4641502 |        50 |\n\nThe number of `trees` and `learning_rate` were pre-defined to be 500 and 0.01 respectively.\n:::\n\n[^3]: This post isn't meant to be so much about the why's and how's of model tuning and training, so I've spared commentary on the code.\n\nAfter identifying our optimal hyperparameters, we fit singular models on the entire training set. These are the models that we'll use to evaluate the effect of team quality on xG.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Fit models on the entire training set after choosing the hyperparameters\"}\nfinalize_tuned_results <- function(tuned_results, model_type) {\n  best_set <- select_best_model(tuned_results, model_type)\n  tuned_results |>\n    hardhat::extract_workflow(model_type) |>\n    tune::finalize_workflow(best_base_set) |> \n    tune::last_fit(\n      split,\n      metrics = yardstick::metric_set(xg_brier_skill_score)\n    )\n}\nlast_base_fit <- finalize_tuned_results(tuned_results, 'base_model')\nlast_elo_fit <- finalize_tuned_results(tuned_results, 'elo_model')\n```\n:::\n\n\n\n\n\n\n### Feature Importance\n\nWe should verify to see that the fitted xG models are behaving as expected. One way of doing so is to look at the importance of the features for model predictions.\n\nIn bespoke models (like the Elo-augmented model), feature importance can be enlightening, as it tell us which features are contributing most to the predicted outcomes. For the base model, I know that shot distance should be the most important feature (by far), as this is found to be the most important features in other similar \"basic\" public xG models, such as [this one](https://github.com/AnshChoudhary/xGModel/blob/main/expected-goals-model-xg.ipynb) and [this one](https://github.com/ML-KULeuven/soccer_xg/blob/master/notebooks/4-creating-custom-xg-pipelines.ipynb).\n\nI like to use [SHAP values](https://christophm.github.io/interpretable-ml-book/shap.html) for feature importance since [they have nice \"local\" explanations and are found to be consistent in their \"global\" structure](https://liuyanguu.github.io/post/2019/07/18/visualization-of-shap-for-xgboost/).\n\n::: {.callout-note collapse=\"true\"}\n## SHAP values\n\nSHAP values quantify the impact of each feature in a predictive model on the model's output. For a binary classification task like ours, SHAP values are expressed on a 0-1 scale.\n\nA SHAP value of 0.9 for a specific feature in a binary classification model doesn't mean that the feature contributes 90% to the prediction, nor does it directly imply that the predicted probability is 90%. Instead, it indicates that this particular feature contributes a value of 0.9 to the shift from the baseline prediction (usually the average of the training data's output) to the specific model output for that observation.\n:::\n\n\n\n\n\n![](var-imp-base.png)\n\nIndeed, we find that shot distance is the most important feature by far with the base xG model.\n\nNow, if we make the same plot and add the aggregate SHAP values for our Elo-augmented xG model, we see that Elo and Elo difference terms are in the middle of the pack in terms of feature importance. That's pretty interesting. That indicates that these features aren't super influential, on average.\n\n\n\n\n\n![](var-imp-compared.png)\n\nFurther, we can verify that the Elo-augmented model predicts xG in a manner that matches intuition using [SHAP dependence plots](https://christophm.github.io/interpretable-ml-book/shap.html#shap-dependence-plot). We should see that the SHAP values are higher when Elo is higher and when Elo difference is higher.[^4]\n\n[^4]: Partial dependence plots aren't quite the gold standard for model interpretability, but I find them useful for getting a sense of the orientation and magnitude of a feature's effect, particularly for non-linear modeling techniques like gradient boosting.\n\n\n\n\n\n![](elo-pdp.png)\n\nIndeed, this is generally what we observe, although the relationships aren't quite monotonic, as I would have expected.[^5]\n\n[^5]: Of course, I could have forced the xgboost model with the Elo terms to have monotonic behavior for those features. I intentionally didn't do that here so as to not confirm my own biases--that higher xG should generally correspond with higher Elo.\n\n### Model Evaluation\n\nSo aggregate SHAP values indicate that our two Elo features aren't playing a big role in contributing to model predictions, although the features do indeed contribute in the way that we'd expect. But SHAP values are just telling us about how and why a model makes certain predictions. What about the performance of the Elo-augmented model compared to the base xG model?\n\n\n\n\n\n| Model Type    | Brier Skill Score |\n|:--------------|------------------:|\n| Base          |            0.1058 |\n| Elo-augmented |            0.1056 |\n\nWell, at least in terms of BSS, it looks like there's basically no difference between the models.[^6]\n\n[^6]: One could run a bootstrap analysis to prove this with more statistical rigor, but I'll leave that as an exercise for the eager reader.\n\nBut that's just one, holistic measure. How well-calibrated is the Elo-augmented model compared to the traditional xG model? Does the augmented model demonstrate better performance across the whole range of shot conversion probabilities?\n\n\n\n\n\n![](compared-calibration.png)\n\nVisually, I wouldn't say there's any significant difference between the calibration curves. A plot of calibration given various buckets for Elo difference would lead to the same conclusion--the Elo-augmented model isn't notably more calibrated.\n\n## Discussion\n\nWe've seen that there is no clear evidence that accounting for team quality improves an xG model. Sure, it can provide value to an xG model--as shown with the SHAP plots for the Elo-augmented model--but it's effectively just providing a different kind of information that could be otherwise captured by other traditional xG features.\n\nSo naturally one may ask--what about player quality? Or goalkeeper quality? What if we get more specific about the measures of quality (more specific than the team strength)?\n\nWhile I do think those would be a little more useful in terms of making a better xG model, the better question is whether we should be accounting for these kinds of \"quality\" features at all. My view is \"no\". I say \"no\" (although I do think it's fine for NFL EP models) due to the manner in which xG models are typically applied.\n\n### Application of xG in soccer vs. EP in American football\n\nAn xG model cannot be used to \"intervene\" with shots in real-time like a fourth-down model might be \"actively\" used to make a choice in the NFL. An NFL coach probably wants a model to incorporate their team's strengths so as to make an informed decision about whether to go for it on fourth down or to kick a field goal.\n\nIn contrast, xG models are more commonly used in a \"passive\" manner, to understand how a team or a specific player might be performing compared to expectation. Soccer coaches and analysts often look at the actual goals scored minus the expected goals accumulated (the \"residual\") to diagnose under and over-performance. Yes, they also look at expected goals in a more broad manner to get a sense of where and how they can get better shots or where they are giving up high-quality shots, but that kind of insight is not acted upon in real-time in the same way that a fourth-down decision is made in real-time.\n\nOne can make the argument that incorporating team or player quality into an xG model can make interpretation more difficult, at least in the manner that we typically use xG to contextualize a game or a season. (xGods, please forgive me for looking at single-game xG.) As an extreme example, let's say that we add an \"Is Messi?\" feature to an xG model (not unlike what Ryan suggested with Justin Tucker in his presentation) to achieve a slightly more accurate xG model. That's all fine and dandy, in theory; but then your game- and season-level xG insights would need to change.\n\nI may look at an Inter Miami game where they \"lost\" on actual goals, let's say 2 to 1, *and* \"lost\" on xG, let's say 0.9 to 1.7, and think \"Oh, that's a fair result\". But if Messi took 4 shots and we're using an expected goals model with the \"Is Messi?\" feature, the Inter Miami xG might appear to be 2.2, perhaps leading to a different take on the game---\"that was Inter Miami's game, but they failed to convert on some key chances\".\n\nOverall, I'd say that it feels circular (and \"wrong\") to build in \"quality\" features into a model used to evaluate shot quality, and, downstream, player and team quality, at least in the current paradigm of soccer analytics.\n\n### In favor of team and/or player quality features\n\nOn the other hand, I do see the argument in favor of identity features in xG models, e.g. a [RAPM-esque](https://www.youtube.com/watch?v=5WRe6SoBh3g) xG model. Such indicator variables allow us [to estimate player and team impact directly](https://www.youtube.com/watch?v=CzKCMUJ9yV8), as well as the uncertainty for those individual estimates. Assuming the model estimation procedure is sound, a direct estimate of player or team skill is better than relying on the \"residual\" of goals minus xG, as is typically done.\n\nFurther, I've been assuming that looking at xGD will always be the primary way that xG is used to evaluate players. If we can reliably estimate player skill directly with an xG model and can tease out how that effect changes over time, then perhaps we should prefer these kinds of estimates.\n\n## Conclusion\n\nI pursued a quantitative analysis to evaluate whether accounting for team strength directly in an expected goals model in soccer might improve predictiveness and calibration, inspired by observed biases in American football's expected points models. Despite SHAP plots indicating some value in Elo-augmented models, the overall performance metrics and calibration plots showed negligible differences compared to the base xG model.\n\nOn the surface, this is a little at odds with [Lars Maurath's ex-post finding](https://www.thesignificantgame.com/portfolio/do-naive-xg-models-underestimate-expected-goals-for-top-teams/) that team quality is useful for explaining xGD, which matches my intuition. However, my ex-ante approach is fairly straightforward and has limitations. Perhaps with better feature engineering and additional features like goalkeeper position, we might find that accounting for team quality directly does improve an xG model.\n\nThe most natural area of future research would be to analyze how measures of player quality might improve an xG model. There is some [prior art](https://statsbomb.com/wp-content/uploads/2022/09/Tahmeed-Tureen-and-Sigrid-Olthof-%E2%80%93-Estimated-Player-Impact-EPI-Quantifying-The-Effects-Of-Individual-Players-On-Football-Actions-Using-Hierarchical-Statistical-Models.pdf) on this, so it would be wise to identify how one could contribute something novel to that work.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}