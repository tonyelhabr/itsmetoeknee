{
  "hash": "169d7cf2868024a1151c8ec7cff2085d",
  "result": {
    "markdown": "---\ntitle: What exactly is an \"expected point\"? (part 1)\ndescription: Calculating and comparing expected points from different expected goals sources\ndate: 2022-09-04\ncategories:\n  - r\n  - soccer\nimage: calib.png\nexecute:\n  include: true\n  echo: true\n---\n\n\n## Introduction\n\n[Expected goals (xG)](https://theanalyst.com/na/2021/07/what-are-expected-goals-xg/) in soccer have gone mainstream and are no longer cool to talk about.\n\n<blockquote class=\"twitter-tweet\">\n\n<p lang=\"en\" dir=\"ltr\">\n\nWhat exactly is an \" expected goal \"? Who decides the criteria ? Is there a list of\" expected goal scorers \" ? Or even \" unexpected ones \" ?\n\n</p>\n\n--- Ian Darke (@IanDarke) <a href=\"https://twitter.com/IanDarke/status/1341904890914885641?ref_src=twsrc%5Etfw\">December 24, 2020</a>\n\n</blockquote>\n\n\n```{=html}\n<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n```\n\nSo let's talk about [expected points (xPts)](https://www.bettingodds.com/news/what-are-expected-points-xp-football-betting). The one sentence explainer for xPts: it's a number between 0 and 3 assigned to each team in a match that we estimate from the xG of each shot in the match. Teams that accumulate more xG than their opponents in the match are more likely to have xPts closer to 3, i.e. the points awarded for a win, and those that accumulate less than their opponents are more likely to earn xPts closer to 0. xPts is convenient for translating a team's xG (relative to it's opponents) to the team's expected placement in the standings.\n\nWhile [several](https://luke-beggs.medium.com/creating-an-expected-points-xp-calculator-for-football-matches-ce4edd18d16f) [outlets](https://theshortfuse.sbnation.com/2017/11/15/16655916/how-to-calculate-xpoints-analysis-stats-xg) have described computing expected points with simulation[^1], [simulation is actually not necessary](https://www.jonaslindstrom.dk/?p=330) if you have the xG for every shot taken in a match.[^2] For example, let's say team A shoots six times with an xG of 0.1 for each shot, and team B shoots three shots with xG's of 0.1, 0.2, and 0.3 respectively. Given these goal probabilities, we can analytically compute xPts as follows.\n\n[^1]: [Danny Page's interactive web app](https://danny.page/expected_goals.html) also uses simulation.\n\n[^2]: Now, if you desire the statistical properties that simulation offers, such as an estimation of error, that's understandable; however, in write-ups that I've seen, such is not mentioned explicitly. Additionally, if one chooses to go down the simulation route because they believe that it helps to suppress flaws with the xG model, that's also understandable. On the other hand, the analytical approach I present should present nearly identical results to that which one would find with simulation, and it offers the advantage of being much faster.\n\nFirst, we find the probability of scoring 0, 1, 2, etc. goals (up to the number of shots taken).[^3]\n\n[^3]: Plotting code is omitted throughout the post since it's not particularly instructive.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nlibrary(poibin)\nxg_a <- rep(0.1, 6)\nxg_b <- c(0.1, 0.2, 0.3)\n\nprobs_a <- dpoibin(seq.int(0, length(xg_a)), xg_a)\nround(probs_a, 2)\n#> [1] 0.53 0.35 0.10 0.01 0.00 0.00 0.00\nprobs_b <- dpoibin(seq.int(0, length(xg_b)), xg_b)\nround(probs_b, 2)\n#> [1] 0.50 0.40 0.09 0.01\n```\n:::\n\n\n\n\n\n\n![](ex.png)\n\nSecond, we convert the goal probabilities to singular probabilities for each team winning the match, as well as the probability of a draw.[^4]\n\n[^4]: How does this work? Under the assumption that xG comes from a [Poisson binomial distribution](https://en.wikipedia.org/wiki/Poisson_binomial_distribution), we look at all combinations of makes and misses of the shots and compare the relative proportion of instances in which one team's number of success, i.e. goals, is greater than, equal to, or less than their opponent's.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nlibrary(gdata)\nouter_prod <- outer(probs_a, probs_b)\np_a <- sum(lowerTriangle(outer_prod))\np_b <- sum(upperTriangle(outer_prod))\np_draw <- sum(diag(outer_prod))\nround(c(p_a, p_b, p_draw), 2)\n#> [1] 0.28 0.30 0.42\n```\n:::\n\n\nFinally, given the match outcome probabilities, the xPts calculation is straightforward.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nxpts_a <- 3 * p_a + 1 * p_draw\nxpts_b <- 3 * p_b + 1 * p_draw\nround(c(xpts_a, xpts_b), 2)\n#> [1] 1.27 1.31\n```\n:::\n\n\nFor this example, we arrive at the interesting result that, despite the two teams total xG being equal (=0.6), team B has a slightly higher probability of winning. There have been plenty of [explanations](https://hockey-graphs.com/2018/12/19/some-people-were-wrong-on-twitter/) on this \"quality vs. quantity\" phenomenon, so I won't go into it in detail. Nonetheless, this simple example illustrates why it can be useful to translate xG into another form---doing so can provide a better perspective on match results and, consequently, team placement in the standings.\n\n### Objectives\n\nSo we've gone over what expected points are and why they're important. Now we set out to do the following.\n\n1.  **Calculate xPts from shot xG for multiple seasons of data.** We'll limit the scope to the 2020/21 and 2021/22 seasons for the English Premier League.[^5]\n2.  **Compare the calibration of the understat and fotmob match outcome probabilities.** `{worldfootballR}` makes it easy for us to get xG from both [understat](https://understat.com/) and [fotmob](https://www.fotmob.com/), and it should be interesting to compare the the predictive performance of the two models.\n3.  **Compare predictions of actual season-long points using xPts that we derive from understat and fotmob xG.** In particular, we'll be interested to see if our conclusions regarding the better source for xG here matches the conclusions for (2).\n\n[^5]: We've limited the scope for several reasons: (1) fotmob only has complete xG data for the 2020/21 and 2021/22 seasons as of writing, (2) I didn't want to have to map team names across the two data sources for a ton of teams; and (3) of all league, I'm most interested in the EPL ðŸ˜„.\n\n## Analysis\n\n### 1. Calculating xPts from xG\n\nLet's start by using the `load_understat_league_shots()` function from `{worldfootballR}` to retrieve understat xG by shot.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tibble)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(lubridate)\nlibrary(worldfootballR) ## version: 0.5.12.5000\nlibrary(janitor)\n\nrename_home_away_teams <- function(df) {\n  df |> \n    mutate(\n      team = ifelse(is_home, home_team, away_team),\n      opponent = ifelse(is_home, away_team, home_team)\n    ) |> \n    select(-c(home_team, away_team)) \n}\n\nconvert_understat_year_to_season <- function(x) {\n  sprintf('%s/%s', x, str_sub(x + 1, 3, 4))\n}\n\n## we'll use all of the shots later when exporing understat data only\nall_understat_shots <- load_understat_league_shots('EPL') |> \n  as_tibble() |> \n  ## camelcase like \"xG\" is for Java scrubs\n  clean_names() |> \n  filter(season <= 2021) |> \n  ## transmute = select + mutate\n  transmute(\n    match_id,\n    ## \"2021/2022\" format so that we have a clear, consistent way to represent season\n    across(season, convert_understat_year_to_season),\n    ## to convert \"2020-09-12 11:30:00\" to a date (\"2020-09-12\")\n    across(date, lubridate::date),\n    home_team,\n    away_team,\n    is_home = h_a == 'h',\n    xg = x_g\n  ) |>\n  rename_home_away_teams() |> \n  arrange(season, date, team)\n\n## but when comparing understat with fotmob, we'll need to limit the seasons to just\n##   those that both sources have\nunderstat_shots <- all_understat_shots |> filter(season >= 2020)\n```\n:::\n\n\nWe can use `load_fotmob_match_details()` to get fotmob's shot xG in a similar fashion.[^6]\n\n[^6]: Note that there are three additional shots in the fotmob data. There's no simple solution to resolving this data discrepancy since we don't have matching shot identifiers in the two data sets ðŸ¤·.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\n## manually created CSV with at least 2 columns: team_understat, team_fotmob.\n##   use the team_understat name to be consistent across sources.\nteam_mapping <- 'https://raw.githubusercontent.com/tonyelhabr/sports_viz/master/59-xg_xpoints/team_mapping.csv' |> \n  read_csv()\n\nrename_fotmob_teams <- function(df) {\n  df |> \n    left_join(\n      team_mapping |> select(team_understat, team_fotmob),\n      by = c('home_team' = 'team_fotmob')\n    ) |> \n    select(-home_team) |> \n    rename(home_team = team_understat) |> \n    left_join(\n      team_mapping |> select(team_understat, team_fotmob),\n      by = c('away_team' = 'team_fotmob')\n    ) |> \n    select(-away_team) |> \n    rename(away_team = team_understat)\n}\n\nfotmob_shots <- load_fotmob_match_details(\n  country = 'ENG',\n  league_name = 'Premier League'\n) |> \n  mutate(\n    ## to convert strings from 'Sat, Sep 12, 2020, 11:30 UTC' to a date\n    date = strptime(match_time_utc, '%a, %b %d, %Y, %H:%M UTC', tz = 'UTC') |> date(),\n    ## fotmob's parent_league_season always reflects the current season, so we need to manually\n    ##   define the season from the date. we would certainly want a more automated approach\n    ##   if working with more seasons and more leagues.\n    season = case_when(\n      date >= ymd('2020-09-12') & date <= ymd('2021-05-23') ~ '2020/21',\n      date >= ymd('2021-08-13') & date <= ymd('2022-05-22') ~ '2021/22',\n      TRUE ~ NA_character_\n    )\n  ) |> \n  ## the NAs are for 2022/2023 (incomplete as of writing) and the partial data for 2019/2020\n  drop_na(season) |> \n  transmute(\n    match_id,\n    season,\n    date,\n    home_team,\n    away_team,\n    is_home = team_id == home_team_id,\n    ## some shots with NAs for some reason\n    xg = coalesce(expected_goals, 0)\n  ) |>\n  rename_fotmob_teams() |> \n  rename_home_away_teams() |> \n  arrange(season, date, team)\n```\n:::\n\n\nAlright, now the fun part. We functionalize the code from the example for calculating the probability that xG will result in 0, 1, 2, etc. goals.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nlibrary(purrr)\npermute_xg <- function(xg) {\n  n <- length(xg)\n  x <- seq.int(0, n)\n  dpoibin(x, xg)\n}\n\ncalculate_permuted_xg <- function(df) {\n  df |> \n    group_by(across(c(everything(), -xg))) |> \n    summarize(across(xg, ~list(.x))) |> \n    mutate(\n      prob = map(xg, ~permute_xg(.x))\n    ) |> \n    select(-c(xg)) |> \n    unnest(cols = c(prob)) |> \n    group_by(across(-c(prob))) |>\n    mutate(\n      g = row_number() - 1L\n    ) |>\n    ungroup() |> \n    arrange(match_id, is_home, g)\n}\n\nunderstat_permuted_xg <- understat_shots |> calculate_permuted_xg()\nfotmob_permuted_xg <- fotmob_shots |> calculate_permuted_xg()\n```\n:::\n\n\nNext, we identify all possible goal combinations using xG as \"weights\" to compute the relative likelihood of each combination, and then analytically calculate the probabilities of winning, losing, and drawing.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nsummarize_pivoted_permuted_xg <- function(prob_away, prob_home) {\n  outer_prod <- outer(prob_away, prob_home)\n  p_draw <- sum(diag(outer_prod), na.rm = TRUE)\n  p_home <- sum(upperTriangle(outer_prod), na.rm = TRUE)\n  p_away <- sum(lowerTriangle(outer_prod), na.rm = TRUE)\n  list(\n    draw = p_draw,\n    home = p_home,\n    away = p_away\n  )\n}\n\n## Bournemouth 0 - 1 Manchester City on 2019-03-02\n## Huddersfield 0 - 0 Swansea on 2018-03-10\npad_for_matches_without_shots_from_one_team <- function(df) {\n  n_teams_per_match <- df |> \n    distinct(match_id, team) |> \n    count(match_id, sort = TRUE)\n  \n  matches_with_no_shots_from_one_team <- n_teams_per_match |> \n    filter(n == 1)\n  \n  dummy_opponents <- df |> \n    distinct(match_id, season, date, team, opponent, is_home) |> \n    semi_join(\n      matches_with_no_shots_from_one_team,\n      by = 'match_id'\n    ) |> \n    mutate(\n      z = team\n    ) |> \n    transmute(\n      match_id, \n      season, \n      date, \n      team = opponent,\n      opponent = z,\n      across(is_home, ~!.x),\n      prob = 1,\n      g = 0L\n    )\n  \n  bind_rows(\n    df,\n    dummy_opponents\n  ) |> \n  arrange(season, date, team, g)\n}\n\nsummarize_permuted_xg_by_match <- function(df) {\n  \n  padded_df <- pad_for_matches_without_shots_from_one_team(df)\n  \n  pivoted <- padded_df |>\n    transmute(\n      match_id,\n      season,\n      date,\n      g,\n      is_home = ifelse(is_home, 'home', 'away'),\n      prob\n    ) |>\n    pivot_wider(\n      names_from = is_home,\n      names_prefix = 'prob_',\n      values_from = prob,\n      values_fill = 0L\n    )\n  \n  pivoted |> \n    select(match_id, season, date, prob_away, prob_home) |>\n    group_by(match_id, season, date) |> \n    summarize(\n      across(starts_with('prob_'), ~list(.x))\n    ) |> \n    ungroup() |> \n    inner_join(\n      padded_df |> distinct(match_id, team, opponent, is_home),\n      by = 'match_id'\n    ) |> \n    mutate(\n      prob = map2(prob_away, prob_home, summarize_pivoted_permuted_xg)\n    ) |> \n    select(-starts_with('prob_')) |> \n    unnest_wider(prob, names_sep = '_') |> \n    mutate(\n      prob_win = ifelse(is_home, prob_home, prob_away),\n      prob_lose = ifelse(is_home, prob_away, prob_home),\n      xpts = 3 * prob_win + 1 * prob_draw\n    ) |> \n    select(-c(prob_home, prob_away))\n}\n\nunderstat_xpts_by_match <- understat_permuted_xg |> summarize_permuted_xg_by_match()\nfotmob_xpts_by_match <- fotmob_permuted_xg |> summarize_permuted_xg_by_match()\n```\n:::\n\n\nLet's take a quick peak at the distributions of xG and xPts, both as a sanity check and to enhance our understanding of the relationship between the two. When plotting xPts as a function xG, we should expect to see a monotonically increasing relationship where xPts bottoms out at zero and tops out at three.\n\n\n\n\n\n![](xg_vs_xpts.png)\n\nFurther, if there is any doubt about the expected points calculation, note that understat offers xPts directly in their data. The mean absolute error of our calculation of xPts with theirs is \\~0.02.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nlibrary(understatr)\n\nall_raw_understat_xpts_by_match <- 2014:2021 |> \n  set_names() |> \n  map_dfr(\n    ~get_league_teams_stats('EPL', .x),\n    .id = 'season'\n  ) |> \n  transmute(\n    across(season, ~convert_understat_year_to_season(as.integer(.x))),\n    date,\n    team = team_name,\n    result,\n    pts,\n    raw_xpts = xpts,\n    xg = xG\n  )\n\nraw_understat_xpts_by_match <- all_raw_understat_xpts_by_match |> \n  inner_join(\n    understat_xpts_by_match |> select(season, date, team, xpts),\n    by = c('season', 'date', 'team')\n  ) |> \n  mutate(\n    xptsd = raw_xpts - xpts\n  ) |> \n  arrange(season, date, team)\n\n## mean absolute error\nround(mean(abs(raw_understat_xpts_by_match$xptsd)), 2)\n#> [1] 0.02\n```\n:::\n\n\n### 2. Match predictive performance[^7]\n\n[^7]: Using the adjective \"predictive\" is a little misleading, since we're not actually making predictions out-of-sample. Rather, we're using models based on xG to evaluate which xG data source better explains the observed results.\n\nAs one might guess, the match outcome probabilities implied by the xG from understat and fotmob are strongly correlated.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nrename_xpts_by_match <- function(df, src) {\n  df |> \n    select(season, date, team, starts_with('prob_'), xpts) |> \n    rename_with(\n      ~sprintf('%s_%s', .x, src), c(starts_with('prob_'), xpts)\n    )\n}\n\nxpts_by_match <- raw_understat_xpts_by_match |> \n  select(season, date, team, result, pts) |> \n  inner_join(\n    understat_xpts_by_match |> rename_xpts_by_match('understat'),\n    by = c('season', 'date', 'team')\n  ) |> \n  inner_join(\n    fotmob_xpts_by_match |> rename_xpts_by_match('fotmob'),\n    by = c('season', 'date', 'team')\n  )\n\ncor_draw <- cor(xpts_by_match$prob_draw_fotmob, xpts_by_match$prob_draw_understat)\ncor_win <- cor(xpts_by_match$prob_win_fotmob, xpts_by_match$prob_win_understat)\ncor_lose <- cor(xpts_by_match$prob_lose_fotmob, xpts_by_match$prob_lose_understat)\nround(c(cor_draw, cor_win, cor_lose), 3)\n#> [1] 0.906 0.958 0.958\n```\n:::\n\n\nNote that the win and loss correlations are identical. This is due to the symmetric nature of the data---we have two records for each match, one from each team's perspective.[^8]\n\n[^8]: Home field advantage is treated as a feature instead of defined directly via columns, i.e. `home_team`, `home_score`, etc., which is good practice in general.\n\n#### Predicting match outcomes with binary logistic regression\n\nNow let's compare how \"good\" the implied probabilities from the two sources are. To do this, we'll create binary logistic regression models to predict a given outcome and compute:\n\n1.  the [mean squared error (MSE)](https://en.wikipedia.org/wiki/Mean_squared_error);\n2.  the [brier skill score (BSS)](https://en.wikipedia.org/wiki/Brier_score#Brier_Skill_Score_(BSS)), treating the empirical proportion of the specified outcome as the reference.[^9][^10]\n3.  a [calibration plot](https://changhsinlee.com/python-calibration-plot/), grouping predictions into \"buckets\" at every 5%.\n\n[^9]: Draws occur for 22.5% of matches in the data set, and wins and losses occur in 38.8% of matches each.\n\n[^10]: Personally, I tend to rely on BSS wherever I can. Not only is it more interpretable---it's a number between 0 and 1, while MSE can take on any value, depending on the context---I like that it forces one to compare to a baseline, which is a good principle in general.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nresult_props <- xpts_by_match |> \n  count(result) |> \n  mutate(prop = n / sum(n))\n\ncompute_mse <- function(truth, estimate) {\n  mean((truth - estimate)^2)\n}\n\ndiagnose_prob_by_match <- function(src, result) {\n  \n  df <- xpts_by_match |> \n    mutate(\n      result = ifelse(result == !!result, 1L, 0L) |> factor()\n    )\n  \n  result_name <- switch(\n    result,\n    'w' = 'win',\n    'l' = 'lose',\n    'd' = 'draw'\n  )\n  col <- sprintf('prob_%s_%s', result_name, src)\n  \n  fit <- glm(\n    df$result ~ df[[col]],\n    family = 'binomial'\n  )\n  \n  probs <- tibble(\n    result_num = as.numeric(df$result) - 1,\n    .prob = unname(predict(fit, type = 'response'))\n  )\n  \n  n_buckets <- 20\n  alpha <- 0.05\n  calib <- probs |>\n    mutate(\n      across(.prob, ~round(.x * n_buckets) / n_buckets)\n    ) |>\n    group_by(.prob) |>\n    summarize(\n      ## Jeffreys' prior\n      ci_lower = qbeta(alpha / 2, sum(result_num) + 0.5, n() - sum(result_num) + 0.5),\n      ci_upper = qbeta(1 - alpha / 2, sum(result_num) + 0.5, n() - sum(result_num) + 0.5),\n      actual = sum(result_num) / n(),\n      n = n()\n    ) |> \n    ungroup()\n  \n  mse <- compute_mse(probs$result_num, probs$.prob)\n  \n  ref_prob <- result_props |> \n    filter(result == !!result) |> \n    pull(prop)\n  \n  ref_mse <- compute_mse(probs$result_num, ref_prob)\n  bss <- 1 - (mse / ref_mse)\n  \n  list(\n    calib = calib,\n    mse = mse,\n    bss = bss\n  )\n}\n\ndiagnostics <- crossing(\n  result = c('w', 'd'),\n  src = c('understat', 'fotmob')\n) |> \n  mutate(\n    diagnostics = map2(src, result, diagnose_prob_by_match)\n  ) |> \n  unnest_wider(diagnostics)\ndiagnostics |> select(-calib)\n#> # A tibble: 4 Ã— 4\n#>   result src         mse    bss\n#>   <chr>  <chr>     <dbl>  <dbl>\n#> 1 d      fotmob    0.170 0.0268\n#> 2 d      understat 0.166 0.0466\n#> 3 w      fotmob    0.173 0.270 \n#> 4 w      understat 0.162 0.317\n```\n:::\n\n\nThe MSE (where lower is \"better\") and the BSS (where higher is \"better\") lead us to the same conclusion---the models based on understat's xG slightly outperform the one based on fotmob's xG.\n\nMoreover, looking at the calibration plot, the understat model predictions seem to stick closer to the 45 degree slope representing perfect calibration.\n\n\n\n\n\n![](calib.png)\n\n#### Predicting points with linear regression\n\nAlternatively, we could regress points on expected points. For linear regression, we can use the [root mean squared error (RMSE)](https://en.wikipedia.org/wiki/Root-mean-square_deviation) (where lower is \"better\") and [R squared](https://en.wikipedia.org/wiki/Coefficient_of_determination) (where higher is \"better\") to compare the models.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\ncompute_rmse <- function(truth, estimate) {\n  sqrt(mean((truth - estimate)^2))\n}\n\ndiagnose_xpts_by_match <- function(src) {\n  \n  col <- sprintf('xpts_%s', src)\n  fit <- lm(xpts_by_match$pts ~ xpts_by_match[[col]])\n  \n  pred <- predict(fit)\n  \n  tibble(\n    rmse = compute_rmse(xpts_by_match$pts, pred),\n    r2 = summary(fit)$r.squared\n  )\n}\n\nc('understat', 'fotmob') |> \n  set_names() |> \n  map_dfr(diagnose_xpts_by_match, .id = 'src')\n#> # A tibble: 2 Ã— 3\n#>   src        rmse    r2\n#>   <chr>     <dbl> <dbl>\n#> 1 understat  1.06 0.374\n#> 2 fotmob     1.10 0.323\n```\n:::\n\n\nThe understat model proves to be better by both metrics, having a lower RMSE and higher R squared than the fotmob model.\n\n#### Predicting match outcomes with multinomial logistic regression\n\nPersonally, I don't like predicting points directly like this since it's a discrete variable that can only take on three values (0, 1, and 3). If we're going to predict points instead of a probability, I think the better approach is to run a multinomial logistic regression and to convert the predicted probabilities to expected points.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nlibrary(nnet)\ndiagnose_implied_xpts_by_match <- function(src) {\n  \n  col_win <- sprintf('prob_win_%s', src)\n  col_draw <- sprintf('prob_draw_%s', src)\n  fit <- multinom(\n    xpts_by_match$result ~ xpts_by_match[[col_win]] + xpts_by_match[[col_draw]],\n    trace = FALSE\n  )\n  probs <- predict(fit, type = 'probs') |> as_tibble()\n  preds <- 3 * probs$w + 1 * probs$d\n  \n  tibble(\n    rmse = compute_rmse(xpts_by_match$pts, preds),\n    r2 = cor(xpts_by_match$pts, preds)^2\n  )\n}\n\nc('understat', 'fotmob') |> \n  set_names() |> \n  map_dfr(diagnose_implied_xpts_by_match, .id = 'src')\n#> # A tibble: 2 Ã— 3\n#>   src        rmse    r2\n#>   <chr>     <dbl> <dbl>\n#> 1 understat  1.06 0.374\n#> 2 fotmob     1.10 0.321\n```\n:::\n\n\nAgain, we see that understat has a lower RMSE and higher R squared. The implication that understat performs slightly better than fotmob agrees with the results from the binary logistic regression approiach for predicting match outcome probabilities and the linear regression approach for predicting points.\n\nOverall, we might say that understat seems to be the better of the two xG sources for explaining individual match results, although the margin is small enough that I would hesitate to say that this is the true across all leagues and all seasons.\n\n### 3. Season predictive performance\n\nHow do the understat and fotmob models fare if we aggregate up the expected points to the season level and predict actual points?[^11]\n\n[^11]: Note that aggregating match-level probabilities to the season-level is not a statistically valid way to use the probabilities, which are intended to be treated independently.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nxpts_by_season <- xpts_by_match |> \n  group_by(season, team) |> \n  summarize(\n    across(c(pts, starts_with('xpts')), sum)\n  ) |> \n  ungroup()\n\ndiagnose_xpts_by_season <- function(src) {\n  \n  col <- sprintf('xpts_%s', src)\n  fit <- lm(xpts_by_season$pts ~ xpts_by_season[[col]])\n  \n  preds <- predict(fit)\n  \n  tibble(\n    rmse = compute_rmse(xpts_by_match$pts, preds),\n    r2 = summary(fit)$r.squared\n  )\n}\n\nc('understat', 'fotmob') |> \n  set_names() |> \n  map_dfr(diagnose_xpts_by_season, .id = 'src')\n#> # A tibble: 2 Ã— 3\n#>   src        rmse    r2\n#>   <chr>     <dbl> <dbl>\n#> 1 understat  53.9 0.845\n#> 2 fotmob     53.8 0.825\n```\n:::\n\n\nThe results are closer than those at the match-level. In fact, fotmob just barely edges out understat in terms of RMSE xPts, although understat outperforms fotmob according to R squared by a relatively comfortable 0.02. It's harder to make a general statement regarding which data source provides better xG for explaining season-long expected points, although we might lean in favor of understat again.\n\n## Conclusion\n\nOverall, we find that understat's xG model seems to very slightly outperform fotmob's in terms of explaining match results and season-long point totals.\n\nIn a follow up post, we'll go more in depth regarding how we can leverage the match outcome probabilities to simulate season-ending points in a more rigorous fashion that done in the last section above.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}