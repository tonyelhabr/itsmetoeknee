[
  {
    "objectID": "posts/decomposition-smoothing-soccer/index.html",
    "href": "posts/decomposition-smoothing-soccer/index.html",
    "title": "Decomposing and Smoothing Soccer Spatial Tendencies",
    "section": "",
    "text": "While reading up on modern soccer analytics, I stumbled upon an excellent set of tutorials written by Devin Pleuler. In particular, his notebook on non-negative matrix factorization (NNMF) caught my eye. I hadn’t really heard of the concept before, but it turned out to be much less daunting once I realized that it is just another type of matrix decomposition. Singular value decomposition (SVD), which I’m much more familiar with, belongs to the same family of calculations (although NNMF and SVD are quite different). In an effort to really gain a better understanding of NNMF, I set out to emulate his notebook.\nIn the process of converting his python code to R, I encountered three challenges with resolutions worth documenting.\nI’ve always considered myself a “whatever gets the job done” kind of person, not insistent on ignoring solutions that use “base” R, {data.table}, python, etc. Nonetheless, replicating Devin’s notebook really underscored the importance of being comfortable outside of a {tidyverse}-centric workflow.\nAnyways, this post outlines the code and my thought process in porting Devin’s code to R. I’ll skip some of the details, emphasizing the things that are most interesting."
  },
  {
    "objectID": "posts/decomposition-smoothing-soccer/index.html#data",
    "href": "posts/decomposition-smoothing-soccer/index.html#data",
    "title": "Decomposing and Smoothing Soccer Spatial Tendencies",
    "section": "Data",
    "text": "Data\nWe’ll be working with the open-sourced StatsBomb data for the 2018 Men’s World Cup, which I’ve called events below. 3\nThis is a relatively large data set with lots of columns (and rows). However, we only need three columns for what we’re going to do: (1) a unique identifier for each player, player_id, along with their (2) x and (3) y coordinates.\n\n\nCode\nlibrary(tidyverse)\ncomps <- StatsBombR::FreeCompetitions()\n\nmatches <- comps %>% \n  filter(competition_id == 43) %>%\n  StatsBombR::FreeMatches() %>% \n  arrange(match_date)\n\nevents <- StatsBombR::StatsBombFreeEvents(matches)\nevents <- StatsBombR::allclean(events)\nevents <- events %>% \n  select(player_id = player.id, x = location.x, y = location.y) %>% \n  drop_na()\n\n\nA quick summary of the data shows that there are 603 unique players, and that the x and y coordinates range from 1 to 120 (yards) and 1 to 80 respectively.\n\n\nCode\nevents %>% \n  summarize(\n    n = n(),\n    n_player = n_distinct(player_id),\n    across(c(x, y), list(min = min, max = max, mean = mean))\n  )\n#> # A tibble: 1 x 8\n#>       n n_player x_min x_max x_mean y_min y_max y_mean\n#>   <int>    <int> <dbl> <dbl>  <dbl> <dbl> <dbl>  <dbl>\n#> 1 224018      603     1   120  60.05     1    80  40.37"
  },
  {
    "objectID": "posts/decomposition-smoothing-soccer/index.html#non-equi-joining-with-data.table",
    "href": "posts/decomposition-smoothing-soccer/index.html#non-equi-joining-with-data.table",
    "title": "Decomposing and Smoothing Soccer Spatial Tendencies",
    "section": "Non-Equi Joining with {data.table}",
    "text": "Non-Equi Joining with {data.table}\nOur first challenge is to convert the following chunk of python.\n\n\nCode\nimport numpy as np\n\nx_scale, y_scale = 30, 20\n\nx_bins = np.linspace(0, 120, x_scale)\ny_bins = np.linspace(0, 80, y_scale)\n\nplayers = {}\n\nfor e in events:\n    if 'player' in e.keys():\n        player_id = e['player']['id']\n        if player_id not in players.keys():\n            players[player_id] = np.zeros((x_scale, y_scale))\n        try:\n            x_bin = int(np.digitize(e['location'][0], x_bins[1:], right=True))\n            y_bin = int(np.digitize(e['location'][1], y_bins[1:], right=True))\n            players[player_id][x_bin][y_bin] += 1\n        except:\n            pass\n\n\nThis code creates a nested dict, where the keys are player id’s and the values are 20x30 matrices. Each element in the matrix is an integer that represents the count of times that the player was recorded being at a certain position on the pitch. (These counts range from 0 to 94 for this data set.)\nSome technical details:\n\nThe python events is actually a pretty heavily nested list4, hence the non-rectangular operations such as e['player']['id'].\nObservations with missing coordinates are ignored with the try-except block.\nx and y values (elements of the 'location' sub-list) are mapped to “bins” using numpy’s digitize() function, which is analogous to base::cut().\n\nHow can we do this same data manipulation in an idiomatic R fashion? We could certainly create a named list element and use base::cut() to closely match the python approach. However, I prefer to stick with data frames and SQL-ish operations since I think these are much more “natural” for R users.5\nSo, going forward with data frames and joins, it’s quickly apparent that we’ll have to do some non-equi joining. {fuzzyjoin} and {sqldf} offer functionality for such an approach, but {data.table} is really the best option. The only minor inconvenience here is that we have to explicitly coerce our events data frame to a data.table.\nWe’ll also need a helper, grid-like data frame to assist with the binning. The 600-row grid_xy_yards data frame (30 x bins * 20 y bins) below is essentially a tidy definition of the cells of the grid upon which we are binning the events data. (One can use whatever flavor of crossing(), expand.grid(), seq(), etc. that you prefer to create a data frame like this.)\nVisually, this grid looks like this.\n\nAnd if you prefer numbers instead of a chart, see the first 10 rows below.\n\n\nCode\ngrid_xy_yards\n#> # A tibble: 600 x 5\n#>     idx     x      y next_y next_x\n#>   <int> <dbl>  <dbl>  <dbl>  <dbl>\n#> 1     1     0  0      4.211  4.138\n#> 2     2     0  4.211  8.421  4.138\n#> 3     3     0  8.421 12.63   4.138\n#> 4     4     0 12.63  16.84   4.138\n#> 5     5     0 16.84  21.05   4.138\n#> 6     6     0 21.05  25.26   4.138\n#> 7     7     0 25.26  29.47   4.138\n#> 8     8     0 29.47  33.68   4.138\n#> 9     9     0 33.68  37.89   4.138\n#> 10    10    0 37.89  42.11   4.138\n#> # ... with 590 more rows\n\n\nTwo things to note about this supplementary data frame:\n\nCells aren’t evenly spaced integers, i.e. x cells are defined at 0, 4.138, 8.276, …, 80 instead of something like 0, 4, 8, …, 80, and y cells are defined at 0, 4.211, 8.421, …, 120 instead of something like 0, 4, 8, …, 120). That’s simply due to using 30 and 20 instead of 31 and 21 to split up the x and y ranges respectively. I point this out because this SQL-ish approach would have been much easier if these numbers were just integers! We could have done an inner join on an integer grid instead of non-equi-joining upon a grid of floating point numbers. Unfortunately, joining on floating point numbers as keys leads to inconsistent results, simply due to the nature of floating points.6\nThe index idx is important! This will come back into play when we do the NNMF procedure, at which point we’ll “flatten” out our x-y pairs into a 1-d format.\n\nOk, on to the actual data joining.\n\n\nCode\nevents_dt <- events %>% drop_na() %>% data.table::as.data.table()\ngrid_xy_yards_dt <- grid_xy_yards %>% data.table::as.data.table()\n\n# We don't even have to load `{data.table}` for this to work!\nevents_binned <- events_dt[grid_xy_yards_dt, on=.(x > x, x <= next_x, y >= y, y < next_y)] %>% \n  as_tibble() %>% \n  select(player_id, idx, x, y)\nevents_binned\n#> # A tibble: 224,038 x 4\n#>    player_id   idx     x     y\n#>        <int> <int> <dbl> <dbl>\n#>  1      5462     1     0     0\n#>  2      5467     1     0     0\n#>  3      5488     1     0     0\n#>  4      3632     1     0     0\n#>  5      5576     1     0     0\n#>  6      5595     1     0     0\n#>  7      5263     1     0     0\n#>  8      4063     1     0     0\n#>  9      5231     1     0     0\n#> 10      5231     1     0     0\n#> # ... with 224,028 more rows\n\n\nIn retrospect, this join was pretty straightforward!\nThe rest of the code below is just doing the actual tallying.\n\nFirst, we make an intermediate data set grid_players, which is the Cartesian product of all possible cells in the grid and all players in events.\nSecond, we “add back” missing cells to events_binned using the intermediate data set grid_players.\n\nIn the end, we end up with a players data frame with 603 player_ids * 30 x bins * 20 y bins = 361,800 rows.\n\n\nCode\n# This `dummy` column approach is an easy way to do a Cartesian join when the two data frames don't share any column names.\ngrid_players <- grid_xy_yards %>% \n  mutate(dummy = 0L) %>% \n  # Cartesian join of all possible cells in the grid and all players in `events`.\n  full_join(\n    events %>% \n      drop_na() %>% \n      distinct(player_id) %>% \n      mutate(dummy = 0L),\n    by = 'dummy'\n  )\n\nplayers <- events_binned %>% \n  group_by(player_id, x, y, idx) %>% \n  summarize(n = n()) %>% \n  ungroup() %>% \n  # Rejoin back on the grid to 'add back' cells with empty counts (i.e. `n = 0`).\n  full_join(grid_players, by = c('player_id', 'x', 'y', 'idx')) %>% \n  select(-dummy, -next_x, -next_y) %>% \n  replace_na(list(n = 0L)) %>% \n  arrange(player_id, x, y)\nplayers\n#> # A tibble: 361,800 x 5\n#>    player_id     x      y   idx     n\n#>        <int> <dbl>  <dbl> <int> <int>\n#>  1      2941     0  0         1     0\n#>  2      2941     0  4.211     2     0\n#>  3      2941     0  8.421     3     0\n#>  4      2941     0 12.63      4     0\n#>  5      2941     0 16.84      5     0\n#>  6      2941     0 21.05      6     0\n#>  7      2941     0 25.26      7     0\n#>  8      2941     0 29.47      8     0\n#>  9      2941     0 33.68      9     0\n#> 10      2941     0 37.89     10     0\n#> # ... with 361,790 more rows\n\n\nTo make this a little bit more tangible, let’s plot Messi’s heatmap. (Is this really a blog post about soccer if it doesn’t mention Messi 😆?)"
  },
  {
    "objectID": "posts/decomposition-smoothing-soccer/index.html#non-negative-matrix-factorization-nnmf-with-reticulate-and-sklearn",
    "href": "posts/decomposition-smoothing-soccer/index.html#non-negative-matrix-factorization-nnmf-with-reticulate-and-sklearn",
    "title": "Decomposing and Smoothing Soccer Spatial Tendencies",
    "section": "Non-Negative Matrix Factorization (NNMF) with {reticulate} and sklearn",
    "text": "Non-Negative Matrix Factorization (NNMF) with {reticulate} and sklearn\nNext up is the actual NNMF calculation. I don’t care if you’re the biggest R stan in the world—you have to admit that the python code to perform the NNMF is quite simple and (dare I say) elegant. The comps=30 here means\n\n\nCode\nfrom sklearn.decomposition import NMF\n\n# Flatten individual player matrices into shape=(600,) which is the product of the original shape components (30 by 20)\nunraveled = [np.matrix.flatten(v) for k, v in players.items()]\ncomps = 30\nmodel = NMF(n_components=comps, init='random', random_state=0)\nW = model.fit_transform(unraveled)\n\n\nMy understanding is that comps=30 is telling the algorithm to reduce our original data (with 603 players) to a lower dimensional space with 30 player “archetypes” that best represent the commonalities among the 603 players.7 Per Devin, the choice of 30 here is somewhat arbitrary. In practice, one might perform some cross validation to identify what number minimizes some loss function, but that’s beyond the scope of what we’re doing here.\nAfter re-formatting our players data into a wide format—equivalent to the numpy.matrix.flatten() call in the python code—we could use the {NMF} package for an R replication.\n\n\nCode\n# Convert from tidy format to wide format (603 rows x 600 columns)\nplayers_mat <- players %>% \n  drop_na() %>% \n  select(player_id, idx, n) %>% \n  pivot_wider(names_from = idx, values_from = n) %>% \n  select(-player_id) %>% \n  as.matrix()\n\ncomps <- 30L\nW <- NMF::nmf(NMF::rmatrix(players_mat), rank = comps, seed = 0, method = 'Frobenius')\n\n\nHowever, I found that the results weren’t all that comparable to the python results. (Perhaps I needed to define the arguments in a different manner.) So why not use {reticulate} and call the sklearn.decomposition.NMF() function to make sure that we exactly emulate the python decomposition?\n\n\nCode\nsklearn <- reticulate::import('sklearn')\n# Won't work if `n_components` aren't explicitly defined as integers!\nmodel <- sklearn$decomposition$NMF(n_components = comps, init = 'random', random_state = 0L)\nW <- model$fit_transform(players_mat)\n\n\nThe result includes 30 20x30 matrices—one 30x20 x-y matrix for each of the 30 components (comps). We have some wrangling left to do to gain anything meaningful from this NNMF procedure, but we have something to work with!"
  },
  {
    "objectID": "posts/decomposition-smoothing-soccer/index.html#gaussian-smoothing-with-spatstat",
    "href": "posts/decomposition-smoothing-soccer/index.html#gaussian-smoothing-with-spatstat",
    "title": "Decomposing and Smoothing Soccer Spatial Tendencies",
    "section": "Gaussian Smoothing with {spatstat}",
    "text": "Gaussian Smoothing with {spatstat}\nThe last thing to do is to post-process the NNMF results and, of course, make pretty plots. The python plotting is pretty standard matplotlib, with the exception of the Gaussian smoothing performed on each component’s matrix model.component_ in the loop to make sub-plots.\n\n\nCode\nfrom scipy.ndimage import gaussian_filter\n\nfor i in range(9):\n    # ...\n    z = np.rot90(gaussian_filter(model.components_[i].reshape(x_scale, y_scale), sigma=1.5), 1)\n    # ...\n\n\nThe first 9 smoothed component matrices come out looking like this. 8\n\nThere’s a couple of steps involved to do the same thing in R.\n\nFirst, we’ll convert the components matrices to a tidy format, decomp_tidy\nSecond, we’ll join our tidied components matrices with our tidy grid of cells, grid_xy_yards, and convert our x and y bins to integers in preparation of the matrix operation performed in the subsequent step.\nLastly, we’ll perform the Gaussian smoothing on nested data frames with a custom function, smoothen_dimension, that wraps spatstat::blur(). This function also maps idx back to field positions (in meters instead of yards) using the supplementary grid_xy_rev_m9 data frame (which is a lot like grid_xy_yards)\n\n\n\nCode\n## 1\ndecomp_tidy <- model$components_ %>% \n  as_tibble() %>% \n  # \"Un-tidy\" tibble with 30 rows (one for each dimension) and 600 columns (one for every `idx`)\n  mutate(dimension = row_number()) %>% \n  # Convert to a tidy tibble with dimensions * x * y rows (30 * 30 * 20 = 18,000)\n  pivot_longer(-dimension, names_to = 'idx', values_to = 'value') %>% \n  # The columns from the matrix are named `V1`, `V2`, ... `V600` by default, so convert them to an integer that can be joined on.\n  mutate(across(idx, ~str_remove(.x, '^V') %>% as.integer()))\n\n## 2\ndecomp <- decomp_tidy %>% \n  # Join on our grid of x-y pairs.\n  inner_join(\n    # Using `dense_rank` because we need indexes here (i.e. 1, 2, ..., 30 instead of 0, 4.1, 8.2, ..., 120 for `x`).\n    grid_xy_yards %>% \n      select(idx, x, y) %>% \n      mutate(across(c(x, y), dense_rank))\n  )\n\n## 3\nsmoothen_component <- function(.data, ...) {\n  mat <- .data %>% \n    select(x, y, value) %>% \n    pivot_wider(names_from = x, values_from = value) %>% \n    select(-y) %>% \n    as.matrix()\n  \n  mat_smoothed <- mat %>% \n    spatstat::as.im() %>% \n    # Pass `sigma` in here.\n    spatstat::blur(...) %>% \n    # Could use `spatstat::as.data.frame.im()`, but it converts directly to x,y,value triplet of columns, which is not the format I want.\n    pluck('v')\n  \n  res <- mat_smoothed %>% \n    # Convert 20x30 y-x matrix to tidy format with 20*30 rows.\n    as_tibble() %>% \n    mutate(y = row_number()) %>% \n    pivot_longer(-y, names_to = 'x', values_to = 'value') %>% \n    # The columns from the matrix are named `V1`, `V2`, ... `V30` by default, so convert them to an integer that can be joined on.\n    mutate(across(x, ~str_remove(.x, '^V') %>% as.integer())) %>% \n    arrange(x, y) %>% \n    # \"Re-index\" rows with `idx`, ranging from 1 to 600.\n    mutate(idx = row_number()) %>% \n    select(-x, -y) %>% \n    # Convert `x` and `y` indexes (i.e. 1, 2, 3, ..., to meters and flip the y-axis).\n    inner_join(grid_xy_rev_m) %>% \n    # Re-scale smoothed values to 0-1 range.\n    mutate(frac = (value - min(value)) / (max(value) - min(value))) %>% \n    ungroup()\n  res\n}\n\ndecomp_smooth <- decomp %>% \n  nest(data = -c(dimension)) %>% \n  # `sigma` passed into `...` of `smoothen_component()`. (`data` passed as first argument.)\n  mutate(data = map(data, smoothen_component, sigma = 1.5)) %>% \n  unnest(data)\ndecomp_smooth\n#> # A tibble: 18,000 x 8\n#>    dimension    value   idx     x     y next_y next_x     frac\n#>        <int>    <dbl> <int> <dbl> <dbl>  <dbl>  <dbl>    <dbl>\n#>  1         1 0.002191     1     0 68     4.211  4.138 0.004569\n#>  2         1 0.004843     2     0 64.42  8.421  4.138 0.01064 \n#>  3         1 0.008334     3     0 60.84 12.63   4.138 0.01863 \n#>  4         1 0.01130      4     0 57.26 16.84   4.138 0.02541 \n#>  5         1 0.01258      5     0 53.68 21.05   4.138 0.02834 \n#>  6         1 0.01208      6     0 50.11 25.26   4.138 0.02719 \n#>  7         1 0.01033      7     0 46.53 29.47   4.138 0.02319 \n#>  8         1 0.008165     8     0 42.95 33.68   4.138 0.01824 \n#>  9         1 0.006156     9     0 39.37 37.89   4.138 0.01364 \n#> 10         1 0.004425    10     0 35.79 42.11   4.138 0.009680\n#> # ... with 17,990 more rows\n\n\nWith the data in the proper format, the plotting is pretty straightforward {ggplot2} code.\n\nViola! I would say that our R version of the python plot is very comparable (just by visual inspection). Note that we could achieve a similar visual profile without the smoothing—see below—but the smoothing undoubtedly makes pattern detection a little less ambiguous.\n\nFrom the smoothed contours, we can discern several different player profiles (in terms of positioning).\n\nComponents 1, 5, 9: left back\nComponents 2: right midfielder\nComponent 3: attacking right midfielder\nComponent 4: wide left midfielder\nComponent 6: central left midfielder\nComponents 7, 8: goalkeeper\n\nThe redundancy with left back and goalkeeper is not ideal. That’s certainly something we could fine tune with more experimentation with components. Anyways, the point of this post wasn’t so much about the insights that could be gained (although that’s ultimately what stakeholders would be interested in if this were a “real” analysis)."
  },
  {
    "objectID": "posts/decomposition-smoothing-soccer/index.html#conclusion",
    "href": "posts/decomposition-smoothing-soccer/index.html#conclusion",
    "title": "Decomposing and Smoothing Soccer Spatial Tendencies",
    "section": "Conclusion",
    "text": "Conclusion\nTranslating python code can be challenging, throwing us off from our typical workflow (for me, being {tidyverse}-centric). But hopefully one can see the value in “doing whatever it takes”, even if it means using “non-tidy” R functions (e.g. {data.table}, matrices, etc.) or a different language altogether."
  },
  {
    "objectID": "posts/dimensionality-reduction-and-clustering/index.html",
    "href": "posts/dimensionality-reduction-and-clustering/index.html",
    "title": "Tired: PCA + kmeans, Wired: UMAP + GMM",
    "section": "",
    "text": "Combining principal component analysis (PCA) and kmeans clustering seems to be a pretty popular 1-2 punch in data science. While there is some debate about whether combining dimensionality reduction and clustering is something we should ever do1, I’m not here to debate that. I’m here to illustrate the potential advantages of upgrading your PCA + kmeans workflow to Uniform Manifold Approximation and Projection (UMAP) + Gaussian Mixture Model (GMM), as noted in my reply here.\n\n\ntired: PCA + kmeanswired: UMAP + GMM\n\n— Tony (@TonyElHabr) June 2, 2021\n\n\nFor this demonstration, I’ll be using this data set pointed out here, including over 100 stats for players from soccer’s “Big 5” leagues.\n\n\nCode\nlibrary(tidyverse)\nraw_df <- 'FBRef 2020-21 T5 League Data.xlsx' %>% \n  readxl::read_excel() %>% \n  janitor::clean_names() %>% \n  mutate(across(where(is.double), ~replace_na(.x, 0)))\n\n# Let's only use players with a 10 matches' worth of minutes.\ndf <- raw_df %>% filter(min > (10 * 90))\ndim(df)\n## [1] 1626  128\n\n\nTrying to infer something from the correlation matrix doesn’t get you very far, so one can see why dimensionality reduction will be useful.\n\nAlso, we don’t really have “labels” here (more on this later), so clustering can be useful for learning something from our data."
  },
  {
    "objectID": "posts/dimensionality-reduction-and-clustering/index.html#unsupervised-evaluation",
    "href": "posts/dimensionality-reduction-and-clustering/index.html#unsupervised-evaluation",
    "title": "Tired: PCA + kmeans, Wired: UMAP + GMM",
    "section": "Unsupervised Evaluation",
    "text": "Unsupervised Evaluation\nWe’ll be feeding in the results from the dimensionality reduction—either PCA or UMAP—to a clustering method—either kmeans or GMM. So, since clustering comes last, all we need to do is figure out how to judge the clustering; this will tell us something about how “good” the combination of dimensionality reduction and clustering is overall.\nI’ll save you from google-ing and just tell you that within-cluster sum of squares (WSS) is typically used for kmeans, and Bayesian Information Criteria (BIC) is the go-to metric for GMM. WSS and BIC are not on the same scale, so we can’t directly compare kmeans and GMM at this point. Nonetheless, we can experiment with different numbers of components—the one major “hyperparameter” for dimensionality reduction—prior to the clustering to identify if more or less components is “better”, given the clustering method. Oh, and why not also vary the number of clusters—the one notable hyperparameter for clustering—while we’re at it?\n\nFor kmeans, we see that WSS decreases with increasing number of clusters, which is typically what we see in “elbow” plots like this. Additionally, we see that WSS decreases with increasing number of components. This makes sense—additional components means more data is accounted for.2 There is definitely a point of “diminishing returns”, somewhere around 3 clusters, after which WSS barely improves.3 Overall, we observe that the kmeans models using UMAP pre-processing do better, compared to those using PCA.\nMoving on to GMM, we observe that BIC generally increases with the number of clusters as well. (Note that, due to the way the {mclust} package defines its objective function, higher BIC is “better”.)\n\nRegarding number of components, we see that the GMM models using more UMAP components do better, as we should have expected. On the other hand, we observe that GMM models using less PCA components do better than those with more components. This is a bit of an odd finding that I don’t have a great explanation for. (Someone please math-splain to me.) Nonetheless, we see that UMAP does better than PCA overall, as we observed with kmeans.\nFor those interested in the code, I map-ed a function across a grid of parameters to generate the data for these plots.4\n\n\nCode\ndo_dimr_clust <- function(df, n, k, dimr, clust, ...) {\n  step_f <- switch(dimr, 'pca' = recipes::step_pca, 'umap' = embed::step_umap)\n  fit_f <- switch(clust, 'kmeans' = stats::kmeans, 'gmm' = mclust::Mclust)\n  \n  d <- recipes::recipe(formula( ~ .), data = df) %>%\n    recipes::step_normalize(recipes::all_numeric_predictors()) %>%\n    step_f(recipes::all_numeric_predictors(), num_comp = n) %>% \n    recipes::prep() %>% \n    recipes::juice() %>% \n    select(where(is.numeric))\n  fit <- fit_f(d, k, ...)\n  broom::glance(fit)\n}\n\nmetrics <- crossing(\n  n = seq.int(2, 8),\n  k = seq.int(2, 8),\n  f_dimr = c('pca', 'umap'),\n  f_clust = c('kmeans', 'mclust')\n) %>%\n  mutate(metrics = pmap(\n    list(n, k, f_dimr, f_clust),\n    ~ do_dimr_clust(\n      df = df,\n      n = ..1,\n      k = ..2,\n      f_dimr = ..3,\n      f_clust = ..4\n    )\n  ))\nmetrics\n#> # A tibble: 196 x 5\n#>        n     k f     g      metrics         \n#>    <int> <int> <chr> <chr>  <list>          \n#>  1     2     2 pca   kmeans <tibble [1 x 4]>\n#>  2     2     2 pca   gmm    <tibble [1 x 7]>\n#>  3     2     2 umap  kmeans <tibble [1 x 4]>\n#>  4     2     2 umap  gmm    <tibble [1 x 7]>\n#>  5     2     3 pca   kmeans <tibble [1 x 4]>\n#>  6     2     3 pca   gmm    <tibble [1 x 7]>\n#>  7     2     3 umap  kmeans <tibble [1 x 4]>\n#>  8     2     3 umap  gmm    <tibble [1 x 7]>\n#>  9     2     4 pca   kmeans <tibble [1 x 4]>\n#> 10     2     4 pca   gmm    <tibble [1 x 7]>\n#> # ... with 186 more rows"
  },
  {
    "objectID": "posts/dimensionality-reduction-and-clustering/index.html#supervised-evaluation",
    "href": "posts/dimensionality-reduction-and-clustering/index.html#supervised-evaluation",
    "title": "Tired: PCA + kmeans, Wired: UMAP + GMM",
    "section": "“Supervised” Evaluation",
    "text": "“Supervised” Evaluation\nWe actually do have something that we can use to help us identify clusters—player position (pos). Let’s treat these position groups as pseudo-labels with which we can gauge the effectiveness of the clustering.\n\n\nCode\ndf <- df %>% \n  mutate(\n    across(\n      pos,\n      ~case_when(\n        .x %in% c('DF,MF', 'MF,DF') ~ 'DM',\n        .x %in% c('DF,FW', 'FW,DF') ~ 'M',\n        .x %in% c('MF,FW', 'FW,MF') ~ 'AM',\n        .x == 'DF' ~ 'D',\n        .x == 'MF' ~ 'M',\n        .x == 'FW' ~ 'F',\n        .x == 'GK' ~ 'G',\n        .x == 'GK,MF' ~ 'G',\n        TRUE ~ .x\n      )\n    )\n  )\ndf %>% count(pos, sort = TRUE)\n#> # A tibble: 6 x 2\n#>   pos       n\n#>   <chr> <int>\n#> 1 D       595\n#> 2 M       364\n#> 3 AM      273\n#> 4 F       196\n#> 5 G       113\n#> 6 DM       85\n\n\nTypically we don’t have labels for clustering tasks; if we do, we’re usually doing some kind of supervised multi-label classification. But our labels aren’t “true” labels in this case, both because:\n\na player’s nominal position often doesn’t completely describe their style of play, and\nthe grouping I did to reduce the number of positions from 11 to 6 was perhaps not optimal.\n\nSo now let’s do the same as before—evaluate different combinations of PCA and UMAP with kmeans and GMM. But now we can use some supervised evaluation metrics: (1) accuracy and (2) mean log loss. While the former is based on the “hard” predictions, the latter is based on probabilities for each class. kmeans returns just hard cluster assignments, so computing accuracy is straightforward; since it doesn’t return probabilities, we’ll treat the hard assignments as having a probability of 1 to compute log loss.5\nWe can compare the two clustering methods more directly now using these two metrics. Since we know that there are 6 position groups, we’ll keep the number of clusters constant at 6. (Note that number of clusters was shown on the x-axis before; but since we have fixed number of components at 6, now we show the number of components on the x-axis.)\nLooking at accuracy first, we see that the best combo depends on our choice for number of components. Overall, we might say that the UMAP combos are better.\n\nNext, looking at average log loss, we see that the GMM clustering methods seem to do better overall (although this may be due to the fact that log loss is not typically used for supervised kmeans). The PCA + GMM does the best across all number of components, with the exception of 7. Note that we get a mean log loss around 28 when we predict the majority class (defender) with a probability of 1 for all observations. (This is a good “baseline” to contextualize our numbers.)\n\nUMAP shines relative to PCA according to accuracy, and GMM beats out kmeans in terms of log loss. Despite these conclusions, we still don’t have clear evidence that UMAP + GMM is the best 1-2 combo; nonetheless, we can at least feel good about its general strength.\n\nAside: Re-coding Clusters\nI won’t bother to show all the code to generate the above plots since it’s mostly just broom::augmment() and {ggplot2}. But, if you have ever worked with supervised stuff like this (if we can call it that), you’ll know that figuring out which of your clusters correspond to your known groups can be difficult. In this case, I started from a variable holding the predicted .class and the true class (pos).\n\n\nCode\nassignments\n#> # A tibble: 1,626 x 2\n#>    .class pos  \n#>     <int> <chr>\n#>  1      1 D    \n#>  2      2 D    \n#>  3      3 M    \n#>  4      3 M    \n#>  5      4 AM   \n#>  6      2 D    \n#>  7      2 D    \n#>  8      4 F    \n#>  9      2 D    \n#> 10      1 D    \n#> # ... with 1,616 more rows\n\n\nI generated a correlation matrix for these two columns, ready to pass into a matching procedure.\n\n\nCode\nassignments %>% \n  fastDummies::dummy_cols(c('.class', 'pos'), remove_selected_columns = TRUE) %>% \n  corrr::correlate(method = 'spearman', quiet = TRUE) %>% \n  filter(term %>% str_detect('pos')) %>% \n  select(term, matches('^[.]class'))\n#> # A tibble: 6 x 7\n#>   term   .class_1 .class_2 .class_3 .class_4 .class_5 .class_6\n#>   <chr>     <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>\n#> 1 pos_AM  -0.208   -0.241   -0.178    0.0251   0.625   -0.123 \n#> 2 pos_D    0.499    0.615   -0.335   -0.264   -0.428   -0.208 \n#> 3 pos_DM   0.0797   0.0330   0.0548  -0.0829  -0.0519  -0.0642\n#> 4 pos_F   -0.171   -0.199   -0.168    0.724    0.0232  -0.101 \n#> 5 pos_G   -0.127   -0.147   -0.124   -0.0964  -0.157    1     \n#> 6 pos_M   -0.222   -0.267    0.724   -0.180    0.0395  -0.147\n\n\nThen I used clue::solve_LSAP() to do the bipartite matching magic. The rest is just pre- and post-processing.\n\n\nCode\nk <- 6 # number of clusters\ncols_idx <- 2:(k+1)\ncors_mat <- as.matrix(cors[,cols_idx]) + 1 # all values have to be positive\nrownames(cors_mat) <- cors$term\ncols <- names(cors)[cols_idx]\ncolnames(cors_mat) <- cols\ncols_idx_min <- clue::solve_LSAP(cors_mat, maximum = TRUE)\ncols_min <- cols[cols_idx_min]\ntibble::tibble(\n  .class = cols_min %>% str_remove('^[.]class_') %>% as.integer(),\n  pos = cors$term %>% str_remove('pos_')\n)\n#> # A tibble: 6 x 2\n#>   .class pos  \n#>    <int> <chr>\n#> 1      5 AM   \n#> 2      2 D    \n#> 3      1 DM   \n#> 4      4 F    \n#> 5      6 G    \n#> 6      3 M \n\n\nThis pairs variable can be used to re-code the .class column in our assignments from before."
  },
  {
    "objectID": "posts/dimensionality-reduction-and-clustering/index.html#case-study-pca-vs.-umap",
    "href": "posts/dimensionality-reduction-and-clustering/index.html#case-study-pca-vs.-umap",
    "title": "Tired: PCA + kmeans, Wired: UMAP + GMM",
    "section": "Case Study: PCA vs. UMAP",
    "text": "Case Study: PCA vs. UMAP\nLet’s step back from the clustering techniques and focus on dimensionality reduction for a moment. One of the ways that dimensionality reduction can be leveraged in sports like soccer is for player similarity metrics.6 Let’s take a look at how this can be done, comparing the PCA and UMAP results while we’re at it.\nDirect comparison of the similarity “scores” we’ll compute—based on Euclidean distance between a chosen player’s components and other players’ components—is not wise given the different ranges of our PCA and UMAP components, so we’ll rely on rankings based on these scores.7 Additionally, fbref provides a “baseline” that we can use to judge our similarity rankings.8\nWe’ll start with Jadon Sancho, a highly discussed player at the moment (as a potential transfer).\n\nWe first need to set up our data into the following format. (This is for 2-component, 6-cluster UMAP + GMM.)\n\n\nCode\nsims_int\n#> # A tibble: 1,664 x 6\n#>    player_1     player_2           comp_1 comp_2 value_1 value_2\n#>    <chr>        <chr>               <int>  <int>   <dbl>   <dbl>\n#>  1 Jadon Sancho Aaron Leya Iseka        1      1  -4.18  -5.14  \n#>  2 Jadon Sancho Aaron Leya Iseka        2      2  -0.678  2.49  \n#>  3 Jadon Sancho Aaron Ramsey            1      1  -4.18  -3.25  \n#>  4 Jadon Sancho Aaron Ramsey            2      2  -0.678 -0.738 \n#>  5 Jadon Sancho Abdoul Kader Bamba      1      1  -4.18  -3.40  \n#>  6 Jadon Sancho Abdoul Kader Bamba      2      2  -0.678  0.0929\n#>  7 Jadon Sancho Abdoulaye Doucouré      1      1  -4.18  -1.36  \n#>  8 Jadon Sancho Abdoulaye Doucouré      2      2  -0.678 -2.66  \n#>  9 Jadon Sancho Abdoulaye Touré         1      1  -4.18  -1.36  \n#> 10 Jadon Sancho Abdoulaye Touré         2      2  -0.678 -2.89  \n#> # ... with 1,654 more rows\n\n\nThen the Euclidean distance calculation is fairly straightforward.\n\n\nCode\nsims_init %>% \n  group_by(player_1, player_2) %>% \n  summarize(\n    d = sqrt(sum((value_1 - value_2)^2))\n  ) %>% \n  ungroup() %>% \n  mutate(score = 1 - ((d - 0) / (max(d) - 0))) %>% \n  mutate(rnk = row_number(desc(score))) %>% \n  arrange(rnk) %>% \n  select(player = player_2, d, score, rnk)\n#> # A tibble: 830 x 4\n#>    player                  d score   rnk\n#>    <chr>               <dbl> <dbl> <int>\n#>  1 Alexis Sánchez     0.0581 0.994     1\n#>  2 Riyad Mahrez       0.120  0.988     2\n#>  3 Serge Gnabry       0.132  0.986     3\n#>  4 Jack Grealish      0.137  0.986     4\n#>  5 Pablo Sarabia      0.171  0.983     5\n#>  6 Thomas Müller      0.214  0.978     6\n#>  7 Leroy Sané         0.223  0.977     7\n#>  8 Callum Hudson-Odoi 0.226  0.977     8\n#>  9 Jesse Lingard      0.260  0.973     9\n#> 10 Ousmane Dembélé    0.263  0.973    10\n#> # ... with 820 more rows\n\n\nDoing the same for PCA and combining all results, we get the following set of rankings.\n\nWe see that the UMAP rankings are “closer” overall to the fbref rankings. Of course, there are some caveats:\n\nThis is just one player.\nThis is with a specific number of components and clusters.\nWe are comparing to similarity rankings based on a separate methodology.\n\nOur observation here (that UMAP > PCA) shouldn’t be taken out of context to conclude that UMAP > PCA in all contexts. Nonetheless, I think this is an interesting use case for dimensionality reduction, where one can justify PCA, UMAP, or any other similar technique, depending on how intuitive the results are."
  },
  {
    "objectID": "posts/dimensionality-reduction-and-clustering/index.html#case-study-umap-gmm",
    "href": "posts/dimensionality-reduction-and-clustering/index.html#case-study-umap-gmm",
    "title": "Tired: PCA + kmeans, Wired: UMAP + GMM",
    "section": "Case Study: UMAP + GMM",
    "text": "Case Study: UMAP + GMM\nFinally, let’s bring clustering back into the conversation. We’re going to focus on how the heralded UMAP + GMM combo can be visualized to provide insight that supports (or debunks) our prior understanding.\nWith a 2-component UMAP + 6-cluster GMM, we can see how the 6 position groups can be identified in a 2-D space.\n\nFor those curious, using PCA instead of UMAP also leads to an identifiable set of clusters. However, uncertainties are generally higher across the board (larger point sizes, more overlap between covariance ellipsoids).\n\nIf we exclude keepers (G) and defenders (D) to focus on the other 4 positions with our UMAP + GMM approach, we can better see how some individual points —at the edges or outside of covariance ellipsoids—are classified with a higher degree of uncertainty.9\n\nNow, highlighting incorrect classifications, we can see how the defensive midfielder (DM) position group (upper left) seems to be a blind spot in our approach.\n\nA more traditional confusion matrix10 also illustrates the inaccuracy with classifying DMs. (Note the lack of dark grey fill in the DM column.)\n\nDMs are often classified as defenders instead. I think this poor result has is more so due to my lazy grouping of players with \"MF,DF\" or \"DF,MF\" positions in the original data set than a fault in our approach."
  },
  {
    "objectID": "posts/dimensionality-reduction-and-clustering/index.html#conclusion",
    "href": "posts/dimensionality-reduction-and-clustering/index.html#conclusion",
    "title": "Tired: PCA + kmeans, Wired: UMAP + GMM",
    "section": "Conclusion",
    "text": "Conclusion\nSo, should our overall conclusion be that we should never use PCA or kmeans? No, not necessarily. They can both be much faster to compute than UMAP and GMMs respectively, which can be a huge positive if computation is a concern. PCA is linear while UMAP is not, so you may want to choose PCA to make it easier to explain to your friends. Regarding clustering, kmeans is technically a specific form of a GMM, so if you want to sound cool to your friends and tell them that you use GMMs, you can do that.\nAnyways, I hope I’ve shown why you should try out UMAP and GMM the next time you think about using PCA and kmeans."
  },
  {
    "objectID": "posts/epl-xpts-simulation-1/index.html",
    "href": "posts/epl-xpts-simulation-1/index.html",
    "title": "What exactly is an “expected point”? (part 1)",
    "section": "",
    "text": "Expected goals (xG) in soccer have gone mainstream and are no longer cool to talk about.\n\n\nWhat exactly is an ” expected goal “? Who decides the criteria ? Is there a list of” expected goal scorers ” ? Or even ” unexpected ones ” ?\n\n— Ian Darke (@IanDarke) December 24, 2020\n\n\nSo let’s talk about expected points (xPts). The one sentence explainer for xPts: it’s a number between 0 and 3 assigned to each team in a match that we estimate from the xG of each shot in the match. Teams that accumulate more xG than their opponents in the match are more likely to have xPts closer to 3, i.e. the points awarded for a win, and those that accumulate less than their opponents are more likely to earn xPts closer to 0. xPts is convenient for translating a team’s xG (relative to it’s opponents) to the team’s expected placement in the standings.\nWhile several outlets have described computing expected points with simulation1, simulation is actually not necessary if you have the xG for every shot taken in a match.2 For example, let’s say team A shoots six times with an xG of 0.1 for each shot, and team B shoots three shots with xG’s of 0.1, 0.2, and 0.3 respectively. Given these goal probabilities, we can analytically compute xPts as follows.\nFirst, we find the probability of scoring 0, 1, 2, etc. goals (up to the number of shots taken).3\n\n\nCode\nlibrary(poibin)\nxg_a <- rep(0.1, 6)\nxg_b <- c(0.1, 0.2, 0.3)\n\nprobs_a <- dpoibin(seq.int(0, length(xg_a)), xg_a)\nround(probs_a, 2)\n#> [1] 0.53 0.35 0.10 0.01 0.00 0.00 0.00\nprobs_b <- dpoibin(seq.int(0, length(xg_b)), xg_b)\nround(probs_b, 2)\n#> [1] 0.50 0.40 0.09 0.01\n\n\n\nSecond, we convert the goal probabilities to singular probabilities for each team winning the match, as well as the probability of a draw.4\n\n\nCode\nlibrary(gdata)\nouter_prod <- outer(probs_a, probs_b)\np_a <- sum(upperTriangle(outer_prod))\np_b <- sum(lowerTriangle(outer_prod))\np_draw <- sum(diag(outer_prod))\nround(c(p_a, p_b, p_draw), 2)\n#> [1] 0.30 0.28 0.42\n\n\nFinally, given the match outcome probabilities, the xPts calculation is straightforward.\n\n\nCode\nxpts_a <- 3 * p_a + 1 * p_draw\nxpts_b <- 3 * p_b + 1 * p_draw\nround(c(xpts_a, xpts_b), 2)\n#> [1] 1.31 1.27\n\n\nFor this example, we arrive at the interesting result that, despite the two teams total xG being equal (=0.6), team A has a slightly higher probability of winning. There have been plenty of explanations on this “quality vs. quantity” phenomenon, so I won’t go into it in detail. Nonetheless, this simple example illustrates why it can be useful to translate xG into another form—doing so can provide a better perspective on match results and, consequently, team placement in the standings.\n\n\nSo we’ve gone over what expected points are and why they’re important. Now we set out to do the following.\n\nCalculate xPts from shot xG for multiple seasons of data. We’ll limit the scope to the 2020/21 and 2021/22 seasons for the English Premier League.5\nCompare the calibration of the understat and fotmob match outcome probabilities. {worldfootballR} makes it easy for us to get xG from both understat and fotmob, and it should be interesting to compare the the predictive performance of the two models.\nCompare predictions of actual season-long points using xPts that we derive from understat and fotmob xG. In particular, we’ll be interested to see if our conclusions regarding the better source for xG here matches the conclusions for (2)."
  },
  {
    "objectID": "posts/epl-xpts-simulation-1/index.html#analysis",
    "href": "posts/epl-xpts-simulation-1/index.html#analysis",
    "title": "What exactly is an “expected point”? (part 1)",
    "section": "Analysis",
    "text": "Analysis\n\n1. Calculating xPts from xG\nLet’s start by using the load_understat_league_shots() function from {worldfootballR} to retrieve understat xG by shot.\n\n\nCode\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tibble)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(lubridate)\nlibrary(worldfootballR) ## version: 0.5.12.5000\nlibrary(janitor)\n\nrename_home_away_teams <- function(df) {\n  df |> \n    mutate(\n      team = ifelse(is_home, home_team, away_team),\n      opponent = ifelse(is_home, away_team, home_team)\n    ) |> \n    select(-c(home_team, away_team)) \n}\n\nconvert_understat_year_to_season <- function(x) {\n  sprintf('%s/%s', x, str_sub(x + 1, 3, 4))\n}\n\n## we'll use all of the shots later when exporing understat data only\nall_understat_shots <- load_understat_league_shots('EPL') |> \n  as_tibble() |> \n  ## camelcase like \"xG\" is for Java scrubs\n  clean_names() |> \n  filter(season <= 2021) |> \n  ## transmute = select + mutate\n  transmute(\n    match_id,\n    ## \"2021/2022\" format so that we have a clear, consistent way to represent season\n    across(season, convert_understat_year_to_season),\n    ## to convert \"2020-09-12 11:30:00\" to a date (\"2020-09-12\")\n    across(date, lubridate::date),\n    home_team,\n    away_team,\n    is_home = h_a == 'h',\n    xg = x_g\n  ) |>\n  rename_home_away_teams() |> \n  arrange(season, date, team)\n\n## but when comparing understat with fotmob, we'll need to limit the seasons to just\n##   those that both sources have\nunderstat_shots <- all_understat_shots |> filter(season >= 2020)\n\n\nWe can use load_fotmob_match_details() to get fotmob’s shot xG in a similar fashion.6\n\n\nCode\n## manually created CSV with at least 2 columns: team_understat, team_fotmob.\n##   use the team_understat name to be consistent across sources.\nteam_mapping <- 'https://raw.githubusercontent.com/tonyelhabr/sports_viz/master/59-xg_xpoints/team_mapping.csv' |> \n  read_csv()\n\nrename_fotmob_teams <- function(df) {\n  df |> \n    left_join(\n      team_mapping |> select(team_understat, team_fotmob),\n      by = c('home_team' = 'team_fotmob')\n    ) |> \n    select(-home_team) |> \n    rename(home_team = team_understat) |> \n    left_join(\n      team_mapping |> select(team_understat, team_fotmob),\n      by = c('away_team' = 'team_fotmob')\n    ) |> \n    select(-away_team) |> \n    rename(away_team = team_understat)\n}\n\nfotmob_shots <- load_fotmob_match_details(\n  country = 'ENG',\n  league_name = 'Premier League'\n) |> \n  mutate(\n    ## to convert strings from 'Sat, Sep 12, 2020, 11:30 UTC' to a date\n    date = strptime(match_time_utc, '%a, %b %d, %Y, %H:%M UTC', tz = 'UTC') |> date(),\n    ## fotmob's parent_league_season always reflects the current season, so we need to manually\n    ##   define the season from the date. we would certainly want a more automated approach\n    ##   if working with more seasons and more leagues.\n    season = case_when(\n      date >= ymd('2020-09-12') & date <= ymd('2021-05-23') ~ '2020/21',\n      date >= ymd('2021-08-13') & date <= ymd('2022-05-22') ~ '2021/22',\n      TRUE ~ NA_character_\n    )\n  ) |> \n  ## the NAs are for 2022/2023 (incomplete as of writing) and the partial data for 2019/2020\n  drop_na(season) |> \n  transmute(\n    match_id,\n    season,\n    date,\n    home_team,\n    away_team,\n    is_home = team_id == home_team_id,\n    ## some shots with NAs for some reason\n    xg = coalesce(expected_goals, 0)\n  ) |>\n  rename_fotmob_teams() |> \n  rename_home_away_teams() |> \n  arrange(season, date, team)\n\n\nAlright, now the fun part. We functionalize the code from the example for calculating the probability that xG will result in 0, 1, 2, etc. goals.\n\n\nCode\nlibrary(purrr)\npermute_xg <- function(xg) {\n  n <- length(xg)\n  x <- seq.int(0, n)\n  dpoibin(x, xg)\n}\n\ncalculate_permuted_xg <- function(df) {\n  df |> \n    group_by(across(c(everything(), -xg))) |> \n    summarize(across(xg, ~list(.x))) |> \n    mutate(\n      prob = map(xg, ~permute_xg(.x))\n    ) |> \n    select(-c(xg)) |> \n    unnest(cols = c(prob)) |> \n    group_by(across(-c(prob))) |>\n    mutate(\n      g = row_number() - 1L\n    ) |>\n    ungroup() |> \n    arrange(match_id, is_home, g)\n}\n\nunderstat_permuted_xg <- understat_shots |> calculate_permuted_xg()\nfotmob_permuted_xg <- fotmob_shots |> calculate_permuted_xg()\n\n\nNext, we identify all possible goal combinations using xG as “weights” to compute the relative likelihood of each combination, and then analytically calculate the probabilities of winning, losing, and drawing.\n\n\nCode\nsummarize_pivoted_permuted_xg <- function(prob_away, prob_home) {\n  outer_prod <- outer(prob_away, prob_home)\n  p_draw <- sum(diag(outer_prod), na.rm = TRUE)\n  p_home <- sum(upperTriangle(outer_prod), na.rm = TRUE)\n  p_away <- sum(lowerTriangle(outer_prod), na.rm = TRUE)\n  list(\n    draw = p_draw,\n    home = p_home,\n    away = p_away\n  )\n}\n\n## Bournemouth 0 - 1 Manchester City on 2019-03-02\n## Huddersfield 0 - 0 Swansea on 2018-03-10\npad_for_matches_without_shots_from_one_team <- function(df) {\n  n_teams_per_match <- df |> \n    distinct(match_id, team) |> \n    count(match_id, sort = TRUE)\n  \n  matches_with_no_shots_from_one_team <- n_teams_per_match |> \n    filter(n == 1)\n  \n  dummy_opponents <- df |> \n    distinct(match_id, season, date, team, opponent, is_home) |> \n    semi_join(\n      matches_with_no_shots_from_one_team,\n      by = 'match_id'\n    ) |> \n    mutate(\n      z = team\n    ) |> \n    transmute(\n      match_id, \n      season, \n      date, \n      team = opponent,\n      opponent = z,\n      across(is_home, ~!.x),\n      prob = 1,\n      g = 0L\n    )\n  \n  bind_rows(\n    df,\n    dummy_opponents\n  ) |> \n  arrange(season, date, team, g)\n}\n\nsummarize_permuted_xg_by_match <- function(df) {\n  \n  padded_df <- pad_for_matches_without_shots_from_one_team(df)\n  \n  pivoted <- padded_df |>\n    transmute(\n      match_id,\n      season,\n      date,\n      g,\n      is_home = ifelse(is_home, 'home', 'away'),\n      prob\n    ) |>\n    pivot_wider(\n      names_from = is_home,\n      names_prefix = 'prob_',\n      values_from = prob,\n      values_fill = 0L\n    )\n  \n  pivoted |> \n    select(match_id, season, date, prob_away, prob_home) |>\n    group_by(match_id, season, date) |> \n    summarize(\n      across(starts_with('prob_'), ~list(.x))\n    ) |> \n    ungroup() |> \n    inner_join(\n      padded_df |> distinct(match_id, team, opponent, is_home),\n      by = 'match_id'\n    ) |> \n    mutate(\n      prob = map2(prob_away, prob_home, summarize_pivoted_permuted_xg)\n    ) |> \n    select(-starts_with('prob_')) |> \n    unnest_wider(prob, names_sep = '_') |> \n    mutate(\n      prob_win = ifelse(is_home, prob_home, prob_away),\n      prob_lose = ifelse(is_home, prob_away, prob_home),\n      xpts = 3 * prob_win + 1 * prob_draw\n    ) |> \n    select(-c(prob_home, prob_away))\n}\n\nunderstat_xpts_by_match <- understat_permuted_xg |> summarize_permuted_xg_by_match()\nfotmob_xpts_by_match <- fotmob_permuted_xg |> summarize_permuted_xg_by_match()\n\n\nLet’s take a quick peak at the distributions of xG and xPts, both as a sanity check and to enhance our understanding of the relationship between the two. When plotting xPts as a function xG, we should expect to see a monotonically increasing relationship where xPts bottoms out at zero and tops out at three.\n\nFurther, if there is any doubt about the expected points calculation, note that understat offers xPts directly in their data. The mean absolute error of our calculation of xPts with theirs is ~0.02.\n\n\nCode\nlibrary(understatr)\n\nall_raw_understat_xpts_by_match <- 2014:2021 |> \n  set_names() |> \n  map_dfr(\n    ~get_league_teams_stats('EPL', .x),\n    .id = 'season'\n  ) |> \n  transmute(\n    across(season, ~convert_understat_year_to_season(as.integer(.x))),\n    date,\n    team = team_name,\n    result,\n    pts,\n    raw_xpts = xpts,\n    xg = xG\n  )\n\nraw_understat_xpts_by_match <- all_raw_understat_xpts_by_match |> \n  inner_join(\n    understat_xpts_by_match |> select(season, date, team, xpts),\n    by = c('season', 'date', 'team')\n  ) |> \n  mutate(\n    xptsd = raw_xpts - xpts\n  ) |> \n  arrange(season, date, team)\n\n## mean absolute error\nround(mean(abs(raw_understat_xpts_by_match$xptsd)), 2)\n#> [1] 0.02\n\n\n\n\n2. Match predictive performance7\nAs one might guess, the match outcome probabilities implied by the xG from understat and fotmob are strongly correlated.\n\n\nCode\nrename_xpts_by_match <- function(df, src) {\n  df |> \n    select(season, date, team, starts_with('prob_'), xpts) |> \n    rename_with(\n      ~sprintf('%s_%s', .x, src), c(starts_with('prob_'), xpts)\n    )\n}\n\nxpts_by_match <- raw_understat_xpts_by_match |> \n  select(season, date, team, result, pts) |> \n  inner_join(\n    understat_xpts_by_match |> rename_xpts_by_match('understat'),\n    by = c('season', 'date', 'team')\n  ) |> \n  inner_join(\n    fotmob_xpts_by_match |> rename_xpts_by_match('fotmob'),\n    by = c('season', 'date', 'team')\n  )\n\ncor_draw <- cor(xpts_by_match$prob_draw_fotmob, xpts_by_match$prob_draw_understat)\ncor_win <- cor(xpts_by_match$prob_win_fotmob, xpts_by_match$prob_win_understat)\ncor_lose <- cor(xpts_by_match$prob_lose_fotmob, xpts_by_match$prob_lose_understat)\nround(c(cor_draw, cor_win, cor_lose), 3)\n#> [1] 0.906 0.958 0.958\n\n\nNote that the win and loss correlations are identical. This is due to the symmetric nature of the data—we have two records for each match, one from each team’s perspective.8\n\nPredicting match outcomes with binary logistic regression\nNow let’s compare how “good” the implied probabilities from the two sources are. To do this, we’ll create binary logistic regression models to predict a given outcome and compute:\n\nthe mean squared error (MSE);\nthe brier skill score (BSS), treating the empirical proportion of the specified outcome as the reference.910\na calibration plot, grouping predictions into “buckets” at every 5%.\n\n\n\nCode\nresult_props <- xpts_by_match |> \n  count(result) |> \n  mutate(prop = n / sum(n))\n\ncompute_mse <- function(truth, estimate) {\n  mean((truth - estimate)^2)\n}\n\ndiagnose_prob_by_match <- function(src, result) {\n  \n  df <- xpts_by_match |> \n    mutate(\n      result = ifelse(result == !!result, 1L, 0L) |> factor()\n    )\n  \n  result_name <- switch(\n    result,\n    'w' = 'win',\n    'l' = 'lose',\n    'd' = 'draw'\n  )\n  col <- sprintf('prob_%s_%s', result_name, src)\n  \n  fit <- glm(\n    df$result ~ df[[col]],\n    family = 'binomial'\n  )\n  \n  probs <- tibble(\n    result_num = as.numeric(df$result) - 1,\n    .prob = unname(predict(fit, type = 'response'))\n  )\n  \n  n_buckets <- 20\n  alpha <- 0.05\n  calib <- probs |>\n    mutate(\n      across(.prob, ~round(.x * n_buckets) / n_buckets)\n    ) |>\n    group_by(.prob) |>\n    summarize(\n      ## Jeffreys' prior\n      ci_lower = qbeta(alpha / 2, sum(result_num) + 0.5, n() - sum(result_num) + 0.5),\n      ci_upper = qbeta(1 - alpha / 2, sum(result_num) + 0.5, n() - sum(result_num) + 0.5),\n      actual = sum(result_num) / n(),\n      n = n()\n    ) |> \n    ungroup()\n  \n  mse <- compute_mse(probs$result_num, probs$.prob)\n  \n  ref_prob <- result_props |> \n    filter(result == !!result) |> \n    pull(prop)\n  \n  ref_mse <- compute_mse(probs$result_num, ref_prob)\n  bss <- 1 - (mse / ref_mse)\n  \n  list(\n    calib = calib,\n    mse = mse,\n    bss = bss\n  )\n}\n\ndiagnostics <- crossing(\n  result = c('w', 'd'),\n  src = c('understat', 'fotmob')\n) |> \n  mutate(\n    diagnostics = map2(src, result, diagnose_prob_by_match)\n  ) |> \n  unnest_wider(diagnostics)\ndiagnostics |> select(-calib)\n#> # A tibble: 4 × 4\n#>   result src         mse    bss\n#>   <chr>  <chr>     <dbl>  <dbl>\n#> 1 d      fotmob    0.170 0.0268\n#> 2 d      understat 0.166 0.0466\n#> 3 w      fotmob    0.173 0.270 \n#> 4 w      understat 0.162 0.317\n\n\nThe MSE (where lower is “better”) and the BSS (where higher is “better”) lead us to the same conclusion—the models based on understat’s xG slightly outperform the one based on fotmob’s xG.\nMoreover, looking at the calibration plot, the understat model predictions seem to stick closer to the 45 degree slope representing perfect calibration.\n\n\n\nPredicting points with linear regression\nAlternatively, we could regress points on expected points. For linear regression, we can use the root mean squared error (RMSE) (where lower is “better”) and R squared (where higher is “better”) to compare the models.\n\n\nCode\ncompute_rmse <- function(truth, estimate) {\n  sqrt(mean((truth - estimate)^2))\n}\n\ndiagnose_xpts_by_match <- function(src) {\n  \n  col <- sprintf('xpts_%s', src)\n  fit <- lm(xpts_by_match$pts ~ xpts_by_match[[col]])\n  \n  pred <- predict(fit)\n  \n  tibble(\n    rmse = compute_rmse(xpts_by_match$pts, pred),\n    r2 = summary(fit)$r.squared\n  )\n}\n\nc('understat', 'fotmob') |> \n  set_names() |> \n  map_dfr(diagnose_xpts_by_match, .id = 'src')\n#> # A tibble: 2 × 3\n#>   src        rmse    r2\n#>   <chr>     <dbl> <dbl>\n#> 1 understat  1.06 0.374\n#> 2 fotmob     1.10 0.323\n\n\nThe understat model proves to be better by both metrics, having a lower RMSE and higher R squared than the fotmob model.\n\n\nPredicting match outcomes with multinomial logistic regression\nPersonally, I don’t like predicting points directly like this since it’s a discrete variable that can only take on three values (0, 1, and 3). If we’re going to predict points instead of a probability, I think the better approach is to run a multinomial logistic regression and to convert the predicted probabilities to expected points.\n\n\nCode\nlibrary(nnet)\ndiagnose_implied_xpts_by_match <- function(src) {\n  \n  col_win <- sprintf('prob_win_%s', src)\n  col_draw <- sprintf('prob_draw_%s', src)\n  fit <- multinom(\n    xpts_by_match$result ~ xpts_by_match[[col_win]] + xpts_by_match[[col_draw]],\n    trace = FALSE\n  )\n  probs <- predict(fit, type = 'probs') |> as_tibble()\n  preds <- 3 * probs$w + 1 * probs$d\n  \n  tibble(\n    rmse = compute_rmse(xpts_by_match$pts, preds),\n    r2 = cor(xpts_by_match$pts, preds)^2\n  )\n}\n\nc('understat', 'fotmob') |> \n  set_names() |> \n  map_dfr(diagnose_implied_xpts_by_match, .id = 'src')\n#> # A tibble: 2 × 3\n#>   src        rmse    r2\n#>   <chr>     <dbl> <dbl>\n#> 1 understat  1.06 0.374\n#> 2 fotmob     1.10 0.321\n\n\nAgain, we see that understat has a lower RMSE and higher R squared. The implication that understat performs slightly better than fotmob agrees with the results from the binary logistic regression approiach for predicting match outcome probabilities and the linear regression approach for predicting points.\nOverall, we might say that understat seems to be the better of the two xG sources for explaining individual match results, although the margin is small enough that I would hesitate to say that this is the true across all leagues and all seasons.\n\n\n\n3. Season predictive performance\nHow do the understat and fotmob models fare if we aggregate up the expected points to the season level and predict actual points?11\n\n\nCode\nxpts_by_season <- xpts_by_match |> \n  group_by(season, team) |> \n  summarize(\n    across(c(pts, starts_with('xpts')), sum)\n  ) |> \n  ungroup()\n\ndiagnose_xpts_by_season <- function(src) {\n  \n  col <- sprintf('xpts_%s', src)\n  fit <- lm(xpts_by_season$pts ~ xpts_by_season[[col]])\n  \n  preds <- predict(fit)\n  \n  tibble(\n    rmse = compute_rmse(xpts_by_match$pts, preds),\n    r2 = summary(fit)$r.squared\n  )\n}\n\nc('understat', 'fotmob') |> \n  set_names() |> \n  map_dfr(diagnose_xpts_by_season, .id = 'src')\n#> # A tibble: 2 × 3\n#>   src        rmse    r2\n#>   <chr>     <dbl> <dbl>\n#> 1 understat  53.9 0.845\n#> 2 fotmob     53.8 0.825\n\n\nThe results are closer than those at the match-level. In fact, fotmob just barely edges out understat in terms of RMSE xPts, although understat outperforms fotmob according to R squared by a relatively comfortable 0.02. It’s harder to make a general statement regarding which data source provides better xG for explaining season-long expected points, although we might lean in favor of understat again."
  },
  {
    "objectID": "posts/epl-xpts-simulation-1/index.html#conclusion",
    "href": "posts/epl-xpts-simulation-1/index.html#conclusion",
    "title": "What exactly is an “expected point”? (part 1)",
    "section": "Conclusion",
    "text": "Conclusion\nOverall, we find that understat’s xG model seems to very slightly outperform fotmob’s in terms of explaining match results and season-long point totals.\nIn a follow up post, we’ll go more in depth regarding how we can leverage the match outcome probabilities to simulate season-ending points in a more rigorous fashion that done in the last section above."
  },
  {
    "objectID": "posts/epl-xpts-simulation-2/index.html",
    "href": "posts/epl-xpts-simulation-2/index.html",
    "title": "What exactly is an “expected point”? (part 2)",
    "section": "",
    "text": "I’ll be picking up where I left off in my last post, so stop everything that you’re doing and go read that if you haven’t already. In this post we’ll do two things:\n\nWe’ll compare how well season-level expected goal difference (xGD), season-level xPts, and aggregated match-level xPts predict season-long points for a given team.\nWe’ll use the match-level probabilites to answer the questions “Which teams had the most unlikely placings in the table given the quality of all their shots across the season?” and “How unlikely were such placings?”"
  },
  {
    "objectID": "posts/epl-xpts-simulation-2/index.html#analysis",
    "href": "posts/epl-xpts-simulation-2/index.html#analysis",
    "title": "What exactly is an “expected point”? (part 2)",
    "section": "Analysis",
    "text": "Analysis\nBefore we start, we need to step back and retrieve data on the actual placings. We could theoretically calculate this from the shot data we already have. However, the logic for handling own goals is a little complicated. We’re probably better off using worldfootballR::understat_league_match_results()—which returns goals at the match-level—to calculate the table. 1\n\n\nCode\nmatch_results <- 2014:2021 |> \n  map_dfr(~understat_league_match_results('EPL', .x)) |> \n  as_tibble()\n\ninit_table <- match_results |> \n  transmute(\n    match_id,\n    across(season, ~str_replace(.x, '\\\\/20', '/')),\n    date = strptime(datetime, '%Y-%m-%d %H:%M:%S', tz = 'UTC') |> date(),\n    home_team,\n    home_goals,\n    away_team,\n    away_goals\n  )\n\ntable <- bind_rows(\n  init_table |>\n    mutate(is_home = TRUE) |> \n    rename_home_away_teams() |> \n    select(season, team, opponent, goals = home_goals, opponent_goals = away_goals),\n  init_table |> \n    mutate(is_home = FALSE) |> \n    rename_home_away_teams() |> \n    select(season, team, opponent, goals = away_goals,opponent_goals = home_goals)\n) |> \n  mutate(\n    pts = case_when(\n      goals > opponent_goals ~ 3L,\n      goals < opponent_goals ~ 0L,\n      TRUE ~ 1L\n    )\n  ) |> \n  group_by(season, team) |> \n  summarize(across(c(goals, opponent_goals, pts), sum)) |> \n  ungroup() |> \n  mutate(gd = goals - opponent_goals) |> \n  arrange(season, desc(pts), desc(gd)) |> \n  group_by(season) |> \n  mutate(rank = row_number(desc(pts))) |> \n  ungroup() |> \n  arrange(season, rank)\n\n\nThis data will help us contextualize predicted placings with actual placings.\n\n1. Predicting season-long points\n\nWith season-long xPts and xGD\nWe start with the all_raw_understat_xpts_by_match variable from the prior post, adding the opponent’s expected goals to create a column for expected goal difference (xgd).\n\n\nCode\nall_raw_understat_xpts_by_match_with_opponent <- all_raw_understat_xpts_by_match |> \n  inner_join(\n    all_understat_shots |> distinct(match_id, season, date, team, opponent),\n    by = c('season', 'date', 'team')\n  )\n\n## we've already determined that the raw_xpts (provided directly by understat) \n##   is close to our calculated xpts, so we'll just use the raw_xpts.\nall_raw_understat_xpts_xgd_by_match <- all_raw_understat_xpts_by_match_with_opponent |> \n  select(match_id, season, date, team, opponent, pts, xpts = raw_xpts, xg) |> \n  inner_join(\n    all_raw_understat_xpts_by_match_with_opponent |> \n      select(match_id, opponent = team, opponent_xg = xg),\n    by = c('match_id', 'opponent')\n  ) |> \n  mutate(xgd = xg - opponent_xg)\n\n\nNext, we aggregate up to the season-level.\n\n\nCode\nall_raw_understat_xpts_xgd_by_season <- all_raw_understat_xpts_xgd_by_match |> \n  group_by(season, team) |> \n  summarize(across(c(pts, xpts, xgd), sum)) |> \n  ungroup() |> \n  group_by(season) |> \n  mutate(xrank = row_number(desc(xpts))) |> \n  ungroup() |> \n  arrange(season, desc(pts), team)\n\n\nFinally, we compute RMSE and R squared, like we did in the last post.\n\n\nCode\ndiagnose_season_feature <- function(df, col) {\n  fit <- lm(df$pts ~ df[[col]])\n  tibble(\n    rmse = compute_rmse(df$pts, predict(fit)),\n    r2 = summary(fit)$r.squared\n  )\n}\n\nc('xgd', 'xpts') |> \n  set_names() |> \n  map_dfr(\n    ~diagnose_season_feature(all_raw_understat_xpts_xgd_by_season, .x), \n    .id = 'feature'\n  )\n#> # A tibble: 2 × 3\n#>   feature  rmse    r2\n#>   <chr>   <dbl> <dbl>\n#> 1 xgd      7.33 0.831\n#> 2 xpts     7.20 0.837\n\n\nAs we should expect, a model using season-long xPts to predict final points outperforms one using season-long xGD as a feature, although maybe the difference between the two is smaller than we might have expected.\n\n\nWith match-level outcome probabilities\nFirst, we use the full understat shot data set and the custom functions from the prior post to calculate xPts by match.\n\n\nCode\nall_understat_xpts_by_match <- all_understat_shots |> \n  calculate_permuted_xg() |> \n  summarize_permuted_xg_by_match()\n\n\nNext, the fun part: simulating match outcomes using the xG-implied match outcome probabilities. This is computationally intense, so we parallelize the calculation.\n\n\nCode\nlibrary(parallel)\nlibrary(future)\nlibrary(furrr)\n\nunderstat_probs_by_match <- all_understat_xpts_by_match |> \n  select(match_id, season, team, opponent, is_home, starts_with('prob')) |> \n  rename_with(~str_remove(.x, '^prob_'), starts_with('prob')) |> \n  pivot_longer(\n    c(win, lose, draw),\n    names_to = 'result',\n    values_to = 'prob'\n  )\n\nsimulate_season_xpts <- function(...) {\n  sim_home_pts_by_match <- understat_probs_by_match |> \n    filter(is_home) |> \n    group_by(team, season, match_id) |> \n    slice_sample(n = 1, weight_by = prob) |> \n    ungroup() |>\n    mutate(\n      pts = case_when(\n        result == 'win' ~ 3L,\n        result == 'lose' ~ 0L,\n        TRUE ~ 1L\n      )\n    )\n  \n  sim_pts_by_match <- bind_rows(\n    sim_home_pts_by_match |> select(match_id, season, team, pts),\n    sim_home_pts_by_match |> \n      transmute(\n        match_id,\n        season,\n        team = opponent,\n        pts = case_when(\n          result == 'win' ~ 0L,\n          result == 'lose' ~ 3L,\n          TRUE ~ 1L\n        )\n      )\n  ) |> \n    group_by(season, team) |> \n    summarize(across(pts, sum)) |> \n    ungroup()\n  \n  sim_pts_by_match |> \n    group_by(season, team) |> \n    summarize(across(pts, sum)) |> \n    ungroup() |> \n    group_by(season) |> \n    mutate(rank = row_number(desc(pts))) |> \n    ungroup() |> \n    arrange(season, rank)\n}\n\nn_cores <- detectCores()\ncores_for_parallel <- ceiling(n_cores * 0.5)\nplan(\n  multisession,\n  workers = cores_for_parallel\n)\n\n## set seed both prior to the future_map_dfr and in .options to guarantee determinstic results\nset.seed(42)\nn_sims <- 10000\nunderstat_sim_pts_by_season <- set_names(1:n_sims) |> \n  future_map_dfr(\n    simulate_season_xpts, \n    .id = 'sim_idx', \n    .options = furrr_options(seed = 42)\n  )\n\n## back to normal processing\nplan(sequential)\n\n\nNext, we aggregate the season-long points across simulations, calculating the relative proportion of simulations in which a given team ends up at a given rank.\n\n\nCode\nunderstat_sim_placings <- understat_sim_pts_by_season |> \n  group_by(season, team, xrank = rank) |> \n  summarize(n = n(), xpts = mean(pts)) |> \n  ungroup() |> \n  group_by(season, team) |> \n  mutate(prop = n / sum(n)) |> \n  ungroup()\n\n\nFinally, we calculate the weighted average of expected points that a team ends up with, and run the same regression that we ran earlier with season-long xPts and xGD.\n\n\nCode\nunderstat_sim_placings_agg <- understat_sim_placings |> \n  group_by(season, team) |> \n  summarize(xpts = sum(xpts * prop)) |> \n  ungroup() |>\n  select(season, team, xpts) |>\n  inner_join(\n    all_raw_understat_xpts_xgd_by_season |> select(season, team, pts),\n    by = c('season', 'team')\n  ) |> \n  arrange(season, desc(xpts))\n\ndiagnose_season_feature(understat_sim_placings_agg, 'xpts')\n#> # A tibble: 1 × 2\n#>    rmse    r2\n#>   <dbl> <dbl>\n#> 1  7.16 0.839\n\n\nInterestingly, the RMSE and R squared values are almost identical to those for the season-long xPts. Perhaps this is not too surprising—match-level outcome probabilities simulated and averaged to arrive at a singular estimate of season-long xPts should give us something very close to just computing season-long xPts directly.\nWhile the null result may be discouraging, the simulations are useful in and of themselves. They can be used to understand the distribution of outcomes for team in a given season, as seen in the table below.2\n\nWe can see that Leicester, Wolves, and Newcastle all placed at the uppper end of their simulated placings, indicating that they over-achieved relative to expectation; on the other hand, Crystal Palace, Brentford, and Leeds placed on the lower end of the distribution of placings, indicating that they under-achieved.\nIn fact, we can go beyond simple observational judgement of whether teams over- and under-achieved—we can use the relative proportion of simulations where a team ends up at a given placing (or “rank”) in the standings to quantify just how unexpected actual end-of-season placings were.\n\n\n\n2. Identifying un-expected placings\nFirst, we join the table of end-of-season placements (actual_rank) to the simulation results that describe the frequency with which a given team places at a given rank (xrank).\n\n\nCode\nunderstat_sim_placings_with_actual_ranks <- understat_sim_placings |> \n  inner_join(\n    table |> select(season, team, actual_pts = pts, actual_rank = rank),\n    by = c('season', 'team')\n  ) |> \n  inner_join(\n    all_raw_understat_xpts_xgd_by_season |> \n      select(season, team, actual_xpts = xpts, xgd),\n    by = c('season', 'team')\n  )\n\n\nFinally, to identify over-achieving teams, we find the teams that had the lowest cumulative probability of placing at their actual placing or better; and to identify under-achieving teams, we find the teams with the lowest cumulative probability of placing at their actual placing or worse.\n\nThis table certainly passes the eye test. Brighton’s sixteenth place finish in the 2020/21 season was discussed ad nauseum in the analytics sphere. Brighton under-performed historically given their massively positive xGD.\nOn the other end of the spectrum, it’s not hyperbole to say that Manchester United’s second place finish in the 2017/18 season was an over-achievement. Although they ended up with the third best goal differential that season, they were closely followed by several teams. And their xGD was sixth in the league that season.\n\nComparison with a simpler approach\nNotably, we could get somewhat similar results by simply looking at the largest residuals of a model that regresses the actual final table placing on just xGD, which is how most people tend to think of “unexpected placings”.\n\n\nCode\ntable_with_xgd <- table |> \n  select(season, team, actual_pts = pts, actual_rank = rank) |> \n  inner_join(\n    all_raw_understat_xpts_xgd_by_season |> select(season, team, xgd),\n    by = c('season', 'team')\n  )\n\nxgd_rank_fit <- lm(actual_rank ~ xgd, table_with_xgd)\n\n\nThe table below shows the top five over- and under-achieving teams according to our regression explaining season-ending placing with season-ending xGD. Three of the top five over- and under-performing teams appear in the respective top fives according to the ranks from the simulations of match probabilities shown before.\n\nWe can also look at this from the opposite perspective. Where do the top five over- and under-achievers according to our simulations with match outcome probabilities fall among the season-ending xGD ranks for unlikelihood?\n\nOutside of the the three team-season pairs appearing in the both of the top five over- and under-achievers that we already saw before, one team-season pair is not too far off from the top five—the 2017/18 Manchester United squad ranked as the sixth biggest over-performers by the season-long xGD regression.3 However, the other over-perfomer, 2019/20 Newcastle, and the two remaining under-performers, 2021/22 Crystal Palace and 2021/22 Brentford, have somewhat large ranking discrepancies. So yes, the two methods can lead to somewhat similar results in some cases, but there is some observational evidence to suggest that there are non-trivial differences in other cases.\nIn fact, there are some big differences between the two methods once we look outside the top five or so biggest over- and under- achievers. For example, in terms of over-achieving, Arsenal’s fifth place finish 2018/19 season is given just a 7.50% chance of occurring given their season-long xGD, ranking them as the 17th biggest over-performer (among 80 team-season pairs considered to have over-achieved from 2014/15).4 On the other hand, the match-level simulation approach marks the likelihood of them finishing fourth as 31.63% (37th of 80). The season-long xGD model essentially sees that they finished with an xGD of 7.5—less than any other fifth place finisher from 2014/15 - 2021/22—and penalizes them, while the match-level approach contextualizes them among their competition more robustly.\nAs another example, Manchester United’s under-achieving sixth place finish in the 2016/17 is given a fairly reasonable 37.36% chance (66th of 80 under-achieving teams) of occurring given their season-long 25.9 xGD, most for any sixth place team in the data set. On the other hand, the match-level simulation approach sees their sixth place finish as more unlikely, at just a 15.88% probability (16th of 80). While one might have their own opinion regarding which approach seems more “correct”, I’d say that likelihoods from the simulation approach seem more appropriate for extreme examples such as this one and the 2018/19 Arsenal example\nOverall, both approaches seem reasonable to use to answer the question “Which teams had the most unlikely placings in the table given the quality of all their shots across the season?” But the approach based on simulations using match probabilities seems more appropriate to use to quantify exactly how unlikely a team’s final placing was. While the simpler regression approach can also be used to quantify likelihood, it is more brittle, dependent on statistical assumptions. Additionally, while it contextualizes a team’s xGD with historical xGD, it does not contextualize a team’s xGD among the other teams in the league, meaning that it does not do a good job with capturing likelihood when there are strong xGD over- and under-performances among a set of teams in a given season."
  },
  {
    "objectID": "posts/epl-xpts-simulation-2/index.html#conclusion",
    "href": "posts/epl-xpts-simulation-2/index.html#conclusion",
    "title": "What exactly is an “expected point”? (part 2)",
    "section": "Conclusion",
    "text": "Conclusion\nWhile aggregating match-level outcome probabilities to try to predict season-ending points does no better than more direct approaches with season-long xGD or xPts, simulating seasons using match-level outcome probabilities can be used in a perhaps more interesting way—to quantify just how unlikely a team’s placement in the table is, given xG for all of their shots across the season."
  },
  {
    "objectID": "posts/fantasy-football-schedule-problem/index.html",
    "href": "posts/fantasy-football-schedule-problem/index.html",
    "title": "Fantasy Football and the Classical Scheduling Problem",
    "section": "",
    "text": "Every year I play in several fantasy football (American) leagues. For those who are unaware, it’s a game that occurs every year in sync with the National Football League (NFL) where participants play in weekly head-to-head games as general managers of virtual football teams. (Yes, it’s very silly.) The winner at the end of the season is often not the player with the team that scores the most points; often a fortunate sequence of matchups dictates who comes out on top.\nI didn’t fare so well this year in one of my leagues, but my disappointing placement was not due to my team struggling to score points; rather, I was extremely unlucky. I finished the season in 7th place despite scoring the most points!\nThis inspired me to quantify just how unlikely I was. The most common way to calculate the likelihood of a given team’s ranking in a league with is with a Monte Carlo simulation based on some parameterized model of scoring to generate probabilities for the final standings. FiveThirtyEight uses such a model for their soccer models, for example. For a setting in which team scores are independent of one another, such as fantasy football, another approach is to simply calculate what each team’s record would be if they had played every other team each week. (So, if your league has 10 teams and each plays each other once, each team would have a hypothetical count of 90 games played.) However, I was particularly interested in answering the question: “In how many different schedules would I have finished where I did?”"
  },
  {
    "objectID": "posts/fantasy-football-schedule-problem/index.html#problem",
    "href": "posts/fantasy-football-schedule-problem/index.html#problem",
    "title": "Fantasy Football and the Classical Scheduling Problem",
    "section": "Problem",
    "text": "Problem\nFiguring out how unlucky I was to finish 7th requires me to first figure out how many possible schedules there are. Formally, the problem can be put as follows1:\n\nLet \\(T={t_1, ..., t_n}\\) be a set of an even \\(n\\) teams. Let \\(R\\) denote a round consisting of a set of pairs \\((t_i, t_j)\\) (denoting a match), such that \\(0 < i < j <= n\\), and such that each team in \\(T\\) is participates exactly once in \\(R\\). Let \\(S\\) be a schedule consisting of a tuple of \\(n - 1\\) valid rounds \\((R_1, ..., R_{n-1})\\), such that all rounds in \\(S\\) are pair-wise disjoint (no round shares a match). How many valid constructions of \\(S\\) are there for \\(n\\) input teams?\n\nFor a small number of teams, it’s fairly simple to write out all possible combinations of matchups. For example, for a two-team league (where each team plays each other once), there is only one possible schedule (solution)—Team 1 vs. Team 2. For a four-team league, there are six possible schedules. Two are shown below.\n\n\n\nsolution\nround\nteam1\nteam2\n\n\n\n\n1\n1\n1\n2\n\n\n\n\n3\n4\n\n\n\n2\n1\n3\n\n\n\n\n2\n4\n\n\n\n3\n1\n4\n\n\n\n\n2\n3\n\n\n2\n1\n1\n3\n\n\n\n\n2\n4\n\n\n\n2\n1\n2\n\n\n\n\n3\n4\n\n\n\n3\n1\n4\n\n\n\n\n2\n3\n\n\n\nNote that there is no concept of “home advantage” in fantasy football, so the order of teams in a given matchup does not matter. Also, note that if our restriction (“constraint”) that each team must play each other once and only once, implies that the number of teams has to be an even number."
  },
  {
    "objectID": "posts/fantasy-football-schedule-problem/index.html#constraint-programming",
    "href": "posts/fantasy-football-schedule-problem/index.html#constraint-programming",
    "title": "Fantasy Football and the Classical Scheduling Problem",
    "section": "Constraint Programming",
    "text": "Constraint Programming\nTo truly answer this question, we can turn to constraint programming. If you’re familiar with constraint programming, then you’ll notice that this set-up is similar to the canonical nurse scheduling problem and is a specific form of the tournament problem.\nBelow is some python code that is able to identify the number feasible solutions for four teams. I print out the first solution for illustrative purposes.\n\n\nCode\nfrom ortools.sat.python import cp_model\n\nclass SolutionPrinter(cp_model.CpSolverSolutionCallback):\n    def __init__(self, games, n_team, n_show=None):\n        cp_model.CpSolverSolutionCallback.__init__(self)\n        self._games = games\n        self._n_show = n_show\n        self._n_team = n_team\n        self._n_sol = 0\n\n    def on_solution_callback(self):\n        self._n_sol += 1\n        print()\n        if self._n_show is None or self._n_sol <= self._n_show:\n            print(f'Solution {self._n_sol}.')\n            for team1 in range(self._n_team):\n                for team2 in range(self._n_team):\n                    if team1 != team2:\n                        print(\n                            f'Team {team1 + 1} vs. Team {team2 + 1} in Round {self.Value(self._games[(team1, team2)])}'\n                        )\n        else:\n            print(f'Found solution {self._n_sol}.')\n\n    def get_n_sol(self):\n        return self._n_sol\n\nn_team = 4\nn_w = n_team - 1\nmodel = cp_model.CpModel()\ngames = {}\nfor team1 in range(n_team):\n    for team2 in range(n_team):\n        if team1 != team2:\n            games[(team1, team2)] = model.NewIntVar(1, n_w, f'{team1:02}_{team2:02}')\n\nfor team1 in range(n_team):\n    for team2 in range(n_team):\n        if team1 != team2:\n            model.Add(games[(team1, team2)] == games[(team2, team1)])\n\n\n# Each team can only play in 1 game each week\nfor t in range(n_team):\n    model.AddAllDifferent(\n        [games[(t, team2)] for team2 in range(n_team) if t != team2]\n    )\n\nsolver = cp_model.CpSolver()\nsolution_printer = SolutionPrinter(games, n_team=n_team, n_show=2)\nstatus = solver.SearchForAllSolutions(model, solution_printer)\n\nprint()\nprint(f'Solve status: {solver.StatusName(status)}')\nprint(f'Solutions found: {solution_printer.get_n_sol()}')\n#> Solution 1.\n#> Team 1 vs. Team 2 in Round 3\n#> Team 1 vs. Team 3 in Round 2\n#> Team 1 vs. Team 4 in Round 1\n#> Team 2 vs. Team 1 in Round 3\n#> Team 2 vs. Team 3 in Round 1\n#> Team 2 vs. Team 4 in Round 2\n#> Team 3 vs. Team 1 in Round 2\n#> Team 3 vs. Team 2 in Round 1\n#> Team 3 vs. Team 4 in Round 3\n#> Team 4 vs. Team 1 in Round 1\n#> Team 4 vs. Team 2 in Round 2\n#> Team 4 vs. Team 3 in Round 3\n#> \n#> Found solution 2.\n#> \n#> Found solution 3.\n#> \n#> Found solution 4.\n#> \n#> Found solution 5.\n#> \n#> Found solution 6.\n#> \n#> Solve status: OPTIMAL\n#> Solutions found: 6\n\n\nEasy enough to run for 10 teams and get an answer, right? WRONG. Turns out this the number of feasible solutions (schedules) starts to blow up really quickly. In fact, I believe the number of solutions for this particular problem is only known up to 14 teams. (I’ve intentionally left the numbers un-rounded to emphasize just how much the number of solutions increases as a function of the number of teams.)\n\n\n\nn\nsolutions\n\n\n\n\n2\n1\n\n\n4\n6\n\n\n6\n720\n\n\n8\n31,449,600\n\n\n10\n444,733,651,353,600\n\n\n12\n10,070,314,878,246,925,803,220,024\n\n\n14\n614,972,203,951,464,579,840,082,248,206,026,604,282\n\n\n\nUnless you happen to be an expert in graph theory and combinatorics, you probably wouldn’t be able to figure this out by hand; for us non-experts out there, we can refer to a known sequence of 1-factorizations of a complete graph \\(K_{2n}\\) and use our brain to figure out permutations in a given round. (Don’t worry if that makes no sense.)\nWhy do I bring this up? Well, I realized that generating all possible schedules for a 10-team league (such as my aforementioned league) is just not reasonable for anyone without a supercomputer and a lot of time. I enhanced the above python code a bit and tried it out for a 10-team league and was only able to generate a couple of million solutions after 3 hours."
  },
  {
    "objectID": "posts/fantasy-football-schedule-problem/index.html#alternative-exhaustive-search",
    "href": "posts/fantasy-football-schedule-problem/index.html#alternative-exhaustive-search",
    "title": "Fantasy Football and the Classical Scheduling Problem",
    "section": "Alternative: Exhaustive Search",
    "text": "Alternative: Exhaustive Search\nThe failure to generate all solutions made me reconsider things a bit. If I can’t reasonably “have it all”, I should simplify things a bit. By “simplify”, I mean perform an “exhaustive” (or “brute-force) search that stops after a specified number of solutions. And, by re-writing things in R, I can eliminate dependencies on Google’s ortools package and python. (Both are great, but, nonetheless, they are potential obstacles for R users.)\nWriting a script to perform an exhaustive search is not so easy itself, and, in this case, requires a completely different approach to the problem. My steps are as follows:\n\nSet up an \\(n\\) x \\(n-1\\) matrix, where the \\(n\\) rows designate teams and the \\(n-1\\) columns designate rounds.\n\n\n\nCode\nleague_size = 4\nrounds <- league_size - 1\nmat <- matrix(nrow = league_size, ncol = rounds)\nmat\n#>      [,1] [,2] [,3]\n#> [1,]   NA   NA   NA\n#> [2,]   NA   NA   NA\n#> [3,]   NA   NA   NA\n#> [4,]   NA   NA   NA\n\n\n\nRandomly select the opponent of team 1 in round 1.\n\n\n\nCode\nteam_i <- 1\nround_i <- 1\nretry_i <- 1\nidx_team <- 1:league_size\nset.seed(1)\n\nteam_1_round_1 <- sample(2:league_size, 1, replace = FALSE)\nmat[team_i, round_i] <- team_1_round_1\nmat\n#>      [,1] [,2] [,3]\n#> [1,]    2   NA   NA\n#> [2,]   NA   NA   NA\n#> [3,]   NA   NA   NA\n#> [4,]   NA   NA   NA\n\n\n\nFind a unique set of opponents for teams 2 through \\(n\\) to fill the rest of the cells in column 1.\n\n\n\nCode\nwhile(team_i <= league_size) {\n  if(team_i %in% teams_already_matched) {\n    team_i_round_i <- which(team_i == teams_already_matched)\n    mat[team_i, round_i] <- team_i_round_i\n    team_i <- team_i + 1\n  } else {\n    teams_cant_match <- unique(c(teams_already_indexed, teams_already_matched))\n    teams_unmatched <- setdiff(teams_possible, teams_cant_match)\n    n_matched <- length(teams_unmatched)\n    if(n_matched == 0) {\n      mat[2:league_size, round_i] <- NA\n      team_i <- 2\n    } else {\n      team_i_round_i <- if(n_matched == 1) {\n        teams_unmatched\n      } else {\n        sample(teams_unmatched, 1)\n      }\n\n      mat[team_i, round_i] <- team_i_round_i\n      team_i <- team_i + 1\n    }\n  }\n}\n#>      [,1] [,2] [,3]\n#> [1,]    2   NA   NA\n#> [2,]    1   NA   NA\n#> [3,]    4   NA   NA\n#> [4,]    3   NA   NA\n\n\n\nIdentify a unique set of opponents for team 1 for all other rounds (rounds 2 through \\(n-1\\)).\n\n\n\nCode\nteams_possible <- setdiff(idx_team, c(1, team_1_round_1))\nteam1_all_rounds <- sample(teams_possible, size = length(teams_possible))\nmat[1, 2:rounds] <- team1_all_rounds\nmat\n#>      [,1] [,2] [,3]\n#> [1,]    2    3    4\n#> [2,]    1   NA   NA\n#> [3,]    4   NA   NA\n#> [4,]    3   NA   NA\n\n\n\nRepeat step 3 for rounds 2 through \\(n-2\\) (penultimate round).\n\n\n\nCode\nwhile(round_i < rounds) {\n  team_i <- 2\n  while(team_i <= league_size) {\n    teams_possible <- setdiff(idx_team, team_i)\n    teams_already_indexed <- 1:(team_i - 1)\n    teams_already_matched <- mat[teams_already_indexed, round_i]\n    teams_already_played <- mat[team_i, 1:(round_i - 1)]\n    reset <- FALSE\n    if(team_i %in% teams_already_matched) {\n      team_i_round_i <- which(team_i == teams_already_matched)\n      if(any(team_i_round_i == teams_already_played)) {\n        reset <- TRUE\n      }\n    } else {\n      teams_cant_match <-\n        unique(c(teams_already_indexed, teams_already_matched, teams_already_played))\n      teams_unmatched <- setdiff(teams_possible, teams_cant_match)\n      n_matched <- length(teams_unmatched)\n      if (n_matched == 0) {\n        reset <- TRUE\n      } else {\n        team_i_round_i <- if(n_matched == 1) {\n          teams_unmatched\n        } else {\n          sample(teams_unmatched, 1)\n        }\n      }\n    }\n    \n    if(reset) {\n      mat[2:league_size, round_i] <- NA\n      team_i <- 2\n      retry_i <- retry_i + 1\n    } else {\n      mat[team_i, round_i] <- team_i_round_i\n      team_i <- team_i + 1\n    }\n  }\n  round_i <- round_i + 1\n}\nmat\n#>      [,1] [,2] [,3]\n#> [1,]    2    3    4\n#> [2,]    1    4   NA\n#> [3,]    4    1   NA\n#> [4,]    3    2   NA\n\n\n\nIdentify the only valid set of matchups for the last round \\(n-1\\).\n\n\n\nCode\nidx_not1 <- 2:league_size\ntotal <- Reduce(sum, idx_team) - idx_not1\nrs <- rowSums(mat[idx_not1, 1:(rounds - 1)])\nteams_last <- total - rs\nmat[idx_not1, rounds] <- teams_last\nmat\n#>      [,1] [,2] [,3]\n#> [1,]    2    3    4\n#> [2,]    1    4    3\n#> [3,]    4    1    2\n#> [4,]    3    2    1\n\n\nThat is the core of the solution. The rest of the work2 involves repeating the steps for however many times you want, always checking for duplicates of previous solutions, i.e. sampling without replacement. (Or, if you don’t care about schedules being unique, i.e. sampling with replacement, it’s even easier.)"
  },
  {
    "objectID": "posts/fantasy-football-schedule-problem/index.html#application",
    "href": "posts/fantasy-football-schedule-problem/index.html#application",
    "title": "Fantasy Football and the Classical Scheduling Problem",
    "section": "Application",
    "text": "Application\nSince generating unique schedules is something I’d like to be able to do every year for my fantasy football leagues, I wrote a package for it, called {ffsched}. The package includes functionality to retrieve your league’s fantasy scores from ESPN, which you can combine with the simulated schedules to generate a plot such as the following.\n\nIt’s immediately evident how un-lucky I (“Tony El Tigre”) was. In the 100,000 simulations, I never finished below 7th, and I only finished 7th 1.1% of the time!\nIn the previous year I scored the most points and finished first. “The Juggernaut” got the short end of the stick in 2019, finishing 7th. He only finished 7th or lower in 6.6% of schedules."
  },
  {
    "objectID": "posts/fantasy-football-schedule-problem/index.html#take-away",
    "href": "posts/fantasy-football-schedule-problem/index.html#take-away",
    "title": "Fantasy Football and the Classical Scheduling Problem",
    "section": "Take-away",
    "text": "Take-away\nAn exhaustive search as a work-around for true constraint programming isn’t always elegant and can be difficult to implement, but if you’re motivated enough to do it—as I was to prove my extreme lack of fortune—it can generate what you need to make a compelling point. My use case (for generating unique fantasy generating football schedules) is inconsequential, but such techniques are often immensely important in real world contexts."
  },
  {
    "objectID": "posts/opta-xg-model-calibration/index.html",
    "href": "posts/opta-xg-model-calibration/index.html",
    "title": "xG Model Calibration",
    "section": "",
    "text": "Recently, I pointed out what seemed to be a bug with the expected goals (xG) data shown on FBref. In particular, the difference between non-penalty goals (npG) and non-penalty xG (npxG)1 seemed to be an outlier for the 2021/22 season across the Big 5 leagues.\n\n\n“aLl xG mOdeLs ArE thE sAme”my brother in christ wut is this then pic.twitter.com/7tjp1VFkoc\n\n— Tony (@TonyElHabr) January 14, 2023\n\n\nAs it turns out FBref and their data provider, Opta, agreed! On Feb. 8, 2023, they posted an update indicating that they adjusted their 2021/22 xG such that the difference between npG and npxG is much more in line with other seasons.\nThe FBref/Opta update gave me two ideas:\n\nCompare pre- and post-update xG to identify where/how adjustments were applied. Where were the “blind spot(s)”?2\nQuantify the calibration of their xG model. Are there obvious weak points with the model?"
  },
  {
    "objectID": "posts/opta-xg-model-calibration/index.html#pre--and-post-update-xg-comparison",
    "href": "posts/opta-xg-model-calibration/index.html#pre--and-post-update-xg-comparison",
    "title": "xG Model Calibration",
    "section": "1. Pre- and post-update xG comparison",
    "text": "1. Pre- and post-update xG comparison\n\n\n\n\n\n\nFirst, let’s take a wholistic look at all of the shots for the 2021/22 seasons played in Big 5 leagues.\n\n\n\n\nOf the 44,986 shots in the data set, 30,326 (67.2%) had changes to their xG values.3 Of those that changed, 23, 584 (78.0%) were reduced, i.e. the pre-update xG value was higher. The average change was pretty minimal, just about ~0.01 xG.\n\n\nCode\nglimpse(discretized_updated_np_shots)\n#> Rows: 44,986\n#> Columns: 16\n#> $ league             <fct> ENG, ENG, ENG, ENG, ENG, ENG, ENG, ENG, ENG, ENG, E…\n#> $ date               <date> 2021-08-13, 2021-08-13, 2021-08-13, 2021-08-13, 20…\n#> $ half               <dbl> 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, …\n#> $ minute             <chr> \"11\", \"12\", \"22\", \"28\", \"30\", \"66\", \"73\", \"80\", \"2\"…\n#> $ team               <chr> \"Brentford\", \"Brentford\", \"Brentford\", \"Brentford\",…\n#> $ player             <chr> \"Frank Onyeka\", \"Bryan Mbeumo\", \"Sergi Canós\", \"Ser…\n#> $ new_xg             <dbl> 0.08, 0.09, 0.02, 0.06, 0.26, 0.06, 0.40, 0.28, 0.0…\n#> $ old_xg             <dbl> 0.09, 0.14, 0.04, 0.07, 0.31, 0.13, 0.58, 0.27, 0.0…\n#> $ is_goal            <fct> no, no, yes, no, no, no, yes, no, no, no, no, no, n…\n#> $ distance           <fct> \"(8,10]\", \"(12,14]\", \"(16,18]\", \"(20,25]\", \"(12,14]…\n#> $ sca1               <fct> pass_live, pass_live, pass_live, pass_live, take_on…\n#> $ body_part          <fct> Head, Right Foot, Right Foot, Right Foot, Right Foo…\n#> $ is_from_deflection <fct> no, no, no, no, no, no, no, no, no, no, no, no, no,…\n#> $ is_from_volley     <fct> no, no, no, no, no, no, no, no, no, yes, no, no, no…\n#> $ is_free_kick       <fct> no, no, no, no, no, no, no, no, no, no, no, no, no,…\n#> $ is_primary_foot    <fct> missing, no, yes, yes, no, yes, missing, missing, y…\n\ndiscretized_updated_np_shots |> \n  mutate(xgd = old_xg - new_xg) |> \n  pull(xgd) |> \n  mean()\n#> [1] 0.0095014\n\n\nTo get more insight into how/why xG changed, we can look at changes to xG values grouped by various features that we can derive from data that FBref publishes alongside each shot’s xG, including distance (yards), sca1 (first shot-creating action), body_part, is_from_deflection, is_from_volley, is_free_kick, and is_primary_foot.45\n\n\n\nThe table below shows that the reductions in npxG occurred most frequently for longer distances, suggesting that the pre-update xG model was over-predicting xG for longer shots. Interestingly, xG for shots when interceptions were the shot-creating action that led directly to the shot, and xG for shots with body_part = \"other\" (non-foot, non-header) were also frequently reduced, in the cases where xG was changed.\n\n\n\n\n\n\n\n\n\n\nFeature\nGroup\n# of non-penalty shots\n# of shots with changed npxG\n# of shots with lower post-update npxG of those that changed\n\n\n\n\ndistance\n(25,30]\n6,061\n3,659 (60.4%)\n3,437 (93.9%)\n\n\ndistance\n(20,25]\n6,760\n4,463 (66.0%)\n4,088 (91.6%)\n\n\nsca1\n\"interception\"\n149\n96 (64.4%)\n87 (90.6%)\n\n\ndistance\n(18,20]\n1,889\n1,232 (65.2%)\n1,088 (88.3%)\n\n\ndistance\n(30,35]\n2,725\n1,267 (46.5%)\n1,117 (88.2%)\n\n\nbody_part\n\"Other\"\n191\n153 (80.1%)\n130 (85.0%)\n\n\n\nOn the other end of the spectrum, reductions in npxG occurred least frequently for shorter distance buckets ((0,2], (2,4], (4,6], (6,8)). Reductions still occurred a majority of the time when there was a change—note that each has >50% for the last column—for all but the shortest distance group, (0,2].\n\n\n\n\n\n\n\n\n\n\nFeature\nGroup\n# of non-penalty shots\n# of shots with changed npxG\n# of shots with lower post-update npxG of those that changed\n\n\n\n\ndistance\n(0,2]\n173\n130 (75.1%)\n51 (39.2%)\n\n\ndistance\n(2,4]\n1,087\n826 (76.0%)\n428 (51.8%)\n\n\ndistance\n(4,6]\n2,003\n1,479 (73.8%)\n831 (56.2%)\n\n\ndistance\n(35,Inf]\n539\n313 (58.1%)\n177 (56.5%)\n\n\ndistance\n(6,8]\n2,557\n1,882 (73.6%)\n1,183 (62.9%)\n\n\nis_free_kick\n\"yes\"\n1,576\n882 (56.0%)\n557 (63.2%)"
  },
  {
    "objectID": "posts/opta-xg-model-calibration/index.html#xg-model-calibration",
    "href": "posts/opta-xg-model-calibration/index.html#xg-model-calibration",
    "title": "xG Model Calibration",
    "section": "2. xG Model Calibration",
    "text": "2. xG Model Calibration\nI’ve touched on model calibration before, when discussing xG-implied match outcome probabilities. There, I wrote my own code to create a calibration plot. Since then, the {tidymodels} team has added calibration plot functionality to the {probably} package. Let’s try it out.\nHere, we’ll use a big sample of data—all 2017/18 - 2021/22 non-penalty shots for the Big 5 leagues and several other first and second tier leagues.6\n\n\n\n\n\nCode\nnp_shots |> count(league, name = 'n_shots')\n#> # A tibble: 13 × 2\n#>    league    n_shots\n#>    <chr>       <int>\n#>  1 BRA_1st_M   39380\n#>  2 ENG_1st_F   11366\n#>  3 ENG_1st_M   46766\n#>  4 ENG_2nd_M   52701\n#>  5 ESP_1st_M   43398\n#>  6 FRA_1st_M   43021\n#>  7 GER_1st_M   39148\n#>  8 ITA_1st_M   49903\n#>  9 MEX_1st_M   32650\n#> 10 NED_1st_M   29803\n#> 11 POR_1st_M   27366\n#> 12 USA_1st_F    9887\n#> 13 USA_1st_M   31047\n\n\n\nCalibration plot\nWe can use probably::cal_plot_breaks() to visually assess whether the observed rate of non-penalty goals (y-axis) is close to the predicted probability of goals (npxG, x-axis).7 If the xG model’s predictions are “well calibrated”, the calibration points will align with the “ideal” line having slope 1 and intercept 0. Points at which the curve is below the diagonal line indicate where the model is more likelty to overpredict; and, likewise, points where the curve is above the diagonal line indicate where the model is underpredicting.\n\n\nCode\nlibrary(probably) ## 0.1.0.9007\n\noverall_calibration <- cal_plot_breaks(\n  np_shots,\n  truth = is_goal,\n  estimate = xg,\n  num_breaks = 20,\n  conf_level = 0.9,\n  event_level = 'second'\n)\n\n\n\n\n\n\nWe can see that the model is pretty well calibrated on the lower end of the spectrum, when xG < 0.25. This makes up a larger majority of the shots (~90%). However, the model is not as well calibrated for higher xG values, tending to overpredict. For example, at the calibration point where npxG is 0.675, the actual goal rate is 0.6.\nObserving the miscalibration for shots with xG > 0.25, one has to wonder whether the removal of the “big chance” feature in favor of other contextual features—an update that Opta made sometime in the first half of 2022—may have (unintentionally) made the model worse in some ways.8 Unfortunately, I didn’t have accessed to Opta xG data prior to that update, so I can’t evaluate this hypothesis. (For all we know, the model calibration for high xG shots might have been worse before!)\n\n\nBrier Skill Score (BSS)\nOne thing that is not provided in the {tidymodels} realm (specifically, the {yardstick} package) is a function to compute Brier score. Nonetheless, we can define a Brier score function ourselves by closely following the mean squared error custom metric example provided by the {tidymodels} team.9\n\n\nCode\nlibrary(yardstick)\nlibrary(rlang)\nbrier_score <- function(data, ...) {\n  UseMethod('brier_score')\n}\n\nbrier_score <- yardstick::new_prob_metric(brier_score, direction = 'minimize')\n\nbrier_score_vec <- function(truth, estimate, na_rm = TRUE, event_level, ...) {\n  \n  brier_score_impl <- function(truth, estimate, event_level, ...) {\n    truth <- 1 - (as.numeric(truth) - 1)\n    \n    if (event_level == 'second') {\n      truth <- 1 - truth\n    }\n    \n    mean((truth - estimate)^2)\n  }\n  \n  ## Recycle the estimate value if it's scalar-ish.\n  if (length(estimate) == 1) {\n    estimate <- rep(estimate, length(truth))\n  }\n  \n  yardstick::metric_vec_template(\n    metric_impl = brier_score_impl,\n    truth = truth,\n    estimate = estimate,\n    na_rm = na_rm,\n    cls = c('factor', 'numeric'),\n    estimator = 'binary',\n    event_level = event_level,\n    ...\n  )\n}\n\nbrier_score.data.frame <- function(data, truth, estimate, na_rm = TRUE, event_level = 'first', ...) {\n  yardstick::metric_summarizer(\n    metric_nm = 'brier_score',\n    metric_fn = brier_score_vec,\n    data = data,\n    truth = !!rlang::enquo(truth),\n    estimate = !!rlang::enquo(estimate),\n    na_rm = na_rm,\n    event_level = event_level,\n    ...\n  )\n}\n\n\nLet’s compute the Brier scores for (1) the overall goal rate (i.e. shots per goal) and (2) xG. We should expect the Brier score for the latter to be closer to 0 (perfect model), since xG should be a better predictor of goals than the naive goal rate.\n\n\nCode\nnp_goal_rate <- np_shots |> \n  count(is_goal) |> \n  mutate(prop = n / sum(n)) |> \n  filter(is_goal == 'yes') |> \n  pull(prop)\nnp_goal_rate\n#> 0.0960288\n\nnp_goal_rate_brier_score <- np_shots |> \n  brier_score(\n    truth = is_goal,\n    estimate = !!np_goal_rate,\n    event_level = 'second'\n  ) |> \n  pull(.estimate)\nnp_goal_rate_brier_score\n#> [1] 0.08680727\n\nnpxg_brier_score <- np_shots |> \n  brier_score(\n    truth = is_goal,\n    estimate = xg,\n    event_level = 'second'\n  ) |> \n  pull(.estimate)\nnpxg_brier_score\n#> [1] 0.07150071\n\n\nNow we can go on to compute Brier skill score (BSS) using an appropriate reference Brier score.10 In this context, the average goal rate seems to be a good choice for a baseline. In contrast to the Brier score, a higher BSS is ideal. (A perfect model would have a BSS of 1.)\n\n\nCode\n1 - (npxg_brier_score / np_goal_rate_brier_score)\n#> [1] 0.176328\n\n\nA BSS of ~0.18 is not bad! This is better than FiveThirtyEight’s BSS for predicting the results for men’s World Cup matches (~0.12 at time of writing) and right around their BSS for predicting WNBA playoff game outcomes (~0.18).11\n\n\nGrouped Calibration and BSS\nNow let’s take a look at model calibration under specific criteria. Is the model worse for shots that follow a dribble (take_on) shot-creating action? After a live_ball pass? etc.\n\n\n\n\nA couple of observations and thoughts:\n\nxG of shots following another shot are over-predicted so much that it causes the BSS to be negative.12 This means that the model is actually doing worse in its xG assignment than simply predicting naive goal rate for shots after another shot!\nA relatively “jagged” calibration plot may not correspond with a worse (lower) BSS score; and visa versa, a relatively “smooth” calibration plot may not correspond with a better (higher) BSS.\n\nNote that the fouled calibration looks jagged for higher predicted xG, but the fact that goals are only scored on about 5% shots immediately following a foul means that inprecise probabilities are not “penalized” quite as much. On the other hand, while the pass_live calibration looks relatively smooth, the 10% goal rate following live ball passes (2x the frequency for shots following fouls) means that it is more penalized for imprecision than an otherwise equivalent post-fouled shot. In fact, this is one of the shortcomings of BSS—it does not do a great job with evaluation of relatively infrequent events.\n\n\nNext, let’s take a look at calibration of shots coming after deflections (of other shots).\n\n\n\n\n\nThe model doesn’t seem to be very well calibrated for shots following deflections! Like shots following other shots in the shot-creating action calibration plot, the BSS for shots after deflections is negative. And, perhaps more interestingly, the model seems to underpredict post-deflection shots, which is the opposite of it’s general tendency to overpredict. (See the wholistic calibration plot from before.)\n\nI’d suspect that there’s lots of confounders that might explain the lack of calibration after deflections. For one, it could be the case that there are often zero defenders between the shot-taker and keeper for shots following a deflection. As far as I know, the Opta model doesn’t have an explicit feature for this.\nAs with the overall model calibration, one has to wonder whether the model may have performed better on shots following deflections with the binary big chance feature.\nThe high goal rate on shots after deflections relative to the goal rate on all other shots certainly contributes to the negative BSS, as we saw earlier with shots following other shots.\n\n\nMoving on, let’s look at calibration of the xG model by groups of leagues, splitting out by tier and gender.\n\n\n\n\n\nStatsBomb has talked about how a “gender-aware” model outperformed a baseline model, so one might expect the calibration of Opta’s singular model to be weaker for the women’s game. It turns out that while, yes, the calibration seems to be a bit worse for shots in the women’s leagues, the overall difference in model performance for men’s and women’s leagues seems to be trivial for the sample here.\nInterestingly, the calibration of the model for non-Big 5 leagues and the English men’s Championship league are slightly better according to BSS, although the differences (both visually, with the calibration curve, and with BSS) are very minimal.\n\nFinally, let’s look at how footedness may play a role in model calibration. As far as I know, whether or not a footed shot is take by a player’s primary foot is not an input into the Opta model, so it may be particularly interesting to look at.\n\n\n\n\n\nDespite my suspicion that xG for shots taken by a player’s weaker foot (right foot shot, left-footed and left foot shot, right-footed) might be severely overpredicted, this doesn’t really seem to be the case. Yes, the model tends to overpredict for these kinds of shots, but the degree to which overprediction occurs doesn’t seem out of line with the whole model.\n\nI can think of least two types of “selection bias” at play here that might explain why the calibration isn’t as bad as I might have guessed for weak-footed shots:\n\nPlayers are more likely to take weak-footed shots when they’re closer to the goal, where shots are likely to have higher xG, but also where shots are more likely to go in.\nPlayers are less likely to take more difficult shots with their weak foot, so they’re not taking as many shots that are unlikely to go in, holding all else equal.\n\n\nOf the non-footed shots, it’s interesting to see that the BSS for headers and shots from other body parts are not particularly well calibrated. In fact, the latter has a negative BSS, indicating that we’d better off with a model that predicted the average goal rate for such shots.\n\nA high goal rate on such shots compared to other types of shots seems to, once again, be the reason that BSS looks particularly bad here."
  },
  {
    "objectID": "posts/opta-xg-model-calibration/index.html#conclusion",
    "href": "posts/opta-xg-model-calibration/index.html#conclusion",
    "title": "xG Model Calibration",
    "section": "Conclusion",
    "text": "Conclusion\nWe’ve explored the wonderful world of model calibration, making friends with BSS and calibration curves in our investigation of a public xG model. Are BSS and calibration curves the be-all and end-all when it comes to model evaluation? Of course not! But they’re useful tools that may or may not be appropriate for your use case.\nWhen it comes to the Opta xG model specifically, am I implying that the model is bad? Of course not (again)! Yes, faceted calibration curves and feature-specific BSS can make a model look bad, but we must keep in mind that there are trade-offs to be made with modeling. Fine-tuning a model to be more well calibrated under certain conditions, e.g. shots after deflections, may make other parts of the model worse! It’s all about trade-offs."
  },
  {
    "objectID": "posts/soccer-league-strength/index.html",
    "href": "posts/soccer-league-strength/index.html",
    "title": "Quantifying Relative Soccer League Strength",
    "section": "",
    "text": "Arguing about domestic league strength is something that soccer fans seems to never tire of. (“Could Messi do it on a cold rainy night in Stoke?”) Many of these conversations are anecdotal, leading to “hot takes” that are unfalsifiable. While we’ll probably never move away from these kinds of discussions, we can at least try to inform them with a quantitative approach.\nPerhaps the obvious way to do so is to take match results from international tournaments (e.g. Champions League, Europa). But such an approach can be flawed—there’s not a large sample, and match results may not be reflective of “true” team strength (e.g. one team may win on xG by a large margin, but lose the game.)"
  },
  {
    "objectID": "posts/soccer-league-strength/index.html#methodology",
    "href": "posts/soccer-league-strength/index.html#methodology",
    "title": "Quantifying Relative Soccer League Strength",
    "section": "Methodology",
    "text": "Methodology\nBut what if we used an approach rooted in player performance? I asked myself that very question and came up with the following approach. (Thanks to Cahnzhi Ye for the data.)\n\nIdentify players who played in more than one league within the same season or across consecutive seasons. Calculate the difference in each player’s atomic VAEP1 per 90 minutes (VAEP/90) after changing leagues.\n\nWhy VAEP? Theoretically it should capture more about in-game actions (including defense) than other stats such as xG, which is biased in favor of attacking players. VAEP is not perfect by any means (e.g. it does not capture off-ball actions), but, in theory, it should be a better measure of overall performance. 2\nNotably, we give up a little in interpretability in using VAEP, since it’s not directly translatable to goals. 3 The following table of top season-long xG totals since 2012 to contextualize the magnitudes of xG and VAEP.\nAnd a scatter plot, because who doesn’t love a graph.\n\n\nConvert the player-level VAEP/90 differences to z-scores by position and age group.\n\nWhy grouping? This is intended to account for the fact that attacking players and “peaking” players (usually age 24-30) tend to have higher VAEP/90, so their league-to-league differences have larger variation. The choice to normalize is perhaps more questionable. The mean of differences is ~0 for all groups already, but the dispersion is smaller without normalization (i.e. standard deviations are closer to 0). So, in this case, normalization should help the linear model capture variation.\n\nRun a single regression where the response variable is the z-transformed VAEP/90 difference, and the features are indicators for leagues, where -1 indicates player departure, a +1 indicates player arrival, and all other values are 0.4 5\n\nFor those familiar with basketball and hockey, this is similar to the set-up for an adjusted plus-minus (APM) calculation. Here, each feature column is a league (instead of a player), each row represents a player (instead of a “stint”), and the response is transformed VAEP/90 (instead of net points per possession).\nThe result is a set of coefficient estimates corresponding to each league. Notably, these are all positive (even if subtracting the intercept), and the Netherlands coefficient is NA due to multi-collinearity in the data. 6\nFor hockey/basketball APM, we would say the coefficient estimate represents how much a player contributes relative to an “average” player. We might be tempted to try to interpret these coefficients directly as well. Yes, we can infer the league “power rankings” from just this singular coefficient list (Premier League as the strongest and Bundesliga 2 as the weakest), but there are some issues.\n\nWe first need to “un-transform” this back to the VAEP/90 scale. (See next step.)\nNote that this is not a zero-sum situation (even after un-transforming). There is no notion of a matchup between one league and another like there is in hockey/basketball with players on the ice/court. Instead, our data is more analogous to a player playing against themselves (not a set of players versus another set of players).\nEven if this were a zero-sum type of problem and the model returned some negative coefficient estimates, it’s unclear what the intercept (or 0) even means. Does it mean “average”? If so, what is an “average” league?\nWe accounted for minutes played—the “per 90” denominator—prior to subtracting rates (difference in VAEP/90), which is different than how APM works. In APM, the minutes played is directly accounted for in the response variable (net points, divided by possessions).\n\nThe take-away here is that we can only interpret the model coefficients on a relative basis.\n\n“Un-transform” the coefficients of the regression using a “weighted-average” standard deviation and mean from the z-transformations of groups. 7\n\nInterpretation after this transformation can be a little tricky. The differences between a specified pair of these post-transformed coefficients represents the expected change in an “average” player’s VAEP/90 (Diff. (VAEP/90)) when moving between the specified leagues.\nTo interpret these differences as a percentage (so that we can “scale” the properly for a player with a VAEP/90 of 1.5, compared to a player with a lower VAEP/90 rate), we use the median VAEP/90 across all leagues as a “baseline”. For example, for Bundesliga -> Premier League, since the overall median VAEP/90 is 0.305 and the Diff. (VAEP/90) between league A and league B is 0.0509, the % Difference is 0.0509/0.305 = 17%."
  },
  {
    "objectID": "posts/soccer-league-strength/index.html#improvements-further-work",
    "href": "posts/soccer-league-strength/index.html#improvements-further-work",
    "title": "Quantifying Relative Soccer League Strength",
    "section": "Improvements & Further Work",
    "text": "Improvements & Further Work\n\nAlthough my approach does eventually get back to the original units (VAEP/90), it does feel a little convoluted. Aditya Kothari proposed re-defining the target variable in the regression to be the ratio of VAEP/minute (instead of a z-transformed difference in VAEP/90) between the leagues that a player moves to and from. (See his full post.) In my eyes, the main advantage of such an approach is that it is more direct. A player-level ratio embeds information about position and age—a forward will tend to have higher VAEP/minute than a defender, and will continue to have higher VAEP/minute than a defender after transferring—so normalizing for age and position is not necessarily justified. Additionally, the model’s league coefficients can be directly interpreted, unlike my approach. Perhaps the main disadvantage is sensitivity to low minutes played. 8\nAnother weakness in my approach is the assumption that relative league strengths are the same every year, which is most certainly not true. One could apply a decaying weight to past seasons to account for varying league strength.\nI would be hesitant to use my results to directly infer how a specific player will translate going from one league to another. My approach focuses on leagues and is more about the “average” player. One aught to include additional features about play style (e.g. touches, progressive passes, team role) if interested in predicting individual player performance with a high degree of accuracy.\nOne can swap out the response variable with other reasonable metrics of player performance, such as xG (which is more readily available than atomic VAEP). In fact, I did this myself and came up with the result below (showing in units of xG/90 instead of as a percentage, since most fans are accustomed to seeing xG and are used to its relative magnitude).\n\n\n\nOne could stay in the realm of just purely “power rankings” and focus more on the estimates and error. For example, in an earlier iteration of this methodology, I used a Bradley-Terry approach to come up with a distribution of estimates for each league.9 Here, the x-axis could be loosely interpreted as the log odds of one league winning in a match versus another league, although it’s not clear exactly what that means. (An “average” team from both leagues? A matchup of teams composed of “average” players from any team in each league?)\n\n\n\nNotably, I’m not using match results at all! Certainly a model could learn something from international and tournament matches. However, using match-level data would require a whole new approach. Also, most would agree that tournament data can be biased by atypical lineups. For example, a manager on one side may opt to rest their best players, saving them for domestic league games, while the other manager may play their side at full strength.10\n\n\n\nSample size is an issue on two levels: (1) the number of transfers (more data would be better) and (2) minutes played.\n\nRegarding (1), one could expand the data set by including all seasons played by a player that has played in more than one league, taking all combinations of seasons in different leagues (i.e. relaxing the the same-season or subsequent-season criteria). I actually did attempt this and found that overall the results were somewhat similar, but there were more questionable results overall. (Brazil’s Serie A was found to be the second strongest league overall with this approach.).\nRegarding (2), one has to make a choice to drop players with low minutes played to prevent outliers affecting the results of the model. However, in some cases, a loaned player coming in at the end of the season and making a huge impact can tell us a lot about the difference in strength of two leagues, so we may not want to drop some of the records after all. An empirical bayes adjustment to VAEP/90, not unlike the one described here by David Robinson, can help overcome this. Below shows how such an adjustment slightly “shrinks” VAEP/90 numbers, especially for those who played less.\n\nOn the topic of “shrinking”, we could have used ridge regression (regression with some penalty) to get more robust league estimates overall. However, there is a downside to ridge regression—we give up some level of interpretability.11 Nonetheless, the relative ranking of leagues would be more reliable with ridge regression."
  },
  {
    "objectID": "posts/soccer-league-strength/index.html#ancillary-take-away",
    "href": "posts/soccer-league-strength/index.html#ancillary-take-away",
    "title": "Quantifying Relative Soccer League Strength",
    "section": "Ancillary Take-away",
    "text": "Ancillary Take-away\nOne final thing I’d like to point out here: I think this whole approach really showcases the inference made possible by player stats (xG, possession value metrics like atomic VAEP, etc.) aggregated over long periods of time. While such stats are often used to evaluate player performance in single games or even for singular in-game actions, they are most effective in providing insight when employed in higher-level analyses."
  },
  {
    "objectID": "posts/soccer-pass-network-max-cut/index.html",
    "href": "posts/soccer-pass-network-max-cut/index.html",
    "title": "Yet Another (Advanced?) Soccer Statistic",
    "section": "",
    "text": "Pass networks are a common visualization form used to summarize a team’s behavior in a soccer match. Nodes represent average player position on passes that they are involved with, and edges represent passes between players. Most pass networks also weight node size and edge width by the total number of passes.\n\nWhile pass networks provide a nice visual tool for providing insight that can (and should) be supplemented by more detailed analysis, they’re often just that—purely a visual tool. In order to gain meaning beyond just anecdotal insight (“Look at how far the wingbacks were up the field!”), practitioners may leverage graph theory concepts such as centrality to quantify relationships.1\nInspired by the findings of Eliakim et al. in “The development of metrics for measuring the level of symmetry in team formation and ball movement flow, and their association with performance”, I wanted to evaluate a graph theory concept that has not been explored in relation to soccer pass networks (except for by Eliakim et al.): maximum cuts.\nTo do so, I’ll be using data from the 2017/18 - 2020/21 Premier League seasons, along with the games up through Boxing Day of the 2021/22 season. Passes and other events are only considered up through the first substitution or red card of each match."
  },
  {
    "objectID": "posts/soccer-pass-network-max-cut/index.html#examples",
    "href": "posts/soccer-pass-network-max-cut/index.html#examples",
    "title": "Yet Another (Advanced?) Soccer Statistic",
    "section": "Examples",
    "text": "Examples\n\nSimple\nWhat is a maximum cut? Visually, it’s an arbitrary line that you can draw through the edges of a network that maximizes the sum of the edge weights.\n\nFor this example, 15 is actually a weighted max cut, since edges are treated differently, according to their assigned value. (An unweighted max cut would assign each edge a value of 1.)\nOn the other side of things, the min cut would be 2+4=6 for this example.\n\n\nIn Soccer\nA 4-node, 5-edge network is nice for illustration, but how does this bear out in soccer?\nTo give a soccer example, Here’s the pass network and weighted max cut numbers for the match between Liverpool and Manchester City on October 3, 2021. 2\n\nZooming out from a single example to all games in our data set, the distribution of weighted max cuts per 90 minutes looks relatively normal, perhaps log-normal. Note: It’s important to adjust for time since not all games have the same number of minutes played due to variance in the occurrence of the first formation change."
  },
  {
    "objectID": "posts/soccer-pass-network-max-cut/index.html#but-is-max-cut-useful",
    "href": "posts/soccer-pass-network-max-cut/index.html#but-is-max-cut-useful",
    "title": "Yet Another (Advanced?) Soccer Statistic",
    "section": "But is Max Cut Useful?",
    "text": "But is Max Cut Useful?\n\nSetup\nTo quantify the impact of weighted max cuts, we’ll look at two measures of quality of play.\n\nexpected goals (xG): xG tells the story of shot quality and quantity, which is massively important in a low-scoring game like soccer.\nexpected threat (xT): xT quantifies scoring opportunities more generally, looking beyond shots.\n\nI’d argue that xT is more informative for our purposes since max cut is related to passes and xT accounts for passes; xG is so tied up in shots that their relationship to passes leading up to those shots may be lost. Nonetheless, we’ll be considering both since both are commonly used for judging overall “value” in soccer.\nWe’ll be transforming these xG and xT in two manners.\n\nVolume-adjusting, i.e. taking each value per 90 minutes. The justification for adjusting max cut for time also applies here.\nOpponent-adjusting, or “differencing”, i.e. subtracting one side’s value from the other’s value. Sure, having a lot of touches in the opponent’s half and taking a lot of shots means scoring is more likely, but if you’re also giving up a ton of shots, then that effort is essentially negated.\n\nGiven that we’ll be making our two quality-of-play stats—xG and xT—relative to the opponent, I prefer to opponent-adjust weighted max cut in the same manner. I’d argue that weighted max cut differential is more informative than just the weighted max cut of one side or the other. Suppressing your opponent’s weighted max cut is reflective of limiting their pass volume, and, consequently, makes it more likely that your weighted max cut is higher.\nThe relationship between weighted max cut and weighted max cut differential is very linear, so, ultimately, it shouldn’t matter too much if we look at opponent-adjust weighted max cut versus just raw weighted max cut.\n\nDifferencing has the added benefit of making our distributions look more “normal”, by making them symmetric about 0. This generally is beneficial for regression analysis, which we go on to conduct.\n\n\nCorrelations\nA first step in looking at the relationship between weighted max cut with xG and xT is a correlation.\n\nWeighted max cut compares favorably to other network stats for summarizing (pass) networks. Perhaps this isn’t too surprising; Eliakim et al. argue that, in relation to soccer, maximum cut surmises what is captured separately by various measures of centrality (betweenness, indegree and outdegree, etc.).\nWeighted max cut has a similar correlation to traditional pass metrics such as relative percentage of passes, but not as strong as counting stats for shots. We really shouldn’t expect any metric to have as strong a relation with xG and xT (especially xG) as shot-based metrics since these are derived from shots and their outcomes.\nOverall, the game-level correlations are not super strong, indicating that we can’t read too much into them for individual games. The correlations are much stronger at the season-level, showing the same ordinality in magnitude of correlation.\n\nObserving the difference in the game-level and season-level correlations should be a good reminder that single-game stats should not be scrutinized too heavily when evaluating a team’s performance over the course of a season. The same is true for max cuts!\n\n\nAccounting for Confounders\nThe correlation approach for quantifying the descriptive role of max cuts in quality of play is a bit naive. Single-variable regressions, i.e. correlations, overstate the impact of the “treatment” variables.\nIf we regress max cut on xG and xT with z-score normalized counts of shots and passes as confounders, we see that the influence of max cut is negated.\n\nIn the case of xG, the coefficient estimate for weighted max cut is offset by the coefficient for the passing term. This is due to their collinearity (over 90%) and their lack of explanatory value in the presence of shot counts, which directly informs xG. For xT, the weighted max cut coefficient is completely suppressed, likely due to collinearity with passing.\nOf course, we could be a little more sophisticated here, drawing out directed acyclic graphs (DAG) and running a more formal causal analysis. But my intuition is that we would come to the same general conclusion: in the face of more traditional metrics like shot and pass counts, possibly the most robust pass-network-derived statistic—weighted max cut—provides minimal additional descriptive power for quantifying quality of play.\n\n\nWhy Haven’t People Tried this Before?\nI can think of a couple of reasons why weighted max cut isn’t commonly seen in soccer literature:\n\nThe calculation requires an un-directed network. In our context, this requires treating passes between players as equal, regardless of who is the passer and who is the receiver. This can distort the role of a striker, who may receive much more than pass, or a keeper, who may pass much more than receive.\nIt’s difficult to visualize beyond just reporting a single number, and, thus, may not resonate with an audience.\nIt’s not super easy to calculate! In fact, {igraph}—the most popular R framework for network analysis—doesn’t have a function for it!\n\nFor what it’s worth, the code that I wrote to calculate the weighted maximum cut for a pass network looks something like this.3 (This is the calculation for the 4-node example network shown before.)\n\n\nCode\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(sdpt3r) ## Semi-Definite Quadratic Linear Programming Solver\n\ndf <- tibble(\n  from = c('a', 'a', 'a', 'b', 'b', 'b', 'c', 'c', 'c', 'd', 'd', 'd'),\n  to   = c('b', 'c', 'd', 'a', 'c', 'd', 'a', 'b', 'd', 'a', 'b', 'c'),\n  n    = c( 1L,  0L,  3L,  1L,  1L,  1L,  0L,  2L,  1L,  1L,  5L,  4L)\n)\n\nwide_df <- df %>% \n  pivot_wider(\n    names_from = to,\n    values_from = n,\n    values_fill = 0L\n  ) %>% \n  select(from, a, b, c, d) %>% \n  arrange(from) %>% \n  select(-from)\nwide_df\n#> # A tibble: 4 x 4\n#>       a     b     c     d\n#>   <int> <int> <int> <int>\n#> 1     0     1     0     3\n#> 2     1     0     1     1\n#> 3     0     2     0     1\n#> 4     1     5     4     0\n\nm <- as.matrix(wide_df)\nsymmetric_m <- m + t(m) ## must be symmetric\nmc <- maxcut(symmetric_m)\nmax_cut <- -round(mc$pobj, 0)\nmax_cut\n#> [1] 15\n\n\nOr, perhaps, people have actually evaluated max cut for pass networks, but have found the same non-significant result that I have found, and have simply opted not to write about it. 🤷"
  },
  {
    "objectID": "posts/soccer-pass-network-max-cut/index.html#conclusion",
    "href": "posts/soccer-pass-network-max-cut/index.html#conclusion",
    "title": "Yet Another (Advanced?) Soccer Statistic",
    "section": "Conclusion",
    "text": "Conclusion\nWeighted max cut can be a very informative metric for summarizing pass volume and pass network structure, as seen in a correlation analysis. It’s merit surpasses that of other summary network stats and is nearly equivalent to traditional pass-derived stats for explaining xG and xT. However, I don’t think it should supersede more traditional stats like shots and passes if purely evaluating attacking quality."
  },
  {
    "objectID": "posts/soccer-pitch-control-r/index.html",
    "href": "posts/soccer-pitch-control-r/index.html",
    "title": "Creating a Soccer Pitch Control Model",
    "section": "",
    "text": "There’s never been a better time to be involved in sports analytics. There is a wealth of open-sourced data and code (not to mention well-researched and public analysis) to digest and use. Both people working for teams and people just doing at as a hobby are publishing new and interesting analyses every day.\nIn particular, the FriendsOfTracking (FOT) group, co-led by Professor and author David Sumpter1 have put together an awesome series of videos on YouTube discussing modern soccer analytics, along with a collection of repositories on GitHub sharing the code shown in videos.\nLaurie Shaw has shared code that implements the pitch control model described in William Spearman’s paper “Beyond Expected Goals” is interesting to me. The model is different than the one that I used to create some animations on Twitter. Those were based on the pitch control model described by Javier Fernandez and Luke Bornn in their paper “Wide Open Spaces” (code courtesy of Rob Hickman). (Apologies for the onslaught of links!)\nNow, I am not one for language wars—and, in fact, I use python often—but I thought it would be awesome to be able to plot Spearman’s pitch control model directly with {ggplot2} and friends. Thus, I set out to convert Laurie’s code to R, attempting to give it a “native” R feel while I was at it.\nMost of the process of translating python to R was relatively straightforward (slicing and dicing data frames and arrays/vectors is just part of data cleaning), so I won’t detail them here. However, there was one part that was particularly interesting—the conversion of a python class object. This was actually the key (and most challenging part) of the conversion process.\nThere are some great resources for describing how to implement object-orientated programming (OOP) in R, including a couple of chapter’s from Hadley Wickham’s Advanced R book and a very practical write-up from Earo Wang. Every object-oriented task has its unique aspects, so hopefully my discussion here has something to add to what has already been written on the subject matter.\nFor demonstration purposes, I’m going to walk through my steps for converting the python class object as if I were doing it for the first time."
  },
  {
    "objectID": "posts/soccer-pitch-control-r/index.html#constructor",
    "href": "posts/soccer-pitch-control-r/index.html#constructor",
    "title": "Creating a Soccer Pitch Control Model",
    "section": "Constructor",
    "text": "Constructor\nBelow is a stripped down version of Laurie’s code, showing the “essence” of what we need to replicate.2\n\n\nCode\nclass player(object):\n    def __init__(self,player_id,frame):\n        self.id = player_id\n        self.get_position(frame)\n        self.get_velocity(frame)\n        \n    def get_position(self,frame):\n        self.position = np.array(frame[self.player_id + 'x', self.player_id + 'y'])\n        \n    def get_velocity(self,frame):\n        self.velocity = np.array(frame[self.player_id + 'x_v', self.player_id + 'y_v'])\n    \n    def tti(self,final_position):\n        reaction_time = 0.7 # in s\n        vmax = 5 # in m/s\n        reaction_position = self.position + self.velocity * reaction_time\n        self.tti = reaction_time + np.linalg.norm(final_positon - reaction_position)/vmax\n\n    def p_intercept(self,t):\n        tti_sigma = 0.45\n        den = 1 + np.exp(-np.pi/np.sqrt(3.0)/tti_sigma * (t-self.tti)))\n        return 1 / den\n\n\nLet’s make some notes and come back to these as we develop our R class.\n\nWe need a unique identifier: player_id. This is just a “best practice” thing for object-oriented programming and makes sense given our context. For a sport like soccer, a unique identifier could just be the player’s name, a combination of the team name and the player jersey number, a league unique identifier, etc.\nA single-row data frame frame is passed to several of the methods, including the constructor __init__. This single row data frame is sourced from a much larger tracking data frame, with rows for every 0.04 second time interval (25 frames per second, or one frame per 0.04 seconds) in the game.\nThe python code stores both the player’s position and velocity as 2x1 arrays. This works well with the unpacking that is done in other places in Laurie’s code.\ntti, short for “time to intercept (a target location)”, uses the player’s position and velocity to define the attribute tti (not to be confused with the method itself). This implies that position and velocity should be defined before tti() is ever called, as they are in __init__. tti needs the position_final 2x1 array to calculate tti which is not known upon instantiation; rather, tti can only be properly defined when called to do a specific calculation relating the player’s position and velocity (both defined implicitly in the class, without needing user-specification) with a user-supplied position_final pair of x and y values.\np_intercept, short for “probability to intercept (a target location)” depends on tti and an additional parameter t, a user-specified value representing how much time is allotted to reach the ball. Like tti, p_intercept is only “properly” defined when actually doing a calculation on the player’s attributes. Unlike tti, there is no attribute in the player instance that stores this probability; it’s value must be saved in a variable external to the player class if the user wants to use it for something other than an ephemeral calculation.3\n\nTime to intercept a “target” location (tti) may not be intuitive to comprehend immediately. The plot4 below annotates the tti of a “target” location on the pitch (which does not have to be where the ball actually is). tti assumes that the player continues moving at their current speed (annotated by the arrows) for reaction_time seconds before running at vmax (full speed) to the target position. tti for each player is independent of the tti of all other players, which is a relatively reasonable assumption. 5\n\nThe probability of reaching the “target” location (p_intercept) is directly related to the player’s tti. Uncertainty about how long it will take the player to reach the target location is quantified by the constant tti_sigma in the calculation. (tti is the mean and tti_sigma is the standard deviation of the distribution for a player’s time to arrive at the target location.)\n\nNotably, this probability is independent of all other players’ probabilities (which explains how it is possible that both players are shown to have probabilities greater than 50% when t = 6 above). When adjusting for all players’ probabilities (by dividing by the sum of all probabilities), the numbers change. This probability adjustment is key when we calculate pitch control.\n\nOk, on to the R code. We’ll be using S3 and the {vctrs} package to help create our player class. (As with the python class, I’ve simplified the actual implementation for demonstration purposes.)\nFirst, we start with the constructor new_player(). Note that there is no direct __init__ equivalent in R. Here we will make a function that is prefixed with new_ and ends with the name of our class (player).\n\n\nCode\nnew_player <- function(\n  player_id = integer(),\n  x = double(),\n  y = double(),\n  x_v = double(),\n  y_v = double()\n) {\n  vctrs::new_rcrd(\n    list(\n      player_id = player_id,\n      x = x,\n      y = y,\n      x_v = x_v,\n      y_v = y_v,\n      tti = -1 # dummy value\n    ),\n    class = 'player'\n  )\n}\n\n\nNow let’s reflect upon our prior notes.\n\nWe have the player_id in this constructor.\nWe don’t pass the data frame tracking here. We’ll do it in our helper function. We might say that our constructor is “low-level”, not intended for the user to call directly.\nWe split the position and velocity vectors into their individual x and y components, resulting in four total variables instead of two. I don’t think a vector (unnamed or named), list, or matrix are particularly compelling data types to use for an x-y pair of values in R. None natively support unpacking (although R vectors do have some form of “broadcasting” with their recycling behavior).\nWe assign a “dummy” value (-1) to tti when initializing the class instance. We will have a method to update tti based on x and y components.\nLike tti, we will need a separate p_intercept method to be used to calculate the probabililty of intercepting a ball given a player’s position, speed, and the final position of the ball (all fed as inputs to tti), as well as the additional user-specified t, representing how much time is allotted to reach the ball."
  },
  {
    "objectID": "posts/soccer-pitch-control-r/index.html#validator",
    "href": "posts/soccer-pitch-control-r/index.html#validator",
    "title": "Creating a Soccer Pitch Control Model",
    "section": "Validator",
    "text": "Validator\nLet’s proceed by creating a validator function to, you guessed it, validate fields in the player class. It is good practice to check the values used to construct the class. The python code did not have any validation like this, but I don’t think it was ever expected to be extremely robust to any user input.\n\n\nCode\nvalidate_player <- function(player) {\n  vctrs::vec_assert(vctrs::field(player, 'player_id'), integer())\n  vctrs::vec_assert(vctrs::field(player, 'x'), double())\n  vctrs::vec_assert(vctrs::field(player, 'y'), double())\n  vctrs::vec_assert(vctrs::field(player, 'tti'), double())\n  player\n}\n\n\nNote that we could have simply done this validation in the constructor function, but I think it makes sense to put the validation in its own function so that the constructor is more direct (especially if the validation checks are complex)."
  },
  {
    "objectID": "posts/soccer-pitch-control-r/index.html#helper",
    "href": "posts/soccer-pitch-control-r/index.html#helper",
    "title": "Creating a Soccer Pitch Control Model",
    "section": "Helper",
    "text": "Helper\nFinally, we’ll create a helper player() function, which is our “user-facing” function that we expect/want users to use to instantiate objects.\n\n\nCode\nplayer <- function(player_id, frame, tracking) {\n    \n    player_id <- as.integer(player_id)\n    frame <- as.integer(frame)\n\n    assertthat::assert_that(is.data.frame(tracking))\n    nms_req <- c('player_id', 'frame', 'x', 'y', 'x_v', 'y_v')\n    assertthat::assert_that(all(nms_req %in% names(tracking)))\n    \n    # `!!` to make sure that we filter using the integer values, not the column itself.\n    tracking_filt <- tracking %>% filter(player_id == !!player_id, frame == !!frame)\n    assertthat::assert_that(nrow(tracking_filt) == 1L)\n    \n    player <-\n      new_player(\n        player_id = player_id,\n        x = tracking_filt[['x']],\n        y = tracking_filt[['y']],\n        x_v = tracking_filt[['x_v']],\n        y_v = tracking_filt[['y_v']]\n      )\n    validate_player(player)\n  }\n\n\nNote the following:\n\nWe coerce player_id and frame to integers instead of doubles (particularly since they are expected to be integers in the constructor). This ensures that the new player is instantiated properly by the constructor and passes our validation.\nWe pass in our entire tracking data frame (that has rows for every 0.04 second interval in the game), as well as the frame to slice out of it. (player_id is also used to filter tracking.) This makes it convenient for user to instantiate new player objects when operating on the tracking data frame. There is no need to extract the singular initial position and velocity components “manually”; instead, the helper function does it for the user."
  },
  {
    "objectID": "posts/soccer-pitch-control-r/index.html#aside",
    "href": "posts/soccer-pitch-control-r/index.html#aside",
    "title": "Creating a Soccer Pitch Control Model",
    "section": "Aside",
    "text": "Aside\nR’s S3 framework is not a formal OOP framework (not even close really). Note that it does not have a reserved keyword to represent the instance of the class like self in python. Also, it is not actually necessary for most of what is done above (with the constructor, validator, and helper).\nFor example, we don’t actually have to create a formal-ish constructor prefixed with new_. We don’t even need a constructor function at all in S3. We could do something like class(var) <- 'player' to create a a player object. Of course, this is prone to errors down the line, so we don’t do that. Likewise with the validator and helper functions. The point of these constructs is to add clarity to our class code. They aren’t strictly necessary."
  },
  {
    "objectID": "posts/soccer-pitch-control-r/index.html#printing",
    "href": "posts/soccer-pitch-control-r/index.html#printing",
    "title": "Creating a Soccer Pitch Control Model",
    "section": "Printing",
    "text": "Printing\nLet’s do one more thing for our player class—create a custom print method. (Writing a custom print method is not required whatsoever, but it can be very helpful for debugging.) If we weren’t using {vctrs} and just S3, we would do this by writing a print.player function. However, {vctrs} provides a “pretty” header for us auto-magically (that looks like <player[1]>) if we use it to write our print method.\nTo take advantage of the pretty-printing functionality offered by {vctrs}, we write a format.player() method that will be called by a subclass of the generic vctrs::obj_print_data method6, which itself is called whenever we print out an object (whether explicitly with print or just by typing the name of the variable representing our player instance). We’ll add the player’s position and velocity components to the print out.\n\n\nCode\nformat.player <- function(player, ...) {\n  if(vctrs::field(player, 'in_frame')) {\n    suffix <- \n      sprintf(\n        'with `position = (%.2f, %.2f)` and `velocity = <%.1f, %.1f>`', \n        vctrs::field(player, 'player_id'), \n        vctrs::field(player, 'y'), \n        vctrs::field(player, 'x_v'),\n        vctrs::field(player, 'y_v')\n      )\n  } else {\n    suffix <- 'is not on the pitch'\n  }\n  prefix <- sprintf('`player_id = %s` ', vctrs::field(player, 'player_id'))\n  msg <- sprintf('%s%s', prefix, suffix)\n  paste(msg, sep = '\\n')\n}\n\nobj_print_data.player <- function(player) {\n  cat(format(player), sep = '\\n')\n}"
  },
  {
    "objectID": "posts/soccer-pitch-control-r/index.html#basic-usage",
    "href": "posts/soccer-pitch-control-r/index.html#basic-usage",
    "title": "Creating a Soccer Pitch Control Model",
    "section": "Basic Usage",
    "text": "Basic Usage\nOk, so that is all fine and dandy, but how would we go about instantiating players in a normal workflow?\nLet’s say that we want to calculate the pitch control for a single frame in the tracking data (called tracking_start below).7\n\n\nCode\ntracking_start\n#> # A tibble: 26 x 9\n#>    frame ball_x ball_y side  player_id     x     y   x_v    y_v\n#>    <int>  <dbl>  <dbl> <chr>     <int> <dbl> <dbl> <dbl>  <dbl>\n#>  1 53027  93.71  24.56 home          1 90.72 39.37 5.906 -3.985\n#>  2 53027  93.71  24.56 home          2 95.10 27.14 1.5   -2.023\n#>  3 53027  93.71  24.56 home          3 96.01 23.32 1.418  2.395\n#>  4 53027  93.71  24.56 home          4 92.39 15.64 1.005  3.473\n#>  5 53027  93.71  24.56 home          5 83.96 24.69 4.238  1.2  \n#>  6 53027  93.71  24.56 home          6 82.19 35.63 3.893 -0.619\n#>  7 53027  93.71  24.56 home          7 85.79 17.34 1.703  1.523\n#>  8 53027  93.71  24.56 home          8 76.06 50.16 2.018 -0.493\n#>  9 53027  93.71  24.56 home          9 61.22 25.35 0.863 -0.77 \n#> 10 53027  93.71  24.56 home         10 59.69 35.10 0.9   -0.573\n#> # ... with 16 more rows\n\n\nLet’s convert players with id’s 10 through 12 (on the home team) to player instances and see how they look when printed out.\n\n\nCode\n10L:12L %>% map(~player(player_id = .x, frame = 53027L, tracking = tracking_start))\n#> [[1]]\n#> <player[1]>\n#> `player_id = 10` with `position = (10.00, 35.09)` and `velocity = <0.9, -0.6>`\n#> \n#> [[2]]\n#> <player[1]>\n#> `player_id = 11` with `position = (11.00, 32.28)` and `velocity = <-0.3, 0.6>`\n#> \n#> [[3]]\n#> <player[1]>\n#> `player_id = 12` is not on the pitch"
  },
  {
    "objectID": "posts/soccer-pitch-control-r/index.html#pseudo-encapsulation",
    "href": "posts/soccer-pitch-control-r/index.html#pseudo-encapsulation",
    "title": "Creating a Soccer Pitch Control Model",
    "section": "Pseudo-Encapsulation",
    "text": "Pseudo-Encapsulation\nWe still need to implement analogues for the tti and p_intercept methods in the python player class. Starting with tti, let’s use some pseudo-encapsulation (with getters and setters) for a player’s tti value.\n\n\nCode\n# Frobenious norm\neuclidean_norm <- function(x1, x2, y1, y2) {\n  m <- matrix(c(x1, y1)) - matrix(c(x2, y2))\n  sqrt(sum(m^2))\n}\n\n.get_tti.player <- function(player, x2, y2) {\n  ri <- 0.7 # in s\n  vmax <- 5 # in m/s\n  x1 <- vctrs::field(player, 'x') + vctrs::field(player, 'x_v') * ri\n  y1 <- vctrs::field(player, 'y') + vctrs::field(player, 'y_v') * ri\n  ri + euclidean_norm(x1, x2, y1, y2) / vmax\n}\n\n.msg_cls_err <- function(player, f) {\n  cls <- class(player)[1]\n  sprintf('`%s()` doesn\\'t know how to handle class `%s`!', f, cls) \n}\n\n.get_tti.default <- function(player, ...) {\n  stop(.msg_cls_err(player, '.get_tti'), call. = FALSE)\n}\n\n.get_tti <- function(player, ...) {\n  UseMethod('.get_tti')\n}\n\n`.set_tti<-.player` <- function(player, value) {\n  vctrs::field(player, 'tti') <- value\n  player\n}\n\n`.set_tti<-.default` <- function(player, ...) {\n  stop(.msg_cls_err(player, '.set_tti'), call. = FALSE)\n}\n\n`.set_tti<-` <- function(player, ...) {\n  UseMethod('.set_tti<-')\n}\n\n\nThere’s a couple of things going on here:\n\nThe .get_tti and .set_tti functions that call UseMethod are true S3 generics that perform method dispatch, i.e. find the correct method for the object passed to the generic (based on the class of the object). The .get_tti.player and .set_tti.player with the .player “suffix” so that they only work in their defined manners when passed in a player instance. (They won’t be called with an object that is not of the player class.)\nThe ellipses (...) in the S3 generic function signatures may be a bit mysterious since they aren’t passed explicitly to UseMethod. Any non-player arguments are captured in these ellipses and passed to whatever method that is called from the generic (e.g. .get_tti.player method called from the .get_tti generic). For .get_tti, the ellipses is intended to capture x2 and y2, and for .set_tti, it captures value.\nWe must use the “strange” syntax .set_tti<-.player (instead of just .set_tti.player, which may seem more “natural”) in order to update an attribute in an already instantiated class. 8\nWe define the function euclidean_norm() outside of .get_tti.player simply because it is not something that is specific to the time to intercept calculation for a player; it can work with any two pairs of x and y coordinates.9\nri and vmax, representing a player’s reaction time and a player’s maximum velocity respectively, are constants defined in the Spearman paper. We could change these if we wanted to, or even make them dynamic (i.e. configurable via other function parameters, or even at instantiation time).\n\nTo really complete our getter and setter methods for tti, we should write methods to handle the case when a non-player object is passed to them. The generic .get_tti and .set_tti methods will dispatch to these functions if the object passed to them (the first argument named player) doesn’t actually inherit from the player class.\n\n\nCode\n.get_tti.default <- function(player, ...) {\n  stop(.msg_cls_err(player, '.get_tti'), call. = FALSE)\n}\n\n.set_tti.default <- function(player, ...) {\n  stop(.msg_cls_err(player, '.get_tti'), call. = FALSE)\n}\n\n\nLet’s see how our pseudo-encapsulation looks in action.\n\n\nCode\nplayers <- 8L:10L %>% map(~player(player_id = .x, frame = 53027L, tracking = tracking_start))\nmap(players, ~vctrs::field(.x, 'tti'))\n#> [[1]]\n#> [1] -1\n#> \n#> [[2]]\n#> [1] -1\n#> \n#> [[3]]\n#> [1] -1\n\n\n\n\nCode\ntarget_x <- 94\ntarget_y <- 63\nfor(i in seq_along(players)) {\n  value <- .get_tti(players[[i]], x2 = target_x, y2 = target_y)\n  .set_tti(players[[i]]) <- value\n}\nmap(players, ~vctrs::field(.x, 'tti'))\n#> [[1]]\n#> [1] 4.92839\n#> \n#> [[2]]\n#> [1] 10.6878\n#> \n#> [[3]]\n#> [1] 9.49904\n\n\nNote how the player tti values changed after we defined them for a specified target_x and target_y.\nOur approach to p_intercept is very similar to that for tti, so I don’t show most of it here. As before, we define getters and setters, as well as generics for the class (the intended target of method dispatch), as well as a default class to handle unexpected inputs. Probably the only interesting part is the calculation itself, as shown below. If you compare it to the p_intercept method in the python object definition, you’ll see it’s basically identical.\n\n\nCode\n.get_p_intercept.player <- function(player, t) {\n  tti_sigma <- 0.45\n  den <- 1 + exp((-base::pi / sqrt(3) / tti_sigma) * (t - vctrs::field(player, 'tti')))\n  1 / den\n}\n\n\nThere is certainly more to show, especially for what is needed to calculate pitch control. (We need to integrate probabilities across all players over time, and do it for the entire pitch.) Nonetheless, the player class and the pseudo-encapsulation that we’ve implemented with S3 and {vctrs} is really the key component underlying the whole pitch control calculation."
  },
  {
    "objectID": "posts/soccer-pitch-control-r/index.html#advanced-usage",
    "href": "posts/soccer-pitch-control-r/index.html#advanced-usage",
    "title": "Creating a Soccer Pitch Control Model",
    "section": "Advanced Usage",
    "text": "Advanced Usage\nTo really motivate the reader, let’s see what this implementation allows us to do.\nFirst, let’s emulate the pitch control plot of event 823, which is a pass by the away (blue) team in the home (red) team’s penalty area preceding a successful shot.\n\nCompare this to the python version.\n\nIt’s not a perfect replication, but I think it’s very close overall.\nSecond, let’s replicate the expected possession value (EPV) plot of the same event, including the EPV added by the pass.\n\nAgain, we can compare this plot to the python equivalent.\n\nCool, my R version seems to be very close to the python original. We do have a small discrepancy in the EPV added calculation. (This EPV is actually an “expected” EPV calculation that uses pitch control to weight the pre-learned EPV grid). I believe this is probably due to discrepancies in the integration done in the pitch control calculation and not due to a significant code issue.\nThe code to prepare the data for these plots gets more complex, which is why I have excluded it here.10 However, none of it is unreasonably difficult to understand or implement once we have a properly defined player object."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tony ElHabr",
    "section": "",
    "text": "Data scientist at CollegeVine"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Tony's Blog",
    "section": "",
    "text": "I co-maintain {worldfootballR}, an R package for extracting world football (soccer) data from several sites. I work mostly with the Fotmob functions.\nI wrote {valorantr}, an R package for pro Valorant data from rib.gg.\nI contributed functions for scraping data from ESPN to {ffscrapr}, an R API client for several fantasy football league platforms."
  },
  {
    "objectID": "projects.html#projects",
    "href": "projects.html#projects",
    "title": "Tony's Blog",
    "section": "Projects",
    "text": "Projects\n\n{geekswhodrink}: Collection nationwide Geeks Who Drink pub trivia scores (work in progress)\n2022 Carnegie Melon Sports Analytics Conference (CMSAC) paper\n\n“The Hot Hand Fallacy in Call of Duty Search and Destroy”\n\n“Bundesliga tax” analysis\n\nCited in The Athletic, Tifo Football, ESPN, Grace Robertson, and more\n\n2021 Big Data Bowl (Big Data Bowl) team submission (finalist)\n\n“Weighted Assessment of Defender Affectiveness (WADE)”: A framework for quantifying NFL defensive back coverage and contest skills\n\n2021 BDB individual submission\n\n“Assessing Pick Plays and How They’re Defended”: Quantifying the causal impact of pick routes run by NFL receivers using tracking data\n\nSports analytics visualization gallery\n\nA collection of visualizations (mostly soccer) that I’ve tweeted out\n\n{xengagement}\n\nAn R package, Twitter bot, and python Dash app for predicting, visualizing, and summarizing the amount of Twitter engagement that xGPhilosophy receives with its end-of-match xG summary tweets\n\nNBA win probability\n\nCode for calculating NBA win probability and discussion (8-hour interview project)\n\nNBA Regularized Adjusted Plus Minus (RAPM)\n\nCode for calculating RAPM given cleaned NBA play-by-play data"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Tony ElHabr",
    "section": "",
    "text": "Data scientist at CollegeVine"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Tony ElHabr",
    "section": "Education",
    "text": "Education\nM.S. in Analytics, May 2020 | Georgia Institute of TechnologyB.S. in Electrical Engineering, May 2016 | The University of Texas at Austin\n\nInterests\nsports analyticsdata sciencememes\n\n\nOther\nContestant on seasons 0 and 1 of “Sliced”RWeekly newsletter curator"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Tony ElHabr",
    "section": "Education",
    "text": "Education\nM.S. in Analytics, May 2020 | Georgia Institute of TechnologyB.S. in Electrical Engineering, May 2016 | The University of Texas at Austin\n\nInterests\nsports analyticsdata sciencememes\n\n\nOther\nContestant on seasons 0 and 1 of “Sliced”RWeekly newsletter curator"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Tony's Blog",
    "section": "",
    "text": "xG Model Calibration\n\n\n\n\n\n\n\nr\n\n\nsoccer\n\n\n\n\nEvaluating Opta’s xG model performance with Brier skill score (BSS) and calibration curves\n\n\n\n\n\n\nFeb 20, 2023\n\n\nTony ElHabr\n\n\n\n\n\n\n  \n\n\n\n\nWhat exactly is an “expected point”? (part 2)\n\n\n\n\n\n\n\nr\n\n\nsoccer\n\n\n\n\nEvaluating how we can use match outcome probabilites for season-long insights\n\n\n\n\n\n\nSep 5, 2022\n\n\nTony ElHabr\n\n\n\n\n\n\n  \n\n\n\n\nWhat exactly is an “expected point”? (part 1)\n\n\n\n\n\n\n\nr\n\n\nsoccer\n\n\n\n\nCalculating and comparing expected points from different expected goals sources\n\n\n\n\n\n\nSep 4, 2022\n\n\nTony ElHabr\n\n\n\n\n\n\n  \n\n\n\n\nYet Another (Advanced?) Soccer Statistic\n\n\n\n\n\n\n\nr\n\n\nsoccer\n\n\n\n\nQuantifying soccer pass networks with weighted maximum cuts\n\n\n\n\n\n\nJan 31, 2022\n\n\nTony ElHabr\n\n\n\n\n\n\n  \n\n\n\n\nTired: PCA + kmeans, Wired: UMAP + GMM\n\n\n\n\n\n\n\nr\n\n\nsoccer\n\n\n\n\nAn Alternative to the Classic Approach to Dimension Reduction + Clustering\n\n\n\n\n\n\nJun 30, 2021\n\n\nTony ElHabr\n\n\n\n\n\n\n  \n\n\n\n\nQuantifying Relative Soccer League Strength\n\n\n\n\n\n\n\nr\n\n\nsoccer\n\n\n\n\nWith Atomic VAEP\n\n\n\n\n\n\nJun 26, 2021\n\n\nTony ElHabr\n\n\n\n\n\n\n  \n\n\n\n\nFantasy Football and the Classical Scheduling Problem\n\n\n\n\n\n\n\nr\n\n\npython\n\n\nfootball (american)\n\n\n\n\nBrute Force Programming Go Brrr\n\n\n\n\n\n\nJan 11, 2021\n\n\nTony ElHabr\n\n\n\n\n\n\n  \n\n\n\n\nDecomposing and Smoothing Soccer Spatial Tendencies\n\n\n\n\n\n\n\nr\n\n\nsoccer\n\n\n\n\nWith data.table, reticulate, and spatstat\n\n\n\n\n\n\nOct 14, 2020\n\n\nTony ElHabr\n\n\n\n\n\n\n  \n\n\n\n\nCreating a Soccer Pitch Control Model\n\n\n\n\n\n\n\nr\n\n\nsoccer\n\n\n\n\nWith S3 Classes and vctrs\n\n\n\n\n\n\nSep 23, 2020\n\n\nTony ElHabr\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Tony's Blog",
    "section": "",
    "text": "Measuring Shooting Overperformance in Soccer\n\n\n\n\n\n\n\nr\n\n\nsoccer\n\n\n\n\nUsing empirical Bayes and the Gamma-Poisson conjugate pair\n\n\n\n\n\n\nAug 28, 2023\n\n\nTony ElHabr\n\n\n\n\n\n\n  \n\n\n\n\nxG Model Calibration\n\n\n\n\n\n\n\nr\n\n\nsoccer\n\n\n\n\nEvaluating Opta’s xG model performance with Brier skill score (BSS) and calibration curves\n\n\n\n\n\n\nFeb 20, 2023\n\n\nTony ElHabr\n\n\n\n\n\n\n  \n\n\n\n\nWhat exactly is an “expected point”? (part 2)\n\n\n\n\n\n\n\nr\n\n\nsoccer\n\n\n\n\nEvaluating how we can use match outcome probabilites for season-long insights\n\n\n\n\n\n\nSep 5, 2022\n\n\nTony ElHabr\n\n\n\n\n\n\n  \n\n\n\n\nWhat exactly is an “expected point”? (part 1)\n\n\n\n\n\n\n\nr\n\n\nsoccer\n\n\n\n\nCalculating and comparing expected points from different expected goals sources\n\n\n\n\n\n\nSep 4, 2022\n\n\nTony ElHabr\n\n\n\n\n\n\n  \n\n\n\n\nYet Another (Advanced?) Soccer Statistic\n\n\n\n\n\n\n\nr\n\n\nsoccer\n\n\n\n\nQuantifying soccer pass networks with weighted maximum cuts\n\n\n\n\n\n\nJan 31, 2022\n\n\nTony ElHabr\n\n\n\n\n\n\n  \n\n\n\n\nTired: PCA + kmeans, Wired: UMAP + GMM\n\n\n\n\n\n\n\nr\n\n\nsoccer\n\n\n\n\nAn Alternative to the Classic Approach to Dimension Reduction + Clustering\n\n\n\n\n\n\nJun 30, 2021\n\n\nTony ElHabr\n\n\n\n\n\n\n  \n\n\n\n\nQuantifying Relative Soccer League Strength\n\n\n\n\n\n\n\nr\n\n\nsoccer\n\n\n\n\nWith Atomic VAEP\n\n\n\n\n\n\nJun 26, 2021\n\n\nTony ElHabr\n\n\n\n\n\n\n  \n\n\n\n\nFantasy Football and the Classical Scheduling Problem\n\n\n\n\n\n\n\nr\n\n\npython\n\n\nfootball (american)\n\n\n\n\nBrute Force Programming Go Brrr\n\n\n\n\n\n\nJan 11, 2021\n\n\nTony ElHabr\n\n\n\n\n\n\n  \n\n\n\n\nDecomposing and Smoothing Soccer Spatial Tendencies\n\n\n\n\n\n\n\nr\n\n\nsoccer\n\n\n\n\nWith data.table, reticulate, and spatstat\n\n\n\n\n\n\nOct 14, 2020\n\n\nTony ElHabr\n\n\n\n\n\n\n  \n\n\n\n\nCreating a Soccer Pitch Control Model\n\n\n\n\n\n\n\nr\n\n\nsoccer\n\n\n\n\nWith S3 Classes and vctrs\n\n\n\n\n\n\nSep 23, 2020\n\n\nTony ElHabr\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/xg-ratio-empirical-bayes/index.html",
    "href": "posts/xg-ratio-empirical-bayes/index.html",
    "title": "Measuring Shooting Overperformance in Soccer",
    "section": "",
    "text": "This blog post is my attempt to replicate the results in Laurie Shaw’s 2018 blog post “Exceeding Expected Goals”. Specifically, I want to shed light on how to implement Gamma-Poisson empirical Bayes (EB) estimation. If you don’t care at all about the theory behind EB and its application to this context, then go ahead and skip ahead to the “Implementation” section.\n\n\nEmpirical Bayes (EB) estimation. Wow, just typing that out makes me feel smart. But what is it, really? In short, I’d describe it as a mix of Bayesian and Frequentist inference. We lean into the observed frequencies of the data (Frequentist) while simultaneously refining our initial data assumptions through Bayesian updating. In practice, one might use EB as a (relatively) simple alternative to a full Bayesian analysis, which can feel daunting.\nIn regular Bayesian analysis, you start with your initial “guess” (prior distribution) about something, and as you gather data, you tweak that “guess” using Bayes’ theorem to get a final view (posterior distribution). We combine what we thought about the data beforehand with how likely the data matches (likelihood).\nEmpirical Bayes puts a twist on this. Instead of having a prior guess, you figure out that initial guess from the same data you’re analyzing. This can make things simpler, especially when you’re dealing with tons of guesses but not much initial info.\n\n\n\nDavid Robinson wrote a wonderful blog post about empirical Bayes estimation for estimating batting averages in baseball, notably “shrinking” the battering averages of those with relatively few at bats closer to some “prior” estimate derived from a choice of hyperparameters. For context, batting average, \\(BA\\), is defined as a player’s count of hits, \\(H\\), divided by the count of their at bats, \\(AB\\).\n\\[\nBA = H / AB\n\\tag{1}\\]\n\n\n\n\n\n\nNote\n\n\n\nI’d David’s post must read material before going through this blog post.\n\n\nIn his post, David uses a Beta prior and a binomial posterior together, i.e. a Beta-binomial Bayesian model)12, since this tandem is suitable for proportions and probabilities. The gist of his approach: we add some fixed number of hits, \\(\\alpha_0\\), and a fixed number of at bats, \\(\\beta_0\\), to the numerator and denominator of the battering average equation as so.\n\\[\n(H + \\alpha_0) / (AB + \\alpha_0 + \\beta_0)\n\\tag{2}\\]\nSpecifically, the “prior” estimate of batting average is found by isolating the \\(\\alpha_0\\) and \\(\\beta_0\\) elements:\n\\[\n\\alpha_0 / (\\alpha_0 + \\beta_0)\n\\tag{3}\\]\nIf, for example, alpha0 = 70 and beta0 = 230, then the prior estimate of batting average is effectively 70 / (70 + 230) = 0.3. Note that alpha0 and beta0 are learned from the data using maximum likelihood estimation (MLE), although other approaches, such as “method of moments” could be used. (Heck, you could even defensibly choose these “hyperparameters” yourself, without any fancy statistics, if you feel that you have enough knowledge of the data.)\n\n\n\nNow, for my replication of Shaw’s analysis, we’re going to be focusing on the ratio of a player’s goals, \\(G\\), divided by their expected goals), \\(xG\\), summed up over a fixed period. Shaw refers to this as “overperformance” \\(O\\) for a player \\(p\\):\n\\[\nO_p = \\frac{G_p}{xG_p}\n\\tag{4}\\]\nWhile one might be tempted to use Beta-Binomial EB since this setup seems similar to batting average in Equation 1, Shaw used a Gamma-Poisson EB adjustment, and justifiably so. Gamma-Poisson makes more sense when the underlying data consists of counts and what you’re trying to estimate is a rate or ratio, not a proportion bounded between 0 and 1. Note that a \\(O_p\\) ratio of 1 indicates that a player is scoring as many goals as expected; a ratio greater than 1 indicates underperformance; and a ratio less than 1 indicates overperformance. On, the other hand, batting average is bounded between 0 and 1.\nNow, despite the naming of conjugate prior pairs–e.g. “Beta-Binomial” and “Gamma-Poisson”, where the prior distribution is represented by the first distribution and the likelihood distribution is indicated by the second–let’s not forget that there is a third distribution to be noted: the posterior. In the case of the Gamma-Poisson model, the unnormalized posterior distribution of the “kernel” (i.e. the prior and likelihood pair) is a Gamma distribution. (This is always the case with Gamma-Prior kernels.)\nIn practice, this means that we’ll be using the Gamma distribution for both estimating hyperparameters and posterior sampling. Perhaps surprising to the reader, you won’t need any Poisson functions in the code implementation. Rather, the Poisson distribution is pertinent to implementation only to the extent that the Gamma distribution happens to be the most reasonable distribution to pair with it.\nI’ve woefully explained away a lot of details here, but hopefully this all makes sense to those with a basic understanding of the Gamma and Poisson distributions themselves."
  },
  {
    "objectID": "posts/xg-ratio-empirical-bayes/index.html#implementation",
    "href": "posts/xg-ratio-empirical-bayes/index.html#implementation",
    "title": "Measuring Shooting Overperformance in Soccer",
    "section": "Implementation",
    "text": "Implementation\nOk, so with all of that context provided, now let’s do the replication of Shaw’s findings.\n\nData\nFirst, let’s pull the data we’ll need–2016/17 and 2017/18 English Premier League goals and expected goals (xG) by player. I’m using understat since it is a reliable source of data3 and is easy to retrieve data from via the {worldfootballR} package.4\nNote that Shaw used data from a provider, Stratagem, that no long provides data, as far as I can tell. For at least this reason, I won’t be able to exactly match his reason.5\n\n\nCode\n## data wrangling\nlibrary(worldfootballR)\nlibrary(dplyr)\nlibrary(tibble)\n\n## distribution fitting and wrangling\nlibrary(MASS, include.only = 'fitdistr') ## to avoid `select` name conflict with dplyr\nlibrary(withr)\nlibrary(purrr)\nlibrary(tidyr)\n\nraw_shots <- worldfootballR::load_understat_league_shots(league = 'EPL')\nshots <- raw_shots |> \n  tibble::as_tibble() |> \n  dplyr::filter(\n    season %in% c(2016L, 2017L), ## 2016/17 and 2017/18 seasons\n    situation != 'DirectFreeKick' ## \"excluding free-kicks\" in the blog post\n  ) |> \n  dplyr::arrange(id) |> \n  dplyr::transmute(\n    id,\n    player,\n    xg = x_g,\n    g = as.integer(result == 'Goal')\n  )\nshots\n#> # A tibble: 19,047 × 4\n#>        id player               xg     g\n#>     <dbl> <chr>             <dbl> <int>\n#>  1 112088 Aaron Ramsey    0.0695      0\n#>  2 112089 Nathaniel Clyne 0.0293      0\n#>  3 112090 Aaron Ramsey    0.00734     0\n#>  4 112091 Roberto Firmino 0.0856      0\n#>  5 112092 Roberto Firmino 0.0441      0\n#>  6 112093 Sadio Mané      0.0607      0\n#>  7 112094 Ragnar Klavan   0.0742      0\n#>  8 112095 Theo Walcott    0.761       0\n#>  9 112096 Theo Walcott    0.0721      1\n#> 10 112097 Roberto Firmino 0.0241      0\n#> # ℹ 19,037 more rows\n\n\nAbove was pulling in every record of shots, with 1 row per shot. Now we aggregate to the player level, such that we have one row per player.\n\n\nCode\nshots_by_player <- shots |> \n  dplyr::group_by(player) |> \n  dplyr::summarize(\n    shots = dplyr::n(),\n    dplyr::across(c(g, xg), sum)\n  ) |> \n  dplyr::ungroup() |> \n  dplyr::mutate(raw_ratio = g / xg) |> \n  dplyr::arrange(dplyr::desc(shots))\nshots_by_player\n#> # A tibble: 588 × 5\n#>    player            shots     g    xg raw_ratio\n#>    <chr>             <int> <int> <dbl>     <dbl>\n#>  1 Harry Kane          293    59  46.7     1.26 \n#>  2 Sergio Agüero       234    41  41.2     0.994\n#>  3 Christian Eriksen   229    18  16.1     1.12 \n#>  4 Alexis Sánchez      217    33  29.1     1.13 \n#>  5 Romelu Lukaku       196    41  32.1     1.28 \n#>  6 Roberto Firmino     184    26  21.1     1.23 \n#>  7 Kevin De Bruyne     179    14  12.2     1.15 \n#>  8 Salomón Rondón      171    15  16.2     0.924\n#>  9 Paul Pogba          168    11  14.3     0.768\n#> 10 Christian Benteke   164    18  28.5     0.631\n#> # ℹ 578 more rows\n\n\n\n\nEB step 1: estimate prior hyperparameters\nNext, we estimate hyperparameters for our prior gamma distribution using MLE. (With R’s dgamma, the hyperparameters are shape and rate6). I subset the data down to players having taken at least 50 shots for estimating these hyperparameters, as this is what Shaw does. In general, you’d want to filter your data here to records that provide good “signal”, and, therefore, will provide reliable estimates of your hyperparameters.7\n\n\nCode\nprior_shots_by_player <- dplyr::filter(\n  shots_by_player, \n  shots >= 50,\n  g > 0 ## prevent error with fitting prior distribution\n)\n\nprior_distr <- MASS::fitdistr(\n  prior_shots_by_player$raw_ratio,\n  dgamma,\n  start = list(shape = 1, rate = 1)\n)\nprior_shape <- prior_distr$estimate[1]\nprior_rate <- prior_distr$estimate[2]\nlist(prior_shape = round(prior_shape, 2), prior_rate = round(prior_rate, 2))\n#> $prior_shape\n#> shape \n#>  9.39 \n#> \n#> $prior_rate\n#> rate \n#> 8.93\n\n\n\n\nEB step 2: Use prior distribution to sample the posterior\nNow we use our prior distribution’s hyperparameters to update all players’ \\(O\\) ratio based on their individual volume of evidence, i.e. their goals and xG.\n\nsimulate_gamma_posterior <- function(\n    successes, \n    trials, \n    prior_shape, \n    prior_rate, \n    n_sims = 10000,\n    seed = 42\n) {\n  posterior_shape <- prior_shape + successes\n  posterior_rate <- prior_rate + trials\n  withr::local_seed(seed)\n  posterior_sample <- rgamma(n = n_sims, shape = posterior_shape, rate = posterior_rate)\n  list(\n    mean = mean(posterior_sample),\n    sd = sd(posterior_sample)\n  )\n}\n\nshots_by_player$adj_ratio <- purrr::map2(\n  shots_by_player$g, shots_by_player$xg,\n  function(g, xg) {\n    simulate_gamma_posterior(\n      successes = g,\n      trials = xg,\n      prior_shape = prior_shape,\n      prior_rate = prior_rate\n    )\n  }\n)\n\nadj_ratio_by_player <- shots_by_player |> \n  tidyr::unnest_wider(\n    adj_ratio, \n    names_sep = '_'\n  ) |> \n  dplyr::arrange(dplyr::desc(adj_ratio_mean))\nadj_ratio_by_player\n#> # A tibble: 588 × 7\n#>    player            shots     g    xg raw_ratio adj_ratio_mean adj_ratio_sd\n#>    <chr>             <int> <int> <dbl>     <dbl>          <dbl>        <dbl>\n#>  1 Fernando Llorente    57    16  9.19      1.74           1.40        0.281\n#>  2 Philippe Coutinho   160    20 12.5       1.60           1.37        0.256\n#>  3 Shkodran Mustafi     37     5  1.82      2.75           1.34        0.357\n#>  4 Pascal Groß          43     7  3.34      2.10           1.34        0.334\n#>  5 Ryan Fraser          55     8  4.09      1.95           1.34        0.324\n#>  6 Eden Hazard         148    28 19.2       1.45           1.33        0.219\n#>  7 James McArthur       53    10  5.93      1.69           1.31        0.299\n#>  8 Charlie Daniels      39     5  2.18      2.30           1.30        0.346\n#>  9 Xherdan Shaqiri     117    12  7.61      1.58           1.29        0.282\n#> 10 Andy Carroll         67    10  6.09      1.64           1.29        0.296\n#> # ℹ 578 more rows\n\nFinally, let’s plot our results, plotting our adjusted mean estimates of \\(O_p\\), ±1 standard deviation about the adjusted mean.8 As noted earlier, we won’t achieve exactly the same results as Shaw due to using a different set of xG values, but evidently we’ve achieved results reasonably close to his.\n\n\nCode\nlibrary(ggplot2)\nlibrary(forcats)\nlibrary(ggh4x)\nlibrary(magick)\n\nshaw_players <- c(\n  'Eden Hazard' = 'E. Hazard',\n  'Mohamed Salah' = 'Mohamed Salah',\n  'Son Heung-Min' = 'Heung-Min Son',\n  'Joshua King' = 'J. King',\n  'Romelu Lukaku' = 'R. Lukaku',\n  'Harry Kane' = 'H. Kane',\n  'Sadio Mané' = 'S. Mane',\n  'Dele Alli' = 'D. Ali',\n  'Riyad Mahrez' = 'R. Mahrez',\n  'Christian Eriksen' = 'C. Eriksen',\n  'Pedro' = 'Pedro',\n  'Alexis Sánchez' = 'A. Sanchez',\n  'Roberto Firmino' = 'Roberto Firmino',\n  'Jamie Vardy' = 'J. Vardy',\n  'Xherdan Shaqiri' = 'X. Shaqiri',\n  'Wilfried Zaha' = 'W. Zaha',\n  'Nathan Redmond' = 'N. Redmond',\n  'Gylfi Sigurdsson' = 'G. Sigurdsson',\n  'Kevin De Bruyne' = 'K. De Bruyne',\n  'Andros Townsend' = 'A. Townsend',\n  'Sergio Agüero' = 'S. Aguero',\n  'Marcus Rashford' = 'M. Rashford',\n  'Jermain Defoe' = 'J. Defoe',\n  'Raheem Sterling' = 'R. Sterling',\n  'Marko Arnautovic' = 'M. Arnautovic',\n  'Paul Pogba' = 'P. Pogba',\n  'Salomón Rondón' = 'S. Rondon',\n  'Christian Benteke' = 'C. Benteke'\n)\n\nordinal_adj_ratio_by_player <- adj_ratio_by_player |>\n  dplyr::filter(\n    player %in% names(shaw_players)\n  ) |> \n  dplyr::mutate(\n    player = forcats::fct_reorder(shaw_players[player], adj_ratio_mean)\n  )\n\nadj_ratio_plot <- ordinal_adj_ratio_by_player |>\n  ggplot2::ggplot() +\n  ggplot2::aes(y = player) +\n  ggplot2::geom_errorbarh(\n    aes(\n      xmin = adj_ratio_mean - adj_ratio_sd,\n      xmax = adj_ratio_mean + adj_ratio_sd\n    ),\n    color = 'blue',\n    linewidth = 0.1,\n    height = 0.3\n  ) +\n  ggplot2::geom_point(\n    ggplot2::aes(x = adj_ratio_mean),\n    shape = 23,\n    size = 0.75,\n    stroke = 0.15,\n    fill = 'red',\n    color = 'black'\n  ) +\n  ggplot2::geom_vline(\n    ggplot2::aes(xintercept = 1), \n    linewidth = 0.1, \n    linetype = 2\n  ) +\n  ## add duplicate axis for ticks: https://stackoverflow.com/questions/56247205/r-ggplot2-add-ticks-on-top-and-right-sides-of-all-facets\n  ggplot2::scale_x_continuous(sec.axis = ggplot2::dup_axis()) +\n  ## ggplot2 doesn't support duplicated and creatinga  second axis for discrete variables:\n  ##   https://github.com/tidyverse/ggplot2/issues/3171.\n  ##   using ggh4x is a workaround.\n  ggplot2::guides(\n    y.sec = ggh4x::guide_axis_manual(\n      breaks = ordinal_adj_ratio_by_player$player,\n      labels = ordinal_adj_ratio_by_player$player\n    )\n  ) +\n  ggplot2::theme_linedraw(base_family = 'DejaVu Sans', base_size = 4) +\n  ggplot2::theme(\n    plot.title = ggplot2::element_text(hjust = 0.5, size = 4.25, face = 'plain'),\n    axis.ticks.length = ggplot2::unit(-1, 'pt'),\n    axis.ticks = ggplot2::element_line(linewidth = 0.05),\n    panel.grid.major.y = ggplot2::element_blank(),\n    panel.grid.minor = ggplot2::element_blank(),\n    panel.grid.major.x = ggplot2::element_line(linetype = 2),\n    axis.text.x.top = ggplot2::element_blank(),\n    axis.text.y.right = ggplot2::element_blank(),\n    axis.title.x.top = ggplot2::element_blank(),\n    axis.title.y.right = ggplot2::element_blank()\n  ) +\n  ggplot2::labs(\n    title = 'Shots from 2016/17 & 2017/18 seasons',\n    y = NULL,\n    x = 'Outperformance (= G/xG)'\n  )\n\nproj_dir <- 'posts/xg-ratio-empirical-bayes'\nplot_path <- file.path(proj_dir, 'shaw-figure-1-replication.png')\nggplot2::ggsave(\n  adj_ratio_plot,\n  filename = plot_path,\n  units = 'px',\n  width = 549,\n  height = 640\n)\n\norig_image <- magick::image_read(file.path(proj_dir, 'shaw-figure-1.png'))\nreplicated_image_with_asa_logo <- magick::image_read(plot_with_asa_logo_path)\ncombined_image_with_tony_logo <- magick::image_append(\n  c(orig_image, replicated_image_with_tony_logo), \n  stack = TRUE\n)\n\nmagick::image_write(\n  combined_image_with_tony_logo, \n  path = file.path(proj_dir, 'shaw-figure-1-compared-w-tony-logo.png')\n)\n\n\n\n\n\nBeyond Replication\nFor the sake of having a pretty plot that’s not just an attempt to replicate the original, let’s run it all back, this time with EPL 2021/22 and 2022/23 data.\n\n\nCode\nraw_shots <- worldfootballR::load_understat_league_shots(league = 'EPL')\nshots <- raw_shots |> \n  tibble::as_tibble() |> \n  dplyr::filter(\n    season %in% c(2021L, 2022L),\n    situation != 'DirectFreeKick'\n  ) |> \n  dplyr::arrange(id) |> \n  dplyr::transmute(\n    id,\n    player,\n    ## since 2022/23, xG is filled out, not x_g\n    xg = dplyr::coalesce(x_g, xG),\n    g = as.integer(result == 'Goal')\n  )\n\nshots_by_player <- shots |> \n  dplyr::group_by(player) |> \n  dplyr::summarize(\n    shots = dplyr::n(),\n    dplyr::across(c(g, xg), sum)\n  ) |> \n  dplyr::ungroup() |> \n  dplyr::mutate(raw_ratio = g / xg) |> \n  dplyr::arrange(dplyr::desc(shots))\nshots_by_player\n\nshots_by_player$adj_ratio <- purrr::map2(\n  shots_by_player$g, shots_by_player$xg,\n  function(g, xg) {\n    simulate_gamma_posterior(\n      successes = g,\n      trials = xg,\n      prior_shape = prior_shape,\n      prior_rate = prior_rate\n    )\n  }\n)\n\nadj_ratio_by_player <- shots_by_player |> \n  tidyr::unnest_wider(\n    adj_ratio, \n    names_sep = '_'\n  ) |> \n  dplyr::arrange(dplyr::desc(adj_ratio_mean))\n\nordinal_adj_ratio_by_player <- adj_ratio_by_player |>\n  dplyr::filter(\n    player %in% names(shaw_players)\n  ) |> \n  dplyr::mutate(\n    player = forcats::fct_reorder(shaw_players[player], adj_ratio_mean)\n  )\n\nlibrary(htmltools)\nlibrary(sysfonts)\nlibrary(showtext)\n\nblackish_background <- '#1f1f1f'\nfont <- 'Titillium Web'\nsysfonts::font_add_google(font, font)\nsysfonts::font_add('fb', 'Font Awesome 6 Brands-Regular-400.otf')\nshowtext::showtext_auto()\nplot_resolution <- 300\nshowtext::showtext_opts(dpi = plot_resolution)\n## https://github.com/tashapiro/tanya-data-viz/blob/1dfad735bca1a7f335969f0eafc94cf971345075/nba-shot-chart/nba-shots.R#L64\n\ntag_lab <- htmltools::tagList(\n  htmltools::tags$span(htmltools::HTML(enc2utf8(\"&#xf099;\")), style='font-family:fb'),\n  htmltools::tags$span(\"@TonyElHabr\"),\n)\n\nbeyond_replication_adj_ratio_plot <- ordinal_adj_ratio_by_player |>\n  ggplot2::ggplot() +\n  ggplot2::aes(y = player) +\n  ggplot2::geom_vline(\n    ggplot2::aes(xintercept = 1), \n    linewidth = 1.5,\n    linetype = 2,\n    color = 'white'\n  ) +\n  ggplot2::geom_errorbarh(\n    ggplot2::aes(\n      xmin = adj_ratio_mean - adj_ratio_sd,\n      xmax = adj_ratio_mean + adj_ratio_sd\n    ),\n    color = 'white',\n    height = 0.5\n  ) +\n  ggplot2::geom_point(\n    ggplot2::aes(x = adj_ratio_mean, size = shots),\n    color = 'white'\n  ) +\n  ggplot2::theme_minimal() +\n  ggplot2::theme(\n    text = ggplot2::element_text(family = font, color = 'white'),\n    title = ggplot2::element_text(size = 14, color = 'white'),\n    plot.title = ggplot2::element_text(face = 'bold', size = 16, color = 'white', hjust = 0),\n    plot.title.position = 'plot',\n    plot.subtitle = ggplot2::element_text(size = 14, color = 'white', hjust = 0),\n    plot.margin = ggplot2::margin(10, 20, 10, 20),\n    plot.caption = ggtext::element_markdown(color = 'white', hjust = 0, size = 10, face = 'plain', lineheight = 1.1),\n    plot.caption.position = 'plot',\n    plot.tag = ggtext::element_markdown(size = 10, color = 'white', hjust = 1),\n    plot.tag.position = c(0.99, 0.01),\n    panel.grid.major.y = ggplot2::element_blank(),\n    panel.grid.minor.x = ggplot2::element_blank(),\n    panel.grid.major.x = ggplot2::element_line(linewidth = 0.1),\n    plot.background = ggplot2::element_rect(fill = blackish_background, color = blackish_background),\n    panel.background = ggplot2::element_rect(fill = blackish_background, color = blackish_background),\n    axis.title = ggplot2::element_text(color = 'white', size = 14, face = 'bold', hjust = 0.99),\n    axis.line = ggplot2::element_blank(),\n    axis.text = ggplot2::element_text(color = 'white', size = 12),\n    axis.text.y = ggtext::element_markdown(),\n    legend.text = ggplot2::element_text(color = 'white', size = 12),\n    legend.position = 'top'\n  ) +\n  ggplot2::labs(\n    title = 'Top 20 shooting overperformers in the EPL',\n    subtitle = 'EPL 2021/22 and 2022/23 seasons. ',\n    caption = 'Players sorted according to descending adjusted G/xG ratio. Minimum 100 shots.<br/>**Source**: understat.',\n    y = NULL,\n    x = 'Adjusted G/xG Ratio',\n    tag = tag_lab\n  )\n\nbeyond_replication_plot_path <- file.path(proj_dir, 'beyond-replication.png')\nggplot2::ggsave(\n  beyond_replication_adj_ratio_plot,\n  filename = beyond_replication_plot_path,\n  width = 7,\n  height = 7\n)\n\n\n\nFor those who follow the EPL, The usual suspects, like Heung-Min Son, show up among the best of the best HERE. The adjusted mean minus one standard deviation value exceeds zero for Son, so one might say that he was a significantly skilled shooter over the past two season.9"
  },
  {
    "objectID": "posts/xg-ratio-empirical-bayes/index.html#conclusion",
    "href": "posts/xg-ratio-empirical-bayes/index.html#conclusion",
    "title": "Measuring Shooting Overperformance in Soccer",
    "section": "Conclusion",
    "text": "Conclusion\nHaving not seen a very clear example of Gamma-Prior EB implementation on the internet–although there are several good explanations with toy examples such as this e-book chapter from Hyvönen and Topias Tolonen–I hope I’ve perhaps un-muddied the waters for at least one interested reader.\nAs for the actual application of \\(G / xG\\) for evaluating shooting performance, I have mixed feelings about it. Further analysis shows that it has zero year-over-year stability, i.e. one shouldn’t use a player’s raw or adjusted G/xG ratio in one season to try to predict whether their overperformance ratio will sustain in the next season. On the other hand, by simply making player estimates more comparable, the EB adjustment of \\(O_p\\) certainly is an improvement over raw \\(G / xG\\) itself.\nComparing hypothetical player A with 6 goals on 2 xG (\\(O_p = 3\\)) vs. player B with 120 goals on 100 xG directly (\\(O_p = 1.2\\)) is unfair; the former could be performing at an unsustainable rate, while the latter has demonstrated sustained overperformance over a lot more time. Indeed, applying the EB adjustment to these hypothetical numbers, player A’s \\(O_p\\) would be shrunken back towards 1, and player B’s adjustment would be effectively nothing, indicating that player B’s shooting performance is stronger, on average."
  }
]