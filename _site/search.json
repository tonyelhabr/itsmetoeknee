[
  {
    "objectID": "posts/decomposition-smoothing-soccer/index.html",
    "href": "posts/decomposition-smoothing-soccer/index.html",
    "title": "Decomposing and Smoothing Soccer Spatial Tendencies",
    "section": "",
    "text": "While reading up on modern soccer analytics, I stumbled upon an excellent set of tutorials written by Devin Pleuler. In particular, his notebook on non-negative matrix factorization (NNMF) caught my eye. I hadn‚Äôt really heard of the concept before, but it turned out to be much less daunting once I realized that it is just another type of matrix decomposition. Singular value decomposition (SVD), which I‚Äôm much more familiar with, belongs to the same family of calculations (although NNMF and SVD are quite different). In an effort to really gain a better understanding of NNMF, I set out to emulate his notebook.\nIn the process of converting his python code to R, I encountered three challenges with resolutions worth documenting.\nI‚Äôve always considered myself a ‚Äúwhatever gets the job done‚Äù kind of person, not insistent on ignoring solutions that use ‚Äúbase‚Äù R, {data.table}, python, etc. Nonetheless, replicating Devin‚Äôs notebook really underscored the importance of being comfortable outside of a {tidyverse}-centric workflow.\nAnyways, this post outlines the code and my thought process in porting Devin‚Äôs code to R. I‚Äôll skip some of the details, emphasizing the things that are most interesting."
  },
  {
    "objectID": "posts/decomposition-smoothing-soccer/index.html#data",
    "href": "posts/decomposition-smoothing-soccer/index.html#data",
    "title": "Decomposing and Smoothing Soccer Spatial Tendencies",
    "section": "Data",
    "text": "Data\nWe‚Äôll be working with the open-sourced StatsBomb data for the 2018 Men‚Äôs World Cup, which I‚Äôve called events below. 3\nThis is a relatively large data set with lots of columns (and rows). However, we only need three columns for what we‚Äôre going to do: (1) a unique identifier for each player, player_id, along with their (2) x and (3) y coordinates.\n\n\nCode\nlibrary(tidyverse)\ncomps <- StatsBombR::FreeCompetitions()\n\nmatches <- comps %>% \n  filter(competition_id == 43) %>%\n  StatsBombR::FreeMatches() %>% \n  arrange(match_date)\n\nevents <- StatsBombR::StatsBombFreeEvents(matches)\nevents <- StatsBombR::allclean(events)\nevents <- events %>% \n  select(player_id = player.id, x = location.x, y = location.y) %>% \n  drop_na()\n\n\nA quick summary of the data shows that there are 603 unique players, and that the x and y coordinates range from 1 to 120 (yards) and 1 to 80 respectively.\n\n\nCode\nevents %>% \n  summarize(\n    n = n(),\n    n_player = n_distinct(player_id),\n    across(c(x, y), list(min = min, max = max, mean = mean))\n  )\n#> # A tibble: 1 x 8\n#>       n n_player x_min x_max x_mean y_min y_max y_mean\n#>   <int>    <int> <dbl> <dbl>  <dbl> <dbl> <dbl>  <dbl>\n#> 1 224018      603     1   120  60.05     1    80  40.37"
  },
  {
    "objectID": "posts/decomposition-smoothing-soccer/index.html#non-equi-joining-with-data.table",
    "href": "posts/decomposition-smoothing-soccer/index.html#non-equi-joining-with-data.table",
    "title": "Decomposing and Smoothing Soccer Spatial Tendencies",
    "section": "Non-Equi Joining with {data.table}",
    "text": "Non-Equi Joining with {data.table}\nOur first challenge is to convert the following chunk of python.\n\n\nCode\nimport numpy as np\n\nx_scale, y_scale = 30, 20\n\nx_bins = np.linspace(0, 120, x_scale)\ny_bins = np.linspace(0, 80, y_scale)\n\nplayers = {}\n\nfor e in events:\n    if 'player' in e.keys():\n        player_id = e['player']['id']\n        if player_id not in players.keys():\n            players[player_id] = np.zeros((x_scale, y_scale))\n        try:\n            x_bin = int(np.digitize(e['location'][0], x_bins[1:], right=True))\n            y_bin = int(np.digitize(e['location'][1], y_bins[1:], right=True))\n            players[player_id][x_bin][y_bin] += 1\n        except:\n            pass\n\n\nThis code creates a nested dict, where the keys are player id‚Äôs and the values are 20x30 matrices. Each element in the matrix is an integer that represents the count of times that the player was recorded being at a certain position on the pitch. (These counts range from 0 to 94 for this data set.)\nSome technical details:\n\nThe python events is actually a pretty heavily nested list4, hence the non-rectangular operations such as e['player']['id'].\nObservations with missing coordinates are ignored with the try-except block.\nx and y values (elements of the 'location' sub-list) are mapped to ‚Äúbins‚Äù using numpy‚Äôs digitize() function, which is analogous to base::cut().\n\nHow can we do this same data manipulation in an idiomatic R fashion? We could certainly create a named list element and use base::cut() to closely match the python approach. However, I prefer to stick with data frames and SQL-ish operations since I think these are much more ‚Äúnatural‚Äù for R users.5\nSo, going forward with data frames and joins, it‚Äôs quickly apparent that we‚Äôll have to do some non-equi joining. {fuzzyjoin} and {sqldf} offer functionality for such an approach, but {data.table} is really the best option. The only minor inconvenience here is that we have to explicitly coerce our events data frame to a data.table.\nWe‚Äôll also need a helper, grid-like data frame to assist with the binning. The 600-row grid_xy_yards data frame (30 x bins * 20 y bins) below is essentially a tidy definition of the cells of the grid upon which we are binning the events data. (One can use whatever flavor of crossing(), expand.grid(), seq(), etc. that you prefer to create a data frame like this.)\nVisually, this grid looks like this.\n\nAnd if you prefer numbers instead of a chart, see the first 10 rows below.\n\n\nCode\ngrid_xy_yards\n#> # A tibble: 600 x 5\n#>     idx     x      y next_y next_x\n#>   <int> <dbl>  <dbl>  <dbl>  <dbl>\n#> 1     1     0  0      4.211  4.138\n#> 2     2     0  4.211  8.421  4.138\n#> 3     3     0  8.421 12.63   4.138\n#> 4     4     0 12.63  16.84   4.138\n#> 5     5     0 16.84  21.05   4.138\n#> 6     6     0 21.05  25.26   4.138\n#> 7     7     0 25.26  29.47   4.138\n#> 8     8     0 29.47  33.68   4.138\n#> 9     9     0 33.68  37.89   4.138\n#> 10    10    0 37.89  42.11   4.138\n#> # ... with 590 more rows\n\n\nTwo things to note about this supplementary data frame:\n\nCells aren‚Äôt evenly spaced integers, i.e.¬†x cells are defined at 0, 4.138, 8.276, ‚Ä¶, 80 instead of something like 0, 4, 8, ‚Ä¶, 80, and y cells are defined at 0, 4.211, 8.421, ‚Ä¶, 120 instead of something like 0, 4, 8, ‚Ä¶, 120). That‚Äôs simply due to using 30 and 20 instead of 31 and 21 to split up the x and y ranges respectively. I point this out because this SQL-ish approach would have been much easier if these numbers were just integers! We could have done an inner join on an integer grid instead of non-equi-joining upon a grid of floating point numbers. Unfortunately, joining on floating point numbers as keys leads to inconsistent results, simply due to the nature of floating points.6\nThe index idx is important! This will come back into play when we do the NNMF procedure, at which point we‚Äôll ‚Äúflatten‚Äù out our x-y pairs into a 1-d format.\n\nOk, on to the actual data joining.\n\n\nCode\nevents_dt <- events %>% drop_na() %>% data.table::as.data.table()\ngrid_xy_yards_dt <- grid_xy_yards %>% data.table::as.data.table()\n\n# We don't even have to load `{data.table}` for this to work!\nevents_binned <- events_dt[grid_xy_yards_dt, on=.(x > x, x <= next_x, y >= y, y < next_y)] %>% \n  as_tibble() %>% \n  select(player_id, idx, x, y)\nevents_binned\n#> # A tibble: 224,038 x 4\n#>    player_id   idx     x     y\n#>        <int> <int> <dbl> <dbl>\n#>  1      5462     1     0     0\n#>  2      5467     1     0     0\n#>  3      5488     1     0     0\n#>  4      3632     1     0     0\n#>  5      5576     1     0     0\n#>  6      5595     1     0     0\n#>  7      5263     1     0     0\n#>  8      4063     1     0     0\n#>  9      5231     1     0     0\n#> 10      5231     1     0     0\n#> # ... with 224,028 more rows\n\n\nIn retrospect, this join was pretty straightforward!\nThe rest of the code below is just doing the actual tallying.\n\nFirst, we make an intermediate data set grid_players, which is the Cartesian product of all possible cells in the grid and all players in events.\nSecond, we ‚Äúadd back‚Äù missing cells to events_binned using the intermediate data set grid_players.\n\nIn the end, we end up with a players data frame with 603 player_ids * 30 x bins * 20 y bins = 361,800 rows.\n\n\nCode\n# This `dummy` column approach is an easy way to do a Cartesian join when the two data frames don't share any column names.\ngrid_players <- grid_xy_yards %>% \n  mutate(dummy = 0L) %>% \n  # Cartesian join of all possible cells in the grid and all players in `events`.\n  full_join(\n    events %>% \n      drop_na() %>% \n      distinct(player_id) %>% \n      mutate(dummy = 0L),\n    by = 'dummy'\n  )\n\nplayers <- events_binned %>% \n  group_by(player_id, x, y, idx) %>% \n  summarize(n = n()) %>% \n  ungroup() %>% \n  # Rejoin back on the grid to 'add back' cells with empty counts (i.e. `n = 0`).\n  full_join(grid_players, by = c('player_id', 'x', 'y', 'idx')) %>% \n  select(-dummy, -next_x, -next_y) %>% \n  replace_na(list(n = 0L)) %>% \n  arrange(player_id, x, y)\nplayers\n#> # A tibble: 361,800 x 5\n#>    player_id     x      y   idx     n\n#>        <int> <dbl>  <dbl> <int> <int>\n#>  1      2941     0  0         1     0\n#>  2      2941     0  4.211     2     0\n#>  3      2941     0  8.421     3     0\n#>  4      2941     0 12.63      4     0\n#>  5      2941     0 16.84      5     0\n#>  6      2941     0 21.05      6     0\n#>  7      2941     0 25.26      7     0\n#>  8      2941     0 29.47      8     0\n#>  9      2941     0 33.68      9     0\n#> 10      2941     0 37.89     10     0\n#> # ... with 361,790 more rows\n\n\nTo make this a little bit more tangible, let‚Äôs plot Messi‚Äôs heatmap. (Is this really a blog post about soccer if it doesn‚Äôt mention Messi üòÜ?)"
  },
  {
    "objectID": "posts/decomposition-smoothing-soccer/index.html#non-negative-matrix-factorization-nnmf-with-reticulate-and-sklearn",
    "href": "posts/decomposition-smoothing-soccer/index.html#non-negative-matrix-factorization-nnmf-with-reticulate-and-sklearn",
    "title": "Decomposing and Smoothing Soccer Spatial Tendencies",
    "section": "Non-Negative Matrix Factorization (NNMF) with {reticulate} and sklearn",
    "text": "Non-Negative Matrix Factorization (NNMF) with {reticulate} and sklearn\nNext up is the actual NNMF calculation. I don‚Äôt care if you‚Äôre the biggest R stan in the world‚Äîyou have to admit that the python code to perform the NNMF is quite simple and (dare I say) elegant. The comps=30 here means\n\n\nCode\nfrom sklearn.decomposition import NMF\n\n# Flatten individual player matrices into shape=(600,) which is the product of the original shape components (30 by 20)\nunraveled = [np.matrix.flatten(v) for k, v in players.items()]\ncomps = 30\nmodel = NMF(n_components=comps, init='random', random_state=0)\nW = model.fit_transform(unraveled)\n\n\nMy understanding is that comps=30 is telling the algorithm to reduce our original data (with 603 players) to a lower dimensional space with 30 player ‚Äúarchetypes‚Äù that best represent the commonalities among the 603 players.7 Per Devin, the choice of 30 here is somewhat arbitrary. In practice, one might perform some cross validation to identify what number minimizes some loss function, but that‚Äôs beyond the scope of what we‚Äôre doing here.\nAfter re-formatting our players data into a wide format‚Äîequivalent to the numpy.matrix.flatten() call in the python code‚Äîwe could use the {NMF} package for an R replication.\n\n\nCode\n# Convert from tidy format to wide format (603 rows x 600 columns)\nplayers_mat <- players %>% \n  drop_na() %>% \n  select(player_id, idx, n) %>% \n  pivot_wider(names_from = idx, values_from = n) %>% \n  select(-player_id) %>% \n  as.matrix()\n\ncomps <- 30L\nW <- NMF::nmf(NMF::rmatrix(players_mat), rank = comps, seed = 0, method = 'Frobenius')\n\n\nHowever, I found that the results weren‚Äôt all that comparable to the python results. (Perhaps I needed to define the arguments in a different manner.) So why not use {reticulate} and call the sklearn.decomposition.NMF() function to make sure that we exactly emulate the python decomposition?\n\n\nCode\nsklearn <- reticulate::import('sklearn')\n# Won't work if `n_components` aren't explicitly defined as integers!\nmodel <- sklearn$decomposition$NMF(n_components = comps, init = 'random', random_state = 0L)\nW <- model$fit_transform(players_mat)\n\n\nThe result includes 30 20x30 matrices‚Äîone 30x20 x-y matrix for each of the 30 components (comps). We have some wrangling left to do to gain anything meaningful from this NNMF procedure, but we have something to work with!"
  },
  {
    "objectID": "posts/decomposition-smoothing-soccer/index.html#gaussian-smoothing-with-spatstat",
    "href": "posts/decomposition-smoothing-soccer/index.html#gaussian-smoothing-with-spatstat",
    "title": "Decomposing and Smoothing Soccer Spatial Tendencies",
    "section": "Gaussian Smoothing with {spatstat}",
    "text": "Gaussian Smoothing with {spatstat}\nThe last thing to do is to post-process the NNMF results and, of course, make pretty plots. The python plotting is pretty standard matplotlib, with the exception of the Gaussian smoothing performed on each component‚Äôs matrix model.component_ in the loop to make sub-plots.\n\n\nCode\nfrom scipy.ndimage import gaussian_filter\n\nfor i in range(9):\n    # ...\n    z = np.rot90(gaussian_filter(model.components_[i].reshape(x_scale, y_scale), sigma=1.5), 1)\n    # ...\n\n\nThe first 9 smoothed component matrices come out looking like this. 8\n\nThere‚Äôs a couple of steps involved to do the same thing in R.\n\nFirst, we‚Äôll convert the components matrices to a tidy format, decomp_tidy\nSecond, we‚Äôll join our tidied components matrices with our tidy grid of cells, grid_xy_yards, and convert our x and y bins to integers in preparation of the matrix operation performed in the subsequent step.\nLastly, we‚Äôll perform the Gaussian smoothing on nested data frames with a custom function, smoothen_dimension, that wraps spatstat::blur(). This function also maps idx back to field positions (in meters instead of yards) using the supplementary grid_xy_rev_m9 data frame (which is a lot like grid_xy_yards)\n\n\n\nCode\n## 1\ndecomp_tidy <- model$components_ %>% \n  as_tibble() %>% \n  # \"Un-tidy\" tibble with 30 rows (one for each dimension) and 600 columns (one for every `idx`)\n  mutate(dimension = row_number()) %>% \n  # Convert to a tidy tibble with dimensions * x * y rows (30 * 30 * 20 = 18,000)\n  pivot_longer(-dimension, names_to = 'idx', values_to = 'value') %>% \n  # The columns from the matrix are named `V1`, `V2`, ... `V600` by default, so convert them to an integer that can be joined on.\n  mutate(across(idx, ~str_remove(.x, '^V') %>% as.integer()))\n\n## 2\ndecomp <- decomp_tidy %>% \n  # Join on our grid of x-y pairs.\n  inner_join(\n    # Using `dense_rank` because we need indexes here (i.e. 1, 2, ..., 30 instead of 0, 4.1, 8.2, ..., 120 for `x`).\n    grid_xy_yards %>% \n      select(idx, x, y) %>% \n      mutate(across(c(x, y), dense_rank))\n  )\n\n## 3\nsmoothen_component <- function(.data, ...) {\n  mat <- .data %>% \n    select(x, y, value) %>% \n    pivot_wider(names_from = x, values_from = value) %>% \n    select(-y) %>% \n    as.matrix()\n  \n  mat_smoothed <- mat %>% \n    spatstat::as.im() %>% \n    # Pass `sigma` in here.\n    spatstat::blur(...) %>% \n    # Could use `spatstat::as.data.frame.im()`, but it converts directly to x,y,value triplet of columns, which is not the format I want.\n    pluck('v')\n  \n  res <- mat_smoothed %>% \n    # Convert 20x30 y-x matrix to tidy format with 20*30 rows.\n    as_tibble() %>% \n    mutate(y = row_number()) %>% \n    pivot_longer(-y, names_to = 'x', values_to = 'value') %>% \n    # The columns from the matrix are named `V1`, `V2`, ... `V30` by default, so convert them to an integer that can be joined on.\n    mutate(across(x, ~str_remove(.x, '^V') %>% as.integer())) %>% \n    arrange(x, y) %>% \n    # \"Re-index\" rows with `idx`, ranging from 1 to 600.\n    mutate(idx = row_number()) %>% \n    select(-x, -y) %>% \n    # Convert `x` and `y` indexes (i.e. 1, 2, 3, ..., to meters and flip the y-axis).\n    inner_join(grid_xy_rev_m) %>% \n    # Re-scale smoothed values to 0-1 range.\n    mutate(frac = (value - min(value)) / (max(value) - min(value))) %>% \n    ungroup()\n  res\n}\n\ndecomp_smooth <- decomp %>% \n  nest(data = -c(dimension)) %>% \n  # `sigma` passed into `...` of `smoothen_component()`. (`data` passed as first argument.)\n  mutate(data = map(data, smoothen_component, sigma = 1.5)) %>% \n  unnest(data)\ndecomp_smooth\n#> # A tibble: 18,000 x 8\n#>    dimension    value   idx     x     y next_y next_x     frac\n#>        <int>    <dbl> <int> <dbl> <dbl>  <dbl>  <dbl>    <dbl>\n#>  1         1 0.002191     1     0 68     4.211  4.138 0.004569\n#>  2         1 0.004843     2     0 64.42  8.421  4.138 0.01064 \n#>  3         1 0.008334     3     0 60.84 12.63   4.138 0.01863 \n#>  4         1 0.01130      4     0 57.26 16.84   4.138 0.02541 \n#>  5         1 0.01258      5     0 53.68 21.05   4.138 0.02834 \n#>  6         1 0.01208      6     0 50.11 25.26   4.138 0.02719 \n#>  7         1 0.01033      7     0 46.53 29.47   4.138 0.02319 \n#>  8         1 0.008165     8     0 42.95 33.68   4.138 0.01824 \n#>  9         1 0.006156     9     0 39.37 37.89   4.138 0.01364 \n#> 10         1 0.004425    10     0 35.79 42.11   4.138 0.009680\n#> # ... with 17,990 more rows\n\n\nWith the data in the proper format, the plotting is pretty straightforward {ggplot2} code.\n\nViola! I would say that our R version of the python plot is very comparable (just by visual inspection). Note that we could achieve a similar visual profile without the smoothing‚Äîsee below‚Äîbut the smoothing undoubtedly makes pattern detection a little less ambiguous.\n\nFrom the smoothed contours, we can discern several different player profiles (in terms of positioning).\n\nComponents 1, 5, 9: left back\nComponents 2: right midfielder\nComponent 3: attacking right midfielder\nComponent 4: wide left midfielder\nComponent 6: central left midfielder\nComponents 7, 8: goalkeeper\n\nThe redundancy with left back and goalkeeper is not ideal. That‚Äôs certainly something we could fine tune with more experimentation with components. Anyways, the point of this post wasn‚Äôt so much about the insights that could be gained (although that‚Äôs ultimately what stakeholders would be interested in if this were a ‚Äúreal‚Äù analysis)."
  },
  {
    "objectID": "posts/decomposition-smoothing-soccer/index.html#conclusion",
    "href": "posts/decomposition-smoothing-soccer/index.html#conclusion",
    "title": "Decomposing and Smoothing Soccer Spatial Tendencies",
    "section": "Conclusion",
    "text": "Conclusion\nTranslating python code can be challenging, throwing us off from our typical workflow (for me, being {tidyverse}-centric). But hopefully one can see the value in ‚Äúdoing whatever it takes‚Äù, even if it means using ‚Äúnon-tidy‚Äù R functions (e.g.¬†{data.table}, matrices, etc.) or a different language altogether."
  },
  {
    "objectID": "posts/dimensionality-reduction-and-clustering/index.html",
    "href": "posts/dimensionality-reduction-and-clustering/index.html",
    "title": "Tired: PCA + kmeans, Wired: UMAP + GMM",
    "section": "",
    "text": "Combining principal component analysis (PCA) and kmeans clustering seems to be a pretty popular 1-2 punch in data science. While there is some debate about whether combining dimensionality reduction and clustering is something we should ever do1, I‚Äôm not here to debate that. I‚Äôm here to illustrate the potential advantages of upgrading your PCA + kmeans workflow to Uniform Manifold Approximation and Projection (UMAP) + Gaussian Mixture Model (GMM), as noted in my reply here.\n\n\ntired: PCA + kmeanswired: UMAP + GMM\n\n‚Äî Tony (@TonyElHabr) June 2, 2021\n\n\nFor this demonstration, I‚Äôll be using this data set pointed out here, including over 100 stats for players from soccer‚Äôs ‚ÄúBig 5‚Äù leagues.\n\n\nCode\nlibrary(tidyverse)\nraw_df <- 'FBRef 2020-21 T5 League Data.xlsx' %>% \n  readxl::read_excel() %>% \n  janitor::clean_names() %>% \n  mutate(across(where(is.double), ~replace_na(.x, 0)))\n\n# Let's only use players with a 10 matches' worth of minutes.\ndf <- raw_df %>% filter(min > (10 * 90))\ndim(df)\n## [1] 1626  128\n\n\nTrying to infer something from the correlation matrix doesn‚Äôt get you very far, so one can see why dimensionality reduction will be useful.\n\nAlso, we don‚Äôt really have ‚Äúlabels‚Äù here (more on this later), so clustering can be useful for learning something from our data."
  },
  {
    "objectID": "posts/dimensionality-reduction-and-clustering/index.html#unsupervised-evaluation",
    "href": "posts/dimensionality-reduction-and-clustering/index.html#unsupervised-evaluation",
    "title": "Tired: PCA + kmeans, Wired: UMAP + GMM",
    "section": "Unsupervised Evaluation",
    "text": "Unsupervised Evaluation\nWe‚Äôll be feeding in the results from the dimensionality reduction‚Äîeither PCA or UMAP‚Äîto a clustering method‚Äîeither kmeans or GMM. So, since clustering comes last, all we need to do is figure out how to judge the clustering; this will tell us something about how ‚Äúgood‚Äù the combination of dimensionality reduction and clustering is overall.\nI‚Äôll save you from google-ing and just tell you that within-cluster sum of squares (WSS) is typically used for kmeans, and Bayesian Information Criteria (BIC) is the go-to metric for GMM. WSS and BIC are not on the same scale, so we can‚Äôt directly compare kmeans and GMM at this point. Nonetheless, we can experiment with different numbers of components‚Äîthe one major ‚Äúhyperparameter‚Äù for dimensionality reduction‚Äîprior to the clustering to identify if more or less components is ‚Äúbetter‚Äù, given the clustering method. Oh, and why not also vary the number of clusters‚Äîthe one notable hyperparameter for clustering‚Äîwhile we‚Äôre at it?\n\nFor kmeans, we see that WSS decreases with increasing number of clusters, which is typically what we see in ‚Äúelbow‚Äù plots like this. Additionally, we see that WSS decreases with increasing number of components. This makes sense‚Äîadditional components means more data is accounted for.2 There is definitely a point of ‚Äúdiminishing returns‚Äù, somewhere around 3 clusters, after which WSS barely improves.3 Overall, we observe that the kmeans models using UMAP pre-processing do better, compared to those using PCA.\nMoving on to GMM, we observe that BIC generally increases with the number of clusters as well. (Note that, due to the way the {mclust} package defines its objective function, higher BIC is ‚Äúbetter‚Äù.)\n\nRegarding number of components, we see that the GMM models using more UMAP components do better, as we should have expected. On the other hand, we observe that GMM models using less PCA components do better than those with more components. This is a bit of an odd finding that I don‚Äôt have a great explanation for. (Someone please math-splain to me.) Nonetheless, we see that UMAP does better than PCA overall, as we observed with kmeans.\nFor those interested in the code, I map-ed a function across a grid of parameters to generate the data for these plots.4\n\n\nCode\ndo_dimr_clust <- function(df, n, k, dimr, clust, ...) {\n  step_f <- switch(dimr, 'pca' = recipes::step_pca, 'umap' = embed::step_umap)\n  fit_f <- switch(clust, 'kmeans' = stats::kmeans, 'gmm' = mclust::Mclust)\n  \n  d <- recipes::recipe(formula( ~ .), data = df) %>%\n    recipes::step_normalize(recipes::all_numeric_predictors()) %>%\n    step_f(recipes::all_numeric_predictors(), num_comp = n) %>% \n    recipes::prep() %>% \n    recipes::juice() %>% \n    select(where(is.numeric))\n  fit <- fit_f(d, k, ...)\n  broom::glance(fit)\n}\n\nmetrics <- crossing(\n  n = seq.int(2, 8),\n  k = seq.int(2, 8),\n  f_dimr = c('pca', 'umap'),\n  f_clust = c('kmeans', 'mclust')\n) %>%\n  mutate(metrics = pmap(\n    list(n, k, f_dimr, f_clust),\n    ~ do_dimr_clust(\n      df = df,\n      n = ..1,\n      k = ..2,\n      f_dimr = ..3,\n      f_clust = ..4\n    )\n  ))\nmetrics\n#> # A tibble: 196 x 5\n#>        n     k f     g      metrics         \n#>    <int> <int> <chr> <chr>  <list>          \n#>  1     2     2 pca   kmeans <tibble [1 x 4]>\n#>  2     2     2 pca   gmm    <tibble [1 x 7]>\n#>  3     2     2 umap  kmeans <tibble [1 x 4]>\n#>  4     2     2 umap  gmm    <tibble [1 x 7]>\n#>  5     2     3 pca   kmeans <tibble [1 x 4]>\n#>  6     2     3 pca   gmm    <tibble [1 x 7]>\n#>  7     2     3 umap  kmeans <tibble [1 x 4]>\n#>  8     2     3 umap  gmm    <tibble [1 x 7]>\n#>  9     2     4 pca   kmeans <tibble [1 x 4]>\n#> 10     2     4 pca   gmm    <tibble [1 x 7]>\n#> # ... with 186 more rows"
  },
  {
    "objectID": "posts/dimensionality-reduction-and-clustering/index.html#supervised-evaluation",
    "href": "posts/dimensionality-reduction-and-clustering/index.html#supervised-evaluation",
    "title": "Tired: PCA + kmeans, Wired: UMAP + GMM",
    "section": "‚ÄúSupervised‚Äù Evaluation",
    "text": "‚ÄúSupervised‚Äù Evaluation\nWe actually do have something that we can use to help us identify clusters‚Äîplayer position (pos). Let‚Äôs treat these position groups as pseudo-labels with which we can gauge the effectiveness of the clustering.\n\n\nCode\ndf <- df %>% \n  mutate(\n    across(\n      pos,\n      ~case_when(\n        .x %in% c('DF,MF', 'MF,DF') ~ 'DM',\n        .x %in% c('DF,FW', 'FW,DF') ~ 'M',\n        .x %in% c('MF,FW', 'FW,MF') ~ 'AM',\n        .x == 'DF' ~ 'D',\n        .x == 'MF' ~ 'M',\n        .x == 'FW' ~ 'F',\n        .x == 'GK' ~ 'G',\n        .x == 'GK,MF' ~ 'G',\n        TRUE ~ .x\n      )\n    )\n  )\ndf %>% count(pos, sort = TRUE)\n#> # A tibble: 6 x 2\n#>   pos       n\n#>   <chr> <int>\n#> 1 D       595\n#> 2 M       364\n#> 3 AM      273\n#> 4 F       196\n#> 5 G       113\n#> 6 DM       85\n\n\nTypically we don‚Äôt have labels for clustering tasks; if we do, we‚Äôre usually doing some kind of supervised multi-label classification. But our labels aren‚Äôt ‚Äútrue‚Äù labels in this case, both because:\n\na player‚Äôs nominal position often doesn‚Äôt completely describe their style of play, and\nthe grouping I did to reduce the number of positions from 11 to 6 was perhaps not optimal.\n\nSo now let‚Äôs do the same as before‚Äîevaluate different combinations of PCA and UMAP with kmeans and GMM. But now we can use some supervised evaluation metrics: (1) accuracy and (2) mean log loss. While the former is based on the ‚Äúhard‚Äù predictions, the latter is based on probabilities for each class. kmeans returns just hard cluster assignments, so computing accuracy is straightforward; since it doesn‚Äôt return probabilities, we‚Äôll treat the hard assignments as having a probability of 1 to compute log loss.5\nWe can compare the two clustering methods more directly now using these two metrics. Since we know that there are 6 position groups, we‚Äôll keep the number of clusters constant at 6. (Note that number of clusters was shown on the x-axis before; but since we have fixed number of components at 6, now we show the number of components on the x-axis.)\nLooking at accuracy first, we see that the best combo depends on our choice for number of components. Overall, we might say that the UMAP combos are better.\n\nNext, looking at average log loss, we see that the GMM clustering methods seem to do better overall (although this may be due to the fact that log loss is not typically used for supervised kmeans). The PCA + GMM does the best across all number of components, with the exception of 7. Note that we get a mean log loss around 28 when we predict the majority class (defender) with a probability of 1 for all observations. (This is a good ‚Äúbaseline‚Äù to contextualize our numbers.)\n\nUMAP shines relative to PCA according to accuracy, and GMM beats out kmeans in terms of log loss. Despite these conclusions, we still don‚Äôt have clear evidence that UMAP + GMM is the best 1-2 combo; nonetheless, we can at least feel good about its general strength.\n\nAside: Re-coding Clusters\nI won‚Äôt bother to show all the code to generate the above plots since it‚Äôs mostly just broom::augmment() and {ggplot2}. But, if you have ever worked with supervised stuff like this (if we can call it that), you‚Äôll know that figuring out which of your clusters correspond to your known groups can be difficult. In this case, I started from a variable holding the predicted .class and the true class (pos).\n\n\nCode\nassignments\n#> # A tibble: 1,626 x 2\n#>    .class pos  \n#>     <int> <chr>\n#>  1      1 D    \n#>  2      2 D    \n#>  3      3 M    \n#>  4      3 M    \n#>  5      4 AM   \n#>  6      2 D    \n#>  7      2 D    \n#>  8      4 F    \n#>  9      2 D    \n#> 10      1 D    \n#> # ... with 1,616 more rows\n\n\nI generated a correlation matrix for these two columns, ready to pass into a matching procedure.\n\n\nCode\nassignments %>% \n  fastDummies::dummy_cols(c('.class', 'pos'), remove_selected_columns = TRUE) %>% \n  corrr::correlate(method = 'spearman', quiet = TRUE) %>% \n  filter(term %>% str_detect('pos')) %>% \n  select(term, matches('^[.]class'))\n#> # A tibble: 6 x 7\n#>   term   .class_1 .class_2 .class_3 .class_4 .class_5 .class_6\n#>   <chr>     <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>\n#> 1 pos_AM  -0.208   -0.241   -0.178    0.0251   0.625   -0.123 \n#> 2 pos_D    0.499    0.615   -0.335   -0.264   -0.428   -0.208 \n#> 3 pos_DM   0.0797   0.0330   0.0548  -0.0829  -0.0519  -0.0642\n#> 4 pos_F   -0.171   -0.199   -0.168    0.724    0.0232  -0.101 \n#> 5 pos_G   -0.127   -0.147   -0.124   -0.0964  -0.157    1     \n#> 6 pos_M   -0.222   -0.267    0.724   -0.180    0.0395  -0.147\n\n\nThen I used clue::solve_LSAP() to do the bipartite matching magic. The rest is just pre- and post-processing.\n\n\nCode\nk <- 6 # number of clusters\ncols_idx <- 2:(k+1)\ncors_mat <- as.matrix(cors[,cols_idx]) + 1 # all values have to be positive\nrownames(cors_mat) <- cors$term\ncols <- names(cors)[cols_idx]\ncolnames(cors_mat) <- cols\ncols_idx_min <- clue::solve_LSAP(cors_mat, maximum = TRUE)\ncols_min <- cols[cols_idx_min]\ntibble::tibble(\n  .class = cols_min %>% str_remove('^[.]class_') %>% as.integer(),\n  pos = cors$term %>% str_remove('pos_')\n)\n#> # A tibble: 6 x 2\n#>   .class pos  \n#>    <int> <chr>\n#> 1      5 AM   \n#> 2      2 D    \n#> 3      1 DM   \n#> 4      4 F    \n#> 5      6 G    \n#> 6      3 M \n\n\nThis pairs variable can be used to re-code the .class column in our assignments from before."
  },
  {
    "objectID": "posts/dimensionality-reduction-and-clustering/index.html#case-study-pca-vs.-umap",
    "href": "posts/dimensionality-reduction-and-clustering/index.html#case-study-pca-vs.-umap",
    "title": "Tired: PCA + kmeans, Wired: UMAP + GMM",
    "section": "Case Study: PCA vs.¬†UMAP",
    "text": "Case Study: PCA vs.¬†UMAP\nLet‚Äôs step back from the clustering techniques and focus on dimensionality reduction for a moment. One of the ways that dimensionality reduction can be leveraged in sports like soccer is for player similarity metrics.6 Let‚Äôs take a look at how this can be done, comparing the PCA and UMAP results while we‚Äôre at it.\nDirect comparison of the similarity ‚Äúscores‚Äù we‚Äôll compute‚Äîbased on Euclidean distance between a chosen player‚Äôs components and other players‚Äô components‚Äîis not wise given the different ranges of our PCA and UMAP components, so we‚Äôll rely on rankings based on these scores.7 Additionally, fbref provides a ‚Äúbaseline‚Äù that we can use to judge our similarity rankings.8\nWe‚Äôll start with Jadon Sancho, a highly discussed player at the moment (as a potential transfer).\n\nWe first need to set up our data into the following format. (This is for 2-component, 6-cluster UMAP + GMM.)\n\n\nCode\nsims_int\n#> # A tibble: 1,664 x 6\n#>    player_1     player_2           comp_1 comp_2 value_1 value_2\n#>    <chr>        <chr>               <int>  <int>   <dbl>   <dbl>\n#>  1 Jadon Sancho Aaron Leya Iseka        1      1  -4.18  -5.14  \n#>  2 Jadon Sancho Aaron Leya Iseka        2      2  -0.678  2.49  \n#>  3 Jadon Sancho Aaron Ramsey            1      1  -4.18  -3.25  \n#>  4 Jadon Sancho Aaron Ramsey            2      2  -0.678 -0.738 \n#>  5 Jadon Sancho Abdoul Kader Bamba      1      1  -4.18  -3.40  \n#>  6 Jadon Sancho Abdoul Kader Bamba      2      2  -0.678  0.0929\n#>  7 Jadon Sancho Abdoulaye Doucour√©      1      1  -4.18  -1.36  \n#>  8 Jadon Sancho Abdoulaye Doucour√©      2      2  -0.678 -2.66  \n#>  9 Jadon Sancho Abdoulaye Tour√©         1      1  -4.18  -1.36  \n#> 10 Jadon Sancho Abdoulaye Tour√©         2      2  -0.678 -2.89  \n#> # ... with 1,654 more rows\n\n\nThen the Euclidean distance calculation is fairly straightforward.\n\n\nCode\nsims_init %>% \n  group_by(player_1, player_2) %>% \n  summarize(\n    d = sqrt(sum((value_1 - value_2)^2))\n  ) %>% \n  ungroup() %>% \n  mutate(score = 1 - ((d - 0) / (max(d) - 0))) %>% \n  mutate(rnk = row_number(desc(score))) %>% \n  arrange(rnk) %>% \n  select(player = player_2, d, score, rnk)\n#> # A tibble: 830 x 4\n#>    player                  d score   rnk\n#>    <chr>               <dbl> <dbl> <int>\n#>  1 Alexis S√°nchez     0.0581 0.994     1\n#>  2 Riyad Mahrez       0.120  0.988     2\n#>  3 Serge Gnabry       0.132  0.986     3\n#>  4 Jack Grealish      0.137  0.986     4\n#>  5 Pablo Sarabia      0.171  0.983     5\n#>  6 Thomas M√ºller      0.214  0.978     6\n#>  7 Leroy San√©         0.223  0.977     7\n#>  8 Callum Hudson-Odoi 0.226  0.977     8\n#>  9 Jesse Lingard      0.260  0.973     9\n#> 10 Ousmane Demb√©l√©    0.263  0.973    10\n#> # ... with 820 more rows\n\n\nDoing the same for PCA and combining all results, we get the following set of rankings.\n\nWe see that the UMAP rankings are ‚Äúcloser‚Äù overall to the fbref rankings. Of course, there are some caveats:\n\nThis is just one player.\nThis is with a specific number of components and clusters.\nWe are comparing to similarity rankings based on a separate methodology.\n\nOur observation here (that UMAP > PCA) shouldn‚Äôt be taken out of context to conclude that UMAP > PCA in all contexts. Nonetheless, I think this is an interesting use case for dimensionality reduction, where one can justify PCA, UMAP, or any other similar technique, depending on how intuitive the results are."
  },
  {
    "objectID": "posts/dimensionality-reduction-and-clustering/index.html#case-study-umap-gmm",
    "href": "posts/dimensionality-reduction-and-clustering/index.html#case-study-umap-gmm",
    "title": "Tired: PCA + kmeans, Wired: UMAP + GMM",
    "section": "Case Study: UMAP + GMM",
    "text": "Case Study: UMAP + GMM\nFinally, let‚Äôs bring clustering back into the conversation. We‚Äôre going to focus on how the heralded UMAP + GMM combo can be visualized to provide insight that supports (or debunks) our prior understanding.\nWith a 2-component UMAP + 6-cluster GMM, we can see how the 6 position groups can be identified in a 2-D space.\n\nFor those curious, using PCA instead of UMAP also leads to an identifiable set of clusters. However, uncertainties are generally higher across the board (larger point sizes, more overlap between covariance ellipsoids).\n\nIf we exclude keepers (G) and defenders (D) to focus on the other 4 positions with our UMAP + GMM approach, we can better see how some individual points ‚Äîat the edges or outside of covariance ellipsoids‚Äîare classified with a higher degree of uncertainty.9\n\nNow, highlighting incorrect classifications, we can see how the defensive midfielder (DM) position group (upper left) seems to be a blind spot in our approach.\n\nA more traditional confusion matrix10 also illustrates the inaccuracy with classifying DMs. (Note the lack of dark grey fill in the DM column.)\n\nDMs are often classified as defenders instead. I think this poor result has is more so due to my lazy grouping of players with \"MF,DF\" or \"DF,MF\" positions in the original data set than a fault in our approach."
  },
  {
    "objectID": "posts/dimensionality-reduction-and-clustering/index.html#conclusion",
    "href": "posts/dimensionality-reduction-and-clustering/index.html#conclusion",
    "title": "Tired: PCA + kmeans, Wired: UMAP + GMM",
    "section": "Conclusion",
    "text": "Conclusion\nSo, should our overall conclusion be that we should never use PCA or kmeans? No, not necessarily. They can both be much faster to compute than UMAP and GMMs respectively, which can be a huge positive if computation is a concern. PCA is linear while UMAP is not, so you may want to choose PCA to make it easier to explain to your friends. Regarding clustering, kmeans is technically a specific form of a GMM, so if you want to sound cool to your friends and tell them that you use GMMs, you can do that.\nAnyways, I hope I‚Äôve shown why you should try out UMAP and GMM the next time you think about using PCA and kmeans."
  },
  {
    "objectID": "posts/epl-xpts-simulation-1/index.html",
    "href": "posts/epl-xpts-simulation-1/index.html",
    "title": "What exactly is an ‚Äúexpected point‚Äù? (part 1)",
    "section": "",
    "text": "Expected goals (xG) in soccer have gone mainstream and are no longer cool to talk about.\n\n\nWhat exactly is an ‚Äù expected goal ‚Äú? Who decides the criteria ? Is there a list of‚Äù expected goal scorers ‚Äù ? Or even ‚Äù unexpected ones ‚Äù ?\n\n‚Äî Ian Darke (@IanDarke) December 24, 2020\n\n\nSo let‚Äôs talk about expected points (xPts). The one sentence explainer for xPts: it‚Äôs a number between 0 and 3 assigned to each team in a match that we estimate from the xG of each shot in the match. Teams that accumulate more xG than their opponents in the match are more likely to have xPts closer to 3, i.e.¬†the points awarded for a win, and those that accumulate less than their opponents are more likely to earn xPts closer to 0. xPts is convenient for translating a team‚Äôs xG (relative to it‚Äôs opponents) to the team‚Äôs expected placement in the standings.\nWhile several outlets have described computing expected points with simulation1, simulation is actually not necessary if you have the xG for every shot taken in a match.2 For example, let‚Äôs say team A shoots six times with an xG of 0.1 for each shot, and team B shoots three shots with xG‚Äôs of 0.1, 0.2, and 0.3 respectively. Given these goal probabilities, we can analytically compute xPts as follows.\nFirst, we find the probability of scoring 0, 1, 2, etc. goals (up to the number of shots taken).3\n\n\nCode\nlibrary(poibin)\nxg_a &lt;- rep(0.1, 6)\nxg_b &lt;- c(0.1, 0.2, 0.3)\n\nprobs_a &lt;- dpoibin(seq.int(0, length(xg_a)), xg_a)\nround(probs_a, 2)\n#&gt; [1] 0.53 0.35 0.10 0.01 0.00 0.00 0.00\nprobs_b &lt;- dpoibin(seq.int(0, length(xg_b)), xg_b)\nround(probs_b, 2)\n#&gt; [1] 0.50 0.40 0.09 0.01\n\n\n\nSecond, we convert the goal probabilities to singular probabilities for each team winning the match, as well as the probability of a draw.4\n\n\nCode\nlibrary(gdata)\nouter_prod &lt;- outer(probs_a, probs_b)\np_a &lt;- sum(lowerTriangle(outer_prod))\np_b &lt;- sum(upperTriangle(outer_prod))\np_draw &lt;- sum(diag(outer_prod))\nround(c(p_a, p_b, p_draw), 2)\n#&gt; [1] 0.28 0.30 0.42\n\n\nFinally, given the match outcome probabilities, the xPts calculation is straightforward.\n\n\nCode\nxpts_a &lt;- 3 * p_a + 1 * p_draw\nxpts_b &lt;- 3 * p_b + 1 * p_draw\nround(c(xpts_a, xpts_b), 2)\n#&gt; [1] 1.27 1.31\n\n\nFor this example, we arrive at the interesting result that, despite the two teams total xG being equal (=0.6), team B has a slightly higher probability of winning. There have been plenty of explanations on this ‚Äúquality vs.¬†quantity‚Äù phenomenon, so I won‚Äôt go into it in detail. Nonetheless, this simple example illustrates why it can be useful to translate xG into another form‚Äîdoing so can provide a better perspective on match results and, consequently, team placement in the standings.\n\n\nSo we‚Äôve gone over what expected points are and why they‚Äôre important. Now we set out to do the following.\n\nCalculate xPts from shot xG for multiple seasons of data. We‚Äôll limit the scope to the 2020/21 and 2021/22 seasons for the English Premier League.5\nCompare the calibration of the understat and fotmob match outcome probabilities. {worldfootballR} makes it easy for us to get xG from both understat and fotmob, and it should be interesting to compare the the predictive performance of the two models.\nCompare predictions of actual season-long points using xPts that we derive from understat and fotmob xG. In particular, we‚Äôll be interested to see if our conclusions regarding the better source for xG here matches the conclusions for (2)."
  },
  {
    "objectID": "posts/epl-xpts-simulation-1/index.html#analysis",
    "href": "posts/epl-xpts-simulation-1/index.html#analysis",
    "title": "What exactly is an ‚Äúexpected point‚Äù? (part 1)",
    "section": "Analysis",
    "text": "Analysis\n\n1. Calculating xPts from xG\nLet‚Äôs start by using the load_understat_league_shots() function from {worldfootballR} to retrieve understat xG by shot.\n\n\nCode\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tibble)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(lubridate)\nlibrary(worldfootballR) ## version: 0.5.12.5000\nlibrary(janitor)\n\nrename_home_away_teams &lt;- function(df) {\n  df |&gt; \n    mutate(\n      team = ifelse(is_home, home_team, away_team),\n      opponent = ifelse(is_home, away_team, home_team)\n    ) |&gt; \n    select(-c(home_team, away_team)) \n}\n\nconvert_understat_year_to_season &lt;- function(x) {\n  sprintf('%s/%s', x, str_sub(x + 1, 3, 4))\n}\n\n## we'll use all of the shots later when exporing understat data only\nall_understat_shots &lt;- load_understat_league_shots('EPL') |&gt; \n  as_tibble() |&gt; \n  ## camelcase like \"xG\" is for Java scrubs\n  clean_names() |&gt; \n  filter(season &lt;= 2021) |&gt; \n  ## transmute = select + mutate\n  transmute(\n    match_id,\n    ## \"2021/2022\" format so that we have a clear, consistent way to represent season\n    across(season, convert_understat_year_to_season),\n    ## to convert \"2020-09-12 11:30:00\" to a date (\"2020-09-12\")\n    across(date, lubridate::date),\n    home_team,\n    away_team,\n    is_home = h_a == 'h',\n    xg = x_g\n  ) |&gt;\n  rename_home_away_teams() |&gt; \n  arrange(season, date, team)\n\n## but when comparing understat with fotmob, we'll need to limit the seasons to just\n##   those that both sources have\nunderstat_shots &lt;- all_understat_shots |&gt; filter(season &gt;= 2020)\n\n\nWe can use load_fotmob_match_details() to get fotmob‚Äôs shot xG in a similar fashion.6\n\n\nCode\n## manually created CSV with at least 2 columns: team_understat, team_fotmob.\n##   use the team_understat name to be consistent across sources.\nteam_mapping &lt;- 'https://raw.githubusercontent.com/tonyelhabr/sports_viz/master/59-xg_xpoints/team_mapping.csv' |&gt; \n  read_csv()\n\nrename_fotmob_teams &lt;- function(df) {\n  df |&gt; \n    left_join(\n      team_mapping |&gt; select(team_understat, team_fotmob),\n      by = c('home_team' = 'team_fotmob')\n    ) |&gt; \n    select(-home_team) |&gt; \n    rename(home_team = team_understat) |&gt; \n    left_join(\n      team_mapping |&gt; select(team_understat, team_fotmob),\n      by = c('away_team' = 'team_fotmob')\n    ) |&gt; \n    select(-away_team) |&gt; \n    rename(away_team = team_understat)\n}\n\nfotmob_shots &lt;- load_fotmob_match_details(\n  country = 'ENG',\n  league_name = 'Premier League'\n) |&gt; \n  mutate(\n    ## to convert strings from 'Sat, Sep 12, 2020, 11:30 UTC' to a date\n    date = strptime(match_time_utc, '%a, %b %d, %Y, %H:%M UTC', tz = 'UTC') |&gt; date(),\n    ## fotmob's parent_league_season always reflects the current season, so we need to manually\n    ##   define the season from the date. we would certainly want a more automated approach\n    ##   if working with more seasons and more leagues.\n    season = case_when(\n      date &gt;= ymd('2020-09-12') & date &lt;= ymd('2021-05-23') ~ '2020/21',\n      date &gt;= ymd('2021-08-13') & date &lt;= ymd('2022-05-22') ~ '2021/22',\n      TRUE ~ NA_character_\n    )\n  ) |&gt; \n  ## the NAs are for 2022/2023 (incomplete as of writing) and the partial data for 2019/2020\n  drop_na(season) |&gt; \n  transmute(\n    match_id,\n    season,\n    date,\n    home_team,\n    away_team,\n    is_home = team_id == home_team_id,\n    ## some shots with NAs for some reason\n    xg = coalesce(expected_goals, 0)\n  ) |&gt;\n  rename_fotmob_teams() |&gt; \n  rename_home_away_teams() |&gt; \n  arrange(season, date, team)\n\n\nAlright, now the fun part. We functionalize the code from the example for calculating the probability that xG will result in 0, 1, 2, etc. goals.\n\n\nCode\nlibrary(purrr)\npermute_xg &lt;- function(xg) {\n  n &lt;- length(xg)\n  x &lt;- seq.int(0, n)\n  dpoibin(x, xg)\n}\n\ncalculate_permuted_xg &lt;- function(df) {\n  df |&gt; \n    group_by(across(c(everything(), -xg))) |&gt; \n    summarize(across(xg, ~list(.x))) |&gt; \n    mutate(\n      prob = map(xg, ~permute_xg(.x))\n    ) |&gt; \n    select(-c(xg)) |&gt; \n    unnest(cols = c(prob)) |&gt; \n    group_by(across(-c(prob))) |&gt;\n    mutate(\n      g = row_number() - 1L\n    ) |&gt;\n    ungroup() |&gt; \n    arrange(match_id, is_home, g)\n}\n\nunderstat_permuted_xg &lt;- understat_shots |&gt; calculate_permuted_xg()\nfotmob_permuted_xg &lt;- fotmob_shots |&gt; calculate_permuted_xg()\n\n\nNext, we identify all possible goal combinations using xG as ‚Äúweights‚Äù to compute the relative likelihood of each combination, and then analytically calculate the probabilities of winning, losing, and drawing.\n\n\nCode\nsummarize_pivoted_permuted_xg &lt;- function(prob_away, prob_home) {\n  outer_prod &lt;- outer(prob_away, prob_home)\n  p_draw &lt;- sum(diag(outer_prod), na.rm = TRUE)\n  p_home &lt;- sum(upperTriangle(outer_prod), na.rm = TRUE)\n  p_away &lt;- sum(lowerTriangle(outer_prod), na.rm = TRUE)\n  list(\n    draw = p_draw,\n    home = p_home,\n    away = p_away\n  )\n}\n\n## Bournemouth 0 - 1 Manchester City on 2019-03-02\n## Huddersfield 0 - 0 Swansea on 2018-03-10\npad_for_matches_without_shots_from_one_team &lt;- function(df) {\n  n_teams_per_match &lt;- df |&gt; \n    distinct(match_id, team) |&gt; \n    count(match_id, sort = TRUE)\n  \n  matches_with_no_shots_from_one_team &lt;- n_teams_per_match |&gt; \n    filter(n == 1)\n  \n  dummy_opponents &lt;- df |&gt; \n    distinct(match_id, season, date, team, opponent, is_home) |&gt; \n    semi_join(\n      matches_with_no_shots_from_one_team,\n      by = 'match_id'\n    ) |&gt; \n    mutate(\n      z = team\n    ) |&gt; \n    transmute(\n      match_id, \n      season, \n      date, \n      team = opponent,\n      opponent = z,\n      across(is_home, ~!.x),\n      prob = 1,\n      g = 0L\n    )\n  \n  bind_rows(\n    df,\n    dummy_opponents\n  ) |&gt; \n  arrange(season, date, team, g)\n}\n\nsummarize_permuted_xg_by_match &lt;- function(df) {\n  \n  padded_df &lt;- pad_for_matches_without_shots_from_one_team(df)\n  \n  pivoted &lt;- padded_df |&gt;\n    transmute(\n      match_id,\n      season,\n      date,\n      g,\n      is_home = ifelse(is_home, 'home', 'away'),\n      prob\n    ) |&gt;\n    pivot_wider(\n      names_from = is_home,\n      names_prefix = 'prob_',\n      values_from = prob,\n      values_fill = 0L\n    )\n  \n  pivoted |&gt; \n    select(match_id, season, date, prob_away, prob_home) |&gt;\n    group_by(match_id, season, date) |&gt; \n    summarize(\n      across(starts_with('prob_'), ~list(.x))\n    ) |&gt; \n    ungroup() |&gt; \n    inner_join(\n      padded_df |&gt; distinct(match_id, team, opponent, is_home),\n      by = 'match_id'\n    ) |&gt; \n    mutate(\n      prob = map2(prob_away, prob_home, summarize_pivoted_permuted_xg)\n    ) |&gt; \n    select(-starts_with('prob_')) |&gt; \n    unnest_wider(prob, names_sep = '_') |&gt; \n    mutate(\n      prob_win = ifelse(is_home, prob_home, prob_away),\n      prob_lose = ifelse(is_home, prob_away, prob_home),\n      xpts = 3 * prob_win + 1 * prob_draw\n    ) |&gt; \n    select(-c(prob_home, prob_away))\n}\n\nunderstat_xpts_by_match &lt;- understat_permuted_xg |&gt; summarize_permuted_xg_by_match()\nfotmob_xpts_by_match &lt;- fotmob_permuted_xg |&gt; summarize_permuted_xg_by_match()\n\n\nLet‚Äôs take a quick peak at the distributions of xG and xPts, both as a sanity check and to enhance our understanding of the relationship between the two. When plotting xPts as a function xG, we should expect to see a monotonically increasing relationship where xPts bottoms out at zero and tops out at three.\n\nFurther, if there is any doubt about the expected points calculation, note that understat offers xPts directly in their data. The mean absolute error of our calculation of xPts with theirs is ~0.02.\n\n\nCode\nlibrary(understatr)\n\nall_raw_understat_xpts_by_match &lt;- 2014:2021 |&gt; \n  set_names() |&gt; \n  map_dfr(\n    ~get_league_teams_stats('EPL', .x),\n    .id = 'season'\n  ) |&gt; \n  transmute(\n    across(season, ~convert_understat_year_to_season(as.integer(.x))),\n    date,\n    team = team_name,\n    result,\n    pts,\n    raw_xpts = xpts,\n    xg = xG\n  )\n\nraw_understat_xpts_by_match &lt;- all_raw_understat_xpts_by_match |&gt; \n  inner_join(\n    understat_xpts_by_match |&gt; select(season, date, team, xpts),\n    by = c('season', 'date', 'team')\n  ) |&gt; \n  mutate(\n    xptsd = raw_xpts - xpts\n  ) |&gt; \n  arrange(season, date, team)\n\n## mean absolute error\nround(mean(abs(raw_understat_xpts_by_match$xptsd)), 2)\n#&gt; [1] 0.02\n\n\n\n\n2. Match predictive performance7\nAs one might guess, the match outcome probabilities implied by the xG from understat and fotmob are strongly correlated.\n\n\nCode\nrename_xpts_by_match &lt;- function(df, src) {\n  df |&gt; \n    select(season, date, team, starts_with('prob_'), xpts) |&gt; \n    rename_with(\n      ~sprintf('%s_%s', .x, src), c(starts_with('prob_'), xpts)\n    )\n}\n\nxpts_by_match &lt;- raw_understat_xpts_by_match |&gt; \n  select(season, date, team, result, pts) |&gt; \n  inner_join(\n    understat_xpts_by_match |&gt; rename_xpts_by_match('understat'),\n    by = c('season', 'date', 'team')\n  ) |&gt; \n  inner_join(\n    fotmob_xpts_by_match |&gt; rename_xpts_by_match('fotmob'),\n    by = c('season', 'date', 'team')\n  )\n\ncor_draw &lt;- cor(xpts_by_match$prob_draw_fotmob, xpts_by_match$prob_draw_understat)\ncor_win &lt;- cor(xpts_by_match$prob_win_fotmob, xpts_by_match$prob_win_understat)\ncor_lose &lt;- cor(xpts_by_match$prob_lose_fotmob, xpts_by_match$prob_lose_understat)\nround(c(cor_draw, cor_win, cor_lose), 3)\n#&gt; [1] 0.906 0.958 0.958\n\n\nNote that the win and loss correlations are identical. This is due to the symmetric nature of the data‚Äîwe have two records for each match, one from each team‚Äôs perspective.8\n\nPredicting match outcomes with binary logistic regression\nNow let‚Äôs compare how ‚Äúgood‚Äù the implied probabilities from the two sources are. To do this, we‚Äôll create binary logistic regression models to predict a given outcome and compute:\n\nthe mean squared error (MSE);\nthe brier skill score (BSS), treating the empirical proportion of the specified outcome as the reference.910\na calibration plot, grouping predictions into ‚Äúbuckets‚Äù at every 5%.\n\n\n\nCode\nresult_props &lt;- xpts_by_match |&gt; \n  count(result) |&gt; \n  mutate(prop = n / sum(n))\n\ncompute_mse &lt;- function(truth, estimate) {\n  mean((truth - estimate)^2)\n}\n\ndiagnose_prob_by_match &lt;- function(src, result) {\n  \n  df &lt;- xpts_by_match |&gt; \n    mutate(\n      result = ifelse(result == !!result, 1L, 0L) |&gt; factor()\n    )\n  \n  result_name &lt;- switch(\n    result,\n    'w' = 'win',\n    'l' = 'lose',\n    'd' = 'draw'\n  )\n  col &lt;- sprintf('prob_%s_%s', result_name, src)\n  \n  fit &lt;- glm(\n    df$result ~ df[[col]],\n    family = 'binomial'\n  )\n  \n  probs &lt;- tibble(\n    result_num = as.numeric(df$result) - 1,\n    .prob = unname(predict(fit, type = 'response'))\n  )\n  \n  n_buckets &lt;- 20\n  alpha &lt;- 0.05\n  calib &lt;- probs |&gt;\n    mutate(\n      across(.prob, ~round(.x * n_buckets) / n_buckets)\n    ) |&gt;\n    group_by(.prob) |&gt;\n    summarize(\n      ## Jeffreys' prior\n      ci_lower = qbeta(alpha / 2, sum(result_num) + 0.5, n() - sum(result_num) + 0.5),\n      ci_upper = qbeta(1 - alpha / 2, sum(result_num) + 0.5, n() - sum(result_num) + 0.5),\n      actual = sum(result_num) / n(),\n      n = n()\n    ) |&gt; \n    ungroup()\n  \n  mse &lt;- compute_mse(probs$result_num, probs$.prob)\n  \n  ref_prob &lt;- result_props |&gt; \n    filter(result == !!result) |&gt; \n    pull(prop)\n  \n  ref_mse &lt;- compute_mse(probs$result_num, ref_prob)\n  bss &lt;- 1 - (mse / ref_mse)\n  \n  list(\n    calib = calib,\n    mse = mse,\n    bss = bss\n  )\n}\n\ndiagnostics &lt;- crossing(\n  result = c('w', 'd'),\n  src = c('understat', 'fotmob')\n) |&gt; \n  mutate(\n    diagnostics = map2(src, result, diagnose_prob_by_match)\n  ) |&gt; \n  unnest_wider(diagnostics)\ndiagnostics |&gt; select(-calib)\n#&gt; # A tibble: 4 √ó 4\n#&gt;   result src         mse    bss\n#&gt;   &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 d      fotmob    0.170 0.0268\n#&gt; 2 d      understat 0.166 0.0466\n#&gt; 3 w      fotmob    0.173 0.270 \n#&gt; 4 w      understat 0.162 0.317\n\n\nThe MSE (where lower is ‚Äúbetter‚Äù) and the BSS (where higher is ‚Äúbetter‚Äù) lead us to the same conclusion‚Äîthe models based on understat‚Äôs xG slightly outperform the one based on fotmob‚Äôs xG.\nMoreover, looking at the calibration plot, the understat model predictions seem to stick closer to the 45 degree slope representing perfect calibration.\n\n\n\nPredicting points with linear regression\nAlternatively, we could regress points on expected points. For linear regression, we can use the root mean squared error (RMSE) (where lower is ‚Äúbetter‚Äù) and R squared (where higher is ‚Äúbetter‚Äù) to compare the models.\n\n\nCode\ncompute_rmse &lt;- function(truth, estimate) {\n  sqrt(mean((truth - estimate)^2))\n}\n\ndiagnose_xpts_by_match &lt;- function(src) {\n  \n  col &lt;- sprintf('xpts_%s', src)\n  fit &lt;- lm(xpts_by_match$pts ~ xpts_by_match[[col]])\n  \n  pred &lt;- predict(fit)\n  \n  tibble(\n    rmse = compute_rmse(xpts_by_match$pts, pred),\n    r2 = summary(fit)$r.squared\n  )\n}\n\nc('understat', 'fotmob') |&gt; \n  set_names() |&gt; \n  map_dfr(diagnose_xpts_by_match, .id = 'src')\n#&gt; # A tibble: 2 √ó 3\n#&gt;   src        rmse    r2\n#&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 understat  1.06 0.374\n#&gt; 2 fotmob     1.10 0.323\n\n\nThe understat model proves to be better by both metrics, having a lower RMSE and higher R squared than the fotmob model.\n\n\nPredicting match outcomes with multinomial logistic regression\nPersonally, I don‚Äôt like predicting points directly like this since it‚Äôs a discrete variable that can only take on three values (0, 1, and 3). If we‚Äôre going to predict points instead of a probability, I think the better approach is to run a multinomial logistic regression and to convert the predicted probabilities to expected points.\n\n\nCode\nlibrary(nnet)\ndiagnose_implied_xpts_by_match &lt;- function(src) {\n  \n  col_win &lt;- sprintf('prob_win_%s', src)\n  col_draw &lt;- sprintf('prob_draw_%s', src)\n  fit &lt;- multinom(\n    xpts_by_match$result ~ xpts_by_match[[col_win]] + xpts_by_match[[col_draw]],\n    trace = FALSE\n  )\n  probs &lt;- predict(fit, type = 'probs') |&gt; as_tibble()\n  preds &lt;- 3 * probs$w + 1 * probs$d\n  \n  tibble(\n    rmse = compute_rmse(xpts_by_match$pts, preds),\n    r2 = cor(xpts_by_match$pts, preds)^2\n  )\n}\n\nc('understat', 'fotmob') |&gt; \n  set_names() |&gt; \n  map_dfr(diagnose_implied_xpts_by_match, .id = 'src')\n#&gt; # A tibble: 2 √ó 3\n#&gt;   src        rmse    r2\n#&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 understat  1.06 0.374\n#&gt; 2 fotmob     1.10 0.321\n\n\nAgain, we see that understat has a lower RMSE and higher R squared. The implication that understat performs slightly better than fotmob agrees with the results from the binary logistic regression approiach for predicting match outcome probabilities and the linear regression approach for predicting points.\nOverall, we might say that understat seems to be the better of the two xG sources for explaining individual match results, although the margin is small enough that I would hesitate to say that this is the true across all leagues and all seasons.\n\n\n\n3. Season predictive performance\nHow do the understat and fotmob models fare if we aggregate up the expected points to the season level and predict actual points?11\n\n\nCode\nxpts_by_season &lt;- xpts_by_match |&gt; \n  group_by(season, team) |&gt; \n  summarize(\n    across(c(pts, starts_with('xpts')), sum)\n  ) |&gt; \n  ungroup()\n\ndiagnose_xpts_by_season &lt;- function(src) {\n  \n  col &lt;- sprintf('xpts_%s', src)\n  fit &lt;- lm(xpts_by_season$pts ~ xpts_by_season[[col]])\n  \n  preds &lt;- predict(fit)\n  \n  tibble(\n    rmse = compute_rmse(xpts_by_match$pts, preds),\n    r2 = summary(fit)$r.squared\n  )\n}\n\nc('understat', 'fotmob') |&gt; \n  set_names() |&gt; \n  map_dfr(diagnose_xpts_by_season, .id = 'src')\n#&gt; # A tibble: 2 √ó 3\n#&gt;   src        rmse    r2\n#&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 understat  53.9 0.845\n#&gt; 2 fotmob     53.8 0.825\n\n\nThe results are closer than those at the match-level. In fact, fotmob just barely edges out understat in terms of RMSE xPts, although understat outperforms fotmob according to R squared by a relatively comfortable 0.02. It‚Äôs harder to make a general statement regarding which data source provides better xG for explaining season-long expected points, although we might lean in favor of understat again."
  },
  {
    "objectID": "posts/epl-xpts-simulation-1/index.html#conclusion",
    "href": "posts/epl-xpts-simulation-1/index.html#conclusion",
    "title": "What exactly is an ‚Äúexpected point‚Äù? (part 1)",
    "section": "Conclusion",
    "text": "Conclusion\nOverall, we find that understat‚Äôs xG model seems to very slightly outperform fotmob‚Äôs in terms of explaining match results and season-long point totals.\nIn a follow up post, we‚Äôll go more in depth regarding how we can leverage the match outcome probabilities to simulate season-ending points in a more rigorous fashion that done in the last section above."
  },
  {
    "objectID": "posts/epl-xpts-simulation-2/index.html",
    "href": "posts/epl-xpts-simulation-2/index.html",
    "title": "What exactly is an ‚Äúexpected point‚Äù? (part 2)",
    "section": "",
    "text": "I‚Äôll be picking up where I left off in my last post, so stop everything that you‚Äôre doing and go read that if you haven‚Äôt already. In this post we‚Äôll do two things:\n\nWe‚Äôll compare how well season-level expected goal difference (xGD), season-level xPts, and aggregated match-level xPts predict season-long points for a given team.\nWe‚Äôll use the match-level probabilites to answer the questions ‚ÄúWhich teams had the most unlikely placings in the table given the quality of all their shots across the season?‚Äù and ‚ÄúHow unlikely were such placings?‚Äù"
  },
  {
    "objectID": "posts/epl-xpts-simulation-2/index.html#analysis",
    "href": "posts/epl-xpts-simulation-2/index.html#analysis",
    "title": "What exactly is an ‚Äúexpected point‚Äù? (part 2)",
    "section": "Analysis",
    "text": "Analysis\nBefore we start, we need to step back and retrieve data on the actual placings. We could theoretically calculate this from the shot data we already have. However, the logic for handling own goals is a little complicated. We‚Äôre probably better off using worldfootballR::understat_league_match_results()‚Äîwhich returns goals at the match-level‚Äîto calculate the table. 1\n\n\nCode\nmatch_results <- 2014:2021 |> \n  map_dfr(~understat_league_match_results('EPL', .x)) |> \n  as_tibble()\n\ninit_table <- match_results |> \n  transmute(\n    match_id,\n    across(season, ~str_replace(.x, '\\\\/20', '/')),\n    date = strptime(datetime, '%Y-%m-%d %H:%M:%S', tz = 'UTC') |> date(),\n    home_team,\n    home_goals,\n    away_team,\n    away_goals\n  )\n\ntable <- bind_rows(\n  init_table |>\n    mutate(is_home = TRUE) |> \n    rename_home_away_teams() |> \n    select(season, team, opponent, goals = home_goals, opponent_goals = away_goals),\n  init_table |> \n    mutate(is_home = FALSE) |> \n    rename_home_away_teams() |> \n    select(season, team, opponent, goals = away_goals,opponent_goals = home_goals)\n) |> \n  mutate(\n    pts = case_when(\n      goals > opponent_goals ~ 3L,\n      goals < opponent_goals ~ 0L,\n      TRUE ~ 1L\n    )\n  ) |> \n  group_by(season, team) |> \n  summarize(across(c(goals, opponent_goals, pts), sum)) |> \n  ungroup() |> \n  mutate(gd = goals - opponent_goals) |> \n  arrange(season, desc(pts), desc(gd)) |> \n  group_by(season) |> \n  mutate(rank = row_number(desc(pts))) |> \n  ungroup() |> \n  arrange(season, rank)\n\n\nThis data will help us contextualize predicted placings with actual placings.\n\n1. Predicting season-long points\n\nWith season-long xPts and xGD\nWe start with the all_raw_understat_xpts_by_match variable from the prior post, adding the opponent‚Äôs expected goals to create a column for expected goal difference (xgd).\n\n\nCode\nall_raw_understat_xpts_by_match_with_opponent <- all_raw_understat_xpts_by_match |> \n  inner_join(\n    all_understat_shots |> distinct(match_id, season, date, team, opponent),\n    by = c('season', 'date', 'team')\n  )\n\n## we've already determined that the raw_xpts (provided directly by understat) \n##   is close to our calculated xpts, so we'll just use the raw_xpts.\nall_raw_understat_xpts_xgd_by_match <- all_raw_understat_xpts_by_match_with_opponent |> \n  select(match_id, season, date, team, opponent, pts, xpts = raw_xpts, xg) |> \n  inner_join(\n    all_raw_understat_xpts_by_match_with_opponent |> \n      select(match_id, opponent = team, opponent_xg = xg),\n    by = c('match_id', 'opponent')\n  ) |> \n  mutate(xgd = xg - opponent_xg)\n\n\nNext, we aggregate up to the season-level.\n\n\nCode\nall_raw_understat_xpts_xgd_by_season <- all_raw_understat_xpts_xgd_by_match |> \n  group_by(season, team) |> \n  summarize(across(c(pts, xpts, xgd), sum)) |> \n  ungroup() |> \n  group_by(season) |> \n  mutate(xrank = row_number(desc(xpts))) |> \n  ungroup() |> \n  arrange(season, desc(pts), team)\n\n\nFinally, we compute RMSE and R squared, like we did in the last post.\n\n\nCode\ndiagnose_season_feature <- function(df, col) {\n  fit <- lm(df$pts ~ df[[col]])\n  tibble(\n    rmse = compute_rmse(df$pts, predict(fit)),\n    r2 = summary(fit)$r.squared\n  )\n}\n\nc('xgd', 'xpts') |> \n  set_names() |> \n  map_dfr(\n    ~diagnose_season_feature(all_raw_understat_xpts_xgd_by_season, .x), \n    .id = 'feature'\n  )\n#> # A tibble: 2 √ó 3\n#>   feature  rmse    r2\n#>   <chr>   <dbl> <dbl>\n#> 1 xgd      7.33 0.831\n#> 2 xpts     7.20 0.837\n\n\nAs we should expect, a model using season-long xPts to predict final points outperforms one using season-long xGD as a feature, although maybe the difference between the two is smaller than we might have expected.\n\n\nWith match-level outcome probabilities\nFirst, we use the full understat shot data set and the custom functions from the prior post to calculate xPts by match.\n\n\nCode\nall_understat_xpts_by_match <- all_understat_shots |> \n  calculate_permuted_xg() |> \n  summarize_permuted_xg_by_match()\n\n\nNext, the fun part: simulating match outcomes using the xG-implied match outcome probabilities. This is computationally intense, so we parallelize the calculation.\n\n\nCode\nlibrary(parallel)\nlibrary(future)\nlibrary(furrr)\n\nunderstat_probs_by_match <- all_understat_xpts_by_match |> \n  select(match_id, season, team, opponent, is_home, starts_with('prob')) |> \n  rename_with(~str_remove(.x, '^prob_'), starts_with('prob')) |> \n  pivot_longer(\n    c(win, lose, draw),\n    names_to = 'result',\n    values_to = 'prob'\n  )\n\nsimulate_season_xpts <- function(...) {\n  sim_home_pts_by_match <- understat_probs_by_match |> \n    filter(is_home) |> \n    group_by(team, season, match_id) |> \n    slice_sample(n = 1, weight_by = prob) |> \n    ungroup() |>\n    mutate(\n      pts = case_when(\n        result == 'win' ~ 3L,\n        result == 'lose' ~ 0L,\n        TRUE ~ 1L\n      )\n    )\n  \n  sim_pts_by_match <- bind_rows(\n    sim_home_pts_by_match |> select(match_id, season, team, pts),\n    sim_home_pts_by_match |> \n      transmute(\n        match_id,\n        season,\n        team = opponent,\n        pts = case_when(\n          result == 'win' ~ 0L,\n          result == 'lose' ~ 3L,\n          TRUE ~ 1L\n        )\n      )\n  ) |> \n    group_by(season, team) |> \n    summarize(across(pts, sum)) |> \n    ungroup()\n  \n  sim_pts_by_match |> \n    group_by(season, team) |> \n    summarize(across(pts, sum)) |> \n    ungroup() |> \n    group_by(season) |> \n    mutate(rank = row_number(desc(pts))) |> \n    ungroup() |> \n    arrange(season, rank)\n}\n\nn_cores <- detectCores()\ncores_for_parallel <- ceiling(n_cores * 0.5)\nplan(\n  multisession,\n  workers = cores_for_parallel\n)\n\n## set seed both prior to the future_map_dfr and in .options to guarantee determinstic results\nset.seed(42)\nn_sims <- 10000\nunderstat_sim_pts_by_season <- set_names(1:n_sims) |> \n  future_map_dfr(\n    simulate_season_xpts, \n    .id = 'sim_idx', \n    .options = furrr_options(seed = 42)\n  )\n\n## back to normal processing\nplan(sequential)\n\n\nNext, we aggregate the season-long points across simulations, calculating the relative proportion of simulations in which a given team ends up at a given rank.\n\n\nCode\nunderstat_sim_placings <- understat_sim_pts_by_season |> \n  group_by(season, team, xrank = rank) |> \n  summarize(n = n(), xpts = mean(pts)) |> \n  ungroup() |> \n  group_by(season, team) |> \n  mutate(prop = n / sum(n)) |> \n  ungroup()\n\n\nFinally, we calculate the weighted average of expected points that a team ends up with, and run the same regression that we ran earlier with season-long xPts and xGD.\n\n\nCode\nunderstat_sim_placings_agg <- understat_sim_placings |> \n  group_by(season, team) |> \n  summarize(xpts = sum(xpts * prop)) |> \n  ungroup() |>\n  select(season, team, xpts) |>\n  inner_join(\n    all_raw_understat_xpts_xgd_by_season |> select(season, team, pts),\n    by = c('season', 'team')\n  ) |> \n  arrange(season, desc(xpts))\n\ndiagnose_season_feature(understat_sim_placings_agg, 'xpts')\n#> # A tibble: 1 √ó 2\n#>    rmse    r2\n#>   <dbl> <dbl>\n#> 1  7.16 0.839\n\n\nInterestingly, the RMSE and R squared values are almost identical to those for the season-long xPts. Perhaps this is not too surprising‚Äîmatch-level outcome probabilities simulated and averaged to arrive at a singular estimate of season-long xPts should give us something very close to just computing season-long xPts directly.\nWhile the null result may be discouraging, the simulations are useful in and of themselves. They can be used to understand the distribution of outcomes for team in a given season, as seen in the table below.2\n\nWe can see that Leicester, Wolves, and Newcastle all placed at the uppper end of their simulated placings, indicating that they over-achieved relative to expectation; on the other hand, Crystal Palace, Brentford, and Leeds placed on the lower end of the distribution of placings, indicating that they under-achieved.\nIn fact, we can go beyond simple observational judgement of whether teams over- and under-achieved‚Äîwe can use the relative proportion of simulations where a team ends up at a given placing (or ‚Äúrank‚Äù) in the standings to quantify just how unexpected actual end-of-season placings were.\n\n\n\n2. Identifying un-expected placings\nFirst, we join the table of end-of-season placements (actual_rank) to the simulation results that describe the frequency with which a given team places at a given rank (xrank).\n\n\nCode\nunderstat_sim_placings_with_actual_ranks <- understat_sim_placings |> \n  inner_join(\n    table |> select(season, team, actual_pts = pts, actual_rank = rank),\n    by = c('season', 'team')\n  ) |> \n  inner_join(\n    all_raw_understat_xpts_xgd_by_season |> \n      select(season, team, actual_xpts = xpts, xgd),\n    by = c('season', 'team')\n  )\n\n\nFinally, to identify over-achieving teams, we find the teams that had the lowest cumulative probability of placing at their actual placing or better; and to identify under-achieving teams, we find the teams with the lowest cumulative probability of placing at their actual placing or worse.\n\nThis table certainly passes the eye test. Brighton‚Äôs sixteenth place finish in the 2020/21 season was discussed ad nauseum in the analytics sphere. Brighton under-performed historically given their massively positive xGD.\nOn the other end of the spectrum, it‚Äôs not hyperbole to say that Manchester United‚Äôs second place finish in the 2017/18 season was an over-achievement. Although they ended up with the third best goal differential that season, they were closely followed by several teams. And their xGD was sixth in the league that season.\n\nComparison with a simpler approach\nNotably, we could get somewhat similar results by simply looking at the largest residuals of a model that regresses the actual final table placing on just xGD, which is how most people tend to think of ‚Äúunexpected placings‚Äù.\n\n\nCode\ntable_with_xgd <- table |> \n  select(season, team, actual_pts = pts, actual_rank = rank) |> \n  inner_join(\n    all_raw_understat_xpts_xgd_by_season |> select(season, team, xgd),\n    by = c('season', 'team')\n  )\n\nxgd_rank_fit <- lm(actual_rank ~ xgd, table_with_xgd)\n\n\nThe table below shows the top five over- and under-achieving teams according to our regression explaining season-ending placing with season-ending xGD. Three of the top five over- and under-performing teams appear in the respective top fives according to the ranks from the simulations of match probabilities shown before.\n\nWe can also look at this from the opposite perspective. Where do the top five over- and under-achievers according to our simulations with match outcome probabilities fall among the season-ending xGD ranks for unlikelihood?\n\nOutside of the the three team-season pairs appearing in the both of the top five over- and under-achievers that we already saw before, one team-season pair is not too far off from the top five‚Äîthe 2017/18 Manchester United squad ranked as the sixth biggest over-performers by the season-long xGD regression.3 However, the other over-perfomer, 2019/20 Newcastle, and the two remaining under-performers, 2021/22 Crystal Palace and 2021/22 Brentford, have somewhat large ranking discrepancies. So yes, the two methods can lead to somewhat similar results in some cases, but there is some observational evidence to suggest that there are non-trivial differences in other cases.\nIn fact, there are some big differences between the two methods once we look outside the top five or so biggest over- and under- achievers. For example, in terms of over-achieving, Arsenal‚Äôs fifth place finish 2018/19 season is given just a 7.50% chance of occurring given their season-long xGD, ranking them as the 17th biggest over-performer (among 80 team-season pairs considered to have over-achieved from 2014/15).4 On the other hand, the match-level simulation approach marks the likelihood of them finishing fourth as 31.63% (37th of 80). The season-long xGD model essentially sees that they finished with an xGD of 7.5‚Äîless than any other fifth place finisher from 2014/15 - 2021/22‚Äîand penalizes them, while the match-level approach contextualizes them among their competition more robustly.\nAs another example, Manchester United‚Äôs under-achieving sixth place finish in the 2016/17 is given a fairly reasonable 37.36% chance (66th of 80 under-achieving teams) of occurring given their season-long 25.9 xGD, most for any sixth place team in the data set. On the other hand, the match-level simulation approach sees their sixth place finish as more unlikely, at just a 15.88% probability (16th of 80). While one might have their own opinion regarding which approach seems more ‚Äúcorrect‚Äù, I‚Äôd say that likelihoods from the simulation approach seem more appropriate for extreme examples such as this one and the 2018/19 Arsenal example\nOverall, both approaches seem reasonable to use to answer the question ‚ÄúWhich teams had the most unlikely placings in the table given the quality of all their shots across the season?‚Äù But the approach based on simulations using match probabilities seems more appropriate to use to quantify exactly how unlikely a team‚Äôs final placing was. While the simpler regression approach can also be used to quantify likelihood, it is more brittle, dependent on statistical assumptions. Additionally, while it contextualizes a team‚Äôs xGD with historical xGD, it does not contextualize a team‚Äôs xGD among the other teams in the league, meaning that it does not do a good job with capturing likelihood when there are strong xGD over- and under-performances among a set of teams in a given season."
  },
  {
    "objectID": "posts/epl-xpts-simulation-2/index.html#conclusion",
    "href": "posts/epl-xpts-simulation-2/index.html#conclusion",
    "title": "What exactly is an ‚Äúexpected point‚Äù? (part 2)",
    "section": "Conclusion",
    "text": "Conclusion\nWhile aggregating match-level outcome probabilities to try to predict season-ending points does no better than more direct approaches with season-long xGD or xPts, simulating seasons using match-level outcome probabilities can be used in a perhaps more interesting way‚Äîto quantify just how unlikely a team‚Äôs placement in the table is, given xG for all of their shots across the season."
  },
  {
    "objectID": "posts/fantasy-football-schedule-problem/index.html",
    "href": "posts/fantasy-football-schedule-problem/index.html",
    "title": "Fantasy Football and the Classical Scheduling Problem",
    "section": "",
    "text": "Every year I play in several fantasy football (American) leagues. For those who are unaware, it‚Äôs a game that occurs every year in sync with the National Football League (NFL) where participants play in weekly head-to-head games as general managers of virtual football teams. (Yes, it‚Äôs very silly.) The winner at the end of the season is often not the player with the team that scores the most points; often a fortunate sequence of matchups dictates who comes out on top.\nI didn‚Äôt fare so well this year in one of my leagues, but my disappointing placement was not due to my team struggling to score points; rather, I was extremely unlucky. I finished the season in 7th place despite scoring the most points!\nThis inspired me to quantify just how unlikely I was. The most common way to calculate the likelihood of a given team‚Äôs ranking in a league with is with a Monte Carlo simulation based on some parameterized model of scoring to generate probabilities for the final standings. FiveThirtyEight uses such a model for their soccer models, for example. For a setting in which team scores are independent of one another, such as fantasy football, another approach is to simply calculate what each team‚Äôs record would be if they had played every other team each week. (So, if your league has 10 teams and each plays each other once, each team would have a hypothetical count of 90 games played.) However, I was particularly interested in answering the question: ‚ÄúIn how many different schedules would I have finished where I did?‚Äù"
  },
  {
    "objectID": "posts/fantasy-football-schedule-problem/index.html#problem",
    "href": "posts/fantasy-football-schedule-problem/index.html#problem",
    "title": "Fantasy Football and the Classical Scheduling Problem",
    "section": "Problem",
    "text": "Problem\nFiguring out how unlucky I was to finish 7th requires me to first figure out how many possible schedules there are. Formally, the problem can be put as follows1:\n\nLet \\(T={t_1, ..., t_n}\\) be a set of an even \\(n\\) teams. Let \\(R\\) denote a round consisting of a set of pairs \\((t_i, t_j)\\) (denoting a match), such that \\(0 &lt; i &lt; j &lt;= n\\), and such that each team in \\(T\\) is participates exactly once in \\(R\\). Let \\(S\\) be a schedule consisting of a tuple of \\(n - 1\\) valid rounds \\((R_1, ..., R_{n-1})\\), such that all rounds in \\(S\\) are pair-wise disjoint (no round shares a match). How many valid constructions of \\(S\\) are there for \\(n\\) input teams?\n\nFor a small number of teams, it‚Äôs fairly simple to write out all possible combinations of matchups. For example, for a two-team league (where each team plays each other once), there is only one possible schedule (solution)‚ÄîTeam 1 vs.¬†Team 2. For a four-team league, there are six possible schedules. Two are shown below.\n\n\n\nsolution\nround\nteam1\nteam2\n\n\n\n\n1\n1\n1\n2\n\n\n\n\n3\n4\n\n\n\n2\n1\n3\n\n\n\n\n2\n4\n\n\n\n3\n1\n4\n\n\n\n\n2\n3\n\n\n2\n1\n1\n3\n\n\n\n\n2\n4\n\n\n\n2\n1\n2\n\n\n\n\n3\n4\n\n\n\n3\n1\n4\n\n\n\n\n2\n3\n\n\n\nNote that there is no concept of ‚Äúhome advantage‚Äù in fantasy football, so the order of teams in a given matchup does not matter. Also, note that if our restriction (‚Äúconstraint‚Äù) that each team must play each other once and only once, implies that the number of teams has to be an even number."
  },
  {
    "objectID": "posts/fantasy-football-schedule-problem/index.html#constraint-programming",
    "href": "posts/fantasy-football-schedule-problem/index.html#constraint-programming",
    "title": "Fantasy Football and the Classical Scheduling Problem",
    "section": "Constraint Programming",
    "text": "Constraint Programming\nTo truly answer this question, we can turn to constraint programming. If you‚Äôre familiar with constraint programming, then you‚Äôll notice that this set-up is similar to the canonical nurse scheduling problem and is a specific form of the tournament problem.\nBelow is some python code that is able to identify the number feasible solutions for four teams. I print out the first solution for illustrative purposes.\n\n\nCode\nfrom ortools.sat.python import cp_model\n\nclass SolutionPrinter(cp_model.CpSolverSolutionCallback):\n    def __init__(self, games, n_team, n_show=None):\n        cp_model.CpSolverSolutionCallback.__init__(self)\n        self._games = games\n        self._n_show = n_show\n        self._n_team = n_team\n        self._n_sol = 0\n\n    def on_solution_callback(self):\n        self._n_sol += 1\n        print()\n        if self._n_show is None or self._n_sol &lt;= self._n_show:\n            print(f'Solution {self._n_sol}.')\n            for team1 in range(self._n_team):\n                for team2 in range(self._n_team):\n                    if team1 != team2:\n                        print(\n                            f'Team {team1 + 1} vs. Team {team2 + 1} in Round {self.Value(self._games[(team1, team2)])}'\n                        )\n        else:\n            print(f'Found solution {self._n_sol}.')\n\n    def get_n_sol(self):\n        return self._n_sol\n\nn_team = 4\nn_w = n_team - 1\nmodel = cp_model.CpModel()\ngames = {}\nfor team1 in range(n_team):\n    for team2 in range(n_team):\n        if team1 != team2:\n            games[(team1, team2)] = model.NewIntVar(1, n_w, f'{team1:02}_{team2:02}')\n\nfor team1 in range(n_team):\n    for team2 in range(n_team):\n        if team1 != team2:\n            model.Add(games[(team1, team2)] == games[(team2, team1)])\n\n\n# Each team can only play in 1 game each week\nfor t in range(n_team):\n    model.AddAllDifferent(\n        [games[(t, team2)] for team2 in range(n_team) if t != team2]\n    )\n\nsolver = cp_model.CpSolver()\nsolution_printer = SolutionPrinter(games, n_team=n_team, n_show=2)\nstatus = solver.SearchForAllSolutions(model, solution_printer)\n\nprint()\nprint(f'Solve status: {solver.StatusName(status)}')\nprint(f'Solutions found: {solution_printer.get_n_sol()}')\n#&gt; Solution 1.\n#&gt; Team 1 vs. Team 2 in Round 3\n#&gt; Team 1 vs. Team 3 in Round 2\n#&gt; Team 1 vs. Team 4 in Round 1\n#&gt; Team 2 vs. Team 1 in Round 3\n#&gt; Team 2 vs. Team 3 in Round 1\n#&gt; Team 2 vs. Team 4 in Round 2\n#&gt; Team 3 vs. Team 1 in Round 2\n#&gt; Team 3 vs. Team 2 in Round 1\n#&gt; Team 3 vs. Team 4 in Round 3\n#&gt; Team 4 vs. Team 1 in Round 1\n#&gt; Team 4 vs. Team 2 in Round 2\n#&gt; Team 4 vs. Team 3 in Round 3\n#&gt; \n#&gt; Found solution 2.\n#&gt; \n#&gt; Found solution 3.\n#&gt; \n#&gt; Found solution 4.\n#&gt; \n#&gt; Found solution 5.\n#&gt; \n#&gt; Found solution 6.\n#&gt; \n#&gt; Solve status: OPTIMAL\n#&gt; Solutions found: 6\n\n\nEasy enough to run for 10 teams and get an answer, right? WRONG. Turns out this the number of feasible solutions (schedules) starts to blow up really quickly. In fact, I believe the number of solutions for this particular problem is only known up to 14 teams. (I‚Äôve intentionally left the numbers un-rounded to emphasize just how much the number of solutions increases as a function of the number of teams.)\n\n\n\nn\nsolutions\n\n\n\n\n2\n1\n\n\n4\n6\n\n\n6\n720\n\n\n8\n31,449,600\n\n\n10\n444,733,651,353,600\n\n\n12\n10,070,314,878,246,925,803,220,024\n\n\n14\n614,972,203,951,464,579,840,082,248,206,026,604,282\n\n\n\nUnless you happen to be an expert in graph theory and combinatorics, you probably wouldn‚Äôt be able to figure this out by hand; for us non-experts out there, we can refer to a known sequence of 1-factorizations of a complete graph \\(K_{2n}\\) and use our brain to figure out permutations in a given round. (Don‚Äôt worry if that makes no sense.)\nWhy do I bring this up? Well, I realized that generating all possible schedules for a 10-team league (such as my aforementioned league) is just not reasonable for anyone without a supercomputer and a lot of time. I enhanced the above python code a bit and tried it out for a 10-team league and was only able to generate a couple of million solutions after 3 hours."
  },
  {
    "objectID": "posts/fantasy-football-schedule-problem/index.html#alternative-exhaustive-search",
    "href": "posts/fantasy-football-schedule-problem/index.html#alternative-exhaustive-search",
    "title": "Fantasy Football and the Classical Scheduling Problem",
    "section": "Alternative: Exhaustive Search",
    "text": "Alternative: Exhaustive Search\nThe failure to generate all solutions made me reconsider things a bit. If I can‚Äôt reasonably ‚Äúhave it all‚Äù, I should simplify things a bit. By ‚Äúsimplify‚Äù, I mean perform an ‚Äúexhaustive‚Äù (or ‚Äúbrute-force) search that stops after a specified number of solutions. And, by re-writing things in R, I can eliminate dependencies on Google‚Äôs ortools package and python. (Both are great, but, nonetheless, they are potential obstacles for R users.)\nWriting a script to perform an exhaustive search is not so easy itself, and, in this case, requires a completely different approach to the problem. My steps are as follows:\n\nSet up an \\(n\\) x \\(n-1\\) matrix, where the \\(n\\) rows designate teams and the \\(n-1\\) columns designate rounds.\n\n\n\nCode\nleague_size = 4\nrounds &lt;- league_size - 1\nmat &lt;- matrix(nrow = league_size, ncol = rounds)\nmat\n#&gt;      [,1] [,2] [,3]\n#&gt; [1,]   NA   NA   NA\n#&gt; [2,]   NA   NA   NA\n#&gt; [3,]   NA   NA   NA\n#&gt; [4,]   NA   NA   NA\n\n\n\nRandomly select the opponent of team 1 in round 1.\n\n\n\nCode\nteam_i &lt;- 1\nround_i &lt;- 1\nretry_i &lt;- 1\nidx_team &lt;- 1:league_size\nset.seed(1)\n\nteam_1_round_1 &lt;- sample(2:league_size, 1, replace = FALSE)\nmat[team_i, round_i] &lt;- team_1_round_1\nmat\n#&gt;      [,1] [,2] [,3]\n#&gt; [1,]    2   NA   NA\n#&gt; [2,]   NA   NA   NA\n#&gt; [3,]   NA   NA   NA\n#&gt; [4,]   NA   NA   NA\n\n\n\nFind a unique set of opponents for teams 2 through \\(n\\) to fill the rest of the cells in column 1.\n\n\n\nCode\nwhile(team_i &lt;= league_size) {\n  if(team_i %in% teams_already_matched) {\n    team_i_round_i &lt;- which(team_i == teams_already_matched)\n    mat[team_i, round_i] &lt;- team_i_round_i\n    team_i &lt;- team_i + 1\n  } else {\n    teams_cant_match &lt;- unique(c(teams_already_indexed, teams_already_matched))\n    teams_unmatched &lt;- setdiff(teams_possible, teams_cant_match)\n    n_matched &lt;- length(teams_unmatched)\n    if(n_matched == 0) {\n      mat[2:league_size, round_i] &lt;- NA\n      team_i &lt;- 2\n    } else {\n      team_i_round_i &lt;- if(n_matched == 1) {\n        teams_unmatched\n      } else {\n        sample(teams_unmatched, 1)\n      }\n\n      mat[team_i, round_i] &lt;- team_i_round_i\n      team_i &lt;- team_i + 1\n    }\n  }\n}\n#&gt;      [,1] [,2] [,3]\n#&gt; [1,]    2   NA   NA\n#&gt; [2,]    1   NA   NA\n#&gt; [3,]    4   NA   NA\n#&gt; [4,]    3   NA   NA\n\n\n\nIdentify a unique set of opponents for team 1 for all other rounds (rounds 2 through \\(n-1\\)).\n\n\n\nCode\nteams_possible &lt;- setdiff(idx_team, c(1, team_1_round_1))\nteam1_all_rounds &lt;- sample(teams_possible, size = length(teams_possible))\nmat[1, 2:rounds] &lt;- team1_all_rounds\nmat\n#&gt;      [,1] [,2] [,3]\n#&gt; [1,]    2    3    4\n#&gt; [2,]    1   NA   NA\n#&gt; [3,]    4   NA   NA\n#&gt; [4,]    3   NA   NA\n\n\n\nRepeat step 3 for rounds 2 through \\(n-2\\) (penultimate round).\n\n\n\nCode\nwhile(round_i &lt; rounds) {\n  team_i &lt;- 2\n  while(team_i &lt;= league_size) {\n    teams_possible &lt;- setdiff(idx_team, team_i)\n    teams_already_indexed &lt;- 1:(team_i - 1)\n    teams_already_matched &lt;- mat[teams_already_indexed, round_i]\n    teams_already_played &lt;- mat[team_i, 1:(round_i - 1)]\n    reset &lt;- FALSE\n    if(team_i %in% teams_already_matched) {\n      team_i_round_i &lt;- which(team_i == teams_already_matched)\n      if(any(team_i_round_i == teams_already_played)) {\n        reset &lt;- TRUE\n      }\n    } else {\n      teams_cant_match &lt;-\n        unique(c(teams_already_indexed, teams_already_matched, teams_already_played))\n      teams_unmatched &lt;- setdiff(teams_possible, teams_cant_match)\n      n_matched &lt;- length(teams_unmatched)\n      if (n_matched == 0) {\n        reset &lt;- TRUE\n      } else {\n        team_i_round_i &lt;- if(n_matched == 1) {\n          teams_unmatched\n        } else {\n          sample(teams_unmatched, 1)\n        }\n      }\n    }\n    \n    if(reset) {\n      mat[2:league_size, round_i] &lt;- NA\n      team_i &lt;- 2\n      retry_i &lt;- retry_i + 1\n    } else {\n      mat[team_i, round_i] &lt;- team_i_round_i\n      team_i &lt;- team_i + 1\n    }\n  }\n  round_i &lt;- round_i + 1\n}\nmat\n#&gt;      [,1] [,2] [,3]\n#&gt; [1,]    2    3    4\n#&gt; [2,]    1    4   NA\n#&gt; [3,]    4    1   NA\n#&gt; [4,]    3    2   NA\n\n\n\nIdentify the only valid set of matchups for the last round \\(n-1\\).\n\n\n\nCode\nidx_not1 &lt;- 2:league_size\ntotal &lt;- Reduce(sum, idx_team) - idx_not1\nrs &lt;- rowSums(mat[idx_not1, 1:(rounds - 1)])\nteams_last &lt;- total - rs\nmat[idx_not1, rounds] &lt;- teams_last\nmat\n#&gt;      [,1] [,2] [,3]\n#&gt; [1,]    2    3    4\n#&gt; [2,]    1    4    3\n#&gt; [3,]    4    1    2\n#&gt; [4,]    3    2    1\n\n\nThat is the core of the solution. The rest of the work2 involves repeating the steps for however many times you want, always checking for duplicates of previous solutions, i.e.¬†sampling without replacement. (Or, if you don‚Äôt care about schedules being unique, i.e.¬†sampling with replacement, it‚Äôs even easier.)"
  },
  {
    "objectID": "posts/fantasy-football-schedule-problem/index.html#application",
    "href": "posts/fantasy-football-schedule-problem/index.html#application",
    "title": "Fantasy Football and the Classical Scheduling Problem",
    "section": "Application",
    "text": "Application\nSince generating unique schedules is something I‚Äôd like to be able to do every year for my fantasy football leagues, I wrote a package for it, called {ffsched}. The package includes functionality to retrieve your league‚Äôs fantasy scores from ESPN, which you can combine with the simulated schedules to generate a plot such as the following.\n\nIt‚Äôs immediately evident how un-lucky I (‚ÄúTony El Tigre‚Äù) was. In the 100,000 simulations, I never finished below 7th, and I only finished 7th 1.1% of the time!\nIn the previous year I scored the most points and finished first. ‚ÄúThe Juggernaut‚Äù got the short end of the stick in 2019, finishing 7th. He only finished 7th or lower in 6.6% of schedules."
  },
  {
    "objectID": "posts/fantasy-football-schedule-problem/index.html#take-away",
    "href": "posts/fantasy-football-schedule-problem/index.html#take-away",
    "title": "Fantasy Football and the Classical Scheduling Problem",
    "section": "Take-away",
    "text": "Take-away\nAn exhaustive search as a work-around for true constraint programming isn‚Äôt always elegant and can be difficult to implement, but if you‚Äôre motivated enough to do it‚Äîas I was to prove my extreme lack of fortune‚Äîit can generate what you need to make a compelling point. My use case (for generating unique fantasy generating football schedules) is inconsequential, but such techniques are often immensely important in real world contexts."
  },
  {
    "objectID": "posts/opta-xg-model-calibration/index.html",
    "href": "posts/opta-xg-model-calibration/index.html",
    "title": "xG Model Calibration",
    "section": "",
    "text": "Recently, I pointed out what seemed to be a bug with the expected goals (xG) data shown on FBref. In particular, the difference between non-penalty goals (npG) and non-penalty xG (npxG)1 seemed to be an outlier for the 2021/22 season across the Big 5 leagues.\n\n\n‚ÄúaLl xG mOdeLs ArE thE sAme‚Äùmy brother in christ wut is this then pic.twitter.com/7tjp1VFkoc\n\n‚Äî Tony (@TonyElHabr) January 14, 2023\n\n\nAs it turns out FBref and their data provider, Opta, agreed! On Feb.¬†8, 2023, they posted an update indicating that they adjusted their 2021/22 xG such that the difference between npG and npxG is much more in line with other seasons.\nThe FBref/Opta update gave me two ideas:\n\nCompare pre- and post-update xG to identify where/how adjustments were applied. Where were the ‚Äúblind spot(s)‚Äù?2\nQuantify the calibration of their xG model. Are there obvious weak points with the model?"
  },
  {
    "objectID": "posts/opta-xg-model-calibration/index.html#pre--and-post-update-xg-comparison",
    "href": "posts/opta-xg-model-calibration/index.html#pre--and-post-update-xg-comparison",
    "title": "xG Model Calibration",
    "section": "1. Pre- and post-update xG comparison",
    "text": "1. Pre- and post-update xG comparison\n\n\n\n\n\n\nFirst, let‚Äôs take a wholistic look at all of the shots for the 2021/22 seasons played in Big 5 leagues.\n\n\n\n\nOf the 44,986 shots in the data set, 30,326 (67.2%) had changes to their xG values.3 Of those that changed, 23, 584 (78.0%) were reduced, i.e.¬†the pre-update xG value was higher. The average change was pretty minimal, just about ~0.01 xG.\n\n\nCode\nglimpse(discretized_updated_np_shots)\n#> Rows: 44,986\n#> Columns: 16\n#> $ league             <fct> ENG, ENG, ENG, ENG, ENG, ENG, ENG, ENG, ENG, ENG, E‚Ä¶\n#> $ date               <date> 2021-08-13, 2021-08-13, 2021-08-13, 2021-08-13, 20‚Ä¶\n#> $ half               <dbl> 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, ‚Ä¶\n#> $ minute             <chr> \"11\", \"12\", \"22\", \"28\", \"30\", \"66\", \"73\", \"80\", \"2\"‚Ä¶\n#> $ team               <chr> \"Brentford\", \"Brentford\", \"Brentford\", \"Brentford\",‚Ä¶\n#> $ player             <chr> \"Frank Onyeka\", \"Bryan Mbeumo\", \"Sergi Can√≥s\", \"Ser‚Ä¶\n#> $ new_xg             <dbl> 0.08, 0.09, 0.02, 0.06, 0.26, 0.06, 0.40, 0.28, 0.0‚Ä¶\n#> $ old_xg             <dbl> 0.09, 0.14, 0.04, 0.07, 0.31, 0.13, 0.58, 0.27, 0.0‚Ä¶\n#> $ is_goal            <fct> no, no, yes, no, no, no, yes, no, no, no, no, no, n‚Ä¶\n#> $ distance           <fct> \"(8,10]\", \"(12,14]\", \"(16,18]\", \"(20,25]\", \"(12,14]‚Ä¶\n#> $ sca1               <fct> pass_live, pass_live, pass_live, pass_live, take_on‚Ä¶\n#> $ body_part          <fct> Head, Right Foot, Right Foot, Right Foot, Right Foo‚Ä¶\n#> $ is_from_deflection <fct> no, no, no, no, no, no, no, no, no, no, no, no, no,‚Ä¶\n#> $ is_from_volley     <fct> no, no, no, no, no, no, no, no, no, yes, no, no, no‚Ä¶\n#> $ is_free_kick       <fct> no, no, no, no, no, no, no, no, no, no, no, no, no,‚Ä¶\n#> $ is_primary_foot    <fct> missing, no, yes, yes, no, yes, missing, missing, y‚Ä¶\n\ndiscretized_updated_np_shots |> \n  mutate(xgd = old_xg - new_xg) |> \n  pull(xgd) |> \n  mean()\n#> [1] 0.0095014\n\n\nTo get more insight into how/why xG changed, we can look at changes to xG values grouped by various features that we can derive from data that FBref publishes alongside each shot‚Äôs xG, including distance (yards), sca1 (first shot-creating action), body_part, is_from_deflection, is_from_volley, is_free_kick, and is_primary_foot.45\n\n\n\nThe table below shows that the reductions in npxG occurred most frequently for longer distances, suggesting that the pre-update xG model was over-predicting xG for longer shots. Interestingly, xG for shots when interceptions were the shot-creating action that led directly to the shot, and xG for shots with body_part = \"other\" (non-foot, non-header) were also frequently reduced, in the cases where xG was changed.\n\n\n\n\n\n\n\n\n\n\nFeature\nGroup\n# of non-penalty shots\n# of shots with changed npxG\n# of shots with lower post-update npxG of those that changed\n\n\n\n\ndistance\n(25,30]\n6,061\n3,659 (60.4%)\n3,437 (93.9%)\n\n\ndistance\n(20,25]\n6,760\n4,463 (66.0%)\n4,088 (91.6%)\n\n\nsca1\n\"interception\"\n149\n96 (64.4%)\n87 (90.6%)\n\n\ndistance\n(18,20]\n1,889\n1,232 (65.2%)\n1,088 (88.3%)\n\n\ndistance\n(30,35]\n2,725\n1,267 (46.5%)\n1,117 (88.2%)\n\n\nbody_part\n\"Other\"\n191\n153 (80.1%)\n130 (85.0%)\n\n\n\nOn the other end of the spectrum, reductions in npxG occurred least frequently for shorter distance buckets ((0,2], (2,4], (4,6], (6,8)). Reductions still occurred a majority of the time when there was a change‚Äînote that each has >50% for the last column‚Äîfor all but the shortest distance group, (0,2].\n\n\n\n\n\n\n\n\n\n\nFeature\nGroup\n# of non-penalty shots\n# of shots with changed npxG\n# of shots with lower post-update npxG of those that changed\n\n\n\n\ndistance\n(0,2]\n173\n130 (75.1%)\n51 (39.2%)\n\n\ndistance\n(2,4]\n1,087\n826 (76.0%)\n428 (51.8%)\n\n\ndistance\n(4,6]\n2,003\n1,479 (73.8%)\n831 (56.2%)\n\n\ndistance\n(35,Inf]\n539\n313 (58.1%)\n177 (56.5%)\n\n\ndistance\n(6,8]\n2,557\n1,882 (73.6%)\n1,183 (62.9%)\n\n\nis_free_kick\n\"yes\"\n1,576\n882 (56.0%)\n557 (63.2%)"
  },
  {
    "objectID": "posts/opta-xg-model-calibration/index.html#xg-model-calibration",
    "href": "posts/opta-xg-model-calibration/index.html#xg-model-calibration",
    "title": "xG Model Calibration",
    "section": "2. xG Model Calibration",
    "text": "2. xG Model Calibration\nI‚Äôve touched on model calibration before, when discussing xG-implied match outcome probabilities. There, I wrote my own code to create a calibration plot. Since then, the {tidymodels} team has added calibration plot functionality to the {probably} package. Let‚Äôs try it out.\nHere, we‚Äôll use a big sample of data‚Äîall 2017/18 - 2021/22 non-penalty shots for the Big 5 leagues and several other first and second tier leagues.6\n\n\n\n\n\nCode\nnp_shots |> count(league, name = 'n_shots')\n#> # A tibble: 13 √ó 2\n#>    league    n_shots\n#>    <chr>       <int>\n#>  1 BRA_1st_M   39380\n#>  2 ENG_1st_F   11366\n#>  3 ENG_1st_M   46766\n#>  4 ENG_2nd_M   52701\n#>  5 ESP_1st_M   43398\n#>  6 FRA_1st_M   43021\n#>  7 GER_1st_M   39148\n#>  8 ITA_1st_M   49903\n#>  9 MEX_1st_M   32650\n#> 10 NED_1st_M   29803\n#> 11 POR_1st_M   27366\n#> 12 USA_1st_F    9887\n#> 13 USA_1st_M   31047\n\n\n\nCalibration plot\nWe can use probably::cal_plot_breaks() to visually assess whether the observed rate of non-penalty goals (y-axis) is close to the predicted probability of goals (npxG, x-axis).7 If the xG model‚Äôs predictions are ‚Äúwell calibrated‚Äù, the calibration points will align with the ‚Äúideal‚Äù line having slope 1 and intercept 0. Points at which the curve is below the diagonal line indicate where the model is more likelty to overpredict; and, likewise, points where the curve is above the diagonal line indicate where the model is underpredicting.\n\n\nCode\nlibrary(probably) ## 0.1.0.9007\n\noverall_calibration <- cal_plot_breaks(\n  np_shots,\n  truth = is_goal,\n  estimate = xg,\n  num_breaks = 20,\n  conf_level = 0.9,\n  event_level = 'second'\n)\n\n\n\n\n\n\nWe can see that the model is pretty well calibrated on the lower end of the spectrum, when xG < 0.25. This makes up a larger majority of the shots (~90%). However, the model is not as well calibrated for higher xG values, tending to overpredict. For example, at the calibration point where npxG is 0.675, the actual goal rate is 0.6.\nObserving the miscalibration for shots with xG > 0.25, one has to wonder whether the removal of the ‚Äúbig chance‚Äù feature in favor of other contextual features‚Äîan update that Opta made sometime in the first half of 2022‚Äîmay have (unintentionally) made the model worse in some ways.8 Unfortunately, I didn‚Äôt have accessed to Opta xG data prior to that update, so I can‚Äôt evaluate this hypothesis. (For all we know, the model calibration for high xG shots might have been worse before!)\n\n\nBrier Skill Score (BSS)\nOne thing that is not provided in the {tidymodels} realm (specifically, the {yardstick} package) is a function to compute Brier score. Nonetheless, we can define a Brier score function ourselves by closely following the mean squared error custom metric example provided by the {tidymodels} team.9\n\n\nCode\nlibrary(yardstick)\nlibrary(rlang)\nbrier_score <- function(data, ...) {\n  UseMethod('brier_score')\n}\n\nbrier_score <- yardstick::new_prob_metric(brier_score, direction = 'minimize')\n\nbrier_score_vec <- function(truth, estimate, na_rm = TRUE, event_level, ...) {\n  \n  brier_score_impl <- function(truth, estimate, event_level, ...) {\n    truth <- 1 - (as.numeric(truth) - 1)\n    \n    if (event_level == 'second') {\n      truth <- 1 - truth\n    }\n    \n    mean((truth - estimate)^2)\n  }\n  \n  ## Recycle the estimate value if it's scalar-ish.\n  if (length(estimate) == 1) {\n    estimate <- rep(estimate, length(truth))\n  }\n  \n  yardstick::metric_vec_template(\n    metric_impl = brier_score_impl,\n    truth = truth,\n    estimate = estimate,\n    na_rm = na_rm,\n    cls = c('factor', 'numeric'),\n    estimator = 'binary',\n    event_level = event_level,\n    ...\n  )\n}\n\nbrier_score.data.frame <- function(data, truth, estimate, na_rm = TRUE, event_level = 'first', ...) {\n  yardstick::metric_summarizer(\n    metric_nm = 'brier_score',\n    metric_fn = brier_score_vec,\n    data = data,\n    truth = !!rlang::enquo(truth),\n    estimate = !!rlang::enquo(estimate),\n    na_rm = na_rm,\n    event_level = event_level,\n    ...\n  )\n}\n\n\nLet‚Äôs compute the Brier scores for (1) the overall goal rate (i.e.¬†shots per goal) and (2) xG. We should expect the Brier score for the latter to be closer to 0 (perfect model), since xG should be a better predictor of goals than the naive goal rate.\n\n\nCode\nnp_goal_rate <- np_shots |> \n  count(is_goal) |> \n  mutate(prop = n / sum(n)) |> \n  filter(is_goal == 'yes') |> \n  pull(prop)\nnp_goal_rate\n#> 0.0960288\n\nnp_goal_rate_brier_score <- np_shots |> \n  brier_score(\n    truth = is_goal,\n    estimate = !!np_goal_rate,\n    event_level = 'second'\n  ) |> \n  pull(.estimate)\nnp_goal_rate_brier_score\n#> [1] 0.08680727\n\nnpxg_brier_score <- np_shots |> \n  brier_score(\n    truth = is_goal,\n    estimate = xg,\n    event_level = 'second'\n  ) |> \n  pull(.estimate)\nnpxg_brier_score\n#> [1] 0.07150071\n\n\nNow we can go on to compute Brier skill score (BSS) using an appropriate reference Brier score.10 In this context, the average goal rate seems to be a good choice for a baseline. In contrast to the Brier score, a higher BSS is ideal. (A perfect model would have a BSS of 1.)\n\n\nCode\n1 - (npxg_brier_score / np_goal_rate_brier_score)\n#> [1] 0.176328\n\n\nA BSS of ~0.18 is not bad! This is better than FiveThirtyEight‚Äôs BSS for predicting the results for men‚Äôs World Cup matches (~0.12 at time of writing) and right around their BSS for predicting WNBA playoff game outcomes (~0.18).11\n\n\nGrouped Calibration and BSS\nNow let‚Äôs take a look at model calibration under specific criteria. Is the model worse for shots that follow a dribble (take_on) shot-creating action? After a live_ball pass? etc.\n\n\n\n\nA couple of observations and thoughts:\n\nxG of shots following another shot are over-predicted so much that it causes the BSS to be negative.12 This means that the model is actually doing worse in its xG assignment than simply predicting naive goal rate for shots after another shot!\nA relatively ‚Äújagged‚Äù calibration plot may not correspond with a worse (lower) BSS score; and visa versa, a relatively ‚Äúsmooth‚Äù calibration plot may not correspond with a better (higher) BSS.\n\nNote that the fouled calibration looks jagged for higher predicted xG, but the fact that goals are only scored on about 5% shots immediately following a foul means that inprecise probabilities are not ‚Äúpenalized‚Äù quite as much. On the other hand, while the pass_live calibration looks relatively smooth, the 10% goal rate following live ball passes (2x the frequency for shots following fouls) means that it is more penalized for imprecision than an otherwise equivalent post-fouled shot. In fact, this is one of the shortcomings of BSS‚Äîit does not do a great job with evaluation of relatively infrequent events.\n\n\nNext, let‚Äôs take a look at calibration of shots coming after deflections (of other shots).\n\n\n\n\n\nThe model doesn‚Äôt seem to be very well calibrated for shots following deflections! Like shots following other shots in the shot-creating action calibration plot, the BSS for shots after deflections is negative. And, perhaps more interestingly, the model seems to underpredict post-deflection shots, which is the opposite of it‚Äôs general tendency to overpredict. (See the wholistic calibration plot from before.)\n\nI‚Äôd suspect that there‚Äôs lots of confounders that might explain the lack of calibration after deflections. For one, it could be the case that there are often zero defenders between the shot-taker and keeper for shots following a deflection. As far as I know, the Opta model doesn‚Äôt have an explicit feature for this.\nAs with the overall model calibration, one has to wonder whether the model may have performed better on shots following deflections with the binary big chance feature.\nThe high goal rate on shots after deflections relative to the goal rate on all other shots certainly contributes to the negative BSS, as we saw earlier with shots following other shots.\n\n\nMoving on, let‚Äôs look at calibration of the xG model by groups of leagues, splitting out by tier and gender.\n\n\n\n\n\nStatsBomb has talked about how a ‚Äúgender-aware‚Äù model outperformed a baseline model, so one might expect the calibration of Opta‚Äôs singular model to be weaker for the women‚Äôs game. It turns out that while, yes, the calibration seems to be a bit worse for shots in the women‚Äôs leagues, the overall difference in model performance for men‚Äôs and women‚Äôs leagues seems to be trivial for the sample here.\nInterestingly, the calibration of the model for non-Big 5 leagues and the English men‚Äôs Championship league are slightly better according to BSS, although the differences (both visually, with the calibration curve, and with BSS) are very minimal.\n\nFinally, let‚Äôs look at how footedness may play a role in model calibration. As far as I know, whether or not a footed shot is take by a player‚Äôs primary foot is not an input into the Opta model, so it may be particularly interesting to look at.\n\n\n\n\n\nDespite my suspicion that xG for shots taken by a player‚Äôs weaker foot (right foot shot, left-footed and left foot shot, right-footed) might be severely overpredicted, this doesn‚Äôt really seem to be the case. Yes, the model tends to overpredict for these kinds of shots, but the degree to which overprediction occurs doesn‚Äôt seem out of line with the whole model.\n\nI can think of least two types of ‚Äúselection bias‚Äù at play here that might explain why the calibration isn‚Äôt as bad as I might have guessed for weak-footed shots:\n\nPlayers are more likely to take weak-footed shots when they‚Äôre closer to the goal, where shots are likely to have higher xG, but also where shots are more likely to go in.\nPlayers are less likely to take more difficult shots with their weak foot, so they‚Äôre not taking as many shots that are unlikely to go in, holding all else equal.\n\n\nOf the non-footed shots, it‚Äôs interesting to see that the BSS for headers and shots from other body parts are not particularly well calibrated. In fact, the latter has a negative BSS, indicating that we‚Äôd better off with a model that predicted the average goal rate for such shots.\n\nA high goal rate on such shots compared to other types of shots seems to, once again, be the reason that BSS looks particularly bad here."
  },
  {
    "objectID": "posts/opta-xg-model-calibration/index.html#conclusion",
    "href": "posts/opta-xg-model-calibration/index.html#conclusion",
    "title": "xG Model Calibration",
    "section": "Conclusion",
    "text": "Conclusion\nWe‚Äôve explored the wonderful world of model calibration, making friends with BSS and calibration curves in our investigation of a public xG model. Are BSS and calibration curves the be-all and end-all when it comes to model evaluation? Of course not! But they‚Äôre useful tools that may or may not be appropriate for your use case.\nWhen it comes to the Opta xG model specifically, am I implying that the model is bad? Of course not (again)! Yes, faceted calibration curves and feature-specific BSS can make a model look bad, but we must keep in mind that there are trade-offs to be made with modeling. Fine-tuning a model to be more well calibrated under certain conditions, e.g.¬†shots after deflections, may make other parts of the model worse! It‚Äôs all about trade-offs."
  },
  {
    "objectID": "posts/soccer-league-strength/index.html",
    "href": "posts/soccer-league-strength/index.html",
    "title": "Quantifying Relative Soccer League Strength",
    "section": "",
    "text": "Arguing about domestic league strength is something that soccer fans seems to never tire of. (‚ÄúCould Messi do it on a cold rainy night in Stoke?‚Äù) Many of these conversations are anecdotal, leading to ‚Äúhot takes‚Äù that are unfalsifiable. While we‚Äôll probably never move away from these kinds of discussions, we can at least try to inform them with a quantitative approach.\nPerhaps the obvious way to do so is to take match results from international tournaments (e.g.¬†Champions League, Europa). But such an approach can be flawed‚Äîthere‚Äôs not a large sample, and match results may not be reflective of ‚Äútrue‚Äù team strength (e.g.¬†one team may win on xG by a large margin, but lose the game.)"
  },
  {
    "objectID": "posts/soccer-league-strength/index.html#methodology",
    "href": "posts/soccer-league-strength/index.html#methodology",
    "title": "Quantifying Relative Soccer League Strength",
    "section": "Methodology",
    "text": "Methodology\nBut what if we used an approach rooted in player performance? I asked myself that very question and came up with the following approach. (Thanks to Cahnzhi Ye for the data.)\n\nIdentify players who played in more than one league within the same season or across consecutive seasons. Calculate the difference in each player‚Äôs atomic VAEP1 per 90 minutes (VAEP/90) after changing leagues.\n\n\n\nCode\n#&gt; # A tibble: 2,462 x 7\n#&gt;    Season Player        `League A`        `League B`       `VAEP/90 A` `VAEP/90 B`   Diff.\n#&gt;     &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;             &lt;chr&gt;                  &lt;dbl&gt;       &lt;dbl&gt;   &lt;dbl&gt;\n#&gt;  1   2020 Timo Werner   Bundesliga 1 (Ge~ Premier League ~       1.25        0.638  0.610 \n#&gt;  2   2020 Alexander S√∏~ Super Lig (Turke~ Bundesliga 1 (G~       1.07        0.773  0.296 \n#&gt;  3   2020 Hakim Ziyech  Eredivisie (Neth~ Premier League ~       0.958       0.365  0.593 \n#&gt;  4   2020 Nicol√°s Gonz~ Bundesliga 2 (Ge~ Bundesliga 1 (G~       0.917       0.943 -0.0256\n#&gt;  5   2020 Fabian Klos   Bundesliga 2 (Ge~ Bundesliga 1 (G~       0.904       0.547  0.358 \n#&gt;  6   2020 Victor Osimh~ Ligue 1 (France)  Serie A (Italy)        0.889       1.01  -0.120 \n#&gt;  7   2020 Eldor Shomur~ Premier League (~ Serie A (Italy)        0.882       0.755  0.126 \n#&gt;  8   2020 Callum Robin~ Championship (En~ Premier League ~       0.880       0.682  0.198 \n#&gt;  9   2020 Jarrod Bowen  Championship (En~ Premier League ~       0.871       0.448  0.423 \n#&gt; 10   2020 Aleksandar M~ Championship (En~ Premier League ~       0.866       0.499  0.367 \n#&gt; # ... with 2,452 more rows\n\n\nWhy VAEP? Theoretically it should capture more about in-game actions (including defense) than other stats such as xG, which is biased in favor of attacking players. VAEP is not perfect by any means (e.g.¬†it does not capture off-ball actions), but, in theory, it should be a better measure of overall performance. 2\nNotably, we give up a little in interpretability in using VAEP, since it‚Äôs not directly translatable to goals. 3 The following table of top season-long xG totals since 2012 to contextualize the magnitudes of xG and VAEP.\n\n\nCode\n#&gt; # A tibble: 36,857 x 5\n#&gt;    Season Player             Minutes    xG  VAEP\n#&gt;     &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1   2015 Lionel Messi          3529  38.1  69.0\n#&gt;  2   2016 Luis Su√°rez           3294  35.6  55.7\n#&gt;  3   2018 Robert Lewandowski    2259  35.1  40.2\n#&gt;  4   2012 Lionel Messi          3425  32.4  76.2\n#&gt;  5   2013 Lionel Messi          2776  31.6  60.0\n#&gt;  6   2015 Cristiano Ronaldo     3236  30.8  58.7\n#&gt;  7   2018 Mohamed Salah         3080  30.4  50.5\n#&gt;  8   2012 Cristiano Ronaldo     3504  30.1  62.7\n#&gt;  9   2017 Edin Dzeko            3216  29.8  49.7\n#&gt; 10   2020 Robert Lewandowski    2902  29.7  49.2\n#&gt; # ... with 36,847 more rows\n\n\nAnd a scatter plot, because who doesn‚Äôt love a graph.\n\n\nConvert the player-level VAEP/90 differences to z-scores by position and age group.\n\n\n\nCode\n#&gt; # A tibble: 2,462 x 7\n#&gt;    Season Player      Position `Age Group` `League A`      `League B`    `VAEP/90 Diff. Z`\n#&gt;     &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;    &lt;chr&gt;       &lt;chr&gt;           &lt;chr&gt;                     &lt;dbl&gt;\n#&gt;  1   2020 Timo Werner AM       24&lt;=x&lt;27    Bundesliga 1 (~ Premier Leagu~            2.28 \n#&gt;  2   2020 Alexander ~ FW       24&lt;=x&lt;27    Super Lig (Tur~ Bundesliga 1 ~            1.10 \n#&gt;  3   2020 Hakim Ziye~ M        27&lt;=x&lt;30    Eredivisie (Ne~ Premier Leagu~            3.20 \n#&gt;  4   2020 Nicol√°s Go~ M        18&lt;=x&lt;24    Bundesliga 2 (~ Bundesliga 1 ~           -0.148\n#&gt;  5   2020 Fabian Klos FW       30&lt;=x&lt;36    Bundesliga 2 (~ Bundesliga 1 ~            1.20 \n#&gt;  6   2020 Victor Osi~ FW       18&lt;=x&lt;24    Ligue 1 (Franc~ Serie A (Ital~           -0.439\n#&gt;  7   2020 Eldor Shom~ FW       24&lt;=x&lt;27    Premier League~ Serie A (Ital~            0.471\n#&gt;  8   2020 Callum Rob~ AM       24&lt;=x&lt;27    Championship (~ Premier Leagu~            0.740\n#&gt;  9   2020 Jarrod Bow~ AM       18&lt;=x&lt;24    Championship (~ Premier Leagu~            1.89 \n#&gt; 10   2020 Aleksandar~ FW       24&lt;=x&lt;27    Championship (~ Premier Leagu~            1.37 \n#&gt; # ... with 2,452 more rows\n\n\nWhy grouping? This is intended to account for the fact that attacking players and ‚Äúpeaking‚Äù players (usually age 24-30) tend to have higher VAEP/90, so their league-to-league differences have larger variation. The choice to normalize is perhaps more questionable. The mean of differences is ~0 for all groups already, but the dispersion is smaller without normalization (i.e.¬†standard deviations are closer to 0). So, in this case, normalization should help the linear model capture variation.\n\n\nCode\n#&gt; # A tibble: 20 x 5\n#&gt;    Position Age Group N    Mean   SD\n#&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt;  1 AM       18&lt;=x&lt;24   138     0 0.224 \n#&gt;  2 AM       24&lt;=x&lt;27   128     0 0.268 \n#&gt;  3 AM       27&lt;=x&lt;30   118     0 0.248 \n#&gt;  4 AM       30&lt;=x&lt;36    68     0 0.248 \n#&gt;  5 D        18&lt;=x&lt;24   203     0 0.112 \n#&gt;  6 D        24&lt;=x&lt;27   268     0 0.101 \n#&gt;  7 D        27&lt;=x&lt;30   295     0 0.0930\n#&gt;  8 D        30&lt;=x&lt;36   316     0 0.0939\n#&gt;  9 DM       18&lt;=x&lt;24    30     0 0.102 \n#&gt; 10 DM       24&lt;=x&lt;27    48     0 0.0913\n#&gt; 11 DM       27&lt;=x&lt;30    20     0 0.105 \n#&gt; 12 DM       30&lt;=x&lt;36    13     0 0.0719\n#&gt; 13 FW       18&lt;=x&lt;24    26     0 0.274 \n#&gt; 14 FW       24&lt;=x&lt;27    67     0 0.268 \n#&gt; 15 FW       27&lt;=x&lt;30    50     0 0.263 \n#&gt; 16 FW       30&lt;=x&lt;36    66     0 0.297 \n#&gt; 17 M        18&lt;=x&lt;24   113     0 0.173 \n#&gt; 18 M        24&lt;=x&lt;27   130     0 0.147 \n#&gt; 19 M        27&lt;=x&lt;30   189     0 0.185 \n#&gt; 20 M        30&lt;=x&lt;36   186     0 0.171  \n\n\n\nRun a single regression where the response variable is the z-transformed VAEP/90 difference, and the features are indicators for leagues, where -1 indicates player departure, a +1 indicates player arrival, and all other values are 0.4 5\n\nFor those familiar with basketball and hockey, this is similar to the set-up for an adjusted plus-minus (APM) calculation. Here, each feature column is a league (instead of a player), each row represents a player (instead of a ‚Äústint‚Äù), and the response is transformed VAEP/90 (instead of net points per possession).\n\n\nCode\n#&gt; tibble [2,472 x 16] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ VAEP/90 Diff Z-Trans     : num [1:2472] -0.0825 0.3285 -0.0143 0.1137 0.1526 ...\n#&gt;  $ Serie A (Italy)          : int [1:2472] -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 ...\n#&gt;  $ Bundesliga 1 (Germany)   : int [1:2472] 1 1 0 0 0 0 0 0 0 0 ...\n#&gt;  $ La Liga (Spain)          : int [1:2472] 0 0 0 0 0 0 0 0 0 0 ...\n#&gt;  $ Serie A (Brazil)         : int [1:2472] 0 0 0 0 0 1 0 0 0 0 ...\n#&gt;  $ Super Lig (Turkey)       : int [1:2472] 0 0 1 0 0 0 0 1 0 0 ...\n#&gt;  $ Premier League (England) : int [1:2472] 0 0 0 0 0 0 0 0 0 0 ...\n#&gt;  $ Super League (China)     : int [1:2472] 0 0 0 0 0 0 0 0 0 1 ...\n#&gt;  $ Major League Soccer (USA): int [1:2472] 0 0 0 0 1 0 1 0 0 0 ...\n#&gt;  $ Primeira Liga (Portugal) : int [1:2472] 0 0 0 0 0 0 0 0 0 0 ...\n#&gt;  $ Ligue 1 (France)         : int [1:2472] 0 0 0 1 0 0 0 0 0 0 ...\n#&gt;  $ Bundesliga 2 (Germany)   : int [1:2472] 0 0 0 0 0 0 0 0 0 0 ...\n#&gt;  $ Championship (England)   : int [1:2472] 0 0 0 0 0 0 0 0 0 0 ...\n#&gt;  $ Premier League (Russia)  : int [1:2472] 0 0 0 0 0 0 0 0 0 0 ...\n#&gt;  $ Superliga (Argentina)    : int [1:2472] 0 0 0 0 0 0 0 0 1 0 ...\n#&gt;  $ Eredivisie (Netherlands) : int [1:2472] 0 0 0 0 0 0 0 0 0 0 ...\n\n\nThe result is a set of coefficient estimates corresponding to each league. Notably, these are all positive (even if subtracting the intercept), and the Netherlands coefficient is NA due to multi-collinearity in the data. 6\n\n\nCode\n#&gt; # A tibble: 16 x 2\n#&gt;    League                    Estimate\n#&gt;    &lt;chr&gt;                        &lt;dbl&gt;\n#&gt;  1 Premier League (England)    0.975 \n#&gt;  2 La Liga (Spain)             0.869 \n#&gt;  3 Ligue 1 (France)            0.786 \n#&gt;  4 Serie A (Italy)             0.738 \n#&gt;  5 Serie A (Brazil)            0.724 \n#&gt;  6 Primeira Liga (Portugal)    0.675 \n#&gt;  7 Bundesliga 1 (Germany)      0.649 \n#&gt;  8 Championship (England)      0.641 \n#&gt;  9 Super Lig (Turkey)          0.617 \n#&gt; 10 Premier League (Russia)     0.485 \n#&gt; 11 Superliga (Argentina)       0.463 \n#&gt; 12 Super League (China)        0.360 \n#&gt; 13 Major League Soccer (USA)   0.281 \n#&gt; 14 Bundesliga 2 (Germany)      0.197 \n#&gt; 15 (Intercept)                 0.0747\n#&gt; 16 Eredivisie (Netherlands)   NA     \n\n\nFor hockey/basketball APM, we would say the coefficient estimate represents how much a player contributes relative to an ‚Äúaverage‚Äù player. We might be tempted to try to interpret these coefficients directly as well. Yes, we can infer the league ‚Äúpower rankings‚Äù from just this singular coefficient list (Premier League as the strongest and Bundesliga 2 as the weakest), but there are some issues.\n\nWe first need to ‚Äúun-transform‚Äù this back to the VAEP/90 scale. (See next step.)\nNote that this is not a zero-sum situation (even after un-transforming). There is no notion of a matchup between one league and another like there is in hockey/basketball with players on the ice/court. Instead, our data is more analogous to a player playing against themselves (not a set of players versus another set of players).\nEven if this were a zero-sum type of problem and the model returned some negative coefficient estimates, it‚Äôs unclear what the intercept (or 0) even means. Does it mean ‚Äúaverage‚Äù? If so, what is an ‚Äúaverage‚Äù league?\nWe accounted for minutes played‚Äîthe ‚Äúper 90‚Äù denominator‚Äîprior to subtracting rates (difference in VAEP/90), which is different than how APM works. In APM, the minutes played is directly accounted for in the response variable (net points, divided by possessions).\n\nThe take-away here is that we can only interpret the model coefficients on a relative basis.\n\n\nCode\n#&gt; # A tibble: 256 x 5\n#&gt;    `League A`               `League B`               `Estimate A` `Estimate B` Diff.\n#&gt;    &lt;fct&gt;                    &lt;fct&gt;                           &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1 Premier League (England) Premier League (England)        0.975        0.975 0    \n#&gt;  2 Premier League (England) La Liga (Spain)                 0.975        0.869 0.107\n#&gt;  3 Premier League (England) Ligue 1 (France)                0.975        0.786 0.189\n#&gt;  4 Premier League (England) Serie A (Italy)                 0.975        0.738 0.238\n#&gt;  5 Premier League (England) Serie A (Brazil)                0.975        0.724 0.252\n#&gt;  6 Premier League (England) Primeira Liga (Portugal)        0.975        0.675 0.300\n#&gt;  7 Premier League (England) Bundesliga 1 (Germany)          0.975        0.649 0.326\n#&gt;  8 Premier League (England) Championship (England)          0.975        0.641 0.334\n#&gt;  9 Premier League (England) Super Lig (Turkey)              0.975        0.617 0.358\n#&gt; 10 Premier League (England) Premier League (Russia)         0.975        0.485 0.491\n#&gt; # ... with 246 more rows\n\n\n\n‚ÄúUn-transform‚Äù the coefficients of the regression using a ‚Äúweighted-average‚Äù standard deviation and mean from the z-transformations of groups. 7\n\nInterpretation after this transformation can be a little tricky. The differences between a specified pair of these post-transformed coefficients represents the expected change in an ‚Äúaverage‚Äù player‚Äôs VAEP/90 (Diff. (VAEP/90)) when moving between the specified leagues.\n\n\nCode\n#&gt; # A tibble: 256 x 4\n#&gt;    `League A`               `League B`               Diff. `Diff. (VAEP/90)`\n#&gt;    &lt;fct&gt;                    &lt;fct&gt;                    &lt;dbl&gt;             &lt;dbl&gt;\n#&gt;  1 Premier League (England) Premier League (England) 0                0     \n#&gt;  2 Premier League (England) La Liga (Spain)          0.107            0.0166\n#&gt;  3 Premier League (England) Ligue 1 (France)         0.189            0.0295\n#&gt;  4 Premier League (England) Serie A (Italy)          0.238            0.0371\n#&gt;  5 Premier League (England) Serie A (Brazil)         0.252            0.0393\n#&gt;  6 Premier League (England) Primeira Liga (Portugal) 0.300            0.0469\n#&gt;  7 Premier League (England) Bundesliga 1 (Germany)   0.326            0.0509\n#&gt;  8 Premier League (England) Championship (England)   0.334            0.0521\n#&gt;  9 Premier League (England) Super Lig (Turkey)       0.358            0.0559\n#&gt; 10 Premier League (England) Premier League (Russia)  0.491            0.0766\n#&gt; # ... with 246 more rows\n\n\nTo interpret these differences as a percentage (so that we can ‚Äúscale‚Äù the properly for a player with a VAEP/90 of 1.5, compared to a player with a lower VAEP/90 rate), we use the median VAEP/90 across all leagues as a ‚Äúbaseline‚Äù. For example, for Bundesliga -&gt; Premier League, since the overall median VAEP/90 is 0.305 and the Diff. (VAEP/90) between league A and league B is 0.0509, the % Difference is 0.0509/0.305 = 17%.\n\n\nCode\n#&gt; # A tibble: 256 x 5\n#&gt;    `League A`               `League B`               Diff. `Diff. (VAEP/90)` `% Diff.`\n#&gt;    &lt;fct&gt;                    &lt;fct&gt;                    &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;    \n#&gt;  1 Premier League (England) Premier League (England) 0                0      0%       \n#&gt;  2 Premier League (England) La Liga (Spain)          0.107            0.0166 5%       \n#&gt;  3 Premier League (England) Ligue 1 (France)         0.189            0.0295 10%      \n#&gt;  4 Premier League (England) Serie A (Italy)          0.238            0.0371 12%      \n#&gt;  5 Premier League (England) Serie A (Brazil)         0.252            0.0393 13%      \n#&gt;  6 Premier League (England) Primeira Liga (Portugal) 0.300            0.0469 15%      \n#&gt;  7 Premier League (England) Bundesliga 1 (Germany)   0.326            0.0509 17%      \n#&gt;  8 Premier League (England) Championship (England)   0.334            0.0521 17%      \n#&gt;  9 Premier League (England) Super Lig (Turkey)       0.358            0.0559 18%      \n#&gt; 10 Premier League (England) Premier League (Russia)  0.491            0.0766 25%      \n#&gt; # ... with 246 more rows"
  },
  {
    "objectID": "posts/soccer-league-strength/index.html#improvements-further-work",
    "href": "posts/soccer-league-strength/index.html#improvements-further-work",
    "title": "Quantifying Relative Soccer League Strength",
    "section": "Improvements & Further Work",
    "text": "Improvements & Further Work\n\nAlthough my approach does eventually get back to the original units (VAEP/90), it does feel a little convoluted. Aditya Kothari proposed re-defining the target variable in the regression to be the ratio of VAEP/minute (instead of a z-transformed difference in VAEP/90) between the leagues that a player moves to and from. (See his full post.) In my eyes, the main advantage of such an approach is that it is more direct. A player-level ratio embeds information about position and age‚Äîa forward will tend to have higher VAEP/minute than a defender, and will continue to have higher VAEP/minute than a defender after transferring‚Äîso normalizing for age and position is not necessarily justified. Additionally, the model‚Äôs league coefficients can be directly interpreted, unlike my approach. Perhaps the main disadvantage is sensitivity to low minutes played. 8\nAnother weakness in my approach is the assumption that relative league strengths are the same every year, which is most certainly not true. One could apply a decaying weight to past seasons to account for varying league strength.\nI would be hesitant to use my results to directly infer how a specific player will translate going from one league to another. My approach focuses on leagues and is more about the ‚Äúaverage‚Äù player. One aught to include additional features about play style (e.g.¬†touches, progressive passes, team role) if interested in predicting individual player performance with a high degree of accuracy.\nOne can swap out the response variable with other reasonable metrics of player performance, such as xG (which is more readily available than atomic VAEP). In fact, I did this myself and came up with the result below (showing in units of xG/90 instead of as a percentage, since most fans are accustomed to seeing xG and are used to its relative magnitude).\n\n\n\nOne could stay in the realm of just purely ‚Äúpower rankings‚Äù and focus more on the estimates and error. For example, in an earlier iteration of this methodology, I used a Bradley-Terry approach to come up with a distribution of estimates for each league.9 Here, the x-axis could be loosely interpreted as the log odds of one league winning in a match versus another league, although it‚Äôs not clear exactly what that means. (An ‚Äúaverage‚Äù team from both leagues? A matchup of teams composed of ‚Äúaverage‚Äù players from any team in each league?)\n\n\n\nNotably, I‚Äôm not using match results at all! Certainly a model could learn something from international and tournament matches. However, using match-level data would require a whole new approach. Also, most would agree that tournament data can be biased by atypical lineups. For example, a manager on one side may opt to rest their best players, saving them for domestic league games, while the other manager may play their side at full strength.10\n\n\n\nSample size is an issue on two levels: (1) the number of transfers (more data would be better) and (2) minutes played.\n\nRegarding (1), one could expand the data set by including all seasons played by a player that has played in more than one league, taking all combinations of seasons in different leagues (i.e.¬†relaxing the the same-season or subsequent-season criteria). I actually did attempt this and found that overall the results were somewhat similar, but there were more questionable results overall. (Brazil‚Äôs Serie A was found to be the second strongest league overall with this approach.).\nRegarding (2), one has to make a choice to drop players with low minutes played to prevent outliers affecting the results of the model. However, in some cases, a loaned player coming in at the end of the season and making a huge impact can tell us a lot about the difference in strength of two leagues, so we may not want to drop some of the records after all. An empirical bayes adjustment to VAEP/90, not unlike the one described here by David Robinson, can help overcome this. Below shows how such an adjustment slightly ‚Äúshrinks‚Äù VAEP/90 numbers, especially for those who played less.\n\nOn the topic of ‚Äúshrinking‚Äù, we could have used ridge regression (regression with some penalty) to get more robust league estimates overall. However, there is a downside to ridge regression‚Äîwe give up some level of interpretability.11 Nonetheless, the relative ranking of leagues would be more reliable with ridge regression."
  },
  {
    "objectID": "posts/soccer-league-strength/index.html#ancillary-take-away",
    "href": "posts/soccer-league-strength/index.html#ancillary-take-away",
    "title": "Quantifying Relative Soccer League Strength",
    "section": "Ancillary Take-away",
    "text": "Ancillary Take-away\nOne final thing I‚Äôd like to point out here: I think this whole approach really showcases the inference made possible by player stats (xG, possession value metrics like atomic VAEP, etc.) aggregated over long periods of time. While such stats are often used to evaluate player performance in single games or even for singular in-game actions, they are most effective in providing insight when employed in higher-level analyses."
  },
  {
    "objectID": "posts/soccer-pass-network-max-cut/index.html",
    "href": "posts/soccer-pass-network-max-cut/index.html",
    "title": "Yet Another (Advanced?) Soccer Statistic",
    "section": "",
    "text": "Pass networks are a common visualization form used to summarize a team‚Äôs behavior in a soccer match. Nodes represent average player position on passes that they are involved with, and edges represent passes between players. Most pass networks also weight node size and edge width by the total number of passes.\n\nWhile pass networks provide a nice visual tool for providing insight that can (and should) be supplemented by more detailed analysis, they‚Äôre often just that‚Äîpurely a visual tool. In order to gain meaning beyond just anecdotal insight (‚ÄúLook at how far the wingbacks were up the field!‚Äù), practitioners may leverage graph theory concepts such as centrality to quantify relationships.1\nInspired by the findings of Eliakim et al.¬†in ‚ÄúThe development of metrics for measuring the level of symmetry in team formation and ball movement flow, and their association with performance‚Äù, I wanted to evaluate a graph theory concept that has not been explored in relation to soccer pass networks (except for by Eliakim et al.): maximum cuts.\nTo do so, I‚Äôll be using data from the 2017/18 - 2020/21 Premier League seasons, along with the games up through Boxing Day of the 2021/22 season. Passes and other events are only considered up through the first substitution or red card of each match."
  },
  {
    "objectID": "posts/soccer-pass-network-max-cut/index.html#examples",
    "href": "posts/soccer-pass-network-max-cut/index.html#examples",
    "title": "Yet Another (Advanced?) Soccer Statistic",
    "section": "Examples",
    "text": "Examples\n\nSimple\nWhat is a maximum cut? Visually, it‚Äôs an arbitrary line that you can draw through the edges of a network that maximizes the sum of the edge weights.\n\nFor this example, 15 is actually a weighted max cut, since edges are treated differently, according to their assigned value. (An unweighted max cut would assign each edge a value of 1.)\nOn the other side of things, the min cut would be 2+4=6 for this example.\n\n\nIn Soccer\nA 4-node, 5-edge network is nice for illustration, but how does this bear out in soccer?\nTo give a soccer example, Here‚Äôs the pass network and weighted max cut numbers for the match between Liverpool and Manchester City on October 3, 2021. 2\n\nZooming out from a single example to all games in our data set, the distribution of weighted max cuts per 90 minutes looks relatively normal, perhaps log-normal. Note: It‚Äôs important to adjust for time since not all games have the same number of minutes played due to variance in the occurrence of the first formation change."
  },
  {
    "objectID": "posts/soccer-pass-network-max-cut/index.html#but-is-max-cut-useful",
    "href": "posts/soccer-pass-network-max-cut/index.html#but-is-max-cut-useful",
    "title": "Yet Another (Advanced?) Soccer Statistic",
    "section": "But is Max Cut Useful?",
    "text": "But is Max Cut Useful?\n\nSetup\nTo quantify the impact of weighted max cuts, we‚Äôll look at two measures of quality of play.\n\nexpected goals (xG): xG tells the story of shot quality and quantity, which is massively important in a low-scoring game like soccer.\nexpected threat (xT): xT quantifies scoring opportunities more generally, looking beyond shots.\n\nI‚Äôd argue that xT is more informative for our purposes since max cut is related to passes and xT accounts for passes; xG is so tied up in shots that their relationship to passes leading up to those shots may be lost. Nonetheless, we‚Äôll be considering both since both are commonly used for judging overall ‚Äúvalue‚Äù in soccer.\nWe‚Äôll be transforming these xG and xT in two manners.\n\nVolume-adjusting, i.e.¬†taking each value per 90 minutes. The justification for adjusting max cut for time also applies here.\nOpponent-adjusting, or ‚Äúdifferencing‚Äù, i.e.¬†subtracting one side‚Äôs value from the other‚Äôs value. Sure, having a lot of touches in the opponent‚Äôs half and taking a lot of shots means scoring is more likely, but if you‚Äôre also giving up a ton of shots, then that effort is essentially negated.\n\nGiven that we‚Äôll be making our two quality-of-play stats‚ÄîxG and xT‚Äîrelative to the opponent, I prefer to opponent-adjust weighted max cut in the same manner. I‚Äôd argue that weighted max cut differential is more informative than just the weighted max cut of one side or the other. Suppressing your opponent‚Äôs weighted max cut is reflective of limiting their pass volume, and, consequently, makes it more likely that your weighted max cut is higher.\nThe relationship between weighted max cut and weighted max cut differential is very linear, so, ultimately, it shouldn‚Äôt matter too much if we look at opponent-adjust weighted max cut versus just raw weighted max cut.\n\nDifferencing has the added benefit of making our distributions look more ‚Äúnormal‚Äù, by making them symmetric about 0. This generally is beneficial for regression analysis, which we go on to conduct.\n\n\nCorrelations\nA first step in looking at the relationship between weighted max cut with xG and xT is a correlation.\n\nWeighted max cut compares favorably to other network stats for summarizing (pass) networks. Perhaps this isn‚Äôt too surprising; Eliakim et al.¬†argue that, in relation to soccer, maximum cut surmises what is captured separately by various measures of centrality (betweenness, indegree and outdegree, etc.).\nWeighted max cut has a similar correlation to traditional pass metrics such as relative percentage of passes, but not as strong as counting stats for shots. We really shouldn‚Äôt expect any metric to have as strong a relation with xG and xT (especially xG) as shot-based metrics since these are derived from shots and their outcomes.\nOverall, the game-level correlations are not super strong, indicating that we can‚Äôt read too much into them for individual games. The correlations are much stronger at the season-level, showing the same ordinality in magnitude of correlation.\n\nObserving the difference in the game-level and season-level correlations should be a good reminder that single-game stats should not be scrutinized too heavily when evaluating a team‚Äôs performance over the course of a season. The same is true for max cuts!\n\n\nAccounting for Confounders\nThe correlation approach for quantifying the descriptive role of max cuts in quality of play is a bit naive. Single-variable regressions, i.e.¬†correlations, overstate the impact of the ‚Äútreatment‚Äù variables.\nIf we regress max cut on xG and xT with z-score normalized counts of shots and passes as confounders, we see that the influence of max cut is negated.\n\nIn the case of xG, the coefficient estimate for weighted max cut is offset by the coefficient for the passing term. This is due to their collinearity (over 90%) and their lack of explanatory value in the presence of shot counts, which directly informs xG. For xT, the weighted max cut coefficient is completely suppressed, likely due to collinearity with passing.\nOf course, we could be a little more sophisticated here, drawing out directed acyclic graphs (DAG) and running a more formal causal analysis. But my intuition is that we would come to the same general conclusion: in the face of more traditional metrics like shot and pass counts, possibly the most robust pass-network-derived statistic‚Äîweighted max cut‚Äîprovides minimal additional descriptive power for quantifying quality of play.\n\n\nWhy Haven‚Äôt People Tried this Before?\nI can think of a couple of reasons why weighted max cut isn‚Äôt commonly seen in soccer literature:\n\nThe calculation requires an un-directed network. In our context, this requires treating passes between players as equal, regardless of who is the passer and who is the receiver. This can distort the role of a striker, who may receive much more than pass, or a keeper, who may pass much more than receive.\nIt‚Äôs difficult to visualize beyond just reporting a single number, and, thus, may not resonate with an audience.\nIt‚Äôs not super easy to calculate! In fact, {igraph}‚Äîthe most popular R framework for network analysis‚Äîdoesn‚Äôt have a function for it!\n\nFor what it‚Äôs worth, the code that I wrote to calculate the weighted maximum cut for a pass network looks something like this.3 (This is the calculation for the 4-node example network shown before.)\n\n\nCode\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(sdpt3r) ## Semi-Definite Quadratic Linear Programming Solver\n\ndf <- tibble(\n  from = c('a', 'a', 'a', 'b', 'b', 'b', 'c', 'c', 'c', 'd', 'd', 'd'),\n  to   = c('b', 'c', 'd', 'a', 'c', 'd', 'a', 'b', 'd', 'a', 'b', 'c'),\n  n    = c( 1L,  0L,  3L,  1L,  1L,  1L,  0L,  2L,  1L,  1L,  5L,  4L)\n)\n\nwide_df <- df %>% \n  pivot_wider(\n    names_from = to,\n    values_from = n,\n    values_fill = 0L\n  ) %>% \n  select(from, a, b, c, d) %>% \n  arrange(from) %>% \n  select(-from)\nwide_df\n#> # A tibble: 4 x 4\n#>       a     b     c     d\n#>   <int> <int> <int> <int>\n#> 1     0     1     0     3\n#> 2     1     0     1     1\n#> 3     0     2     0     1\n#> 4     1     5     4     0\n\nm <- as.matrix(wide_df)\nsymmetric_m <- m + t(m) ## must be symmetric\nmc <- maxcut(symmetric_m)\nmax_cut <- -round(mc$pobj, 0)\nmax_cut\n#> [1] 15\n\n\nOr, perhaps, people have actually evaluated max cut for pass networks, but have found the same non-significant result that I have found, and have simply opted not to write about it. ü§∑"
  },
  {
    "objectID": "posts/soccer-pass-network-max-cut/index.html#conclusion",
    "href": "posts/soccer-pass-network-max-cut/index.html#conclusion",
    "title": "Yet Another (Advanced?) Soccer Statistic",
    "section": "Conclusion",
    "text": "Conclusion\nWeighted max cut can be a very informative metric for summarizing pass volume and pass network structure, as seen in a correlation analysis. It‚Äôs merit surpasses that of other summary network stats and is nearly equivalent to traditional pass-derived stats for explaining xG and xT. However, I don‚Äôt think it should supersede more traditional stats like shots and passes if purely evaluating attacking quality."
  },
  {
    "objectID": "posts/soccer-pitch-control-r/index.html",
    "href": "posts/soccer-pitch-control-r/index.html",
    "title": "Creating a Soccer Pitch Control Model",
    "section": "",
    "text": "There‚Äôs never been a better time to be involved in sports analytics. There is a wealth of open-sourced data and code (not to mention well-researched and public analysis) to digest and use. Both people working for teams and people just doing at as a hobby are publishing new and interesting analyses every day.\nIn particular, the FriendsOfTracking (FOT) group, co-led by Professor and author David Sumpter1 have put together an awesome series of videos on YouTube discussing modern soccer analytics, along with a collection of repositories on GitHub sharing the code shown in videos.\nLaurie Shaw has shared code that implements the pitch control model described in William Spearman‚Äôs paper ‚ÄúBeyond Expected Goals‚Äù is interesting to me. The model is different than the one that I used to create some animations on Twitter. Those were based on the pitch control model described by Javier Fernandez and Luke Bornn in their paper ‚ÄúWide Open Spaces‚Äù (code courtesy of Rob Hickman). (Apologies for the onslaught of links!)\nNow, I am not one for language wars‚Äîand, in fact, I use python often‚Äîbut I thought it would be awesome to be able to plot Spearman‚Äôs pitch control model directly with {ggplot2} and friends. Thus, I set out to convert Laurie‚Äôs code to R, attempting to give it a ‚Äúnative‚Äù R feel while I was at it.\nMost of the process of translating python to R was relatively straightforward (slicing and dicing data frames and arrays/vectors is just part of data cleaning), so I won‚Äôt detail them here. However, there was one part that was particularly interesting‚Äîthe conversion of a python class object. This was actually the key (and most challenging part) of the conversion process.\nThere are some great resources for describing how to implement object-orientated programming (OOP) in R, including a couple of chapter‚Äôs from Hadley Wickham‚Äôs Advanced R book and a very practical write-up from Earo Wang. Every object-oriented task has its unique aspects, so hopefully my discussion here has something to add to what has already been written on the subject matter.\nFor demonstration purposes, I‚Äôm going to walk through my steps for converting the python class object as if I were doing it for the first time."
  },
  {
    "objectID": "posts/soccer-pitch-control-r/index.html#constructor",
    "href": "posts/soccer-pitch-control-r/index.html#constructor",
    "title": "Creating a Soccer Pitch Control Model",
    "section": "Constructor",
    "text": "Constructor\nBelow is a stripped down version of Laurie‚Äôs code, showing the ‚Äúessence‚Äù of what we need to replicate.2\n\n\nCode\nclass player(object):\n    def __init__(self,player_id,frame):\n        self.id = player_id\n        self.get_position(frame)\n        self.get_velocity(frame)\n        \n    def get_position(self,frame):\n        self.position = np.array(frame[self.player_id + 'x', self.player_id + 'y'])\n        \n    def get_velocity(self,frame):\n        self.velocity = np.array(frame[self.player_id + 'x_v', self.player_id + 'y_v'])\n    \n    def tti(self,final_position):\n        reaction_time = 0.7 # in s\n        vmax = 5 # in m/s\n        reaction_position = self.position + self.velocity * reaction_time\n        self.tti = reaction_time + np.linalg.norm(final_positon - reaction_position)/vmax\n\n    def p_intercept(self,t):\n        tti_sigma = 0.45\n        den = 1 + np.exp(-np.pi/np.sqrt(3.0)/tti_sigma * (t-self.tti)))\n        return 1 / den\n\n\nLet‚Äôs make some notes and come back to these as we develop our R class.\n\nWe need a unique identifier: player_id. This is just a ‚Äúbest practice‚Äù thing for object-oriented programming and makes sense given our context. For a sport like soccer, a unique identifier could just be the player‚Äôs name, a combination of the team name and the player jersey number, a league unique identifier, etc.\nA single-row data frame frame is passed to several of the methods, including the constructor __init__. This single row data frame is sourced from a much larger tracking data frame, with rows for every 0.04 second time interval (25 frames per second, or one frame per 0.04 seconds) in the game.\nThe python code stores both the player‚Äôs position and velocity as 2x1 arrays. This works well with the unpacking that is done in other places in Laurie‚Äôs code.\ntti, short for ‚Äútime to intercept (a target location)‚Äù, uses the player‚Äôs position and velocity to define the attribute tti (not to be confused with the method itself). This implies that position and velocity should be defined before tti() is ever called, as they are in __init__. tti needs the position_final 2x1 array to calculate tti which is not known upon instantiation; rather, tti can only be properly defined when called to do a specific calculation relating the player‚Äôs position and velocity (both defined implicitly in the class, without needing user-specification) with a user-supplied position_final pair of x and y values.\np_intercept, short for ‚Äúprobability to intercept (a target location)‚Äù depends on tti and an additional parameter t, a user-specified value representing how much time is allotted to reach the ball. Like tti, p_intercept is only ‚Äúproperly‚Äù defined when actually doing a calculation on the player‚Äôs attributes. Unlike tti, there is no attribute in the player instance that stores this probability; it‚Äôs value must be saved in a variable external to the player class if the user wants to use it for something other than an ephemeral calculation.3\n\nTime to intercept a ‚Äútarget‚Äù location (tti) may not be intuitive to comprehend immediately. The plot4 below annotates the tti of a ‚Äútarget‚Äù location on the pitch (which does not have to be where the ball actually is). tti assumes that the player continues moving at their current speed (annotated by the arrows) for reaction_time seconds before running at vmax (full speed) to the target position. tti for each player is independent of the tti of all other players, which is a relatively reasonable assumption. 5\n\nThe probability of reaching the ‚Äútarget‚Äù location (p_intercept) is directly related to the player‚Äôs tti. Uncertainty about how long it will take the player to reach the target location is quantified by the constant tti_sigma in the calculation. (tti is the mean and tti_sigma is the standard deviation of the distribution for a player‚Äôs time to arrive at the target location.)\n\nNotably, this probability is independent of all other players‚Äô probabilities (which explains how it is possible that both players are shown to have probabilities greater than 50% when t = 6 above). When adjusting for all players‚Äô probabilities (by dividing by the sum of all probabilities), the numbers change. This probability adjustment is key when we calculate pitch control.\n\nOk, on to the R code. We‚Äôll be using S3 and the {vctrs} package to help create our player class. (As with the python class, I‚Äôve simplified the actual implementation for demonstration purposes.)\nFirst, we start with the constructor new_player(). Note that there is no direct __init__ equivalent in R. Here we will make a function that is prefixed with new_ and ends with the name of our class (player).\n\n\nCode\nnew_player <- function(\n  player_id = integer(),\n  x = double(),\n  y = double(),\n  x_v = double(),\n  y_v = double()\n) {\n  vctrs::new_rcrd(\n    list(\n      player_id = player_id,\n      x = x,\n      y = y,\n      x_v = x_v,\n      y_v = y_v,\n      tti = -1 # dummy value\n    ),\n    class = 'player'\n  )\n}\n\n\nNow let‚Äôs reflect upon our prior notes.\n\nWe have the player_id in this constructor.\nWe don‚Äôt pass the data frame tracking here. We‚Äôll do it in our helper function. We might say that our constructor is ‚Äúlow-level‚Äù, not intended for the user to call directly.\nWe split the position and velocity vectors into their individual x and y components, resulting in four total variables instead of two. I don‚Äôt think a vector (unnamed or named), list, or matrix are particularly compelling data types to use for an x-y pair of values in R. None natively support unpacking (although R vectors do have some form of ‚Äúbroadcasting‚Äù with their recycling behavior).\nWe assign a ‚Äúdummy‚Äù value (-1) to tti when initializing the class instance. We will have a method to update tti based on x and y components.\nLike tti, we will need a separate p_intercept method to be used to calculate the probabililty of intercepting a ball given a player‚Äôs position, speed, and the final position of the ball (all fed as inputs to tti), as well as the additional user-specified t, representing how much time is allotted to reach the ball."
  },
  {
    "objectID": "posts/soccer-pitch-control-r/index.html#validator",
    "href": "posts/soccer-pitch-control-r/index.html#validator",
    "title": "Creating a Soccer Pitch Control Model",
    "section": "Validator",
    "text": "Validator\nLet‚Äôs proceed by creating a validator function to, you guessed it, validate fields in the player class. It is good practice to check the values used to construct the class. The python code did not have any validation like this, but I don‚Äôt think it was ever expected to be extremely robust to any user input.\n\n\nCode\nvalidate_player <- function(player) {\n  vctrs::vec_assert(vctrs::field(player, 'player_id'), integer())\n  vctrs::vec_assert(vctrs::field(player, 'x'), double())\n  vctrs::vec_assert(vctrs::field(player, 'y'), double())\n  vctrs::vec_assert(vctrs::field(player, 'tti'), double())\n  player\n}\n\n\nNote that we could have simply done this validation in the constructor function, but I think it makes sense to put the validation in its own function so that the constructor is more direct (especially if the validation checks are complex)."
  },
  {
    "objectID": "posts/soccer-pitch-control-r/index.html#helper",
    "href": "posts/soccer-pitch-control-r/index.html#helper",
    "title": "Creating a Soccer Pitch Control Model",
    "section": "Helper",
    "text": "Helper\nFinally, we‚Äôll create a helper player() function, which is our ‚Äúuser-facing‚Äù function that we expect/want users to use to instantiate objects.\n\n\nCode\nplayer <- function(player_id, frame, tracking) {\n    \n    player_id <- as.integer(player_id)\n    frame <- as.integer(frame)\n\n    assertthat::assert_that(is.data.frame(tracking))\n    nms_req <- c('player_id', 'frame', 'x', 'y', 'x_v', 'y_v')\n    assertthat::assert_that(all(nms_req %in% names(tracking)))\n    \n    # `!!` to make sure that we filter using the integer values, not the column itself.\n    tracking_filt <- tracking %>% filter(player_id == !!player_id, frame == !!frame)\n    assertthat::assert_that(nrow(tracking_filt) == 1L)\n    \n    player <-\n      new_player(\n        player_id = player_id,\n        x = tracking_filt[['x']],\n        y = tracking_filt[['y']],\n        x_v = tracking_filt[['x_v']],\n        y_v = tracking_filt[['y_v']]\n      )\n    validate_player(player)\n  }\n\n\nNote the following:\n\nWe coerce player_id and frame to integers instead of doubles (particularly since they are expected to be integers in the constructor). This ensures that the new player is instantiated properly by the constructor and passes our validation.\nWe pass in our entire tracking data frame (that has rows for every 0.04 second interval in the game), as well as the frame to slice out of it. (player_id is also used to filter tracking.) This makes it convenient for user to instantiate new player objects when operating on the tracking data frame. There is no need to extract the singular initial position and velocity components ‚Äúmanually‚Äù; instead, the helper function does it for the user."
  },
  {
    "objectID": "posts/soccer-pitch-control-r/index.html#aside",
    "href": "posts/soccer-pitch-control-r/index.html#aside",
    "title": "Creating a Soccer Pitch Control Model",
    "section": "Aside",
    "text": "Aside\nR‚Äôs S3 framework is not a formal OOP framework (not even close really). Note that it does not have a reserved keyword to represent the instance of the class like self in python. Also, it is not actually necessary for most of what is done above (with the constructor, validator, and helper).\nFor example, we don‚Äôt actually have to create a formal-ish constructor prefixed with new_. We don‚Äôt even need a constructor function at all in S3. We could do something like class(var) <- 'player' to create a a player object. Of course, this is prone to errors down the line, so we don‚Äôt do that. Likewise with the validator and helper functions. The point of these constructs is to add clarity to our class code. They aren‚Äôt strictly necessary."
  },
  {
    "objectID": "posts/soccer-pitch-control-r/index.html#printing",
    "href": "posts/soccer-pitch-control-r/index.html#printing",
    "title": "Creating a Soccer Pitch Control Model",
    "section": "Printing",
    "text": "Printing\nLet‚Äôs do one more thing for our player class‚Äîcreate a custom print method. (Writing a custom print method is not required whatsoever, but it can be very helpful for debugging.) If we weren‚Äôt using {vctrs} and just S3, we would do this by writing a print.player function. However, {vctrs} provides a ‚Äúpretty‚Äù header for us auto-magically (that looks like <player[1]>) if we use it to write our print method.\nTo take advantage of the pretty-printing functionality offered by {vctrs}, we write a format.player() method that will be called by a subclass of the generic vctrs::obj_print_data method6, which itself is called whenever we print out an object (whether explicitly with print or just by typing the name of the variable representing our player instance). We‚Äôll add the player‚Äôs position and velocity components to the print out.\n\n\nCode\nformat.player <- function(player, ...) {\n  if(vctrs::field(player, 'in_frame')) {\n    suffix <- \n      sprintf(\n        'with `position = (%.2f, %.2f)` and `velocity = <%.1f, %.1f>`', \n        vctrs::field(player, 'player_id'), \n        vctrs::field(player, 'y'), \n        vctrs::field(player, 'x_v'),\n        vctrs::field(player, 'y_v')\n      )\n  } else {\n    suffix <- 'is not on the pitch'\n  }\n  prefix <- sprintf('`player_id = %s` ', vctrs::field(player, 'player_id'))\n  msg <- sprintf('%s%s', prefix, suffix)\n  paste(msg, sep = '\\n')\n}\n\nobj_print_data.player <- function(player) {\n  cat(format(player), sep = '\\n')\n}"
  },
  {
    "objectID": "posts/soccer-pitch-control-r/index.html#basic-usage",
    "href": "posts/soccer-pitch-control-r/index.html#basic-usage",
    "title": "Creating a Soccer Pitch Control Model",
    "section": "Basic Usage",
    "text": "Basic Usage\nOk, so that is all fine and dandy, but how would we go about instantiating players in a normal workflow?\nLet‚Äôs say that we want to calculate the pitch control for a single frame in the tracking data (called tracking_start below).7\n\n\nCode\ntracking_start\n#> # A tibble: 26 x 9\n#>    frame ball_x ball_y side  player_id     x     y   x_v    y_v\n#>    <int>  <dbl>  <dbl> <chr>     <int> <dbl> <dbl> <dbl>  <dbl>\n#>  1 53027  93.71  24.56 home          1 90.72 39.37 5.906 -3.985\n#>  2 53027  93.71  24.56 home          2 95.10 27.14 1.5   -2.023\n#>  3 53027  93.71  24.56 home          3 96.01 23.32 1.418  2.395\n#>  4 53027  93.71  24.56 home          4 92.39 15.64 1.005  3.473\n#>  5 53027  93.71  24.56 home          5 83.96 24.69 4.238  1.2  \n#>  6 53027  93.71  24.56 home          6 82.19 35.63 3.893 -0.619\n#>  7 53027  93.71  24.56 home          7 85.79 17.34 1.703  1.523\n#>  8 53027  93.71  24.56 home          8 76.06 50.16 2.018 -0.493\n#>  9 53027  93.71  24.56 home          9 61.22 25.35 0.863 -0.77 \n#> 10 53027  93.71  24.56 home         10 59.69 35.10 0.9   -0.573\n#> # ... with 16 more rows\n\n\nLet‚Äôs convert players with id‚Äôs 10 through 12 (on the home team) to player instances and see how they look when printed out.\n\n\nCode\n10L:12L %>% map(~player(player_id = .x, frame = 53027L, tracking = tracking_start))\n#> [[1]]\n#> <player[1]>\n#> `player_id = 10` with `position = (10.00, 35.09)` and `velocity = <0.9, -0.6>`\n#> \n#> [[2]]\n#> <player[1]>\n#> `player_id = 11` with `position = (11.00, 32.28)` and `velocity = <-0.3, 0.6>`\n#> \n#> [[3]]\n#> <player[1]>\n#> `player_id = 12` is not on the pitch"
  },
  {
    "objectID": "posts/soccer-pitch-control-r/index.html#pseudo-encapsulation",
    "href": "posts/soccer-pitch-control-r/index.html#pseudo-encapsulation",
    "title": "Creating a Soccer Pitch Control Model",
    "section": "Pseudo-Encapsulation",
    "text": "Pseudo-Encapsulation\nWe still need to implement analogues for the tti and p_intercept methods in the python player class. Starting with tti, let‚Äôs use some pseudo-encapsulation (with getters and setters) for a player‚Äôs tti value.\n\n\nCode\n# Frobenious norm\neuclidean_norm <- function(x1, x2, y1, y2) {\n  m <- matrix(c(x1, y1)) - matrix(c(x2, y2))\n  sqrt(sum(m^2))\n}\n\n.get_tti.player <- function(player, x2, y2) {\n  ri <- 0.7 # in s\n  vmax <- 5 # in m/s\n  x1 <- vctrs::field(player, 'x') + vctrs::field(player, 'x_v') * ri\n  y1 <- vctrs::field(player, 'y') + vctrs::field(player, 'y_v') * ri\n  ri + euclidean_norm(x1, x2, y1, y2) / vmax\n}\n\n.msg_cls_err <- function(player, f) {\n  cls <- class(player)[1]\n  sprintf('`%s()` doesn\\'t know how to handle class `%s`!', f, cls) \n}\n\n.get_tti.default <- function(player, ...) {\n  stop(.msg_cls_err(player, '.get_tti'), call. = FALSE)\n}\n\n.get_tti <- function(player, ...) {\n  UseMethod('.get_tti')\n}\n\n`.set_tti<-.player` <- function(player, value) {\n  vctrs::field(player, 'tti') <- value\n  player\n}\n\n`.set_tti<-.default` <- function(player, ...) {\n  stop(.msg_cls_err(player, '.set_tti'), call. = FALSE)\n}\n\n`.set_tti<-` <- function(player, ...) {\n  UseMethod('.set_tti<-')\n}\n\n\nThere‚Äôs a couple of things going on here:\n\nThe .get_tti and .set_tti functions that call UseMethod are true S3 generics that perform method dispatch, i.e.¬†find the correct method for the object passed to the generic (based on the class of the object). The .get_tti.player and .set_tti.player with the .player ‚Äúsuffix‚Äù so that they only work in their defined manners when passed in a player instance. (They won‚Äôt be called with an object that is not of the player class.)\nThe ellipses (...) in the S3 generic function signatures may be a bit mysterious since they aren‚Äôt passed explicitly to UseMethod. Any non-player arguments are captured in these ellipses and passed to whatever method that is called from the generic (e.g.¬†.get_tti.player method called from the .get_tti generic). For .get_tti, the ellipses is intended to capture x2 and y2, and for .set_tti, it captures value.\nWe must use the ‚Äústrange‚Äù syntax .set_tti<-.player (instead of just .set_tti.player, which may seem more ‚Äúnatural‚Äù) in order to update an attribute in an already instantiated class. 8\nWe define the function euclidean_norm() outside of .get_tti.player simply because it is not something that is specific to the time to intercept calculation for a player; it can work with any two pairs of x and y coordinates.9\nri and vmax, representing a player‚Äôs reaction time and a player‚Äôs maximum velocity respectively, are constants defined in the Spearman paper. We could change these if we wanted to, or even make them dynamic (i.e.¬†configurable via other function parameters, or even at instantiation time).\n\nTo really complete our getter and setter methods for tti, we should write methods to handle the case when a non-player object is passed to them. The generic .get_tti and .set_tti methods will dispatch to these functions if the object passed to them (the first argument named player) doesn‚Äôt actually inherit from the player class.\n\n\nCode\n.get_tti.default <- function(player, ...) {\n  stop(.msg_cls_err(player, '.get_tti'), call. = FALSE)\n}\n\n.set_tti.default <- function(player, ...) {\n  stop(.msg_cls_err(player, '.get_tti'), call. = FALSE)\n}\n\n\nLet‚Äôs see how our pseudo-encapsulation looks in action.\n\n\nCode\nplayers <- 8L:10L %>% map(~player(player_id = .x, frame = 53027L, tracking = tracking_start))\nmap(players, ~vctrs::field(.x, 'tti'))\n#> [[1]]\n#> [1] -1\n#> \n#> [[2]]\n#> [1] -1\n#> \n#> [[3]]\n#> [1] -1\n\n\n\n\nCode\ntarget_x <- 94\ntarget_y <- 63\nfor(i in seq_along(players)) {\n  value <- .get_tti(players[[i]], x2 = target_x, y2 = target_y)\n  .set_tti(players[[i]]) <- value\n}\nmap(players, ~vctrs::field(.x, 'tti'))\n#> [[1]]\n#> [1] 4.92839\n#> \n#> [[2]]\n#> [1] 10.6878\n#> \n#> [[3]]\n#> [1] 9.49904\n\n\nNote how the player tti values changed after we defined them for a specified target_x and target_y.\nOur approach to p_intercept is very similar to that for tti, so I don‚Äôt show most of it here. As before, we define getters and setters, as well as generics for the class (the intended target of method dispatch), as well as a default class to handle unexpected inputs. Probably the only interesting part is the calculation itself, as shown below. If you compare it to the p_intercept method in the python object definition, you‚Äôll see it‚Äôs basically identical.\n\n\nCode\n.get_p_intercept.player <- function(player, t) {\n  tti_sigma <- 0.45\n  den <- 1 + exp((-base::pi / sqrt(3) / tti_sigma) * (t - vctrs::field(player, 'tti')))\n  1 / den\n}\n\n\nThere is certainly more to show, especially for what is needed to calculate pitch control. (We need to integrate probabilities across all players over time, and do it for the entire pitch.) Nonetheless, the player class and the pseudo-encapsulation that we‚Äôve implemented with S3 and {vctrs} is really the key component underlying the whole pitch control calculation."
  },
  {
    "objectID": "posts/soccer-pitch-control-r/index.html#advanced-usage",
    "href": "posts/soccer-pitch-control-r/index.html#advanced-usage",
    "title": "Creating a Soccer Pitch Control Model",
    "section": "Advanced Usage",
    "text": "Advanced Usage\nTo really motivate the reader, let‚Äôs see what this implementation allows us to do.\nFirst, let‚Äôs emulate the pitch control plot of event 823, which is a pass by the away (blue) team in the home (red) team‚Äôs penalty area preceding a successful shot.\n\nCompare this to the python version.\n\nIt‚Äôs not a perfect replication, but I think it‚Äôs very close overall.\nSecond, let‚Äôs replicate the expected possession value (EPV) plot of the same event, including the EPV added by the pass.\n\nAgain, we can compare this plot to the python equivalent.\n\nCool, my R version seems to be very close to the python original. We do have a small discrepancy in the EPV added calculation. (This EPV is actually an ‚Äúexpected‚Äù EPV calculation that uses pitch control to weight the pre-learned EPV grid). I believe this is probably due to discrepancies in the integration done in the pitch control calculation and not due to a significant code issue.\nThe code to prepare the data for these plots gets more complex, which is why I have excluded it here.10 However, none of it is unreasonably difficult to understand or implement once we have a properly defined player object."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tony ElHabr",
    "section": "",
    "text": "Senior Analytics Engineer at Equilibrium Energy"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Tony's Blog",
    "section": "",
    "text": "Co-maintain {worldfootballR}, an R package for extracting world football (soccer) data from several sites. I work mostly with the Fotmob functions. (Removed as of August 2023.)\nContributed functions for scraping data from ESPN to {ffscrapr}, an R API client for several fantasy football league platforms.\nWrote {valorantr}, an R package for pro Valorant data from rib.gg. (Archived due to API changes)"
  },
  {
    "objectID": "projects.html#projects",
    "href": "projects.html#projects",
    "title": "Tony's Blog",
    "section": "Projects",
    "text": "Projects\n\nSports analytics visualization gallery\n\nA collection of visualizations (mostly soccer) that I‚Äôve posted on social media.\n\n‚ÄúBundesliga tax‚Äù analysis\n\nCited in The Athletic, Tifo Football, ESPN, Grace Robertson, and more\n\n2021 Big Data Bowl (Big Data Bowl) team submission (finalist)\n\n‚ÄúWeighted Assessment of Defender Affectiveness (WADE)‚Äù: A framework for quantifying NFL defensive back coverage and contest skills\n\n2022 Carnegie Melon Sports Analytics Conference (CMSAC) paper\n\n‚ÄúThe Hot Hand Fallacy in Call of Duty Search and Destroy‚Äù\n\n{xengagement}\n\nAn R package, Twitter bot, and python Dash app for predicting, visualizing, and summarizing the amount of Twitter engagement that xGPhilosophy receives with its end-of-match xG summary tweets\n\nNBA win probability\n\nCode for calculating NBA win probability and discussion (8-hour interview project)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Tony ElHabr",
    "section": "",
    "text": "Data scientist at CollegeVine"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Tony ElHabr",
    "section": "Education",
    "text": "Education\nM.S. in Analytics, May 2020 | Georgia Institute of TechnologyB.S. in Electrical Engineering, May 2016 | The University of Texas at Austin\n\nInterests\nsports analyticsdata sciencememes\n\n\nOther\nContestant on seasons 0 and 1 of ‚ÄúSliced‚ÄùRWeekly newsletter curator"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Tony ElHabr",
    "section": "Education",
    "text": "Education\nM.S. in Analytics (Online), 2020 | Georgia TechB.S. in Electrical Engineering, 2016 | UT Austin"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Tony's Blog",
    "section": "",
    "text": "xG Model Calibration\n\n\n\n\n\n\n\nr\n\n\nsoccer\n\n\n\n\nEvaluating Opta‚Äôs xG model performance with Brier skill score (BSS) and calibration curves\n\n\n\n\n\n\nFeb 20, 2023\n\n\nTony ElHabr\n\n\n\n\n\n\n  \n\n\n\n\nWhat exactly is an ‚Äúexpected point‚Äù? (part 2)\n\n\n\n\n\n\n\nr\n\n\nsoccer\n\n\n\n\nEvaluating how we can use match outcome probabilites for season-long insights\n\n\n\n\n\n\nSep 5, 2022\n\n\nTony ElHabr\n\n\n\n\n\n\n  \n\n\n\n\nWhat exactly is an ‚Äúexpected point‚Äù? (part 1)\n\n\n\n\n\n\n\nr\n\n\nsoccer\n\n\n\n\nCalculating and comparing expected points from different expected goals sources\n\n\n\n\n\n\nSep 4, 2022\n\n\nTony ElHabr\n\n\n\n\n\n\n  \n\n\n\n\nYet Another (Advanced?) Soccer Statistic\n\n\n\n\n\n\n\nr\n\n\nsoccer\n\n\n\n\nQuantifying soccer pass networks with weighted maximum cuts\n\n\n\n\n\n\nJan 31, 2022\n\n\nTony ElHabr\n\n\n\n\n\n\n  \n\n\n\n\nTired: PCA + kmeans, Wired: UMAP + GMM\n\n\n\n\n\n\n\nr\n\n\nsoccer\n\n\n\n\nAn Alternative to the Classic Approach to Dimension Reduction + Clustering\n\n\n\n\n\n\nJun 30, 2021\n\n\nTony ElHabr\n\n\n\n\n\n\n  \n\n\n\n\nQuantifying Relative Soccer League Strength\n\n\n\n\n\n\n\nr\n\n\nsoccer\n\n\n\n\nWith Atomic VAEP\n\n\n\n\n\n\nJun 26, 2021\n\n\nTony ElHabr\n\n\n\n\n\n\n  \n\n\n\n\nFantasy Football and the Classical Scheduling Problem\n\n\n\n\n\n\n\nr\n\n\npython\n\n\nfootball (american)\n\n\n\n\nBrute Force Programming Go Brrr\n\n\n\n\n\n\nJan 11, 2021\n\n\nTony ElHabr\n\n\n\n\n\n\n  \n\n\n\n\nDecomposing and Smoothing Soccer Spatial Tendencies\n\n\n\n\n\n\n\nr\n\n\nsoccer\n\n\n\n\nWith data.table, reticulate, and spatstat\n\n\n\n\n\n\nOct 14, 2020\n\n\nTony ElHabr\n\n\n\n\n\n\n  \n\n\n\n\nCreating a Soccer Pitch Control Model\n\n\n\n\n\n\n\nr\n\n\nsoccer\n\n\n\n\nWith S3 Classes and vctrs\n\n\n\n\n\n\nSep 23, 2020\n\n\nTony ElHabr\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Tony's Blog",
    "section": "",
    "text": "Expected goals, gamestate, and predictiveness\n\n\n\n\n\n\n\nr\n\n\nsoccer\n\n\n\n\nIs expected goals (xG) in neutral gamestates a better predictor of future performance than xG across all gamestates?\n\n\n\n\n\n\nJun 19, 2024\n\n\nTony ElHabr\n\n\n\n\n\n\n  \n\n\n\n\nEstimating Shooting Performance Unlikeliness\n\n\n\n\n\n\n\nr\n\n\nsoccer\n\n\n\n\nQuantifying how unlikely a player‚Äôs season-long shooting performance is, factoring in their prior shot history\n\n\n\n\n\n\nMay 5, 2024\n\n\nTony ElHabr\n\n\n\n\n\n\n  \n\n\n\n\nBall Progression is All You Need\n\n\n\n\n\n\n\nr\n\n\nsoccer\n\n\nobservablejs\n\n\n\n\nIdentifying where passes (even incomplete ones) have the most positive impact on the pitch.\n\n\n\n\n\n\nApr 20, 2024\n\n\nTony ElHabr\n\n\n\n\n\n\n  \n\n\n\n\nMeasuring manager performance in fantasy football\n\n\n\n\n\n\n\nr\n\n\nfootball (american)\n\n\n\n\nAll-play scores, optimal lineup choices, and more\n\n\n\n\n\n\nDec 31, 2023\n\n\nTony ElHabr\n\n\n\n\n\n\n  \n\n\n\n\nShould we account for team quality in an xG model?\n\n\n\n\n\n\n\nr\n\n\nsoccer\n\n\n\n\nFinnicking around (with an xG model) and finding out\n\n\n\n\n\n\nDec 29, 2023\n\n\nTony ElHabr\n\n\n\n\n\n\n  \n\n\n\n\nGame state with FBref data\n\n\n\n\n\n\n\nr\n\n\nsoccer\n\n\n\n\nCalculating expected goal difference (xGD) with respect to game state, using FBref data.\n\n\n\n\n\n\nOct 25, 2023\n\n\nTony ElHabr\n\n\n\n\n\n\n  \n\n\n\n\nCalibrating Binary Probabilities\n\n\n\n\n\n\n\nr\n\n\nsoccer\n\n\n\n\nUsing calibration to improve classifier model performance\n\n\n\n\n\n\nSep 11, 2023\n\n\nTony ElHabr\n\n\n\n\n\n\n  \n\n\n\n\nMeta-Analytics for Soccer\n\n\n\n\n\n\n\nr\n\n\nsoccer\n\n\n\n\nTo what extent does a state differentiate between players? How much does the stat‚Äôs value change over time?\n\n\n\n\n\n\nSep 8, 2023\n\n\nTony ElHabr\n\n\n\n\n\n\n  \n\n\n\n\nMeasuring Shooting Overperformance in Soccer\n\n\n\n\n\n\n\nr\n\n\nsoccer\n\n\n\n\nUsing empirical Bayes and the Gamma-Poisson conjugate pair\n\n\n\n\n\n\nAug 28, 2023\n\n\nTony ElHabr\n\n\n\n\n\n\n  \n\n\n\n\nxG Model Calibration\n\n\n\n\n\n\n\nr\n\n\nsoccer\n\n\n\n\nEvaluating Opta‚Äôs xG model performance with Brier skill score (BSS) and calibration curves\n\n\n\n\n\n\nFeb 20, 2023\n\n\nTony ElHabr\n\n\n\n\n\n\n  \n\n\n\n\nWhat exactly is an ‚Äúexpected point‚Äù? (part 2)\n\n\n\n\n\n\n\nr\n\n\nsoccer\n\n\n\n\nEvaluating how we can use match outcome probabilites for season-long insights\n\n\n\n\n\n\nSep 5, 2022\n\n\nTony ElHabr\n\n\n\n\n\n\n  \n\n\n\n\nWhat exactly is an ‚Äúexpected point‚Äù? (part 1)\n\n\n\n\n\n\n\nr\n\n\nsoccer\n\n\n\n\nCalculating and comparing expected points from different expected goals sources\n\n\n\n\n\n\nSep 4, 2022\n\n\nTony ElHabr\n\n\n\n\n\n\n  \n\n\n\n\nYet Another (Advanced?) Soccer Statistic\n\n\n\n\n\n\n\nr\n\n\nsoccer\n\n\n\n\nQuantifying soccer pass networks with weighted maximum cuts\n\n\n\n\n\n\nJan 31, 2022\n\n\nTony ElHabr\n\n\n\n\n\n\n  \n\n\n\n\nTired: PCA + kmeans, Wired: UMAP + GMM\n\n\n\n\n\n\n\nr\n\n\nsoccer\n\n\n\n\nAn Alternative to the Classic Approach to Dimension Reduction + Clustering\n\n\n\n\n\n\nJun 30, 2021\n\n\nTony ElHabr\n\n\n\n\n\n\n  \n\n\n\n\nQuantifying Relative Soccer League Strength\n\n\n\n\n\n\n\nr\n\n\nsoccer\n\n\n\n\nWith Atomic VAEP\n\n\n\n\n\n\nJun 26, 2021\n\n\nTony ElHabr\n\n\n\n\n\n\n  \n\n\n\n\nFantasy Football and the Classical Scheduling Problem\n\n\n\n\n\n\n\nr\n\n\npython\n\n\nfootball (american)\n\n\n\n\nBrute Force Programming Go Brrr\n\n\n\n\n\n\nJan 11, 2021\n\n\nTony ElHabr\n\n\n\n\n\n\n  \n\n\n\n\nDecomposing and Smoothing Soccer Spatial Tendencies\n\n\n\n\n\n\n\nr\n\n\nsoccer\n\n\n\n\nWith data.table, reticulate, and spatstat\n\n\n\n\n\n\nOct 14, 2020\n\n\nTony ElHabr\n\n\n\n\n\n\n  \n\n\n\n\nCreating a Soccer Pitch Control Model\n\n\n\n\n\n\n\nr\n\n\nsoccer\n\n\n\n\nWith S3 Classes and vctrs\n\n\n\n\n\n\nSep 23, 2020\n\n\nTony ElHabr\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/xg-ratio-empirical-bayes/index.html",
    "href": "posts/xg-ratio-empirical-bayes/index.html",
    "title": "Measuring Shooting Overperformance in Soccer",
    "section": "",
    "text": "Updates\n\n\n\nWording in this post was updated after the original posting. Originally an outperformance ratio greater than 1 was mistakenly called ‚Äúunderperforming‚Äù, and a ratio less than 1 was mistakenly called ‚Äúoverperforming‚Äù. The wording has been fixed."
  },
  {
    "objectID": "posts/xg-ratio-empirical-bayes/index.html#implementation",
    "href": "posts/xg-ratio-empirical-bayes/index.html#implementation",
    "title": "Measuring Shooting Overperformance in Soccer",
    "section": "Implementation",
    "text": "Implementation\nOk, so with all of that context provided, now let‚Äôs do the replication of Shaw‚Äôs findings.\n\nData\nFirst, let‚Äôs pull the data we‚Äôll need‚Äì2016/17 and 2017/18 English Premier League goals and expected goals (xG) by player. I‚Äôm using understat since it is a reliable source of data3 and is easy to retrieve data from via the {worldfootballR} package.4\nNote that Shaw used data from a provider, Stratagem, that no long provides data, as far as I can tell. For at least this reason, I won‚Äôt be able to exactly match his reason.5\n\n\nCode\n## data wrangling\nlibrary(worldfootballR)\nlibrary(dplyr)\nlibrary(tibble)\n\n## distribution fitting and wrangling\nlibrary(MASS, include.only = 'fitdistr') ## to avoid `select` name conflict with dplyr\nlibrary(withr)\nlibrary(purrr)\nlibrary(tidyr)\n\nraw_shots &lt;- worldfootballR::load_understat_league_shots(league = 'EPL')\nshots &lt;- raw_shots |&gt; \n  tibble::as_tibble() |&gt; \n  dplyr::filter(\n    season %in% c(2016L, 2017L), ## 2016/17 and 2017/18 seasons\n    situation != 'DirectFreeKick' ## \"excluding free-kicks\" in the blog post\n  ) |&gt; \n  dplyr::arrange(id) |&gt; \n  dplyr::transmute(\n    id,\n    player,\n    xg = x_g,\n    g = as.integer(result == 'Goal')\n  )\nshots\n#&gt; # A tibble: 19,047 √ó 4\n#&gt;        id player               xg     g\n#&gt;     &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt; &lt;int&gt;\n#&gt;  1 112088 Aaron Ramsey    0.0695      0\n#&gt;  2 112089 Nathaniel Clyne 0.0293      0\n#&gt;  3 112090 Aaron Ramsey    0.00734     0\n#&gt;  4 112091 Roberto Firmino 0.0856      0\n#&gt;  5 112092 Roberto Firmino 0.0441      0\n#&gt;  6 112093 Sadio Man√©      0.0607      0\n#&gt;  7 112094 Ragnar Klavan   0.0742      0\n#&gt;  8 112095 Theo Walcott    0.761       0\n#&gt;  9 112096 Theo Walcott    0.0721      1\n#&gt; 10 112097 Roberto Firmino 0.0241      0\n#&gt; # ‚Ñπ 19,037 more rows\n\n\nAbove was pulling in every record of shots, with 1 row per shot. Now we aggregate to the player level, such that we have one row per player.\n\n\nCode\nshots_by_player &lt;- shots |&gt; \n  dplyr::group_by(player) |&gt; \n  dplyr::summarize(\n    shots = dplyr::n(),\n    dplyr::across(c(g, xg), sum)\n  ) |&gt; \n  dplyr::ungroup() |&gt; \n  dplyr::mutate(raw_ratio = g / xg) |&gt; \n  dplyr::arrange(dplyr::desc(shots))\nshots_by_player\n#&gt; # A tibble: 588 √ó 5\n#&gt;    player            shots     g    xg raw_ratio\n#&gt;    &lt;chr&gt;             &lt;int&gt; &lt;int&gt; &lt;dbl&gt;     &lt;dbl&gt;\n#&gt;  1 Harry Kane          293    59  46.7     1.26 \n#&gt;  2 Sergio Ag√ºero       234    41  41.2     0.994\n#&gt;  3 Christian Eriksen   229    18  16.1     1.12 \n#&gt;  4 Alexis S√°nchez      217    33  29.1     1.13 \n#&gt;  5 Romelu Lukaku       196    41  32.1     1.28 \n#&gt;  6 Roberto Firmino     184    26  21.1     1.23 \n#&gt;  7 Kevin De Bruyne     179    14  12.2     1.15 \n#&gt;  8 Salom√≥n Rond√≥n      171    15  16.2     0.924\n#&gt;  9 Paul Pogba          168    11  14.3     0.768\n#&gt; 10 Christian Benteke   164    18  28.5     0.631\n#&gt; # ‚Ñπ 578 more rows\n\n\n\n\nEB step 1: estimate prior hyperparameters\nNext, we estimate hyperparameters for our prior gamma distribution using MLE. (With R‚Äôs dgamma, the hyperparameters are shape and rate6). I subset the data down to players having taken at least 50 shots for estimating these hyperparameters, as this is what Shaw does. In general, you‚Äôd want to filter your data here to records that provide good ‚Äúsignal‚Äù, and, therefore, will provide reliable estimates of your hyperparameters.7\n\n\nCode\nprior_shots_by_player &lt;- dplyr::filter(\n  shots_by_player, \n  shots &gt;= 50,\n  g &gt; 0 ## prevent error with fitting prior distribution\n)\n\nprior_distr &lt;- MASS::fitdistr(\n  prior_shots_by_player$raw_ratio,\n  dgamma,\n  start = list(shape = 1, rate = 1)\n)\nprior_shape &lt;- prior_distr$estimate[1]\nprior_rate &lt;- prior_distr$estimate[2]\nlist(prior_shape = round(prior_shape, 2), prior_rate = round(prior_rate, 2))\n#&gt; $prior_shape\n#&gt; shape \n#&gt;  9.39 \n#&gt; \n#&gt; $prior_rate\n#&gt; rate \n#&gt; 8.93\n\n\n\n\nEB step 2: Use prior distribution to sample the posterior\nNow we use our prior distribution‚Äôs hyperparameters to update all players‚Äô \\(O\\) ratio based on their individual volume of evidence, i.e.¬†their goals and xG.\n\nsimulate_gamma_posterior &lt;- function(\n    successes, \n    trials, \n    prior_shape, \n    prior_rate, \n    n_sims = 10000,\n    seed = 42\n) {\n  posterior_shape &lt;- prior_shape + successes\n  posterior_rate &lt;- prior_rate + trials\n  withr::local_seed(seed)\n  posterior_sample &lt;- rgamma(n = n_sims, shape = posterior_shape, rate = posterior_rate)\n  list(\n    mean = mean(posterior_sample),\n    sd = sd(posterior_sample)\n  )\n}\n\nshots_by_player$adj_ratio &lt;- purrr::map2(\n  shots_by_player$g, shots_by_player$xg,\n  function(g, xg) {\n    simulate_gamma_posterior(\n      successes = g,\n      trials = xg,\n      prior_shape = prior_shape,\n      prior_rate = prior_rate\n    )\n  }\n)\n\nadj_ratio_by_player &lt;- shots_by_player |&gt; \n  tidyr::unnest_wider(\n    adj_ratio, \n    names_sep = '_'\n  ) |&gt; \n  dplyr::arrange(dplyr::desc(adj_ratio_mean))\nadj_ratio_by_player\n#&gt; # A tibble: 588 √ó 7\n#&gt;    player            shots     g    xg raw_ratio adj_ratio_mean adj_ratio_sd\n#&gt;    &lt;chr&gt;             &lt;int&gt; &lt;int&gt; &lt;dbl&gt;     &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt;\n#&gt;  1 Fernando Llorente    57    16  9.19      1.74           1.40        0.281\n#&gt;  2 Philippe Coutinho   160    20 12.5       1.60           1.37        0.256\n#&gt;  3 Shkodran Mustafi     37     5  1.82      2.75           1.34        0.357\n#&gt;  4 Pascal Gro√ü          43     7  3.34      2.10           1.34        0.334\n#&gt;  5 Ryan Fraser          55     8  4.09      1.95           1.34        0.324\n#&gt;  6 Eden Hazard         148    28 19.2       1.45           1.33        0.219\n#&gt;  7 James McArthur       53    10  5.93      1.69           1.31        0.299\n#&gt;  8 Charlie Daniels      39     5  2.18      2.30           1.30        0.346\n#&gt;  9 Xherdan Shaqiri     117    12  7.61      1.58           1.29        0.282\n#&gt; 10 Andy Carroll         67    10  6.09      1.64           1.29        0.296\n#&gt; # ‚Ñπ 578 more rows\n\nFinally, let‚Äôs plot our results, plotting our adjusted mean estimates of \\(O_p\\), ¬±1 standard deviation about the adjusted mean.8 As noted earlier, we won‚Äôt achieve exactly the same results as Shaw due to using a different set of xG values, but evidently we‚Äôve achieved results reasonably close to his.\n\n\nCode\nlibrary(ggplot2)\nlibrary(forcats)\nlibrary(ggh4x)\nlibrary(magick)\n\nshaw_players &lt;- c(\n  'Eden Hazard' = 'E. Hazard',\n  'Mohamed Salah' = 'Mohamed Salah',\n  'Son Heung-Min' = 'Heung-Min Son',\n  'Joshua King' = 'J. King',\n  'Romelu Lukaku' = 'R. Lukaku',\n  'Harry Kane' = 'H. Kane',\n  'Sadio Man√©' = 'S. Mane',\n  'Dele Alli' = 'D. Ali',\n  'Riyad Mahrez' = 'R. Mahrez',\n  'Christian Eriksen' = 'C. Eriksen',\n  'Pedro' = 'Pedro',\n  'Alexis S√°nchez' = 'A. Sanchez',\n  'Roberto Firmino' = 'Roberto Firmino',\n  'Jamie Vardy' = 'J. Vardy',\n  'Xherdan Shaqiri' = 'X. Shaqiri',\n  'Wilfried Zaha' = 'W. Zaha',\n  'Nathan Redmond' = 'N. Redmond',\n  'Gylfi Sigurdsson' = 'G. Sigurdsson',\n  'Kevin De Bruyne' = 'K. De Bruyne',\n  'Andros Townsend' = 'A. Townsend',\n  'Sergio Ag√ºero' = 'S. Aguero',\n  'Marcus Rashford' = 'M. Rashford',\n  'Jermain Defoe' = 'J. Defoe',\n  'Raheem Sterling' = 'R. Sterling',\n  'Marko Arnautovic' = 'M. Arnautovic',\n  'Paul Pogba' = 'P. Pogba',\n  'Salom√≥n Rond√≥n' = 'S. Rondon',\n  'Christian Benteke' = 'C. Benteke'\n)\n\nordinal_adj_ratio_by_player &lt;- adj_ratio_by_player |&gt;\n  dplyr::filter(\n    player %in% names(shaw_players)\n  ) |&gt; \n  dplyr::mutate(\n    player = forcats::fct_reorder(shaw_players[player], adj_ratio_mean)\n  )\n\nadj_ratio_plot &lt;- ordinal_adj_ratio_by_player |&gt;\n  ggplot2::ggplot() +\n  ggplot2::aes(y = player) +\n  ggplot2::geom_errorbarh(\n    aes(\n      xmin = adj_ratio_mean - adj_ratio_sd,\n      xmax = adj_ratio_mean + adj_ratio_sd\n    ),\n    color = 'blue',\n    linewidth = 0.1,\n    height = 0.3\n  ) +\n  ggplot2::geom_point(\n    ggplot2::aes(x = adj_ratio_mean),\n    shape = 23,\n    size = 0.75,\n    stroke = 0.15,\n    fill = 'red',\n    color = 'black'\n  ) +\n  ggplot2::geom_vline(\n    ggplot2::aes(xintercept = 1), \n    linewidth = 0.1, \n    linetype = 2\n  ) +\n  ## add duplicate axis for ticks: https://stackoverflow.com/questions/56247205/r-ggplot2-add-ticks-on-top-and-right-sides-of-all-facets\n  ggplot2::scale_x_continuous(sec.axis = ggplot2::dup_axis()) +\n  ## ggplot2 doesn't support duplicated and creatinga  second axis for discrete variables:\n  ##   https://github.com/tidyverse/ggplot2/issues/3171.\n  ##   using ggh4x is a workaround.\n  ggplot2::guides(\n    y.sec = ggh4x::guide_axis_manual(\n      breaks = ordinal_adj_ratio_by_player$player,\n      labels = ordinal_adj_ratio_by_player$player\n    )\n  ) +\n  ggplot2::theme_linedraw(base_family = 'DejaVu Sans', base_size = 4) +\n  ggplot2::theme(\n    plot.title = ggplot2::element_text(hjust = 0.5, size = 4.25, face = 'plain'),\n    axis.ticks.length = ggplot2::unit(-1, 'pt'),\n    axis.ticks = ggplot2::element_line(linewidth = 0.05),\n    panel.grid.major.y = ggplot2::element_blank(),\n    panel.grid.minor = ggplot2::element_blank(),\n    panel.grid.major.x = ggplot2::element_line(linetype = 2),\n    axis.text.x.top = ggplot2::element_blank(),\n    axis.text.y.right = ggplot2::element_blank(),\n    axis.title.x.top = ggplot2::element_blank(),\n    axis.title.y.right = ggplot2::element_blank()\n  ) +\n  ggplot2::labs(\n    title = 'Shots from 2016/17 & 2017/18 seasons',\n    y = NULL,\n    x = 'Outperformance (= G/xG)'\n  )\n\nproj_dir &lt;- 'posts/xg-ratio-empirical-bayes'\nplot_path &lt;- file.path(proj_dir, 'shaw-figure-1-replication.png')\nggplot2::ggsave(\n  adj_ratio_plot,\n  filename = plot_path,\n  units = 'px',\n  width = 549,\n  height = 640\n)\n\norig_image &lt;- magick::image_read(file.path(proj_dir, 'shaw-figure-1.png'))\nreplicated_image_with_asa_logo &lt;- magick::image_read(plot_with_asa_logo_path)\ncombined_image_with_tony_logo &lt;- magick::image_append(\n  c(orig_image, replicated_image_with_tony_logo), \n  stack = TRUE\n)\n\nmagick::image_write(\n  combined_image_with_tony_logo, \n  path = file.path(proj_dir, 'shaw-figure-1-compared-w-tony-logo.png')\n)\n\n\n\n\n\nBeyond Replication\nFor the sake of having a pretty plot that‚Äôs not just an attempt to replicate the original, let‚Äôs run it all back, this time with EPL 2021/22 and 2022/23 data.\n\n\nCode\nraw_shots &lt;- worldfootballR::load_understat_league_shots(league = 'EPL')\nshots &lt;- raw_shots |&gt; \n  tibble::as_tibble() |&gt; \n  dplyr::filter(\n    season %in% c(2021L, 2022L),\n    situation != 'DirectFreeKick'\n  ) |&gt; \n  dplyr::arrange(id) |&gt; \n  dplyr::transmute(\n    id,\n    player,\n    ## since 2022/23, xG is filled out, not x_g\n    xg = dplyr::coalesce(x_g, xG),\n    g = as.integer(result == 'Goal')\n  )\n\nshots_by_player &lt;- shots |&gt; \n  dplyr::group_by(player) |&gt; \n  dplyr::summarize(\n    shots = dplyr::n(),\n    dplyr::across(c(g, xg), sum)\n  ) |&gt; \n  dplyr::ungroup() |&gt; \n  dplyr::mutate(raw_ratio = g / xg) |&gt; \n  dplyr::arrange(dplyr::desc(shots))\nshots_by_player\n\nshots_by_player$adj_ratio &lt;- purrr::map2(\n  shots_by_player$g, shots_by_player$xg,\n  function(g, xg) {\n    simulate_gamma_posterior(\n      successes = g,\n      trials = xg,\n      prior_shape = prior_shape,\n      prior_rate = prior_rate\n    )\n  }\n)\n\nadj_ratio_by_player &lt;- shots_by_player |&gt; \n  tidyr::unnest_wider(\n    adj_ratio, \n    names_sep = '_'\n  ) |&gt; \n  dplyr::arrange(dplyr::desc(adj_ratio_mean))\n\nordinal_adj_ratio_by_player &lt;- adj_ratio_by_player |&gt;\n  dplyr::filter(\n    player %in% names(shaw_players)\n  ) |&gt; \n  dplyr::mutate(\n    player = forcats::fct_reorder(shaw_players[player], adj_ratio_mean)\n  )\n\nlibrary(htmltools)\nlibrary(sysfonts)\nlibrary(showtext)\n\nblackish_background &lt;- '#1f1f1f'\nfont &lt;- 'Titillium Web'\nsysfonts::font_add_google(font, font)\nsysfonts::font_add('fb', 'Font Awesome 6 Brands-Regular-400.otf')\nshowtext::showtext_auto()\nplot_resolution &lt;- 300\nshowtext::showtext_opts(dpi = plot_resolution)\n## https://github.com/tashapiro/tanya-data-viz/blob/1dfad735bca1a7f335969f0eafc94cf971345075/nba-shot-chart/nba-shots.R#L64\n\ntag_lab &lt;- htmltools::tagList(\n  htmltools::tags$span(htmltools::HTML(enc2utf8(\"&#xf099;\")), style='font-family:fb'),\n  htmltools::tags$span(\"@TonyElHabr\"),\n)\n\nbeyond_replication_adj_ratio_plot &lt;- ordinal_adj_ratio_by_player |&gt;\n  ggplot2::ggplot() +\n  ggplot2::aes(y = player) +\n  ggplot2::geom_vline(\n    ggplot2::aes(xintercept = 1), \n    linewidth = 1.5,\n    linetype = 2,\n    color = 'white'\n  ) +\n  ggplot2::geom_errorbarh(\n    ggplot2::aes(\n      xmin = adj_ratio_mean - adj_ratio_sd,\n      xmax = adj_ratio_mean + adj_ratio_sd\n    ),\n    color = 'white',\n    height = 0.5\n  ) +\n  ggplot2::geom_point(\n    ggplot2::aes(x = adj_ratio_mean, size = shots),\n    color = 'white'\n  ) +\n  ggplot2::theme_minimal() +\n  ggplot2::theme(\n    text = ggplot2::element_text(family = font, color = 'white'),\n    title = ggplot2::element_text(size = 14, color = 'white'),\n    plot.title = ggplot2::element_text(face = 'bold', size = 16, color = 'white', hjust = 0),\n    plot.title.position = 'plot',\n    plot.subtitle = ggplot2::element_text(size = 14, color = 'white', hjust = 0),\n    plot.margin = ggplot2::margin(10, 20, 10, 20),\n    plot.caption = ggtext::element_markdown(color = 'white', hjust = 0, size = 10, face = 'plain', lineheight = 1.1),\n    plot.caption.position = 'plot',\n    plot.tag = ggtext::element_markdown(size = 10, color = 'white', hjust = 1),\n    plot.tag.position = c(0.99, 0.01),\n    panel.grid.major.y = ggplot2::element_blank(),\n    panel.grid.minor.x = ggplot2::element_blank(),\n    panel.grid.major.x = ggplot2::element_line(linewidth = 0.1),\n    plot.background = ggplot2::element_rect(fill = blackish_background, color = blackish_background),\n    panel.background = ggplot2::element_rect(fill = blackish_background, color = blackish_background),\n    axis.title = ggplot2::element_text(color = 'white', size = 14, face = 'bold', hjust = 0.99),\n    axis.line = ggplot2::element_blank(),\n    axis.text = ggplot2::element_text(color = 'white', size = 12),\n    axis.text.y = ggtext::element_markdown(),\n    legend.text = ggplot2::element_text(color = 'white', size = 12),\n    legend.position = 'top'\n  ) +\n  ggplot2::labs(\n    title = 'Top 20 shooting overperformers in the EPL',\n    subtitle = 'EPL 2021/22 and 2022/23 seasons. ',\n    caption = 'Players sorted according to descending adjusted G/xG ratio. Minimum 100 shots.&lt;br/&gt;**Source**: understat.',\n    y = NULL,\n    x = 'Adjusted G/xG Ratio',\n    tag = tag_lab\n  )\n\nbeyond_replication_plot_path &lt;- file.path(proj_dir, 'beyond-replication.png')\nggplot2::ggsave(\n  beyond_replication_adj_ratio_plot,\n  filename = beyond_replication_plot_path,\n  width = 7,\n  height = 7\n)\n\n\n\nFor those who follow the EPL, The usual suspects, like Heung-Min Son, show up among the best of the best HERE. The adjusted mean minus one standard deviation value exceeds zero for Son, so one might say that he was a significantly skilled shooter over the past two season.9"
  },
  {
    "objectID": "posts/xg-ratio-empirical-bayes/index.html#conclusion",
    "href": "posts/xg-ratio-empirical-bayes/index.html#conclusion",
    "title": "Measuring Shooting Overperformance in Soccer",
    "section": "Conclusion",
    "text": "Conclusion\nHaving not seen a very clear example of Gamma-Prior EB implementation on the internet‚Äìalthough there are several good explanations with toy examples such as this e-book chapter from Hyv√∂nen and Topias Tolonen‚ÄìI hope I‚Äôve perhaps un-muddied the waters for at least one interested reader.\nAs for the actual application of \\(G / xG\\) for evaluating shooting performance, I have mixed feelings about it. Further analysis shows that it has zero year-over-year stability, i.e.¬†one shouldn‚Äôt use a player‚Äôs raw or adjusted G/xG ratio in one season to try to predict whether their outperformance ratio will sustain in the next season. On the other hand, by simply making player estimates more comparable, the EB adjustment of \\(O_p\\) certainly is an improvement over raw \\(G / xG\\) itself.\nComparing hypothetical player A with 6 goals on 2 xG (\\(O_p = 3\\)) vs.¬†player B with 120 goals on 100 xG directly (\\(O_p = 1.2\\)) is unfair; the former could be performing at an unsustainable rate, while the latter has demonstrated sustained overperformance over a lot more time. Indeed, applying the EB adjustment to these hypothetical numbers, player A‚Äôs \\(O_p\\) would be shrunken back towards 1, and player B‚Äôs adjustment would be effectively nothing, indicating that player B‚Äôs shooting performance is stronger, on average."
  },
  {
    "objectID": "posts/soccer-meta-analytics/index.html",
    "href": "posts/soccer-meta-analytics/index.html",
    "title": "Meta-Analytics for Soccer",
    "section": "",
    "text": "This blog post demonstrates how to calculate the discrimination and stability ‚Äúmeta-metrics‚Äù proposed by Franks et al.¬†(2017) for an array of soccer stats. Before the computations (‚ÄúInference‚Äù), I start by defining these meta-metrics (‚ÄúMethods‚Äù), although I gloss over a lot of details for the sake of brevity. At the end, in ‚ÄúResults and Discussion‚Äù, I briefly discuss how my results compare to those for Franks et al., who analyzed basketball and hockey stats.\n\n\nIn the realm of sports, data analysis has grown significantly, introducing numerous metrics to guide coaching, management, and, of course, fans. However, this proliferation has also led to confusion, with overlapping and sometimes conflicting metrics. To address this, Franks et al.1 propose three ‚Äúmeta-metrics‚Äù to help determine which metrics offer unique and dependable information for decision-makers.\nBy examining sources of variation in sports metrics (e.g.¬†intrinsic player skill), Franks et al.¬†introduce three meta-metrics to assess player performance stats:\n\ndiscrimination: How good is the metric at telling players apart?\nstability: How likely is it that a player maintains the same performance for the metric in the next season?\nindependence: Does the stat tell us something different about players that other stats don‚Äôt tell us?"
  },
  {
    "objectID": "posts/soccer-meta-analytics/index.html#inference",
    "href": "posts/soccer-meta-analytics/index.html#inference",
    "title": "Meta-Analytics for Soccer",
    "section": "Inference",
    "text": "Inference\n\nData\nWe‚Äôll be using public data from FBref for the 2018/19 - 2022/23 seasons of the the Big 5 European soccer leagues.\n\n\nRetrieve and wrangle data\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(tibble)\n\nload_fb_advanced_match_stats &lt;- function(country, gender, tier, stat_type, team_or_player, season_end_year = NA) {\n  \n  url &lt;- sprintf(\n    'https://github.com/JaseZiv/worldfootballR_data/releases/download/fb_advanced_match_stats/%s_%s_%s_%s_%s_advanced_match_stats.rds',\n    country,\n    gender,\n    tier,\n    stat_type,\n    team_or_player\n  )\n  readRDS(url(url))\n}\n\npossibly_load_fb_advanced_match_stats &lt;- purrr::possibly(\n  load_fb_advanced_match_stats, \n  otherwise = tibble::tibble(),\n  quiet = TRUE\n)\n\nparams &lt;- tidyr::expand_grid(\n  country = c('ENG', 'ESP', 'FRA', 'GER', 'ITA'),\n  gender = 'M',\n  tier = '1st',\n  stat_type = 'summary',\n  team_or_player = 'player'\n) |&gt; \n  as.list()\n\nraw_player_match_stats &lt;- purrr::pmap_dfr(\n  params,\n  possibly_load_fb_advanced_match_stats\n) |&gt; \n  dplyr::filter(\n    ## stop at 2022/23 season\n    Season_End_Year &lt; 2024L,\n    ## don't include keepers\n    !grepl('GK', Pos)\n  )\n\nALL_METRICS &lt;- c(\n  'goals' = 'Goals',\n  'assists' = 'Assists',\n  'shots' = 'Shots',\n  'shots_on_target' = 'Shots on Target',\n  'tackles' = 'Tackles',\n  'interceptions' = 'Interceptions',\n  'xg' = 'xG',\n  'xa' = 'xA',\n  'goals_xg_ratio' = 'Goals/xG',\n  'carries' = 'Carries',\n  'shot_conversion_rate' = 'Shots Conversion Rate',\n  'pass_completion_rate' = 'Pass Completion Rate',\n  'goals_p90' = 'Goals/90',\n  'shots_p90' = 'Shots/90',\n  'xg_p90' = 'xG/90'\n)\n\nsafe_divide &lt;- function(num, den) {\n  ifelse(\n    den == 0 | is.na(den),\n    NA_real_,\n    dplyr::coalesce(num / den, 0)\n  )\n}\n\ncoalesce_fraction &lt;- purrr::compose(\n  \\(num, den) safe_divide(num, den),\n  \\(x) ifelse(x &gt; 1, 1, x),\n  \\(x) ifelse(x &lt; 0, 0, x),\n  .dir = 'forward'\n)\n\nadd_rate_and_p90_metric_columns &lt;- function(df) {\n  df |&gt; \n    dplyr::mutate(\n      ## Mark Noble with the epic 1 goal on 0 shots https://fbref.com/en/matches/b56fd899/Watford-West-Ham-United-December-28-2021-Premier-League\n      shot_conversion_rate = coalesce_fraction(goals, shots),\n      pass_completion_rate = coalesce_fraction(passes_completed, passes_attempted),\n      goals_xg_ratio = safe_divide(goals, xg)\n    ) |&gt; \n    dplyr::mutate(\n      dplyr::across(\n        c(goals, shots, xg),\n        list(\n          p90 = \\(.x) 90 * .x / minutes_played\n        )\n      )\n    )\n}\n\nsummarize_all_metric_columns &lt;- function(df, ...) {\n  matches_played &lt;- df |&gt; \n    dplyr::group_by(league, season, team, player) |&gt;\n    dplyr::filter(minutes_played &gt; 0L) |&gt; \n    dplyr::summarize(\n      matches_played = dplyr::n_distinct(match_id)\n    ) |&gt; \n    dplyr::ungroup()\n  \n  df |&gt; \n    dplyr::group_by(..., league, season, team, player) |&gt; \n    dplyr::summarize(\n      dplyr::across(\n        c(minutes_played:dplyr::last_col()),\n        sum\n      )\n    ) |&gt; \n    dplyr::ungroup() |&gt; \n    add_rate_and_p90_metric_columns() |&gt; \n    dplyr::inner_join(\n      matches_played,\n      by = dplyr::join_by(league, season, team, player)\n    ) |&gt; \n    dplyr::relocate(\n      matches_played,\n      .before = minutes_played\n    )\n}\n\n\nplayer_match_stats &lt;- raw_player_match_stats |&gt; \n  dplyr::transmute(\n    league = sprintf('%s-%s-%s', Country, Gender, Tier),\n    season = sprintf('%s/%s', Season_End_Year - 1, substr(Season_End_Year, 3, 4)),\n    date = Match_Date,\n    match_id = basename(dirname(MatchURL)),\n    team = Team,\n    player = Player,\n    \n    minutes_played = Min,\n    \n    goals = Gls, ## includes pks\n    assists = Ast,\n    shots = Sh, ## does not include pk attempts\n    shots_on_target = SoT,\n    tackles = Tkl,\n    interceptions = Int,\n    \n    passes_completed = Cmp_Passes,\n    passes_attempted = Att_Passes,\n    carries = Carries_Carries,\n    \n    xg = xG_Expected,\n    xa = xAG_Expected\n  ) |&gt; \n  add_rate_and_p90_metric_columns()\n\nplayer_season_stats &lt;- summarize_all_metric_columns(player_match_stats)\n\n## Franks et al. used 250 min played for the NBA\n## https://github.com/afranks86/meta-analytics/blob/1871d24762184afa69f29a2b5b348431e70b9b2b/basketballReliability.R#L60\nMIN_MINUTES_PLAYED &lt;- 270\neligible_player_season_stats &lt;- player_season_stats |&gt; \n  dplyr::filter(minutes_played &gt;= MIN_MINUTES_PLAYED)\n\neligible_player_match_stats &lt;- player_match_stats |&gt; \n  dplyr::semi_join(\n    eligible_player_season_stats,\n    by = dplyr::join_by(league, season, team, player)\n  ) |&gt; \n  dplyr::arrange(league, season, team, player)\n\n## drop players with 0s for any given metric across any season?\n##   looks like they only did that for testing a 1-season evaluation:\n##   https://github.com/afranks86/meta-analytics/blob/1871d24762184afa69f29a2b5b348431e70b9b2b/basketballReliability.R#L25\n# eligible_player_season_stats |&gt; \n#   pivot_metric_columns() |&gt; \n#   group_by(league, team, player, metric) |&gt; \n#   summarize(has_any_zero = any(value == 0)) |&gt; \n#   ungroup() |&gt; \n#   filter(has_any_zero)\n\n\nAfter some light wrangling, the player-match data looks like this.\n\ndplyr::glimpse(eligible_player_match_stats)\n#&gt; Rows: 271,416\n#&gt; Columns: 24\n#&gt; $ league               &lt;chr&gt; \"ENG-M-1st\", \"ENG-M-1st\", \"ENG-M-1st\", \"ENG-M-‚Ä¶\n#&gt; $ season               &lt;chr&gt; \"2018/19\", \"2018/19\", \"2018/19\", \"2018/19\", \"2‚Ä¶\n#&gt; $ date                 &lt;chr&gt; \"2018-08-12\", \"2018-08-18\", \"2018-08-25\", \"201‚Ä¶\n#&gt; $ match_id             &lt;chr&gt; \"478e9dab\", \"9b69882c\", \"0014076a\", \"c1503e09\"‚Ä¶\n#&gt; $ team                 &lt;chr&gt; \"Arsenal\", \"Arsenal\", \"Arsenal\", \"Arsenal\", \"A‚Ä¶\n#&gt; $ player               &lt;chr&gt; \"Aaron Ramsey\", \"Aaron Ramsey\", \"Aaron Ramsey\"‚Ä¶\n#&gt; $ minutes_played       &lt;dbl&gt; 53, 23, 90, 90, 79, 79, 62, 24, 11, 13, 18, 16‚Ä¶\n#&gt; $ goals                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n#&gt; $ assists              &lt;dbl&gt; 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 2, 0, 0‚Ä¶\n#&gt; $ shots                &lt;dbl&gt; 1, 2, 2, 2, 0, 1, 0, 1, 0, 0, 0, 1, 0, 3, 1, 0‚Ä¶\n#&gt; $ shots_on_target      &lt;dbl&gt; 1, 1, 1, 2, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0‚Ä¶\n#&gt; $ tackles              &lt;dbl&gt; 2, 0, 2, 1, 2, 0, 2, 0, 1, 0, 0, 0, 1, 2, 1, 0‚Ä¶\n#&gt; $ interceptions        &lt;dbl&gt; 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0‚Ä¶\n#&gt; $ passes_completed     &lt;dbl&gt; 9, 6, 48, 51, 49, 26, 16, 16, 13, 5, 11, 7, 8,‚Ä¶\n#&gt; $ passes_attempted     &lt;dbl&gt; 13, 9, 61, 66, 58, 31, 20, 21, 15, 7, 12, 10, ‚Ä¶\n#&gt; $ carries              &lt;dbl&gt; 5, 7, 41, 49, 43, 27, 21, 16, 11, 8, 11, 7, 7,‚Ä¶\n#&gt; $ xg                   &lt;dbl&gt; 0.0, 0.1, 0.1, 0.1, 0.0, 0.0, 0.0, 0.1, 0.0, 0‚Ä¶\n#&gt; $ xa                   &lt;dbl&gt; 0.1, 0.0, 0.2, 0.2, 0.1, 0.4, 0.0, 0.1, 0.0, 0‚Ä¶\n#&gt; $ shot_conversion_rate &lt;dbl&gt; 0, 0, 0, 0, NA, 0, NA, 1, NA, NA, NA, 0, NA, 0‚Ä¶\n#&gt; $ pass_completion_rate &lt;dbl&gt; 0.6923077, 0.6666667, 0.7868852, 0.7727273, 0.‚Ä¶\n#&gt; $ goals_xg_ratio       &lt;dbl&gt; NA, 0.000000, 0.000000, 0.000000, NA, NA, NA, ‚Ä¶\n#&gt; $ goals_p90            &lt;dbl&gt; 0.000000, 0.000000, 0.000000, 0.000000, 0.0000‚Ä¶\n#&gt; $ shots_p90            &lt;dbl&gt; 1.698113, 7.826087, 2.000000, 2.000000, 0.0000‚Ä¶\n#&gt; $ xg_p90               &lt;dbl&gt; 0.0000000, 0.3913043, 0.1000000, 0.1000000, 0.‚Ä¶\n\nThere are a lot more stats we could pull in, but I‚Äôve opted for a relatively small set of 15 stats2:\n\n‚ÄúUn-adjusted‚Äù (i.e.¬†not normalized for the number of minutes played) counting stats: goals, assists, shots, shots on target, tackles, interceptions, and carries\nUn-adjusted ‚Äúadvanced‚Äù stats: expected goals (xG), expected assists (xA), and goals/xG ratio\nRate metrics: pass completion rate and shot conversion rate (i.e.¬†goals divided by shots)\n‚ÄúPer 90‚Äù versions of other stats: goals/90, shots/90, and xG/90\n\nNow we can start to calculate the meta-metrics.\n\n\nDiscrimination\nWe‚Äôll begin with the discrimination meta-metric, which means we‚Äôll need to calculate ‚Äúbootstrap variance‚Äù, \\(BV\\), and, ‚Äúseasonal variance‚Äù, \\(SV\\), as shown in Equation¬†1.\nStarting with the former, we resample player matches within teams, with replacement.\n\n\nBootstrap observed player-seasons\nresample_stats &lt;- function(player_match_stats) {\n  \n  match_ids &lt;- player_match_stats |&gt;\n    dplyr::distinct(league, season, team, date, match_id) |&gt; \n    dplyr::arrange(league, season, team, date)\n  \n  ## can't just specify to resample 38 matches per team since different leagues \n  ## have different season lengths (Bundesliga)\n  resampled_match_ids &lt;- match_ids |&gt; \n    dplyr::select(league, season, team, match_id) |&gt; \n    dplyr::group_by(league, season, team) |&gt; \n    dplyr::summarize(\n      match_id = list(match_id)\n    ) |&gt; \n    dplyr::ungroup() |&gt; \n    dplyr::mutate(\n      match_id = purrr::map(\n        match_id,\n        \\(.x) {\n          sample(.x, size = length(.x), replace = TRUE)\n        })\n    ) |&gt; \n    tidyr::unnest(match_id)\n  \n  resampled_match_ids |&gt; \n    dplyr::inner_join(\n      player_match_stats,\n      by = dplyr::join_by(league, season, team, match_id),\n      relationship = 'many-to-many'\n    )\n}\n\n## Franks et al. used 20 bootstrap replicates\n## https://github.com/afranks86/meta-analytics/blob/1871d24762184afa69f29a2b5b348431e70b9b2b/basketballReliability.R#L70\nN_BOOSTRAPS &lt;- 20\nset.seed(42)\nresampled_player_match_stats &lt;- purrr::map_dfr(\n  rlang::set_names(1:N_BOOSTRAPS),\n  \\(...) {\n    resample_stats(\n      player_match_stats = eligible_player_match_stats\n    )\n  },\n  .id = 'bootstrap_id'\n) |&gt; \n  dplyr::mutate(bootstrap_id = as.integer(bootstrap_id))\n\n\nAfter we aggregate over the matches in each bootstrap to create player-season summaries per bootstrap, we get a data frame that looks like this.\n\n\nAggregate bootstraps\nresampled_player_match_stats &lt;- resampled_player_match_stats |&gt; \n  ## Franks et al. did this\n  ## https://github.com/afranks86/meta-analytics/blob/1871d24762184afa69f29a2b5b348431e70b9b2b/basketballReliability.R#L78\n  dplyr::filter(minutes_played &lt; (0.75 * MIN_MINUTES_PLAYED))\n\nresampled_player_season_stats &lt;- resampled_player_match_stats |&gt; \n  summarize_all_metric_columns(bootstrap_id) |&gt; \n  dplyr::arrange(bootstrap_id, league, season, team, player)\n\n\n\ndplyr::glimpse(resampled_player_season_stats)\n#&gt; Rows: 234,172\n#&gt; Columns: 24\n#&gt; $ bootstrap_id         &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1‚Ä¶\n#&gt; $ league               &lt;chr&gt; \"ENG-M-1st\", \"ENG-M-1st\", \"ENG-M-1st\", \"ENG-M-‚Ä¶\n#&gt; $ season               &lt;chr&gt; \"2018/19\", \"2018/19\", \"2018/19\", \"2018/19\", \"2‚Ä¶\n#&gt; $ team                 &lt;chr&gt; \"Arsenal\", \"Arsenal\", \"Arsenal\", \"Arsenal\", \"A‚Ä¶\n#&gt; $ player               &lt;chr&gt; \"Aaron Ramsey\", \"Ainsley Maitland-Niles\", \"Ale‚Ä¶\n#&gt; $ matches_played       &lt;int&gt; 28, 16, 35, 35, 29, 25, 19, 17, 34, 33, 24, 8,‚Ä¶\n#&gt; $ minutes_played       &lt;dbl&gt; 1349, 1053, 1714, 2767, 2213, 1357, 989, 1385,‚Ä¶\n#&gt; $ goals                &lt;dbl&gt; 2, 2, 3, 11, 4, 3, 0, 5, 0, 0, 7, 0, 2, 20, 0,‚Ä¶\n#&gt; $ assists              &lt;dbl&gt; 1, 1, 4, 8, 2, 5, 3, 0, 3, 0, 2, 0, 4, 4, 0, 5‚Ä¶\n#&gt; $ shots                &lt;dbl&gt; 28, 8, 31, 102, 24, 30, 7, 5, 21, 20, 14, 9, 7‚Ä¶\n#&gt; $ shots_on_target      &lt;dbl&gt; 9, 6, 15, 28, 11, 11, 4, 5, 1, 4, 9, 0, 6, 27,‚Ä¶\n#&gt; $ tackles              &lt;dbl&gt; 37, 28, 21, 35, 44, 17, 9, 21, 61, 35, 9, 21, ‚Ä¶\n#&gt; $ interceptions        &lt;dbl&gt; 10, 17, 14, 19, 34, 9, 13, 36, 55, 23, 3, 0, 3‚Ä¶\n#&gt; $ passes_completed     &lt;dbl&gt; 733, 444, 619, 677, 1656, 552, 470, 832, 1269,‚Ä¶\n#&gt; $ passes_attempted     &lt;dbl&gt; 906, 561, 847, 933, 2059, 724, 580, 905, 1504,‚Ä¶\n#&gt; $ carries              &lt;dbl&gt; 630, 334, 724, 738, 1085, 454, 333, 602, 978, ‚Ä¶\n#&gt; $ xg                   &lt;dbl&gt; 2.7, 1.6, 4.2, 11.0, 1.0, 3.9, 0.3, 1.9, 0.6, ‚Ä¶\n#&gt; $ xa                   &lt;dbl&gt; 2.4, 0.9, 5.3, 3.6, 2.0, 4.8, 0.9, 0.5, 0.7, 0‚Ä¶\n#&gt; $ shot_conversion_rate &lt;dbl&gt; 0.07142857, 0.25000000, 0.09677419, 0.10784314‚Ä¶\n#&gt; $ pass_completion_rate &lt;dbl&gt; 0.8090508, 0.7914439, 0.7308146, 0.7256163, 0.‚Ä¶\n#&gt; $ goals_xg_ratio       &lt;dbl&gt; 0.7407407, 1.2500000, 0.7142857, 1.0000000, 4.‚Ä¶\n#&gt; $ goals_p90            &lt;dbl&gt; 0.1334322, 0.1709402, 0.1575263, 0.3577882, 0.‚Ä¶\n#&gt; $ shots_p90            &lt;dbl&gt; 1.8680504, 0.6837607, 1.6277713, 3.3176726, 0.‚Ä¶\n#&gt; $ xg_p90               &lt;dbl&gt; 0.180133432, 0.136752137, 0.220536756, 0.35778‚Ä¶\n\nNext, we calculate the variance across the player-season bootstraps, and then average the bootstrap variance over all player-seasons, grouping by season, to arrive at \\(BV\\). We end up with one variance value per season and metric.\n\n\nCalculate average bootstrap variance, BV\npivot_metric_columns &lt;- function(df) {\n  df |&gt; \n    dplyr::select(\n      league,\n      season,\n      team,\n      player,\n      dplyr::any_of(names(ALL_METRICS))\n    ) |&gt; \n    tidyr::pivot_longer(\n      -c(league, season, team, player),\n      names_to = 'metric',\n      values_to = 'value'\n    )\n}\n\nresampled_player_season_variance &lt;- resampled_player_season_stats |&gt; \n  pivot_metric_columns() |&gt; \n  dplyr::group_by(season, team, player, metric) |&gt; \n  dplyr::summarize(\n    bv = var(value, na.rm = TRUE)\n  ) |&gt; \n  dplyr::ungroup()\n\nbv &lt;- resampled_player_season_variance |&gt; \n  dplyr::group_by(season, metric) |&gt; \n  dplyr::summarize(\n    bv = mean(bv, na.rm = TRUE)\n  ) |&gt; \n  dplyr::ungroup() |&gt; \n  dplyr::arrange(season, metric)\n\n\n\nbv\n#&gt; # A tibble: 90 √ó 3\n#&gt;    season  metric                        bv\n#&gt;    &lt;chr&gt;   &lt;chr&gt;                      &lt;dbl&gt;\n#&gt;  1 2017/18 assists                 1.64    \n#&gt;  2 2017/18 carries              7712.      \n#&gt;  3 2017/18 goals                   2.36    \n#&gt;  4 2017/18 goals_p90               0.0102  \n#&gt;  5 2017/18 goals_xg_ratio          1.40    \n#&gt;  6 2017/18 interceptions          33.4     \n#&gt;  7 2017/18 pass_completion_rate    0.000577\n#&gt;  8 2017/18 shot_conversion_rate    0.00604 \n#&gt;  9 2017/18 shots                  32.5     \n#&gt; 10 2017/18 shots_on_target         8.39 \n#&gt; # ‚Ñπ 80 more rows\n\nNext, we move on to the ‚Äúseasonal variance‚Äù, \\(SV\\). This is actually pretty trivial, as it‚Äôs just a direct call to stats::var on the observed player-season aggregates, grouping by season. Again, we end up with one row per season and metric.\n\n\nCalculate seasonal variance, SV\npivoted_player_season_stats &lt;- pivot_metric_columns(player_season_stats)\n\nsv &lt;- pivoted_player_season_stats |&gt;\n  dplyr::group_by(season, metric) |&gt; \n  dplyr::summarize(\n    sv = var(value, na.rm = TRUE)\n  ) |&gt; \n  dplyr::ungroup() |&gt; \n  dplyr::arrange(season, metric)\n\n\n\nsv\n#&gt; # A tibble: 90 √ó 3\n#&gt;    season  metric                         sv\n#&gt;    &lt;chr&gt;   &lt;chr&gt;                       &lt;dbl&gt;\n#&gt;  1 2017/18 assists                   4.33   \n#&gt;  2 2017/18 carries              153542.     \n#&gt;  3 2017/18 goals                    12.4    \n#&gt;  4 2017/18 goals_p90                 0.302  \n#&gt;  5 2017/18 goals_xg_ratio            2.15   \n#&gt;  6 2017/18 interceptions           319.     \n#&gt;  7 2017/18 pass_completion_rate      0.00948\n#&gt;  8 2017/18 shot_conversion_rate      0.0128 \n#&gt;  9 2017/18 shots                   480.     \n#&gt; 10 2017/18 shots_on_target          73.4    \n#&gt; # ‚Ñπ 80 more rows\n\nFinally, we bring \\(BV\\) and \\(SV\\) together to calculate discrimination by season.\n\n\nCalculate discrimination\ndiscrimination &lt;- bv |&gt; \n  dplyr::inner_join(\n    sv,\n    by = dplyr::join_by(season, metric)\n  ) |&gt; \n  dplyr::mutate(\n    discrimination = 1 - bv / sv\n  ) |&gt; \n  dplyr::arrange(season, metric)\n\n\n\ndiscrimination\n#&gt; # A tibble: 90 √ó 5\n#&gt;    season  metric                        bv           sv discrimination\n#&gt;    &lt;chr&gt;   &lt;chr&gt;                      &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;\n#&gt;  1 2017/18 assists                 1.64          4.33             0.621\n#&gt;  2 2017/18 carries              7712.       153542.               0.950\n#&gt;  3 2017/18 goals                   2.36         12.4              0.810\n#&gt;  4 2017/18 goals_p90               0.0102        0.302            0.966\n#&gt;  5 2017/18 goals_xg_ratio          1.40          2.15             0.348\n#&gt;  6 2017/18 interceptions          33.4         319.               0.895\n#&gt;  7 2017/18 pass_completion_rate    0.000577      0.00948          0.939\n#&gt;  8 2017/18 shot_conversion_rate    0.00604       0.0128           0.527\n#&gt;  9 2017/18 shots                  32.5         480.               0.932\n#&gt; 10 2017/18 shots_on_target         8.39         73.4              0.886\n#&gt; # ‚Ñπ 80 more rows\n\nFor the sake of making discrimination directly comparable to stability (which is only calculated per metric, not per metric and season), we can average the discrimination values over all seasons.\n\n\nAverage discrimination\naverage_discrimination &lt;- discrimination |&gt; \n  dplyr::group_by(metric) |&gt; \n  dplyr::summarize(\n    discrimination = mean(discrimination)\n  ) |&gt; \n  dplyr::arrange(metric)\naverage_discrimination\n\n\n\naverage_discrimination\n#&gt; # A tibble: 15 √ó 2\n#&gt;    metric               discrimination\n#&gt;    &lt;chr&gt;                         &lt;dbl&gt;\n#&gt;  1 assists                       0.600\n#&gt;  2 carries                       0.951\n#&gt;  3 goals                         0.792\n#&gt;  4 goals_p90                     0.830\n#&gt;  5 goals_xg_ratio                0.181\n#&gt;  6 interceptions                 0.889\n#&gt;  7 pass_completion_rate          0.950\n#&gt;  8 shot_conversion_rate          0.507\n#&gt;  9 shots                         0.926\n#&gt; 10 shots_on_target               0.875\n#&gt; 11 shots_p90                     0.956\n#&gt; 12 tackles                       0.895\n#&gt; 13 xa                            0.830\n#&gt; 14 xg                            0.898\n#&gt; 15 xg_p90                        0.929\n\n\n\nStability\nFor stability, we need to calculate ‚Äúwithin‚Äù-player variance, \\(WV\\), and ‚Äútotal variance‚Äù, \\(TV\\), as shown in Equation¬†2.\nFor \\(WV\\), we start by grouping on the on everything but the metric values and season variable in our data frame for observed player-season stats, and calculate variance with var(). Then we average the variance across all players with mean().\n\n\nCalculate within-player variance, WV\nwithin_player_variance &lt;- pivoted_player_season_stats |&gt; \n  ## should check for players with the same name\n  dplyr::group_by(league, team, player, metric) |&gt;\n  dplyr::summarize(\n    seasons_played = dplyr::n(),\n    wv = var(value, na.rm = TRUE)\n  ) |&gt; \n  dplyr::ungroup()\n\nwv &lt;- within_player_variance |&gt;\n  ## Franks et al. don't have something quite like this, where they filter for\n  ##   a minimum number of seasons played. I think they didn't find it necessary\n  ##   after dropping player who have ever had zeros, and because they were working\n  ##   with 20 seasons of NBA data. (I'm working with 6.)\n  dplyr::filter(\n    seasons_played &gt;= 3L\n  ) |&gt; \n  dplyr::group_by(metric) |&gt; \n  dplyr::summarize(\n    wv = mean(wv, na.rm = TRUE)\n  ) |&gt; \n  dplyr::ungroup() |&gt; \n  dplyr::arrange(metric)\n\n\n\nwv\n#&gt; # A tibble: 15 √ó 2\n#&gt;    metric                        wv\n#&gt;    &lt;chr&gt;                      &lt;dbl&gt;\n#&gt;  1 assists                  2.71   \n#&gt;  2 carries              77143.     \n#&gt;  3 goals                    5.75   \n#&gt;  4 goals_p90                0.0269 \n#&gt;  5 goals_xg_ratio           1.45   \n#&gt;  6 interceptions          122.     \n#&gt;  7 pass_completion_rate     0.00339\n#&gt;  8 shot_conversion_rate     0.0103 \n#&gt;  9 shots                  183.     \n#&gt; 10 shots_on_target         29.1    \n#&gt; 11 shots_p90                0.821  \n#&gt; 12 tackles                239.     \n#&gt; 13 xa                       1.57   \n#&gt; 14 xg                       3.71   \n#&gt; 15 xg_p90                   0.0141\n\nTo finish our variance calculations, we calculate ‚Äútotal variance‚Äù, \\(TV\\). This is the most straightforward of them all, as it involves just a call to var() for each metric on the observed player-season stats.\n\n\nCalculate total variance, TV\ntv &lt;- pivoted_player_season_stats |&gt; \n  dplyr::group_by(metric) |&gt;\n  dplyr::summarize(\n    tv = var(value, na.rm = TRUE)\n  ) |&gt; \n  dplyr::ungroup() |&gt; \n  dplyr::arrange(metric)\n\n\n\ntv\n#&gt; # A tibble: 15 √ó 2\n#&gt;    metric                        tv\n#&gt;    &lt;chr&gt;                      &lt;dbl&gt;\n#&gt;  1 assists                   4.16  \n#&gt;  2 carries              161078.    \n#&gt;  3 goals                    11.5   \n#&gt;  4 goals_p90                 0.0925\n#&gt;  5 goals_xg_ratio            1.67  \n#&gt;  6 interceptions           252.    \n#&gt;  7 pass_completion_rate      0.0119\n#&gt;  8 shot_conversion_rate      0.0136\n#&gt;  9 shots                   427.    \n#&gt; 10 shots_on_target          65.2   \n#&gt; 11 shots_p90                 4.76  \n#&gt; 12 tackles                 460.    \n#&gt; 13 xa                        3.15  \n#&gt; 14 xg                        9.27  \n#&gt; 15 xg_p90                    0.0729\n\nWe bring \\(BV\\), \\(WV\\) and \\(TV\\) together to calculate stability.\n\n\nCalculate stability\naverage_bv &lt;- bv |&gt; \n  dplyr::group_by(metric) |&gt; \n  dplyr::summarize(\n    bv = mean(bv)\n  ) |&gt; \n  dplyr::ungroup()\n\nstability &lt;- average_bv |&gt; \n  dplyr::inner_join(\n    tv,\n    by = dplyr::join_by(metric)\n  ) |&gt;\n  dplyr::inner_join(\n    wv,\n    by = dplyr::join_by(metric)\n  ) |&gt; \n  dplyr::mutate(\n    stability = 1 - (wv - bv) / (tv - bv)\n  ) |&gt; \n  dplyr::arrange(metric)\n\n\n\nstability\n#&gt; # A tibble: 15 √ó 5\n#&gt;    metric                        bv          tv          wv stability\n#&gt;    &lt;chr&gt;                      &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n#&gt;  1 assists                 1.66          4.16       2.71        0.580\n#&gt;  2 carries              7808.       161078.     77143.          0.548\n#&gt;  3 goals                   2.38         11.5        5.75        0.629\n#&gt;  4 goals_p90               0.0105        0.0925     0.0269      0.799\n#&gt;  5 goals_xg_ratio          1.35          1.67       1.45        0.696\n#&gt;  6 interceptions          27.8         252.       122.          0.577\n#&gt;  7 pass_completion_rate    0.000582      0.0119     0.00339     0.752\n#&gt;  8 shot_conversion_rate    0.00670       0.0136     0.0103      0.487\n#&gt;  9 shots                  31.7         427.       183.          0.618\n#&gt; 10 shots_on_target         8.17         65.2       29.1         0.634\n#&gt; 11 shots_p90               0.118         4.76       0.821       0.849\n#&gt; 12 tackles                48.3         460.       239.          0.537\n#&gt; 13 xa                      0.538         3.15       1.57        0.607\n#&gt; 14 xg                      0.944         9.27       3.71        0.668\n#&gt; 15 xg_p90                  0.00397       0.0729     0.0141      0.853"
  },
  {
    "objectID": "posts/soccer-meta-analytics/index.html#results",
    "href": "posts/soccer-meta-analytics/index.html#results",
    "title": "Meta-Analytics for Soccer",
    "section": "Results",
    "text": "Results\nLet‚Äôs make a scatter plot of the discrimination and stability of our metrics, akin to the plots made by Franks et al.\n\nWe can make the following observations:\n\nUn-adjusted tallying (i.e.¬†‚Äúcount‚Äù) statistics based on total playing time‚Äìlike shots, tackles, and interceptions‚Äìtend to be highly discriminative, but not quite as stable. This makes sense‚Äîsuch statistics have large between-player variance‚Äîthink about the number of shots that a central defender takes compared to a striker. When aggregated for each player throughout a season, they provide a strong signal for different player types, i.e.¬†positions. Indeed, Franks et al.¬†found the same for NBA and NHL statistics.\nWhen not adjusted for minutes played, xG and xA also prove to be more discriminative than stable.\nThe other ‚Äúadvanced‚Äù stat, Goals/xG ratio, stands out. It‚Äôs the only metric that is significantly more stable than discriminative. Although I haven‚Äôt looked into this extensively or given it much more than a few moments of thought, I believe this is because most players do not score goals in individual matches, but often accumulate some non-zero xG via shots. Thus, Goals/xG can be zero very frequently, meaning that it would be hard to differentiate players just on this ratio. And, because the ratio is often zero, it is found to be stable.\nShot conversion rate seems to be the center of the universe, having nearly equal discrimination and stability, both at around 0.5.\nThe ‚Äúper 90‚Äù metrics show higher stability than all other stats evaluated. This speaks to the between-player noise-minimizing benefits of comparing players on an equal-minute basis. Their stability is much closer to their discriminative power than most other other metrics."
  },
  {
    "objectID": "posts/soccer-meta-analytics/index.html#conclusion",
    "href": "posts/soccer-meta-analytics/index.html#conclusion",
    "title": "Meta-Analytics for Soccer",
    "section": "Conclusion",
    "text": "Conclusion\nWe‚Äôve replicated the ideas of a commonly cited sports analytics paper3, applying them to the beautiful game of soccer. While this blog post was more about replication than insights, we‚Äôve confirmed that the key observations regarding the meta-metric properties of volume and rate statistics also apply soccer.\n\nDiscrimination can be used to distinguish between players for a given season. Volume measures like shots and tackles should implicitly provide some indication of player positions and roles. (After all, we wouldn‚Äôt expect a striker to be making more tackles than a defender.)\nStability represents how much a metric can vary over time. Intuitively, stats that adjust for playing time (i.e.¬†‚Äúper 90‚Äù) show less fluctuations from season to season."
  },
  {
    "objectID": "posts/soccer-meta-analytics/index.html#definitions",
    "href": "posts/soccer-meta-analytics/index.html#definitions",
    "title": "Meta-Analytics for Soccer",
    "section": "Definitions",
    "text": "Definitions\nFirst, we should explicitly define the meta-metrics of interest: discrimination and stability. I‚Äôll break things down more formally first, but if you want just the gist of things, at least observe the ‚ÄúSimplified‚Äù formulas.\n\nFull Formulas\nLet \\(X\\) to be a random variable. Given metric \\(m\\) for player \\(p\\) in season \\(s\\), we can represent a single value of the random variable as \\(X_{spm}\\). Given these representations, we specify the following shorthands.\n\\[\n\\begin{array}{rcl}\nE_{spm}[X] & = & E[X|S=s,P=p,M=m] \\\\\nV_{spm}[X] & = & Var[X|S=s,P=p,M=m]. \\\\\n\\end{array}\n\\]\nFranks et al.¬†define discrimination as\n\\[\n\\begin{array}{cc}\n\\text{(Discrimination)} & D_{sm} = 1 - \\frac{E_{sm}[V_{spm}[X]]}{V_{sm}[X]}.\n\\end{array}\n\\tag{1}\\]\nThis describes the fraction of between-player variance due to true differences in player ability. A value closer to 1 indicates a metric that can differentiate between players more precisely. Stability is defined as\n\\[\n\\begin{array}{cc}\n\\text{(Stability)} & S_{m} = 1 - \\frac{E_{m}[V_{spm}[X] - V_{spm}[X]]}{V_{m}[X] - E_{m}[V_{spm}[X]]}.\n\\end{array}\n\\tag{2}\\]\nThis represents the fraction of total variance that is due to within-player changes over time. A value closer to 1 indicates that the metric value for a given player is less likely to change from season to season.\n\n\nSimplified Formulas\nLet‚Äôs abstract one level up from metric \\(m\\) since it should be intuitive that we won‚Äôt be looking at variance across different stats. (Evaluating variance across shots taken and passes completion rate has not practical meaning!) Further, we‚Äôll remove the subscripts and other notation for seasons, players, expectations, and variances for clarity.\nWe re-write Equation¬†1 more plainly as\n\\[\n\\begin{array}{cc}\n\\text{(Discrimination)} & 1 - \\frac{BV}{SV}.\n\\end{array}\n\\tag{3}\\]\n\\(BV\\) is the bootstrapped variance for a given player in a given season (1 record per ‚Äúplayer-season‚Äù), averaged over all player-seasons. \\(SV\\) is the player-season variance for all players for a given season.\nWe re-write Equation¬†2 as\n\\[\n\\begin{array}{cc}\n\\text{(Stability)} & 1 - \\frac{WV - BV}{TV - BV}.\n\\end{array}\n\\tag{4}\\]\n\\(WV\\) represents the variance across all player-seasons within a player. \\(TV\\) is the total variance across all player-season totals over all seasons."
  },
  {
    "objectID": "posts/soccer-meta-analytics/index.html#method",
    "href": "posts/soccer-meta-analytics/index.html#method",
    "title": "Meta-Analytics for Soccer",
    "section": "Method",
    "text": "Method\n\nDiscrimination\nSimplifying the formulation of Franks et al., let‚Äôs define discrimination as follows:\n\\[\n\\begin{array}{cc}\n\\text{(Discrimination)} & 1 - \\frac{BV}{SV}.\n\\end{array}\n\\tag{1}\\]\n\\(BV\\) is the average bootstrapped variance of a metric for a given player in a given season. (We will be resampling the actual game totals for individual players.) \\(SV\\) is the variance of a metric for all players in a given season.\nThis equation describes the fraction of between-player variance due to true differences in player ability. A value closer to 1 indicates a metric that can differentiate between players more precisely.\n\n\nStability\nAgain, simplifying the original formula, let‚Äôs define stability as so:\n\\[\n\\begin{array}{cc}\n\\text{(Stability)} & 1 - \\frac{WV - BV}{TV - BV}.\n\\end{array}\n\\tag{2}\\]\n\\(WV\\) represents the average variance among seasons within a player. \\(TV\\) is the total variance of a metric over all seasons.\nThis representation represents the fraction of total variance that is due to within-player changes over time. A value closer to 1 indicates that the metric value for a given player is less likely to change from season to season."
  },
  {
    "objectID": "posts/soccer-meta-analytics/index.html#results-and-discussion",
    "href": "posts/soccer-meta-analytics/index.html#results-and-discussion",
    "title": "Meta-Analytics for Soccer",
    "section": "Results and Discussion",
    "text": "Results and Discussion\nLet‚Äôs make a scatter plot of the discrimination and stability of our metrics, akin to the plots made by Franks et al.\n\nWe can make the following observations:\n\nUn-adjusted volume (i.e.¬†‚Äúcount‚Äù in the plot) statistics based on total playing time‚Äìlike shots, tackles, and interceptions‚Äìtend to be highly discriminative, but not quite as stable. This makes sense‚Äîsuch statistics have large between-player variance‚Äîthink about the number of shots that a central defender takes compared to a striker. When aggregated for each player throughout a season, they provide a strong signal for different player types, i.e.¬†positions. Indeed, Franks et al.¬†found the same for NBA and NHL statistics.\nAdvanced stats like xG and xA also prove to be more discriminative than stable.\nThe other advanced stat, goals/xG ratio, stands out. It‚Äôs the only metric that is significantly more stable than discriminative. Although I haven‚Äôt looked into this extensively or given it much more than a few moments of thought, I believe this is because most players do not score goals in individual matches, but often accumulate some non-zero xG via shots. Thus, Goals/xG can be zero very frequently, meaning that it would be hard to differentiate players just on this ratio. And, because the ratio is often zero, it is found to be stable.\nShot conversion rate seems to be the center of the universe, having nearly equal discrimination and stability, both at around 0.5.\nThe per 90 metrics show higher stability than all other stats evaluated. This speaks to the between-player noise-minimizing benefits of comparing players on an equal-minute basis. Their stability is much closer to their discriminative power than most other other metrics."
  },
  {
    "objectID": "posts/soccer-meta-analytics/index.html#methods",
    "href": "posts/soccer-meta-analytics/index.html#methods",
    "title": "Meta-Analytics for Soccer",
    "section": "Methods",
    "text": "Methods\n\n\n\n\n\n\nWarning\n\n\n\nI skip over a lot of details from Frank et al.‚Äôs paper here. I‚Äôd encourage an interested reader to review the original paper.\n\n\n\nDiscrimination\nSimplifying the formulation of Franks et al., let‚Äôs define discrimination as follows:\n\\[\n1 - \\frac{BV}{SV}.\n\\tag{1}\\]\n\\(BV\\) is the average sampling variance of a metric after bootstrapping matches. (This variance measure is unique in that it involves resampling, while the others are simply calculated on the observed data.) \\(SV\\) is the total variance of a metric between players in a given season.\nAltogether, this equation describes the fraction of variance between players due to true differences in player ability. A value closer to 1 indicates a metric that can differentiate between players more precisely.\n\n\nStability\nAgain, simplifying the original equation from Franks et al., we define stability as so:\n\\[\n1 - \\frac{WV - BV}{TV - BV}.\n\\tag{2}\\]\n\\(WV\\) represents the average variance among seasons ‚Äúwithin‚Äù a player, before accounting for sampling variance (\\(BV\\)). \\(TV\\) is the total variance of a metric over all seasons, before accounting for sampling variance.\nOn the whole, this equation represent the fraction of total variance that is due to within-player changes over time. A value closer to 1 indicates that the metric value for a given player is less likely to change from season to season."
  },
  {
    "objectID": "posts/probability-calibration/index.html",
    "href": "posts/probability-calibration/index.html",
    "title": "Calibrating Binary Probabilities",
    "section": "",
    "text": "Ever grappled with a classification model that consistently over-predicts or under-predicts an event? Your first thought might be to re-evaluate the model‚Äôs features or framework. But what if tweaking the model isn‚Äôt an option, either due to a lack of resources or access restrictions? The good news is, there‚Äôs another way‚Äìit‚Äôs called calibration.\nCalibration falls in the ‚Äúpost-processing‚Äù step of predictive modeling.1 We modify the output of a model using nothing but the model predictions and labels of the output. We do that by, you guess it, fitting another model, often called a ‚Äúcalibrator‚Äù.\n\n\nthe pre-processing stage (e.g., feature engineering, normalization, etc.)\nmodel fitting (actually training the model)\npost-processing (such as optimizing a probability threshold)\n\n\nOne of my favorite (and relatively new) packages in the {tidymodels} ecosystem is the {probably} package. It provides functions that make it fairly straightforward to do calibration, even for those who are new to the concept. So let‚Äôs use {probably} to demonstrate the power of calibration."
  },
  {
    "objectID": "posts/probability-calibration/index.html#calibrating-a-binary-classifier",
    "href": "posts/probability-calibration/index.html#calibrating-a-binary-classifier",
    "title": "Calibrating Binary Probabilities",
    "section": "Calibrating a Binary Classifier",
    "text": "Calibrating a Binary Classifier\n\nData\nI‚Äôll be using the pre-match win probabilities from FiveThirtyEight (RIP). Specifically, I‚Äôll subset their plethora of historical projections to two women‚Äôs leagues: the Women‚Äôs Super League (WSL) in England and the National Women‚Äôs Soccer League (NWSL) in the U.S. I have a suspicion that the model probabilities are not as calibrated as they could be, as has been observed that gender-agnostic models in soccer can be less performant for the women‚Äôs game.\nTo keep things simple, I‚Äôm going to treat this as a binary classification task, where matches are the ‚Äúpositive‚Äù outcome (\"yes\"), and losses and draws are grouped as the ‚Äúnegative‚Äù outcome (\"no\").\n\n\nRetrieve data\nlibrary(readr)\nlibrary(dplyr)\nmatches &lt;- readr::read_csv('https://projects.fivethirtyeight.com/soccer-api/club/spi_matches.csv') |&gt; \n  dplyr::filter(\n    !is.na(score1), ## match is completed\n    league %in% c(\n      \"FA Women's Super League\",\n      \"National Women's Soccer League\"\n    )\n  ) |&gt; \n  dplyr::transmute(\n    league,\n    season,\n    date,\n    team1,\n    team2,\n    target = factor(ifelse(score1 &gt; score2, 'yes', 'no')),\n    ## .pred_ is sort of a convention among {tidymodels} docs\n    .pred_yes = prob1,\n    .pred_no = 1- .pred_yes \n  )\n\n\n\nmatches\n#&gt; # A tibble: 1,358 √ó 6\n#&gt;    date       team1               team2             target .pred_yes .pred_no\n#&gt;    &lt;date&gt;     &lt;chr&gt;               &lt;chr&gt;             &lt;fct&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n#&gt;  1 2016-07-09 Liverpool Women     Reading           yes        0.439    0.561\n#&gt;  2 2016-07-10 Arsenal Women       Notts County Lad‚Ä¶ yes        0.357    0.643\n#&gt;  3 2016-07-10 Chelsea FC Women    Birmingham City   no         0.480    0.520\n#&gt;  4 2016-07-16 Liverpool Women     Notts County Lad‚Ä¶ no         0.429    0.571\n#&gt;  5 2016-07-17 Chelsea FC Women    Arsenal Women     no         0.412    0.588\n#&gt;  6 2016-07-24 Reading             Birmingham City   no         0.382    0.618\n#&gt;  7 2016-07-24 Notts County Ladies Manchester City ‚Ä¶ no         0.308    0.692\n#&gt;  8 2016-07-31 Reading             Notts County Lad‚Ä¶ no         0.407    0.593\n#&gt;  9 2016-07-31 Arsenal Women       Liverpool Women   no         0.435    0.565\n#&gt; 10 2016-08-03 Reading             Manchester City ‚Ä¶ no         0.306    0.694\n#&gt; # ‚Ñπ 1,348 more rows\n\n\n\nDiagnosis\nWe start with the ‚Äúdiagnosis‚Äù phase: ‚ÄúHow well do the original probabilities perform?‚Äù To evaluate this, we can use one of the several probably::cal_plot_* functions. In this case, we‚Äôll use cal_plot_breaks().2\nThis function neatly divides the range of predicted probabilities from zero to one into distinct bins. For each bin, it calculates the observed event rate using data that has probabilities falling within that bin‚Äôs range. Ideally, if our predictions are calibrated, the curve produced should match up with a straight diagonal line, i.e.¬†a 45-degree slope passing through (0,0) and (1,1). As a bonus, the probably::cal_plot_* family of functions even provides confidence intervals about the calibration curve.\n\nlibrary(probably)\npackageVersion('probably')\n#&gt; [1] ‚Äò1.0.1.9000‚Äô\n\nmatches |&gt; \n  probably::cal_plot_breaks(\n    truth = target,\n    ## the \"_yes\" in `.pred_yes` must match one of the values in `target`\n    estimate = .pred_yes,\n    ## because \"yes\" is the second event level (\"no\" is the first)\n    event_level = 'second'\n  )\n\n{probably} offers some really neat automatic plotting of calibration curves. While I‚Äôd suggest giving them a try, I like to make my curves in a certain manner. In particular, instead of using a ‚Äúrug‚Äù, I like showing sample sizes via points on the curve.\n\nIndeed, it looks like there is some room for improvement with the match probabilities. It seems that the FiveThirtyEight model over-predicts when the actual win rate is low, broadly below 20%; and, likewise, it tends to under-predict when the true win rate is really greater than 60%\n\n\nRemediation\nNow we move on to the ‚Äúremedation‚Äù step. That is, we fit a model, a ‚Äúcalibrator‚Äù, with the binary outcome as the target variable and the probability estimate as the lone input feature. {probably} offers several options with the cal_estimate_*() set of functions.\nI‚Äôve opted for Beta calibration, although logistic calibration would work fine here as well. Beta calibration is a little more flexible, and, consequently, can provide superior probability estimates, especially when the target distribution is skewed.3\n\nmatches |&gt; \n  probably::cal_estimate_beta(\n    truth = target,\n    estimate = dplyr::starts_with('.pred'),\n    event_level = 'second'\n  )\n#&gt;  ‚îÄ‚îÄ Probability Calibration \n#&gt;  Method: Beta calibration\n#&gt;  Type: Binary\n#&gt;  Source class: Data Frame\n#&gt;  Data points: 1,358\n#&gt;  Truth variable: `target`\n#&gt;  Estimate variables:\n#&gt;  `.pred_no` ==&gt; no\n#&gt;  .pred_yes` ==&gt; yes\n\nUnder the hood, cal_estimate_beta() is doing something like this.\n\nlibrary(betacal)\n\nbetacal::beta_calibration(\n  p = matches$.pred_yes,\n  y = matches$target == 'yes',\n  parameters = 'abm'\n)\n\nIt‚Äôs almost shocking how simple the implementation is."
  },
  {
    "objectID": "posts/probability-calibration/index.html#results",
    "href": "posts/probability-calibration/index.html#results",
    "title": "Calibrating Binary Probabilities",
    "section": "Results",
    "text": "Results\nWith the calibrator model in hand, let‚Äôs make a scatter plot of all the points in the data set, viewing how the model has adjusted the original probabilities.\n\nWe observe that the calibrator has increased point estimates on the lower end of the spectrum and decreased estimates on the upper end of the spectrum. The calibration has seemingly fine-tuned under-predicting and over-predicting behavior from the original model.\nTo see the change that calibrator has made, we can re-make our calibration curve plot, adding the ‚Äúcalibrated‚Äù curve alongside the original ‚Äúun-calibrated‚Äù curve.\n\nVisually, it‚Äôs evident that the remediation has improved the probability estimates. The calibrated curve more closely ‚Äúhugs‚Äù the ideal 45 degree slope across the whole probability spectrum.\n\nValidation\nTo quantitatively describe the difference in calibration between the two models, we can compare the Brier Skill Score (BSS) of the un-calibrated and calibrated models.4 Keep in mind that a higher BSS indicates a more calibrated model. (1 is ideal. 0 indicates that the model is no better or worse than a reference model5.)\n\n\nBrier Skill Score (BSS) calculation\nlibrary(yardstick)\nlibrary(rlang)\n\nbrier_skill_score &lt;- function(data, ...) {\n  UseMethod('brier_skill_score')\n}\n\nbrier_skill_score &lt;- yardstick::new_prob_metric(\n  brier_skill_score, \n  direction = 'maximize'\n)\n\nbss &lt;- function(\n    truth, \n    estimate, \n    ref_estimate, \n    event_level,\n    case_weights,\n    ...\n) {\n    \n  if (length(estimate) == 1) {\n    estimate &lt;- rep(estimate, length(truth))\n  }\n  \n  if (length(ref_estimate) == 1) {\n    ref_estimate &lt;- rep(ref_estimate, length(truth))\n  }\n  \n  estimate_brier_score &lt;- brier_class_vec(\n    truth = truth,\n    estimate = estimate,\n    event_level = event_level,\n    case_weights = case_weights,\n    ...\n  )\n  \n  ref_brier_score &lt;- brier_class_vec(\n    truth = truth,\n    estimate = ref_estimate,\n    event_level = event_level,\n    case_weights = case_weights,\n    ...\n  )\n  \n  1 - (estimate_brier_score / ref_brier_score)\n}\n\nbrier_skill_score_estimator_impl &lt;- function(\n    truth, \n    estimate, \n    ref_estimate, \n    event_level,\n    case_weights\n) {\n  bss(\n    truth = truth,\n    estimate = estimate,\n    ref_estimate = ref_estimate,\n    event_level = event_level,\n    case_weights = case_weights\n  )\n}\n\n\nbrier_skill_score_vec &lt;- function(\n    truth, \n    estimate, \n    ref_estimate, \n    na_rm = TRUE, \n    event_level = yardstick:::yardstick_event_level(),\n    case_weights = NULL, \n    ...\n) {\n  \n  yardstick:::abort_if_class_pred(truth)\n  \n  estimator &lt;- yardstick::finalize_estimator(\n    truth, \n    metric_class = 'brier_skill_score'\n  )\n  \n  yardstick::check_prob_metric(truth, estimate, case_weights, estimator)\n  \n  if (na_rm) {\n    result &lt;- yardstick::yardstick_remove_missing(truth, estimate, case_weights)\n    \n    truth &lt;- result$truth\n    estimate &lt;- result$estimate\n    case_weights &lt;- result$case_weights\n  } else if (yardstick::yardstick_any_missing(truth, estimate, case_weights)) {\n    return(NA_real_)\n  }\n\n  brier_skill_score_estimator_impl(\n    truth = truth,\n    estimate = estimate,\n    ref_estimate = ref_estimate,\n    event_level = event_level,\n    case_weights = case_weights\n  )\n}\n\nbrier_skill_score.data.frame &lt;- function(\n    data, \n    truth, \n    ...,\n    na_rm = TRUE,\n    event_level = yardstick:::yardstick_event_level(),\n    case_weights = NULL\n) {\n  yardstick::prob_metric_summarizer(\n    name = 'brier_skill_score',\n    fn = brier_skill_score_vec,\n    data = data,\n    truth = !!rlang::enquo(truth),\n    ...,\n    na_rm = na_rm,\n    event_level = event_level,\n    case_weights = !!rlang::enquo(case_weights),\n    fn_options = list(\n      ref_estimate = ref_estimate\n    )\n  )\n}\n\n## The BSS is more intuitive to caluclate using the match non-win rate.\n##   Accordingly, we'll use .pred_no for our probability.\nREF_ESTIMATE &lt;- matches |&gt; \n  dplyr::count(target) |&gt; \n  dplyr::mutate(prop = n / sum(n)) |&gt; \n  dplyr::filter(target != 'yes') |&gt; \n  dplyr::pull(prop)\n\nraw_brier_skill_score &lt;- brier_skill_score_vec(\n  truth = calibrated$target,\n  estimate = calibrated$.raw_pred_no,\n  ref_estimate = REF_ESTIMATE\n)\n\ncalibrated_brier_skill_score &lt;- brier_skill_score_vec(\n  truth = calibrated$target,\n  estimate = calibrated$.pred_no,\n  ref_estimate = REF_ESTIMATE\n)\n\ncompared_brier_skill_scores &lt;- round(\n  c(\n    'Un-calibrated' = raw_brier_skill_score,\n    'Calibrated' = calibrated_brier_skill_score\n  ),\n  3\n)\n\n\n\ncompared_brier_skill_scores\n#&gt; Un-calibrated    Calibrated \n#&gt;         0.196         0.205\n\nIndeed, we have (marginally) improved the original pre-match win probabilities. But this approach is arguably a little naive‚Äìwe‚Äôve only re-assessed the entire data set a single time without accounting for potential uncertainties.\nFortunately, the {probably} package provides the cal_validate_*() family of functions. These functions allow for a more rigorous assessment of whether the calibration enhances the original probabilities. We can generate resamples from the original data and then compute the average and standard error for our chosen metrics. This lets us compare the calibrated and uncalibrated probabilities more effectively.\nLet‚Äôs do just that, using cross-validation with 10 folds and 10 repeats We‚Äôll again use BSS to evaluate the model probabilities.\n\n\nRobustly evaluate-ing calibrator\nset.seed(42)\nsets &lt;- rsample::vfold_cv(\n  matches, \n  v = 10, \n  repeats = 10\n)\n\n## \"fixed\" in the sense that we're pre-defining the reference estimate.\n##   I'm not sure there's another way of going about this when working in conjunction `yardstick::metric_set()` and `probably::cal_validate_*()`\n##   with\nfixed_brier_skill_score.data.frame &lt;- function(...) {\n  brier_skill_score(\n    ref_estimate = REF_ESTIMATE,\n    ...\n  )\n}\n\nfixed_brier_skill_score &lt;- function(data, ...) {\n  UseMethod('fixed_brier_skill_score')\n}\n\nfixed_brier_skill_score &lt;- yardstick::new_prob_metric(\n  fixed_brier_skill_score,\n  direction = 'maximize'\n)\n\neval_metrics &lt;- yardstick::metric_set(\n  fixed_brier_skill_score\n)\n\nvalidation &lt;- probably::cal_validate_beta(\n  sets,\n  truth = target,\n  metrics = eval_metrics\n)\n\nvalidation_metrics &lt;- validation |&gt; \n  tune::collect_metrics() |&gt; \n  ## i think the `.config` column is bugged (it just says \"config\" for all rows?)\n  dplyr::select(-dplyr::any_of('.config'))\nvalidation_metrics\n\n\n\nvalidation_metrics\n#&gt; # A tibble: 2 √ó 6\n#&gt;   .metric           .type        .estimator  mean     n std_err\n#&gt;   &lt;chr&gt;             &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n#&gt; 1 brier_skill_score uncalibrated binary     0.196   100 0.00494\n#&gt; 2 brier_skill_score calibrated   binary     0.202   100 0.00628\n\nWe find that the calibrator does indeed offer a ‚Äúsignificant‚Äù improvement when assessed through this more statistically rigorous method. Specifically, the mean BSS of the validation set, minus one standard error, exceeds the mean of the uncalibrated BSS by more than one standard error.\n\npivoted_validation_metrics &lt;- validation_metrics |&gt; \n  dplyr::transmute(\n    .type,\n    mean,\n    lower = mean - std_err,\n    upper = mean + std_err\n  )\npivoted_validation_metrics\n#&gt; # A tibble: 2 √ó 4\n#&gt;   .type         mean lower upper\n#&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 uncalibrated 0.196 0.191 0.201\n#&gt; 2 calibrated   0.202 0.196 0.209"
  },
  {
    "objectID": "posts/probability-calibration/index.html#conclusion",
    "href": "posts/probability-calibration/index.html#conclusion",
    "title": "Calibrating Binary Probabilities",
    "section": "Conclusion",
    "text": "Conclusion\nWhile I wouldn‚Äôt say ‚Äúcalibration is all you need‚Äù, it‚Äôs certainly something nice to have in your toolkit when you‚Äôre working on a modeling task. It can be a game-changer, especially when tweaking the original model isn‚Äôt an option, whether due to access limitations or, let‚Äôs be honest, sheer laziness.\nOh, and I failed to mention this earlier‚Äîcalibration isn‚Äôt just for binary classifiers. Multinomial classifiers and even regression models can benefit from this technique as well.\nHappy modeling, folks."
  },
  {
    "objectID": "posts/fbref-gamestate-expected-goal-difference/index.html",
    "href": "posts/fbref-gamestate-expected-goal-difference/index.html",
    "title": "Game state with FBref data",
    "section": "",
    "text": "Soccer is a game defined by its discrete, low-scoring nature, where the dynamics of a match are often dictated by the ‚Äúgame state‚Äù. Whether a team is leading, trailing, or level with their opponent at a specific moment can often influence how aggressively a team plays. Contextualizing statistics like expected goals (xG) can shed new light on how we evaluate team performance.\nConsider this scenario: a team enters a match as the underdog. They play aggressively early on, getting a few shots on target and eventually scoring, taking a 1-0 lead going into halftime. After the half, they decide to switch into a more defensive scheme, pulling everyone back towards their 18-yard box when the opponent has the ball. They end the match with no additional shots or xG accumulated, but they win the game. While their secondhalf statistics look poor because they ‚Äúparked the bus‚Äù, they arguably increased their odds of winning. If we consider the game state when looking at the winning team‚Äôs statistics, we can reason about why their xG looks poor.\nSo, game state is useful, right? Yet, game state analysis remains somewhat under-utilized in soccer analytics, in my opinion. Why is that? Well, it‚Äôs not without its challenges. Contextualizing numbers according to game state can introduce biases, leading us to over-attribute outcomes to tactical choices. Moreover, the calculations involved can be far from trivial.\nSo that‚Äôs what this post is for. I‚Äôll walk through how to calculate expected goals difference (xGD)1‚Äìthe difference between your team‚Äôs expected goals and your opponent‚Äôs‚Äìwith respect to the game state, using data from FBref."
  },
  {
    "objectID": "posts/fbref-gamestate-expected-goal-difference/index.html#data-pull",
    "href": "posts/fbref-gamestate-expected-goal-difference/index.html#data-pull",
    "title": "Game state with FBref data",
    "section": "Data pull",
    "text": "Data pull\nThe 2023 Major League Soccer (MLS) regular season just ended, and I‚Äôm interested to see what we might learn about the teams who qualified for playoffs. So, naturally, I‚Äôve chosen to focus on this past MLS season for our game state calculations.\nTo begin2, we pull raw FBref data from pre-saved {worldfootballR} release data, starting with match shots.\n\n\nCode\n## data scrape\nlibrary(worldfootballR) ## version: 0.6.4.9\n\n## data manipulation\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(tidyr)\n\nCOUNTRY &lt;- 'USA'\nGENDER &lt;- 'M'\nTIER &lt;- '1st'\nSEASON_END_YEAR &lt;- 2023\n\nraw_shots &lt;- worldfootballR::load_fb_match_shooting(\n  country = COUNTRY,\n  gender = GENDER,\n  tier = TIER,\n  season_end_year = SEASON_END_YEAR\n)\n\n\n\ndplyr::glimpse(raw_shots)\n#&gt; Rows: 15,277\n#&gt; Columns: 23\n#&gt; $ MatchURL         &lt;chr&gt; \"https://fbref.com/en/matches/48a684ed/Nashvil‚Ä¶\n#&gt; $ Date             &lt;chr&gt; \"2023-02-25\", \"2023-02-25\", \"2023-02-25\", \"202‚Ä¶\n#&gt; $ Squad            &lt;chr&gt; \"Nashville\", \"Nashville\", \"Nashville\", \"Nashvi‚Ä¶\n#&gt; $ Home_Away        &lt;chr&gt; \"Home\", \"Home\", \"Home\", \"Home\", \"Home\", \"Home\"‚Ä¶\n#&gt; $ Match_Half       &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2‚Ä¶\n#&gt; $ Minute           &lt;chr&gt; \"6\", \"13\", \"31\", \"34\", \"45+1\", \"51\", \"73\", \"80‚Ä¶\n#&gt; $ Player           &lt;chr&gt; \"Jacob Shaffelburg\", \"Sean Davis\", \"Teal Bunbu‚Ä¶\n#&gt; $ Player_Href      &lt;chr&gt; \"/en/players/339a2561/Jacob-Shaffelburg\", \"/en‚Ä¶\n#&gt; $ xG               &lt;chr&gt; \"0.39\", \"0.09\", \"0.03\", \"0.25\", \"0.04\", \"0.02\"‚Ä¶\n#&gt; $ PSxG             &lt;chr&gt; \"0.47\", \"\", \"0.06\", \"0.74\", \"\", \"\", \"\", \"0.96\"‚Ä¶\n#&gt; $ Outcome          &lt;chr&gt; \"Saved\", \"Off Target\", \"Saved\", \"Goal\", \"Off T‚Ä¶\n#&gt; $ Distance         &lt;chr&gt; \"16\", \"18\", \"29\", \"8\", \"17\", \"25\", \"28\", \"11\",‚Ä¶\n#&gt; $ `Body Part`      &lt;chr&gt; \"Right Foot\", \"Left Foot\", \"Right Foot\", \"Righ‚Ä¶\n#&gt; $ Notes            &lt;chr&gt; \"\", \"Volley\", \"Deflected\", \"Volley\", \"\", \"\", \"‚Ä¶\n#&gt; $ Player_SCA_1     &lt;chr&gt; \"Randall Leal\", \"An√≠bal Godoy\", \"Jacob Shaffel‚Ä¶\n#&gt; $ Event_SCA_1      &lt;chr&gt; \"Pass (Live)\", \"Pass (Live)\", \"Pass (Live)\", \"‚Ä¶\n#&gt; $ Player_SCA_2     &lt;chr&gt; \"Walker Zimmerman\", \"Jack Maher\", \"Joe Willis\"‚Ä¶\n#&gt; $ Event_SCA_2      &lt;chr&gt; \"Pass (Live)\", \"Pass (Live)\", \"Pass (Live)\", \"‚Ä¶\n#&gt; $ Competition_Name &lt;chr&gt; \"Major League Soccer\", \"Major League Soccer\", ‚Ä¶\n#&gt; $ Gender           &lt;chr&gt; \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"‚Ä¶\n#&gt; $ Country          &lt;chr&gt; \"USA\", \"USA\", \"USA\", \"USA\", \"USA\", \"USA\", \"USA‚Ä¶\n#&gt; $ Tier             &lt;chr&gt; \"1st\", \"1st\", \"1st\", \"1st\", \"1st\", \"1st\", \"1st‚Ä¶\n#&gt; $ Season_End_Year  &lt;int&gt; 2023, 2023, 2023, 2023, 2023, 2023, 2023, 2023‚Ä¶\n\nGiven a match URL like this, worldfootballR::load_fb_match_shooting() provides data from the ‚ÄúShots‚Äù table on the page.\n\nWhile it might seem like the shots table is all we‚Äôd need to calculate expected goal difference (xGD), FBref‚Äôs match shot log table doesn‚Äôt include own goals. Nonetheless, we can use worldfootballR::load_fb_match_summary() to extract timestamps for own goals from the ‚ÄúMatch Summary‚Äù timeline.\n\n\n\nCode\nraw_match_summaries &lt;- worldfootballR::load_fb_match_summary(\n  country = COUNTRY,\n  gender = GENDER,\n  tier = TIER,\n  season_end_year = SEASON_END_YEAR\n)\n\n\n\ndplyr::glimpse(raw_match_summaries)\n#&gt; Rows: 9,565\n#&gt; Columns: 33\n#&gt; $ MatchURL          &lt;chr&gt; \"https://fbref.com/en/matches/48a684ed/Nashvi‚Ä¶\n#&gt; $ League            &lt;chr&gt; \"Major League Soccer\", \"Major League Soccer\",‚Ä¶\n#&gt; $ Match_Date        &lt;chr&gt; \"2023-02-25\", \"2023-02-25\", \"2023-02-25\", \"20‚Ä¶\n#&gt; $ Matchweek         &lt;chr&gt; \"Major League Soccer (Regular Season)\", \"Majo‚Ä¶\n#&gt; $ Home_Team         &lt;chr&gt; \"Nashville SC\", \"Nashville SC\", \"Nashville SC‚Ä¶\n#&gt; $ Home_Formation    &lt;chr&gt; \"4-2-3-1\", \"4-2-3-1\", \"4-2-3-1\", \"4-2-3-1\", \"‚Ä¶\n#&gt; $ Home_Score        &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ‚Ä¶\n#&gt; $ Home_xG           &lt;dbl&gt; 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, ‚Ä¶\n#&gt; $ Home_Goals        &lt;chr&gt; \"Walker Zimmerman ¬∑ 34&rsquor; Jacob Shaffelb‚Ä¶\n#&gt; $ Home_Yellow_Cards &lt;chr&gt; \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", ‚Ä¶\n#&gt; $ Home_Red_Cards    &lt;chr&gt; \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", ‚Ä¶\n#&gt; $ Away_Team         &lt;chr&gt; \"New York City FC\", \"New York City FC\", \"New ‚Ä¶\n#&gt; $ Away_Formation    &lt;chr&gt; \"4-2-3-1\", \"4-2-3-1\", \"4-2-3-1\", \"4-2-3-1\", \"‚Ä¶\n#&gt; $ Away_Score        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ‚Ä¶\n#&gt; $ Away_xG           &lt;dbl&gt; 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, ‚Ä¶\n#&gt; $ Away_Goals        &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"‚Ä¶\n#&gt; $ Away_Yellow_Cards &lt;chr&gt; \"4\", \"4\", \"4\", \"4\", \"4\", \"4\", \"4\", \"4\", \"4\", ‚Ä¶\n#&gt; $ Away_Red_Cards    &lt;chr&gt; \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", ‚Ä¶\n#&gt; $ Game_URL          &lt;chr&gt; \"https://fbref.com/en/matches/48a684ed/Nashvi‚Ä¶\n#&gt; $ Team              &lt;chr&gt; \"New York City FC\", \"Nashville SC\", \"Nashvill‚Ä¶\n#&gt; $ Home_Away         &lt;chr&gt; \"Away\", \"Home\", \"Home\", \"Away\", \"Away\", \"Home‚Ä¶\n#&gt; $ Event_Time        &lt;dbl&gt; 28, 34, 58, 62, 70, 72, 74, 75, 80, 82, 82, 8‚Ä¶\n#&gt; $ Is_Pens           &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL‚Ä¶\n#&gt; $ Event_Half        &lt;dbl&gt; 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, ‚Ä¶\n#&gt; $ Event_Type        &lt;chr&gt; \"Yellow Card\", \"Goal\", \"Yellow Card\", \"Yellow‚Ä¶\n#&gt; $ Event_Players     &lt;chr&gt; \"Braian Cufr√©\", \"Walker Zimmerman Assist: Faf‚Ä¶\n#&gt; $ Score_Progression &lt;chr&gt; \"0:0\", \"1:0\", \"1:0\", \"1:0\", \"1:0\", \"1:0\", \"1:‚Ä¶\n#&gt; $ Penalty_Number    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n#&gt; $ Competition_Name  &lt;chr&gt; \"Major League Soccer\", \"Major League Soccer\",‚Ä¶\n#&gt; $ Gender            &lt;chr&gt; \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", ‚Ä¶\n#&gt; $ Country           &lt;chr&gt; \"USA\", \"USA\", \"USA\", \"USA\", \"USA\", \"USA\", \"US‚Ä¶\n#&gt; $ Tier              &lt;chr&gt; \"1st\", \"1st\", \"1st\", \"1st\", \"1st\", \"1st\", \"1s‚Ä¶\n#&gt; $ Season_End_Year   &lt;int&gt; 2023, 2023, 2023, 2023, 2023, 2023, 2023, 202‚Ä¶"
  },
  {
    "objectID": "posts/fbref-gamestate-expected-goal-difference/index.html#data-wrangling",
    "href": "posts/fbref-gamestate-expected-goal-difference/index.html#data-wrangling",
    "title": "Game state with FBref data",
    "section": "Data wrangling",
    "text": "Data wrangling\nNow we start to clean up the raw data. Starting with the match summary data, we:\n\nCreate a match_id field, to make it easy to join this data set with the shots data set.\nClean up the time fields, minutes and minutes_added.\nRename existing columns.\n\n\n\nCode\n## Extract the from \"47880eb7\" from \"https://fbref.com/en/matches/47880eb7/Liverpool-Manchester-City-November-10-2019-Premier-League\"\nextract_fbref_match_id &lt;- function(match_url) {\n  basename(dirname(match_url))\n}\n\nmatch_summaries &lt;- raw_match_summaries |&gt; \n  dplyr::group_by(MatchURL) |&gt; \n  dplyr::mutate(\n    match_summary_rn = dplyr::row_number(dplyr::desc(Event_Time)),\n    match_has_no_penalties = all(Event_Type != 'Penalty')\n  ) |&gt; \n  dplyr::ungroup() |&gt; \n  dplyr::mutate(\n    match_has_no_goals = Away_Score == 0 & Home_Score == 0\n  ) |&gt; \n  ## Drop non-shot events, e.g. card and substitution events. \n  ##   Always keep the first timeline event, so that we're not accidentally dropping matches.\n  dplyr::filter(\n    Event_Type %in% c('Goal', 'Own Goal', 'Penalty') | \n      ## don't drop games with no goals\n      (match_has_no_goals & match_has_no_penalties & match_summary_rn == 1)\n  ) |&gt; \n  dplyr::transmute(\n    match_id = extract_fbref_match_id(MatchURL),\n    season = Season_End_Year,\n    gender = Gender,\n    tier = Tier,\n    date = lubridate::ymd(Match_Date),\n    home_team = Home_Team ,\n    away_team = Away_Team,\n    period = as.integer(Event_Half),\n    ## ensure that minutes always has a value\n    minutes = dplyr::case_when(\n      period == 1L & Event_Time &gt; 45L ~ 45L, \n      period == 2L & Event_Time &gt; 90L ~ 90L,\n      .default = Event_Time\n    ) |&gt; as.integer(),\n    minutes_added = dplyr::case_when(\n      period == 1L & Event_Time &gt; 45 ~ Event_Time - 45L, \n      period == 2L & Event_Time &gt; 90 ~ Event_Time - 90L,\n      .default = NA_integer_\n    ),\n    home_g = as.integer(gsub('[:].*$', '', Score_Progression)), ## after event\n    away_g = as.integer(gsub('^.*[:]', '', Score_Progression)),\n    is_own_goal = Event_Type == 'Own Goal',\n    team = Team,\n    player = Event_Players\n  )\n\n\n\ndplyr::glimpse(match_summaries)\n#&gt; Rows: 1,752\n#&gt; Columns: 15\n#&gt; $ match_id      &lt;chr&gt; \"48a684ed\", \"48a684ed\", \"1861e533\", \"1861e533\", \"‚Ä¶\n#&gt; $ season        &lt;int&gt; 2023, 2023, 2023, 2023, 2023, 2023, 2023, 2023, 2‚Ä¶\n#&gt; $ gender        &lt;chr&gt; \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\",‚Ä¶\n#&gt; $ tier          &lt;chr&gt; \"1st\", \"1st\", \"1st\", \"1st\", \"1st\", \"1st\", \"1st\", ‚Ä¶\n#&gt; $ date          &lt;date&gt; 2023-02-25, 2023-02-25, 2023-02-25, 2023-02-25, ‚Ä¶\n#&gt; $ home_team     &lt;chr&gt; \"Nashville SC\", \"Nashville SC\", \"FC Cincinnati\", ‚Ä¶\n#&gt; $ away_team     &lt;chr&gt; \"New York City FC\", \"New York City FC\", \"Houston ‚Ä¶\n#&gt; $ period        &lt;int&gt; 1, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 2, 2, 2, 2, 1‚Ä¶\n#&gt; $ minutes       &lt;int&gt; 34, 80, 19, 45, 48, 48, 12, 39, 90, 90, 28, 45, 5‚Ä¶\n#&gt; $ minutes_added &lt;dbl&gt; NA, NA, NA, 2, NA, NA, NA, NA, 3, 9, NA, 3, NA, N‚Ä¶\n#&gt; $ home_g        &lt;int&gt; 1, 2, 1, 1, 2, 0, 0, 0, 1, 2, 0, 1, 2, 3, 4, 1, 0‚Ä¶\n#&gt; $ away_g        &lt;int&gt; 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1‚Ä¶\n#&gt; $ is_own_goal   &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n#&gt; $ team          &lt;chr&gt; \"Nashville SC\", \"Nashville SC\", \"FC Cincinnati\", ‚Ä¶\n#&gt; $ player        &lt;chr&gt; \"Walker Zimmerman Assist: Faf√† Picault\", \"Jacob S‚Ä¶\n\nNext, we start to clean up the shots data frame. The data wrangling is similar.\n\n\nCode\nshots &lt;- raw_shots |&gt; \n  dplyr::transmute(\n    match_id = extract_fbref_match_id(MatchURL),\n    period = as.integer(Match_Half),\n    ## convert \"45+2\" to \"45\"\n    minutes = ifelse(\n      grepl('[+]', Minute),\n      as.integer(gsub('(^[0-9]+)[+]([0-9]+$)', '\\\\1', Minute)), \n      as.integer(Minute)\n    ),\n    ## convert \"45+2\" to \"2\"\n    minutes_added = ifelse(\n      grepl('[+]', Minute), \n      as.integer(gsub('(^[0-9]+)[+]([0-9]+$)', '\\\\2', Minute)), \n      NA_integer_\n    ),\n    is_home = Home_Away == 'Home',\n    team = Squad,\n    player = Player,\n    is_goal = Outcome == 'Goal',\n    xg = as.double(xG)\n  )\n\n\n\ndplyr::glimpse(shots)\n#&gt; Rows: 15,277\n#&gt; Columns: 9\n#&gt; $ match_id      &lt;chr&gt; \"48a684ed\", \"48a684ed\", \"48a684ed\", \"48a684ed\", \"‚Ä¶\n#&gt; $ period        &lt;int&gt; 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2‚Ä¶\n#&gt; $ minutes       &lt;int&gt; 6, 13, 31, 34, 45, 51, 73, 80, 83, 19, 30, 41, 45‚Ä¶\n#&gt; $ minutes_added &lt;int&gt; NA, NA, NA, NA, 1, NA, NA, NA, NA, NA, NA, NA, 2,‚Ä¶\n#&gt; $ is_home       &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T‚Ä¶\n#&gt; $ team          &lt;chr&gt; \"Nashville\", \"Nashville\", \"Nashville\", \"Nashville‚Ä¶\n#&gt; $ player        &lt;chr&gt; \"Jacob Shaffelburg\", \"Sean Davis\", \"Teal Bunbury\"‚Ä¶\n#&gt; $ is_goal       &lt;lgl&gt; FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, T‚Ä¶\n#&gt; $ xg            &lt;dbl&gt; 0.39, 0.09, 0.03, 0.25, 0.04, 0.02, 0.02, 0.45, 0‚Ä¶\n\n\nAccounting for Own goals\nNow, for the ugliest part of all this shot data wrangling‚Äìhandling own goals.\nFirst, we inject ‚Äúsynthetic‚Äù records into the shots data for every case where the match summary indicates that there is an own goal.\n\n\nCode\nshots_with_own_goals &lt;- dplyr::bind_rows(\n  shots |&gt; \n    dplyr::transmute(\n      match_id,\n      period,\n      minutes,\n      minutes_added,\n      is_home,\n      team,\n      player,\n      is_goal,\n      xg,\n      is_own_goal = FALSE\n    ),\n  ## synthetic events for own goals\n  match_summaries |&gt; \n    dplyr::filter(\n      is_own_goal\n    ) |&gt; \n    dplyr::transmute(\n      match_id,\n      period,\n      minutes,\n      minutes_added,\n      is_home = team == home_team,\n      team,\n      player,\n      is_goal = TRUE,\n      xg = NA_real_,\n      is_own_goal = TRUE\n    )\n)\n\n\nNext, we add proper, cleaned columns for goals and xG.\n\n\nCode\nclean_shots &lt;- shots_with_own_goals |&gt; \n  ## To get meta-information about the game\n  dplyr::inner_join(\n    match_summaries |&gt;\n      dplyr::distinct(match_id, home_team, away_team),\n    by = dplyr::join_by(match_id),\n    relationship = 'many-to-one'\n  ) |&gt; \n  dplyr::mutate(\n    home_g = dplyr::case_when(\n      ## Note that fotmob would list the away team for an own goal but fbref \n      ##   lists the home team\n      (is_goal | is_own_goal) & is_home ~ 1L,\n      is_own_goal & is_home ~ 1L,\n      TRUE ~ 0L\n    ),\n    away_g = dplyr::case_when(\n      (is_goal | is_own_goal) & !is_home ~ 1L,\n      TRUE ~ 0L\n    ),\n    home_xg = dplyr::case_when(\n      is_home ~ dplyr::coalesce(xg, 0),\n      TRUE ~ 0L ## even for own goals\n    ),\n    away_xg = dplyr::case_when(\n      !is_home ~ dplyr::coalesce(xg, 0),\n      TRUE ~ 0L\n    )\n  ) |&gt;\n  dplyr::group_by(match_id) |&gt;\n  ## Differentiate between shots in the same minute.\n  dplyr::mutate(\n    shot_idx = dplyr::row_number((minutes + dplyr::coalesce(minutes_added, 0L)))\n  ) |&gt; \n  dplyr::ungroup() |&gt; \n  dplyr::transmute(\n    shot_id = sprintf('%s-%02d', match_id, shot_idx),\n    match_id,\n    period,\n    minutes,\n    minutes_added,\n    is_home,\n    is_goal,\n    is_own_goal,\n    player,\n    home_team,\n    away_team,\n    home_g,\n    away_g,\n    home_xg,\n    away_xg\n  )\n\n\n\ndplyr::glimpse(clean_shots)\n#&gt; Rows: 15,335\n#&gt; Columns: 15\n#&gt; $ shot_id       &lt;chr&gt; \"48a684ed-01\", \"48a684ed-02\", \"48a684ed-05\", \"48a‚Ä¶\n#&gt; $ match_id      &lt;chr&gt; \"48a684ed\", \"48a684ed\", \"48a684ed\", \"48a684ed\", \"‚Ä¶\n#&gt; $ period        &lt;int&gt; 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2‚Ä¶\n#&gt; $ minutes       &lt;int&gt; 6, 13, 31, 34, 45, 51, 73, 80, 83, 19, 30, 41, 45‚Ä¶\n#&gt; $ minutes_added &lt;dbl&gt; NA, NA, NA, NA, 1, NA, NA, NA, NA, NA, NA, NA, 2,‚Ä¶\n#&gt; $ is_home       &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T‚Ä¶\n#&gt; $ is_goal       &lt;lgl&gt; FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, T‚Ä¶\n#&gt; $ is_own_goal   &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n#&gt; $ player        &lt;chr&gt; \"Jacob Shaffelburg\", \"Sean Davis\", \"Teal Bunbury\"‚Ä¶\n#&gt; $ home_team     &lt;chr&gt; \"Nashville SC\", \"Nashville SC\", \"Nashville SC\", \"‚Ä¶\n#&gt; $ away_team     &lt;chr&gt; \"New York City FC\", \"New York City FC\", \"New York‚Ä¶\n#&gt; $ home_g        &lt;int&gt; 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n#&gt; $ away_g        &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n#&gt; $ home_xg       &lt;dbl&gt; 0.39, 0.09, 0.03, 0.25, 0.04, 0.02, 0.02, 0.45, 0‚Ä¶\n#&gt; $ away_xg       &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0‚Ä¶\n\n\n\nDouble Counting Shot Events\nUp to this point, we have one record per shot. But, to calculate goals and expected goals (xG) conceded for any given team at any given point in a game, we can make our lives easier by ‚Äúdouble counting‚Äù each shot event, once from each team‚Äôs perspective.\nTo do that, we first re-assign (‚Äúre-stack‚Äù) teams and goals based on the home and away teams‚Äô perspectives.\n\n\nCode\nrestacked_shots &lt;- dplyr::bind_rows(\n  clean_shots |&gt; \n    dplyr::filter(is_home) |&gt; \n    dplyr::transmute(\n      shot_id,\n      match_id,\n      period,\n      minutes,\n      minutes_added,\n      is_home,\n      is_goal,\n      is_own_goal,\n      player,\n      team = home_team,\n      opponent = away_team,\n      g = home_g,\n      g_conceded = away_g,\n      xg = home_xg,\n      xg_conceded = away_xg\n    ),\n  clean_shots |&gt; \n    dplyr::filter(!is_home) |&gt; \n    dplyr::transmute(\n      shot_id,\n      match_id,\n      period,\n      minutes,\n      minutes_added,\n      is_home,\n      is_goal,\n      is_own_goal,\n      player,\n      team = away_team,\n      opponent = home_team,\n      g = away_g,\n      g_conceded = home_g,\n      xg = away_xg,\n      xg_conceded = home_xg\n    )\n)\n\n\nThen, we replicate the whole data frame, indicating whether we‚Äôre looking at the shot events from a given team‚Äôs point of view (pov = \"primary\") or their opponents‚Äô point of view (\"secondary\").\n\n\nCode\ndoublecounted_restacked_shots &lt;- dplyr::bind_rows(\n  restacked_shots |&gt; dplyr::mutate(pov = 'primary', .before = 1),\n  restacked_shots |&gt; \n    ## re-assign to temporary variable names first, so that way we don't accidentlaly overwrite information\n    dplyr::rename(\n      team1 = team,\n      team2 = opponent,\n      g1 = g,\n      g2 = g_conceded,\n      xg1 = xg,\n      xg2 = xg_conceded\n    ) |&gt; \n    ## then formally re-assign columns\n    dplyr::rename(\n      team = team2,\n      opponent = team1,\n      g = g2,\n      g_conceded = g1,\n      xg = xg2,\n      xg_conceded = xg1\n    ) |&gt; \n    dplyr::mutate(\n      is_home = !is_home\n    ) |&gt; \n    dplyr::mutate(\n      pov = 'secondary',\n      .before = 1\n    )\n) |&gt; \n  dplyr::arrange(match_id, shot_id, pov)\n\n\n\ndplyr::glimpse(doublecounted_restacked_shots)\n#&gt; Rows: 30,670\n#&gt; Columns: 16\n#&gt; $ pov           &lt;chr&gt; \"primary\", \"secondary\", \"primary\", \"secondary\", \"‚Ä¶\n#&gt; $ shot_id       &lt;chr&gt; \"00069d73-01\", \"00069d73-01\", \"00069d73-02\", \"000‚Ä¶\n#&gt; $ match_id      &lt;chr&gt; \"00069d73\", \"00069d73\", \"00069d73\", \"00069d73\", \"‚Ä¶\n#&gt; $ period        &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1‚Ä¶\n#&gt; $ minutes       &lt;int&gt; 2, 2, 4, 4, 9, 9, 20, 20, 24, 24, 32, 32, 34, 34,‚Ä¶\n#&gt; $ minutes_added &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n#&gt; $ is_home       &lt;lgl&gt; TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, FALS‚Ä¶\n#&gt; $ is_goal       &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n#&gt; $ is_own_goal   &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n#&gt; $ player        &lt;chr&gt; \"Hany Mukhtar\", \"Hany Mukhtar\", \"Teal Bunbury\", \"‚Ä¶\n#&gt; $ team          &lt;chr&gt; \"Nashville SC\", \"Chicago Fire\", \"Nashville SC\", \"‚Ä¶\n#&gt; $ opponent      &lt;chr&gt; \"Chicago Fire\", \"Nashville SC\", \"Chicago Fire\", \"‚Ä¶\n#&gt; $ g             &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n#&gt; $ g_conceded    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n#&gt; $ xg            &lt;dbl&gt; 0.04, 0.00, 0.17, 0.00, 0.02, 0.00, 0.12, 0.00, 0‚Ä¶\n#&gt; $ xg_conceded   &lt;dbl&gt; 0.00, 0.04, 0.00, 0.17, 0.00, 0.02, 0.00, 0.12, 0‚Ä¶\n\n\n\nCalculating Cumulative Goals\nNext, we calculate cumulative goals and xG scored and conceded.\n\n\nCode\ncumu_doublecounted_restacked_shots &lt;- doublecounted_restacked_shots |&gt; \n  dplyr::group_by(match_id, team) |&gt; \n  dplyr::mutate(\n    dplyr::across(\n      c(g, g_conceded),\n      list(cumu = cumsum)\n    )\n  ) |&gt; \n  dplyr::ungroup() |&gt; \n  dplyr::mutate(\n    gamestate = g_cumu - g_conceded_cumu\n  )\n\n\nAnd then we bring everything together to create a singular data frame from which it is straightforward to calculate xGD with respect to game state.3\n\n\nCode\nORDERED_gamestate_LABELS &lt;- c('Trailing', 'Tied', 'Leading')\ngamestate_shots &lt;- cumu_doublecounted_restacked_shots |&gt; \n  dplyr::inner_join(\n    match_summaries |&gt; \n      dplyr::distinct(\n        match_id,\n        season,\n        date,\n        home_team,\n        away_team\n      ),\n    by = dplyr::join_by(match_id)\n  ) |&gt; \n  dplyr::transmute(\n    pov,\n    match_id,\n    season,\n    date,\n    home_team,\n    away_team,\n    team,\n    player,\n    shot_id,\n    period,\n    minutes,\n    minutes_added,\n    time = minutes + dplyr::coalesce(minutes_added, 0L),\n    xg,\n    xgd = xg - xg_conceded,\n    gamestate = cut(\n      gamestate,\n      breaks = c(-Inf, -1, 0, Inf),\n      labels = ORDERED_gamestate_LABELS\n    )\n  ) |&gt; \n  dplyr::group_by(match_id, team) |&gt; \n  dplyr::arrange(shot_id, .by_group = TRUE) |&gt; \n  dplyr::mutate(\n    pre_shot_gamestate = dplyr::lag(gamestate, default = ORDERED_gamestate_LABELS[2])\n  ) |&gt; \n  dplyr::ungroup()\n\n\n\ndplyr::glimpse(gamestate_shots)\n#&gt; Rows: 30,670\n#&gt; Columns: 17\n#&gt; $ pov                &lt;chr&gt; \"secondary\", \"secondary\", \"secondary\", \"seco‚Ä¶\n#&gt; $ match_id           &lt;chr&gt; \"00069d73\", \"00069d73\", \"00069d73\", \"00069d7‚Ä¶\n#&gt; $ season             &lt;int&gt; 2023, 2023, 2023, 2023, 2023, 2023, 2023, 20‚Ä¶\n#&gt; $ date               &lt;date&gt; 2023-05-06, 2023-05-06, 2023-05-06, 2023-05‚Ä¶\n#&gt; $ home_team          &lt;chr&gt; \"Nashville SC\", \"Nashville SC\", \"Nashville S‚Ä¶\n#&gt; $ away_team          &lt;chr&gt; \"Chicago Fire\", \"Chicago Fire\", \"Chicago Fir‚Ä¶\n#&gt; $ team               &lt;chr&gt; \"Chicago Fire\", \"Chicago Fire\", \"Chicago Fir‚Ä¶\n#&gt; $ player             &lt;chr&gt; \"Hany Mukhtar\", \"Teal Bunbury\", \"Hany Mukhta‚Ä¶\n#&gt; $ shot_id            &lt;chr&gt; \"00069d73-01\", \"00069d73-02\", \"00069d73-03\",‚Ä¶\n#&gt; $ period             &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1,‚Ä¶\n#&gt; $ minutes            &lt;int&gt; 2, 4, 9, 20, 24, 32, 34, 35, 39, 40, 41, 42,‚Ä¶\n#&gt; $ minutes_added      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ‚Ä¶\n#&gt; $ time               &lt;dbl&gt; 2, 4, 9, 20, 24, 32, 34, 35, 39, 40, 41, 42,‚Ä¶\n#&gt; $ xg                 &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.‚Ä¶\n#&gt; $ xgd                &lt;dbl&gt; -0.04, -0.17, -0.02, -0.12, -0.05, -0.08, -0‚Ä¶\n#&gt; $ gamestate          &lt;fct&gt; Tied, Tied, Tied, Tied, Tied, Tied, Tied, Ti‚Ä¶\n#&gt; $ pre_shot_gamestate &lt;fct&gt; Tied, Tied, Tied, Tied, Tied, Tied, Tied, Ti‚Ä¶\n\n\n\nPadding end-of-match events\nOh, wait! We should probably account for the amount of time spent in a game state when contextualizing xGD. To do that properly, we should add more synthetic records for the end of halves.\nUnfortunately, FBref does not provide the exact ending minute of each half, as far as I know. Thus, we‚Äôll ‚Äúpad‚Äù our data with artificial records to mark the end of halves‚Äìthe 45th minute in the first half / 90th minute in the second half‚Äìusing some heuristics.\n\nIf there are no shots after the last regular minute in a half, we add 3 minutes. (3 minutes is about the median amount of minutes allocated for extra time.)\nIf the last shot is after the last regular minute in a half, we take the maximum of:\n\nAdding 3 minutes beyond the last regular minute (like (1)) or\nAdding one minute beyond the last shot.\n\n\n\n\nCode\nLAST_MIN_BUFFER &lt;- 3\nlast_min_pad &lt;- gamestate_shots |&gt;\n  dplyr::select(\n    match_id,\n    season,\n    date,\n    team,\n    pre_shot_gamestate,\n    period,\n    time\n  ) |&gt; \n  dplyr::group_by(match_id, team, period) |&gt;\n  dplyr::slice_max(time, n = 1, with_ties = FALSE) |&gt;\n  dplyr::ungroup() |&gt;\n  dplyr::mutate(\n    xg = 0,\n    xgd = 0,\n    last_regular_min = ifelse(period == 1L, 45L, 90L),\n    time = pmax(last_regular_min + LAST_MIN_BUFFER, time + 1)\n  )\n\npadded_gamestate_shots &lt;- dplyr::bind_rows(\n  gamestate_shots,\n  last_min_pad\n) |&gt; \n  dplyr::arrange(match_id, time)\n\ngamestate_shots_and_durations &lt;- padded_gamestate_shots |&gt; \n  dplyr::group_by(match_id, team) |&gt; \n  dplyr::mutate(\n    prev_period = dplyr::lag(period),\n    prev_time = dplyr::lag(time)\n  ) |&gt; \n  dplyr::ungroup() |&gt; \n  dplyr::mutate(\n    duration = dplyr::case_when(\n      period == 1L & is.na(prev_period) ~ time - 0L,\n      period == 2L & period != prev_period ~ time - 45L,\n      TRUE ~ time - prev_time\n    )\n  )\n\n\n\ndplyr::glimpse(gamestate_shots_and_durations)\n#&gt; Rows: 32,642\n#&gt; Columns: 21\n#&gt; $ pov                &lt;chr&gt; \"secondary\", \"primary\", \"secondary\", \"primar‚Ä¶\n#&gt; $ match_id           &lt;chr&gt; \"00069d73\", \"00069d73\", \"00069d73\", \"00069d7‚Ä¶\n#&gt; $ season             &lt;int&gt; 2023, 2023, 2023, 2023, 2023, 2023, 2023, 20‚Ä¶\n#&gt; $ date               &lt;date&gt; 2023-05-06, 2023-05-06, 2023-05-06, 2023-05‚Ä¶\n#&gt; $ home_team          &lt;chr&gt; \"Nashville SC\", \"Nashville SC\", \"Nashville S‚Ä¶\n#&gt; $ away_team          &lt;chr&gt; \"Chicago Fire\", \"Chicago Fire\", \"Chicago Fir‚Ä¶\n#&gt; $ team               &lt;chr&gt; \"Chicago Fire\", \"Nashville SC\", \"Chicago Fir‚Ä¶\n#&gt; $ player             &lt;chr&gt; \"Hany Mukhtar\", \"Hany Mukhtar\", \"Teal Bunbur‚Ä¶\n#&gt; $ shot_id            &lt;chr&gt; \"00069d73-01\", \"00069d73-01\", \"00069d73-02\",‚Ä¶\n#&gt; $ period             &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,‚Ä¶\n#&gt; $ minutes            &lt;int&gt; 2, 2, 4, 4, 9, 9, 20, 20, 24, 24, 32, 32, 34‚Ä¶\n#&gt; $ minutes_added      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ‚Ä¶\n#&gt; $ time               &lt;dbl&gt; 2, 2, 4, 4, 9, 9, 20, 20, 24, 24, 32, 32, 34‚Ä¶\n#&gt; $ xg                 &lt;dbl&gt; 0.00, 0.04, 0.00, 0.17, 0.00, 0.02, 0.00, 0.‚Ä¶\n#&gt; $ xgd                &lt;dbl&gt; -0.04, 0.04, -0.17, 0.17, -0.02, 0.02, -0.12‚Ä¶\n#&gt; $ gamestate          &lt;fct&gt; Tied, Tied, Tied, Tied, Tied, Tied, Tied, Ti‚Ä¶\n#&gt; $ pre_shot_gamestate &lt;fct&gt; Tied, Tied, Tied, Tied, Tied, Tied, Tied, Ti‚Ä¶\n#&gt; $ last_regular_min   &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ‚Ä¶\n#&gt; $ prev_period        &lt;int&gt; NA, NA, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n#&gt; $ prev_time          &lt;dbl&gt; NA, NA, 2, 2, 4, 4, 9, 9, 20, 20, 24, 24, 32‚Ä¶\n#&gt; $ duration           &lt;dbl&gt; 2, 2, 2, 2, 5, 5, 11, 11, 4, 4, 8, 8, 2, 2, ‚Ä¶"
  },
  {
    "objectID": "posts/fbref-gamestate-expected-goal-difference/index.html#data-analysis",
    "href": "posts/fbref-gamestate-expected-goal-difference/index.html#data-analysis",
    "title": "Game state with FBref data",
    "section": "Data analysis",
    "text": "Data analysis\nAs the saying goes, ‚Äú80% of data analysis/science is data cleaning‚Äù. Well, that rings true here, as all we need to do at this point is perform a few common {dplyr} and {tidyr} actions to arrive at xGD by game state. Oh, and we should contextualize game state xGD by how long a team has spent in each game state (duration).\n\n\nCode\nagg_gamestate_xgd &lt;- gamestate_shots_and_durations |&gt; \n  dplyr::group_by(team, pre_shot_gamestate) |&gt; \n  dplyr::summarize(\n    dplyr::across(\n      c(\n        xgd,\n        duration\n      ),\n      \\(.x) sum(.x, na.rm = TRUE)\n    )\n  ) |&gt; \n  dplyr::ungroup() |&gt; \n  dplyr::mutate(\n    xgd_p90 = xgd * 90 / duration\n  ) |&gt; \n  dplyr::group_by(team) |&gt; \n  dplyr::mutate(\n    prop_duration = duration / sum(duration)\n  ) |&gt; \n  dplyr::ungroup() |&gt; \n  dplyr::select(\n    team,\n    pre_shot_gamestate,\n    xgd_p90,\n    prop_duration\n  )\n\n\n\nagg_gamestate_xgd\n#&gt; # A tibble: 87 √ó 4\n#&gt;    team           pre_shot_gamestate  xgd_p90 prop_duration\n#&gt;    &lt;chr&gt;          &lt;fct&gt;                 &lt;dbl&gt;         &lt;dbl&gt;\n#&gt;  1 Atlanta United Trailing            0.277           0.266\n#&gt;  2 Atlanta United Tied               -0.00208         0.385\n#&gt;  3 Atlanta United Leading             0.262           0.350\n#&gt;  4 Austin FC      Trailing            0.0287          0.271\n#&gt;  5 Austin FC      Tied               -0.0978          0.508\n#&gt;  6 Austin FC      Leading            -0.474           0.221\n#&gt;  7 CF Montr√©al    Trailing           -1.20            0.376\n#&gt;  8 CF Montr√©al    Tied               -0.388           0.400\n#&gt;  9 CF Montr√©al    Leading             0.327           0.223\n#&gt; 10 Charlotte FC   Trailing           -0.0656          0.212\n#&gt; # ‚Ñπ 77 more rows\n\nWe did all that work, so let‚Äôs make a pretty graph that conveys both:\n\nthe proportion of time spent in a given game state, and\nthe xGD per 90 per game state\n\nfor every team. Keep in mind that an xGD per 90 of +0.50 means that you‚Äôre accumulating half a goal‚Äôs worth of shot quality more than you‚Äôre giving up over the course of a game. While this number may seem small, that‚Äôs quite a good number over the course of an entire season.\n\n\nCode\n## logo scraping\nlibrary(httr)\nlibrary(jsonlite)\n\n## plotting\nlibrary(ggplot2)\nlibrary(sysfonts)\nlibrary(showtext)\nlibrary(ggtext)\nlibrary(htmltools)\nlibrary(grid)\nlibrary(scales)\n\nTAG_LABEL &lt;- htmltools::tagList(\n  htmltools::tags$span(htmltools::HTML(enc2utf8('&#xf099;')), style = 'font-family:fb'),\n  htmltools::tags$span('@TonyElHabr'),\n)\nCAPTION_LABEL &lt;- '**Data**: Opta via fbref.'\nSUBTITLE_LABEL &lt;- 'MLS, 2023 Season'\nPLOT_RESOLUTION &lt;- 300\nWHITISH_FOREGROUND_COLOR &lt;- 'white'\nCOMPLEMENTARY_FOREGROUND_COLOR &lt;- '#cbcbcb'\nBLACKISH_BACKGROUND_COLOR &lt;- '#1c1c1c'\nCOMPLEMENTARY_BACKGROUND_COLOR &lt;- '#4d4d4d'\nFONT &lt;- 'Titillium Web'\nsysfonts::font_add_google(FONT, FONT)\n## https://github.com/tashapiro/tanya-data-viz/blob/main/chatgpt-lensa/chatgpt-lensa.R for twitter logo\nsysfonts::font_add('fb', 'Font Awesome 6 Brands-Regular-400.otf')\nshowtext::showtext_auto()\nshowtext::showtext_opts(dpi = PLOT_RESOLUTION)\n\nggplot2::theme_set(ggplot2::theme_minimal())\nggplot2::theme_update(\n  text = ggplot2::element_text(family = FONT),\n  title = ggplot2::element_text(size = 20, color = WHITISH_FOREGROUND_COLOR),\n  plot.title = ggtext::element_markdown(face = 'bold', size = 20, color = WHITISH_FOREGROUND_COLOR),\n  plot.title.position = 'plot',\n  plot.subtitle = ggtext::element_markdown(size = 16, color = COMPLEMENTARY_FOREGROUND_COLOR),\n  axis.text = ggplot2::element_text(color = WHITISH_FOREGROUND_COLOR, size = 14),\n  axis.title.x = ggtext::element_markdown(size = 14, color = WHITISH_FOREGROUND_COLOR, face = 'bold', hjust = 0.99),\n  axis.title.y = ggtext::element_markdown(size = 14, color = WHITISH_FOREGROUND_COLOR, face = 'bold', hjust = 0.99),\n  axis.line = ggplot2::element_blank(),\n  strip.text = ggplot2::element_text(size = 14, color = WHITISH_FOREGROUND_COLOR, face = 'bold', hjust = 0),\n  legend.position = 'top',\n  legend.text = ggplot2::element_text(size = 12, color = WHITISH_FOREGROUND_COLOR, face = 'plain'),\n  legend.title = ggplot2::element_text(size = 12, color = WHITISH_FOREGROUND_COLOR, face = 'bold'),\n  panel.grid.major = ggplot2::element_line(color = COMPLEMENTARY_BACKGROUND_COLOR),\n  panel.grid.minor = ggplot2::element_line(color = COMPLEMENTARY_BACKGROUND_COLOR),\n  panel.grid.minor.x = ggplot2::element_blank(),\n  panel.grid.minor.y = ggplot2::element_blank(),\n  plot.margin = ggplot2::margin(10, 20, 10, 20),\n  plot.background = ggplot2::element_rect(fill = BLACKISH_BACKGROUND_COLOR, color = BLACKISH_BACKGROUND_COLOR),\n  plot.caption = ggtext::element_markdown(size = 10, color = WHITISH_FOREGROUND_COLOR, hjust = 0, face = 'plain'),\n  plot.caption.position = 'plot',\n  plot.tag = ggtext::element_markdown(size = 10, color = WHITISH_FOREGROUND_COLOR, hjust = 1),\n  plot.tag.position = c(0.99, 0.01),\n  panel.spacing.x = grid::unit(2, 'lines'),\n  panel.background = ggplot2::element_rect(fill = BLACKISH_BACKGROUND_COLOR, color = BLACKISH_BACKGROUND_COLOR)\n)\nggplot2::update_geom_defaults('text', list(color = WHITISH_FOREGROUND_COLOR, size = 12 / .pt))\n\nGAMESTATE_PAL &lt;- c(\n  'Trailing' = '#ef3e36',\n  'Tied' = COMPLEMENTARY_FOREGROUND_COLOR,\n  'Leading' = '#17bebb'\n)\n\n## There is a way to get team logos from FBref, but they have a white background \n##   by default, and making the background transparent for a plot with a dark\n##   background is kind of a pain in the ass. So let's pull images from fotmob.\n## This function is basically a minified version of what used to exist as\n##   worldfootballR::fotmob_get_league_tables(). I rely on FBref and fotmob listing\n##   teams in the same order alphabetically, which works fine for the MLS. A\n##   better, scalable strategy for binding team names between sources is to\n##   order teams by points / placement in the standings.\n\nget_fotmob_standings &lt;- function() {\n  url &lt;- 'https://www.fotmob.com/api/leagues?id=130'\n  resp &lt;- httr::GET(url)\n  cont &lt;- httr::content(resp, as = 'text')\n  result &lt;- jsonlite::fromJSON(cont)\n  table_init &lt;- result$table$data\n  tables &lt;- dplyr::bind_rows(table_init$tables)\n  tables$table$all[[3]] |&gt; \n    dplyr::transmute(\n      team = name,\n      team_id = id,\n      pts,\n      logo_url = sprintf('https://images.fotmob.com/image_resources/logo/teamlogo/%s.png', team_id)\n    )\n}\n\nfotmob_standings &lt;- get_fotmob_standings()\nteam_logos &lt;- agg_gamestate_xgd |&gt; \n  dplyr::distinct(team) |&gt; \n  dplyr::arrange(team) |&gt; \n  ## Lucky for us, MLS team names line up with the fotmob names alphabetically.\n  dplyr::bind_cols(\n    fotmob_standings |&gt; \n      dplyr::arrange(team) |&gt; \n      dplyr::select(path = logo_url, pts)\n  )\n\nagg_gamestate_xgd_with_logos &lt;- agg_gamestate_xgd |&gt; \n  dplyr::inner_join(\n    team_logos |&gt; \n      dplyr::select(\n        team,\n        pts,\n        path\n      ),\n    by = dplyr::join_by(team)\n  ) |&gt; \n  dplyr::mutate(\n    label = glue::glue(\"&lt;span style='font-size:12px;color:{WHITISH_FOREGROUND_COLOR}'&gt;{team}&lt;/span&gt; &lt;span style='font-size:9px;color:{COMPLEMENTARY_FOREGROUND_COLOR}'&gt;{pts} pts&lt;/span&gt; &lt;img src='{path}' width='14' height='14'/&gt;\")\n  ) |&gt; \n  dplyr::select(-path)\n\nteam_label_order &lt;- agg_gamestate_xgd_with_logos |&gt; \n  dplyr::filter(\n    pre_shot_gamestate == 'Leading'\n  ) |&gt; \n  dplyr::arrange(prop_duration) |&gt; \n  dplyr::pull(team)\n\nprepped_agg_gamestate_xgd &lt;- agg_gamestate_xgd_with_logos |&gt; \n  dplyr::mutate(\n    dplyr::across(\n      team,\n      \\(.x) factor(.x, levels = team_label_order)\n    )\n  ) |&gt; \n  dplyr::arrange(team, desc(pre_shot_gamestate)) |&gt; \n  dplyr::group_by(team) |&gt; \n  dplyr::mutate(\n    cumu_prop_duration = cumsum(prop_duration)\n  ) |&gt; \n  dplyr::ungroup() |&gt; \n  dplyr::mutate(\n    half_cumu_prop_duration = cumu_prop_duration - 0.5 * prop_duration\n  )\n\nxgd_p90_plot &lt;- prepped_agg_gamestate_xgd |&gt; \n  ggplot2::ggplot() +\n  ggplot2::aes(\n    x = prop_duration,\n    y = team\n  ) +\n  ggplot2::scale_y_discrete(\n    name = '',\n    labels = prepped_agg_gamestate_xgd |&gt;\n      dplyr::distinct(team, label) |&gt;\n      tibble::deframe()\n  ) +\n  ggplot2::theme(\n    axis.text.y = ggtext::element_markdown(margin = grid::unit(c(0, 0, 0, 0), 'pt')),\n  ) +\n  ggplot2::geom_col(\n    show.legend = FALSE,\n    alpha = 0.8,\n    ggplot2::aes(\n      fill = pre_shot_gamestate\n    )\n  ) +\n  ggplot2::geom_text(\n    family = FONT,\n    size = 12 / ggplot2::.pt,\n    fontface = 'bold',\n    color = WHITISH_FOREGROUND_COLOR,\n    data = dplyr::filter(prepped_agg_gamestate_xgd, xgd_p90 &gt;= 0),\n    ggplot2::aes(\n      x = half_cumu_prop_duration,\n      y = team,\n      label = scales::number(xgd_p90, accuracy = 0.01, style_positive = 'plus')\n    )\n  ) +\n  ggplot2::geom_text(\n    family = FONT,\n    size = 12 / ggplot2::.pt,\n    fontface = 'bold.italic',\n    color = BLACKISH_BACKGROUND_COLOR,\n    data = dplyr::filter(prepped_agg_gamestate_xgd, xgd_p90 &lt; 0),\n    ggplot2::aes(\n      x = half_cumu_prop_duration,\n      y = team,\n      label = scales::number(xgd_p90, accuracy = 0.01)\n    )\n  ) +\n  ggplot2::scale_x_continuous(\n    labels = scales::percent_format(accuracy = 1),\n    expand = c(0.01, 0.01)\n  ) +\n  ggplot2::scale_fill_manual(\n    values = GAMESTATE_PAL\n  ) +\n  ggplot2::theme(\n    panel.grid.major.y = ggplot2::element_blank(),\n    panel.grid.major.x = ggplot2::element_blank(),\n    legend.position = 'top'\n  ) +\n  ggplot2::labs(\n    title = glue::glue(\"xGD per 90 when &lt;span style='color:{GAMESTATE_PAL[['Leading']]}'&gt;Leading&lt;/span&gt;, &lt;span style='color:{GAMESTATE_PAL[['Tied']]}'&gt;Tied&lt;/span&gt;, and &lt;span style='color:{GAMESTATE_PAL[['Trailing']]}'&gt;Trailing&lt;/spna&gt;\"),\n    subtitle = SUBTITLE_LABEL,\n    y = NULL,\n    tag = TAG_LABEL,\n    caption = paste0(CAPTION_LABEL, '&lt;br/&gt;**xGD**: Expected goals for minus expected goals conceded'),\n    x = '% of Match Time'\n  )\nxgd_p90_plot\n\nxgd_p90_plot_path &lt;- file.path(PROJ_DIR, '2023-mls-xgd-p90.png')\nggplot2::ggsave(\n  xgd_p90_plot,\n  filename = xgd_p90_plot_path,\n  width = 8,\n  height = 8\n)\n\n## https://themockup.blog/posts/2019-01-09-add-a-logo-to-your-plot/\nadd_logo &lt;- function(\n    plot_path,\n    logo_path,\n    logo_scale = 0.1,\n    idx_x = 0.01, ## right-hand side\n    idx_y = 0.99, ## top of plot\n    adjust_x = ifelse(idx_x &lt; 0.5, TRUE, FALSE),\n    adjust_y = ifelse(idx_y &lt; 0.5, TRUE, FALSE)\n) {\n  plot &lt;- magick::image_read(plot_path)\n  logo_raw &lt;- magick::image_read(logo_path)\n  \n  plot_height &lt;- magick::image_info(plot)$height\n  plot_width &lt;- magick::image_info(plot)$width\n  \n  logo &lt;- magick::image_scale(\n    logo_raw,\n    as.character(round(plot_width * logo_scale))\n  )\n  \n  info &lt;- magick::image_info(logo)\n  logo_width &lt;- info$width\n  logo_height &lt;- info$height\n  \n  x_pos &lt;- plot_width - idx_x * plot_width\n  y_pos &lt;- plot_height - idx_y * plot_height\n  \n  if (isTRUE(adjust_x)) {\n    x_pos &lt;- x_pos - logo_width\n  }\n  \n  if (isTRUE(adjust_y)) {\n    y_pos &lt;- y_pos - logo_height\n  }\n  \n  offset &lt;- paste0('+', x_pos, '+', y_pos)\n  \n  new_plot &lt;- magick::image_composite(plot, logo, offset = offset)\n  ext &lt;- tools::file_ext(plot_path)\n  rgx_ext &lt;- sprintf('[.]%s$', ext)\n  \n  magick::image_write(\n    new_plot,\n    plot_path\n  )\n}\n\nadd_logo(\n  xgd_p90_plot_path,\n  logo_path = file.path(PROJ_DIR, 'mls-logo-black-and-white.png'),\n  logo_scale = 0.06\n)\n\n\n\nSo, what can we learn from this perspective?\n\nColumbus, who finished with the third most points, looks to be the most dominant team all-around. They have the most time spent leading (43%) and are one of only three teams with a positive xGD rate in every game state.\nCincinnati‚Äìthe team that ended up with the most points in the regular season‚Äìis 11th in terms of time spent leading (30%). On the other hand, they do have the best xGD per 90 rate (+0.60) out of all teams in neutral (‚ÄúTied‚Äù) game states, which is the most common game state on average.\nOrlando City, who finished with the second most points, has a relatively poor xGD rate in neutral game states (-0.21). This may be something to be concerned about it in the playoffs, where matches can be tighter.\nSporting KC has the fourth-most time spent leading (35%) and one of the better xGD rates when leading (+0.40), but ended up 8th in the Western Conference after accumulating just the 16th most points across all 29 teams. They could be a team to watch out for in the playoffs if they can get a lead early in their matches.\nNew York Red Bulls squeaked into the playoffs, but have a really strong xGD rate (+0.98) when tied. They may be a team to look out to overperform their seeding.\nLos Anegeles FC, like Columbus, is one of the only teams to have a positive xGD in all game states. In fact, they have the strongest xGD in positive game states (+0.97). Evidently, they look to continue to attack once they get a lead.\nSt.¬†Louis City, as analytics folks will tell you, has overperformed in its inaugural season. While they finished with the most points in the Western Conference, the underlying numbers‚Äìthe negative xGD in all game states‚Äìsuggest that they could be in for a rude awakening in the playoffs."
  },
  {
    "objectID": "posts/fbref-gamestate-expected-goal-difference/index.html#conclusion",
    "href": "posts/fbref-gamestate-expected-goal-difference/index.html#conclusion",
    "title": "Game state with FBref data",
    "section": "Conclusion",
    "text": "Conclusion\nAs we‚Äôve seen, calculating stats with respect to game state using data from the biggest public provider of soccer info is‚Ä¶ not exactly straightforward. But the additional layers of insights that we can glean from contextualizing with game state can be rewarding."
  },
  {
    "objectID": "posts/z/index.html",
    "href": "posts/z/index.html",
    "title": "hello",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "posts/xg-team-quality/index.html",
    "href": "posts/xg-team-quality/index.html",
    "title": "Should we account for team quality in an xG model?",
    "section": "",
    "text": "‚ÄúShould we account for team quality in an xG model?‚Äù From a purely philosophical point of view, my opinion is ‚Äúno‚Äù‚ÄìI think that xG models should be agnostic to player and team abilities, even if we were to find that accounting for shot-taker or team identity improves the performance of the model.\nBut I thought it might be fun to entertain the question from a quantitative perspective. If we add features for team strength to an xG model, can we meaningfully improve the model‚Äôs overall predictive performance and calibration?\n\n\nThis write-up is inspired by Ryan Brill‚Äôs recent presentation on fourth-down decision-making in the National Football League (NFL). He points out that expected points (EP) models in the NFL have a selection bias problem‚Äìthey tend to under-rate the probability of a positive outcome for ‚Äúgood‚Äù teams and over-rate such outcomes for ‚Äúbad‚Äù teams.\nExpected goals (xG) in soccer also suffer from this bias. Lars Maurath has a great deep-dive looking into expected goals under-estimation for strong teams like Barcelona.\nThis phenomenon is evident in the English Premier League (EPL) as well. The end-of-season table for the 2022/23 season shows that xGD, i.e.¬†goals (G) minus xG, is very positive for the top 6 teams and very negative for the bottom 6 teams.\n\n\n\nTeam\nPoints\nG\nxG\nxGD\n\n\n\n\nManchester City\n89\n33\n78.6\n46.5\n\n\nArsenal\n84\n43\n71.9\n29.9\n\n\nManchester Utd\n75\n43\n67.7\n17.3\n\n\nNewcastle Utd\n71\n33\n72.0\n32.4\n\n\nLiverpool\n67\n47\n72.6\n21.7\n\n\nBrighton\n62\n53\n73.3\n23.1\n\n\nAston Villa\n61\n46\n50.2\n-2.2\n\n\nTottenham\n60\n63\n57.1\n7.4\n\n\nBrentford\n59\n46\n56.8\n6.8\n\n\nFulham\n52\n53\n46.2\n-17.6\n\n\nCrystal Palace\n45\n49\n39.3\n-8.8\n\n\nChelsea\n44\n47\n49.5\n-3.0\n\n\nWolves\n41\n58\n36.8\n-23.1\n\n\nWest Ham\n40\n55\n49.2\n-3.9\n\n\nBournemouth\n39\n71\n38.6\n-25.3\n\n\nNott‚Äôham Forest\n38\n68\n39.3\n-24.9\n\n\nEverton\n36\n57\n45.2\n-20.5\n\n\nLeicester City\n34\n68\n50.6\n-12.8\n\n\nLeeds United\n31\n78\n47.4\n-19.8\n\n\nSouthampton\n25\n73\n37.7\n-23.3\n\n\n\nLars does an ‚Äúex-post‚Äù analysis‚Äìevaluating the ‚Äúresidual‚Äù of expected goals, or xGD‚Äìto arrive at his conclusion that team quality does seem to explain large deviations between goals and expected goals for top teams.\n\n[W]hen conditioning on‚Ä¶ team quality (Elo ranking) I do not find this bias in either naive or sophisticated models.\n\nI‚Äôm curious to see if we can arrive at a related conclusion in a more direct, ‚Äúex-ante‚Äù fashion. Can we reduce the underestimation of goals for strong teams by directly accounting for team quality in an xG model?"
  },
  {
    "objectID": "posts/xg-team-quality/index.html#introduction",
    "href": "posts/xg-team-quality/index.html#introduction",
    "title": "Should we account for team quality in an xG model?",
    "section": "",
    "text": "‚ÄúShould we account for team quality in an xG model?‚Äù From a purely philosophical point of view, my opinion is ‚Äúno‚Äù‚ÄìI think that xG models should be agnostic to player and team abilities, even if we were to find that accounting for shot-taker or team identity improves the performance of the model.\nBut I thought it might be fun to entertain the question from a quantitative perspective. If we add features for team strength to an xG model, can we meaningfully improve the model‚Äôs overall predictive performance and calibration?\n\n\nThis write-up is inspired by Ryan Brill‚Äôs recent presentation on fourth-down decision-making in the National Football League (NFL). He points out that expected points (EP) models in the NFL have a selection bias problem‚Äìthey tend to under-rate the probability of a positive outcome for ‚Äúgood‚Äù teams and over-rate such outcomes for ‚Äúbad‚Äù teams.\nExpected goals (xG) in soccer also suffer from this bias. Lars Maurath has a great deep-dive looking into expected goals under-estimation for strong teams like Barcelona.\nThis phenomenon is evident in the English Premier League (EPL) as well. The end-of-season table for the 2022/23 season shows that xGD, i.e.¬†goals (G) minus xG, is very positive for the top 6 teams and very negative for the bottom 6 teams.\n\n\n\nTeam\nPoints\nG\nxG\nxGD\n\n\n\n\nManchester City\n89\n33\n78.6\n46.5\n\n\nArsenal\n84\n43\n71.9\n29.9\n\n\nManchester Utd\n75\n43\n67.7\n17.3\n\n\nNewcastle Utd\n71\n33\n72.0\n32.4\n\n\nLiverpool\n67\n47\n72.6\n21.7\n\n\nBrighton\n62\n53\n73.3\n23.1\n\n\nAston Villa\n61\n46\n50.2\n-2.2\n\n\nTottenham\n60\n63\n57.1\n7.4\n\n\nBrentford\n59\n46\n56.8\n6.8\n\n\nFulham\n52\n53\n46.2\n-17.6\n\n\nCrystal Palace\n45\n49\n39.3\n-8.8\n\n\nChelsea\n44\n47\n49.5\n-3.0\n\n\nWolves\n41\n58\n36.8\n-23.1\n\n\nWest Ham\n40\n55\n49.2\n-3.9\n\n\nBournemouth\n39\n71\n38.6\n-25.3\n\n\nNott‚Äôham Forest\n38\n68\n39.3\n-24.9\n\n\nEverton\n36\n57\n45.2\n-20.5\n\n\nLeicester City\n34\n68\n50.6\n-12.8\n\n\nLeeds United\n31\n78\n47.4\n-19.8\n\n\nSouthampton\n25\n73\n37.7\n-23.3\n\n\n\nLars does an ‚Äúex-post‚Äù analysis‚Äìevaluating the ‚Äúresidual‚Äù of expected goals, or xGD‚Äìto arrive at his conclusion that team quality does seem to explain large deviations between goals and expected goals for top teams.\n\n[W]hen conditioning on‚Ä¶ team quality (Elo ranking) I do not find this bias in either naive or sophisticated models.\n\nI‚Äôm curious to see if we can arrive at a related conclusion in a more direct, ‚Äúex-ante‚Äù fashion. Can we reduce the underestimation of goals for strong teams by directly accounting for team quality in an xG model?"
  },
  {
    "objectID": "posts/xg-team-quality/index.html#analysis-and-results",
    "href": "posts/xg-team-quality/index.html#analysis-and-results",
    "title": "Should we account for team quality in an xG model?",
    "section": "Analysis and Results",
    "text": "Analysis and Results\n\nData\nI‚Äôll be using event data that I‚Äôve ingested with the {socceraction} package (which I‚Äôve made available here!) for the 2013/14 through 2022/23 EPL seasons. I‚Äôll focus on just ‚Äúopen-play‚Äù shots, i.e.¬†shots excluding penalties and those taken from set pieces.\nI‚Äôve scraped Elo ratings from ClubElo for my measure of team quality. I‚Äôve chosen Elo because it provides an intuitive, sport-agnostic measure of relative skill. Also, it is calculated independent of the events that take place in a game, so any correlation with measures of shot volume, quality, etc. is only coincidental.1\n\n\nPackage imports and other setup\n## Data retrieval\nlibrary(curl)\nlibrary(arrow)\nlibrary(qs) ## local dev\nlibrary(worldfootballR)\n\n## Data manipulation\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(lubridate)\nlibrary(forcats)\n\n## Modeling\nlibrary(rsample)\nlibrary(recipes)\nlibrary(parsnip)\nlibrary(workflows)\nlibrary(hardhat)\n\n## Model tuning\nlibrary(tune)\nlibrary(dials)\nlibrary(workflowsets)\nlibrary(finetune)\n\n## Model diagnostics\nlibrary(rlang)\nlibrary(yardstick)\nlibrary(SHAPforxgboost)\n\n## Plotting\nlibrary(ggplot2)\nlibrary(sysfonts)\nlibrary(showtext)\nlibrary(ggtext)\nlibrary(htmltools)\nlibrary(scales)\nlibrary(grid)\nlibrary(glue)\nlibrary(knitr)\n\nPROJ_DIR &lt;- 'posts/xg-team-quality'\n\nTAG_LABEL &lt;- htmltools::tagList(\n  htmltools::tags$span(htmltools::HTML(enc2utf8(\"&#xf099;\")), style = 'font-family:fb'),\n  htmltools::tags$span(\"@TonyElHabr\"),\n)\nSUBTITLE_LABEL &lt;- 'English Premier League, 2012/13 - 2022/23'\nPLOT_RESOLUTION &lt;- 300\nWHITISH_FOREGROUND_COLOR &lt;- 'white'\nCOMPLEMENTARY_FOREGROUND_COLOR &lt;- '#cbcbcb' # '#f1f1f1'\nBLACKISH_BACKGROUND_COLOR &lt;- '#1c1c1c'\nCOMPLEMENTARY_BACKGROUND_COLOR &lt;- '#4d4d4d'\nFONT &lt;- 'Titillium Web'\nsysfonts::font_add_google(FONT, FONT)\n## https://github.com/tashapiro/tanya-data-viz/blob/main/chatgpt-lensa/chatgpt-lensa.R for twitter logo\nsysfonts::font_add('fb', 'Font Awesome 6 Brands-Regular-400.otf')\nshowtext::showtext_auto()\nshowtext::showtext_opts(dpi = PLOT_RESOLUTION)\n\nggplot2::theme_set(ggplot2::theme_minimal())\nggplot2::theme_update(\n  text = ggplot2::element_text(family = FONT),\n  title = ggplot2::element_text(size = 20, color = WHITISH_FOREGROUND_COLOR),\n  plot.title = ggtext::element_markdown(face = 'bold', size = 20, color = WHITISH_FOREGROUND_COLOR),\n  plot.title.position = 'plot',\n  plot.subtitle = ggtext::element_markdown(size = 16, color = COMPLEMENTARY_FOREGROUND_COLOR),\n  axis.text = ggplot2::element_text(color = WHITISH_FOREGROUND_COLOR, size = 14),\n  # axis.title = ggplot2::element_text(size = 14, color = WHITISH_FOREGROUND_COLOR, face = 'bold', hjust = 0.99),\n  axis.title.x = ggtext::element_markdown(size = 14, color = WHITISH_FOREGROUND_COLOR, face = 'bold', hjust = 0.99),\n  axis.title.y = ggtext::element_markdown(size = 14, color = WHITISH_FOREGROUND_COLOR, face = 'bold', hjust = 0.99),\n  axis.line = ggplot2::element_blank(),\n  strip.text = ggplot2::element_text(size = 14, color = WHITISH_FOREGROUND_COLOR, face = 'bold', hjust = 0),\n  legend.position = 'top',\n  legend.text = ggplot2::element_text(size = 12, color = WHITISH_FOREGROUND_COLOR, face = 'plain'),\n  legend.title = ggplot2::element_text(size = 12, color = WHITISH_FOREGROUND_COLOR, face = 'bold'),\n  panel.grid.major = ggplot2::element_line(color = COMPLEMENTARY_BACKGROUND_COLOR),\n  panel.grid.minor = ggplot2::element_line(color = COMPLEMENTARY_BACKGROUND_COLOR),\n  panel.grid.minor.x = ggplot2::element_blank(),\n  panel.grid.minor.y = ggplot2::element_blank(),\n  plot.margin = ggplot2::margin(10, 20, 10, 20),\n  plot.background = ggplot2::element_rect(fill = BLACKISH_BACKGROUND_COLOR, color = BLACKISH_BACKGROUND_COLOR),\n  plot.caption = ggtext::element_markdown(color = WHITISH_FOREGROUND_COLOR, hjust = 0, size = 10, face = 'plain'),\n  plot.caption.position = 'plot',\n  plot.tag = ggtext::element_markdown(size = 10, color = WHITISH_FOREGROUND_COLOR, hjust = 1),\n  plot.tag.position = c(0.99, 0.01),\n  panel.spacing.x = grid::unit(2, 'lines'),\n  panel.background = ggplot2::element_rect(fill = BLACKISH_BACKGROUND_COLOR, color = BLACKISH_BACKGROUND_COLOR)\n)\n\n\n\n\nRetrieve and wrangle data\nread_parquet_from_url &lt;- function(url) {\n  load &lt;- curl::curl_fetch_memory(url)\n  arrow::read_parquet(load$content)\n}\n\nREPO &lt;- 'tonyelhabr/socceraction-streamlined'\nread_socceraction_parquet_release &lt;- function(name, tag) {\n  url &lt;- sprintf('https://github.com/%s/releases/download/%s/%s.parquet', REPO, tag, name)\n  read_parquet_from_url(url)\n}\n\nread_socceraction_parquet_releases &lt;- function(name, tag = 'data-processed') {\n  purrr::map_dfr(\n    2013:2022,\n    \\(season_start_year) {\n      basename &lt;- sprintf('8-%s-%s', season_start_year, name)\n      message(basename)\n      read_socceraction_parquet_release(basename, tag = tag)\n    }\n  )\n}\n\nread_socceraction_parquet &lt;- function(name, branch = 'main') {\n  url &lt;- sprintf('https://github.com/%s/raw/%s/%s.parquet', REPO, branch, name)\n  read_parquet_from_url(url)\n}\n\nx &lt;- read_socceraction_parquet_releases('x')\ny &lt;- read_socceraction_parquet_releases('y')\nactions &lt;- read_socceraction_parquet_releases('actions')\ngames &lt;- read_socceraction_parquet_releases('games') |&gt; \n  dplyr::mutate(\n    date = lubridate::date(game_date)\n  )\nteam_elo &lt;- read_socceraction_parquet('data/final/8/2013-2022/clubelo-ratings')\n\nopen_play_shots &lt;- games |&gt;\n  dplyr::transmute(\n    season_id,\n    game_id,\n    date,\n    home_team_id,\n    away_team_id\n  ) |&gt; \n  dplyr::inner_join(\n    x |&gt; \n      dplyr::filter(type_shot_a0 == 1) |&gt; \n      dplyr::select(\n        game_id,\n        action_id,\n        \n        ## features\n        start_x_a0,\n        start_y_a0,\n        start_dist_to_goal_a0,\n        start_angle_to_goal_a0,\n        type_dribble_a1,\n        type_pass_a1,\n        type_cross_a1,\n        type_corner_crossed_a1,\n        type_shot_a1,\n        type_freekick_crossed_a1,\n        bodypart_foot_a0,\n        bodypart_head_a0,\n        bodypart_other_a0\n      ) |&gt; \n      dplyr::mutate(\n        dplyr::across(-c(game_id, action_id), as.integer)\n      ),\n    by = dplyr::join_by(game_id),\n    relationship = 'many-to-many'\n  ) |&gt; \n  dplyr::inner_join(\n    y |&gt; \n      dplyr::transmute(\n        game_id, \n        action_id,\n        scores = ifelse(scores, 'yes', 'no') |&gt; factor(levels = c('yes', 'no'))\n      ),\n    by = dplyr::join_by(game_id, action_id)\n  ) |&gt; \n  dplyr::inner_join(\n    actions |&gt; \n      dplyr::select(\n        game_id,\n        action_id,\n        team_id,\n        player_id\n      ),\n    by = dplyr::join_by(game_id, action_id)\n  ) |&gt; \n  dplyr::left_join(\n    team_elo |&gt; dplyr::select(date, home_team_id = team_id, home_elo = elo),\n    by = dplyr::join_by(date, home_team_id)\n  ) |&gt; \n  dplyr::left_join(\n    team_elo |&gt; dplyr::select(date, away_team_id = team_id, away_elo = elo),\n    by = dplyr::join_by(date, away_team_id)\n  ) |&gt; \n  dplyr::transmute(\n    date,\n    season_id,\n    game_id,\n    team_id,\n    opponent_team_id = ifelse(team_id == home_team_id, away_team_id, home_team_id),\n    action_id,\n    \n    scores,\n    \n    elo = ifelse(team_id == home_team_id, home_elo, away_elo),\n    opponent_elo = ifelse(team_id == home_team_id, away_elo, home_elo),\n    elo_diff = elo - opponent_elo,\n    \n    start_dist_to_goal_a0,\n    start_angle_to_goal_a0,\n    type_dribble_a1,\n    type_pass_a1,\n    type_cross_a1,\n    type_corner_crossed_a1,\n    type_shot_a1,\n    type_freekick_crossed_a1,\n    bodypart_foot_a0,\n    bodypart_head_a0,\n    bodypart_other_a0\n  )\n\n\n\n\nSplit the data for modeling\nsplit &lt;- rsample::make_splits(\n  open_play_shots |&gt; dplyr::filter(season_id %in% c(2013L:2019L)),\n  open_play_shots |&gt; dplyr::filter(season_id %in% c(2020L:2022L))\n)\n\ntrain &lt;- rsample::training(split)\ntest &lt;- rsample::testing(split)\n\n\nIt‚Äôs worth spotlighting Elo a bit more since it‚Äôs the novel feature here. Below is a look at the distribution of pre-match Elo over the course of the entire data set.\n\nTo make Elo values feel a bit more tangible, note that:\n\nMan City‚Äìarguably the best team in the EPL for the past decade‚Äìhas sustained an Elo greater than 1850 for the majority of the past decade.\nBottom-table teams often in the relegation battle tend to have Elos less than 1650.\n\nThe pre-match difference team Elos follows a normal-ish distribution, with most values falling within the ¬±300 range.\n\n\n\nModel Training\nThe feature set for our ‚Äúbase‚Äù xG model consists of the following:\n\nlocation of the shot (distance and angle of the shot to the center of the goal mouth)2.\ntype of action leading to the shot.\nbody part with which the shot was taken.\n\nThese features are essentially what all xG models have in common, although the exact implementation differs. Data providers such as Opta also account for information that is not captured in traditional event data, such as the position of the goalkeeper.\nFor the team-quality-adjusted (or ‚ÄúElo-augmented‚Äù) model, I‚Äôll add two additional features:\n\nthe Elo of the team of the shot-taker.\nthe difference in the Elo of the shot-taking team and the opposing team.\n\nThe former is meant to capture the opponent-agnostic quality of a team, while the latter captures the quality of the team relative to their opponent.\n\n\nSetting up the models\nrec_elo &lt;- recipes::recipe(\n  scores ~ \n    elo +\n    elo_diff +\n    start_dist_to_goal_a0 +\n    start_angle_to_goal_a0 +\n    type_dribble_a1 +\n    type_pass_a1 +\n    type_cross_a1 +\n    type_corner_crossed_a1 +\n    type_shot_a1 +\n    type_freekick_crossed_a1 +\n    bodypart_foot_a0 +\n    bodypart_head_a0 +\n    bodypart_other_a0,\n  data = train\n)\n\nrec_base &lt;- rec_elo |&gt; \n  recipes::step_rm(elo, elo_diff)\n\n\nI‚Äôll be using xgboost for my xG models.3 I‚Äôll choose hyperparameters using an efficient grid search, evaluating models with the Brier skill score (BSS). For the reference Brier score for BSS, I‚Äôll use a dummy model that predicts 12% conversion for all shots.4\n\n\n\n\n\n\nBrier skill score (BSS)\n\n\n\n\n\nIf this isn‚Äôt the first blog post you‚Äôve read of mine, you probably know that I love to use BSS for classification tasks, especially for xG models.\nBut why BSS? Simply put, Brier scores are known as the best evaluation metric to use for classification tasks where you‚Äôre purely interested in probabilities. And BSS goes one step beyond Brier scores, forcing one to contextualize the model evaluation with a reasonable baseline. In the context of xG, BSS helps us directly see whether our fitted model is better than a naive prediction, such as guessing that all shots convert at the observed shot conversion rate.\nKeep in mind that a higher BSS is ideal. A perfect model would have a BSS of 1; a model that is no better than a reference model would have a BSS of 0.\n\n\n\n\n\nFunctions for Brier skill score\n## See also: probability-calibration\nbrier_skill_score &lt;- function(data, ...) {\n  UseMethod('brier_skill_score')\n}\n\nbrier_skill_score &lt;- yardstick::new_prob_metric(\n  brier_skill_score, \n  direction = 'maximize'\n)\n\nbss &lt;- function(\n    truth, \n    estimate, \n    ref_estimate, \n    event_level,\n    case_weights,\n    ...\n) {\n  \n  if (length(estimate) == 1) {\n    estimate &lt;- rep(estimate, length(truth))\n  }\n  \n  if (length(ref_estimate) == 1) {\n    ref_estimate &lt;- rep(ref_estimate, length(truth))\n  }\n  \n  estimate_brier_score &lt;- brier_class_vec(\n    truth = truth,\n    estimate = estimate,\n    event_level = event_level,\n    case_weights = case_weights,\n    ...\n  )\n  \n  ref_brier_score &lt;- brier_class_vec(\n    truth = truth,\n    estimate = ref_estimate,\n    event_level = event_level,\n    case_weights = case_weights,\n    ...\n  )\n  \n  1 - (estimate_brier_score / ref_brier_score)\n}\n\nbrier_skill_score_estimator_impl &lt;- function(\n    truth, \n    estimate, \n    ref_estimate, \n    event_level,\n    case_weights\n) {\n  bss(\n    truth = truth,\n    estimate = estimate,\n    ref_estimate = ref_estimate,\n    event_level = event_level,\n    case_weights = case_weights\n  )\n}\n\nbrier_skill_score_vec &lt;- function(\n    truth, \n    estimate, \n    ref_estimate, \n    na_rm = TRUE, \n    event_level = yardstick:::yardstick_event_level(),\n    case_weights = NULL, \n    ...\n) {\n  \n  yardstick:::abort_if_class_pred(truth)\n  \n  estimator &lt;- yardstick::finalize_estimator(\n    truth, \n    metric_class = 'brier_skill_score'\n  )\n  \n  yardstick::check_prob_metric(truth, estimate, case_weights, estimator)\n  \n  if (na_rm) {\n    result &lt;- yardstick::yardstick_remove_missing(truth, estimate, case_weights)\n    \n    truth &lt;- result$truth\n    estimate &lt;- result$estimate\n    case_weights &lt;- result$case_weights\n  } else if (yardstick::yardstick_any_missing(truth, estimate, case_weights)) {\n    return(NA_real_)\n  }\n  \n  brier_skill_score_estimator_impl(\n    truth = truth,\n    estimate = estimate,\n    ref_estimate = ref_estimate,\n    event_level = event_level,\n    case_weights = case_weights\n  )\n}\n\nbrier_skill_score.data.frame &lt;- function(\n    data, \n    truth, \n    ...,\n    na_rm = TRUE,\n    event_level = yardstick:::yardstick_event_level(),\n    case_weights = NULL,\n    \n    ref_estimate = 0.5,\n    name = 'brier_skill_score'\n) {\n  yardstick::prob_metric_summarizer(\n    name = name,\n    fn = brier_skill_score_vec,\n    data = data,\n    truth = !!rlang::enquo(truth),\n    ...,\n    na_rm = na_rm,\n    event_level = event_level,\n    case_weights = !!rlang::enquo(case_weights),\n    fn_options = list(\n      ref_estimate = ref_estimate\n    )\n  )\n}\n\nxg_brier_skill_score &lt;- function(data, ...) {\n  UseMethod('xg_brier_skill_score')\n}\n\n# REF_ESTIMATE &lt;- open_play_shots |&gt;\n#   dplyr::summarize(goal_rate = sum(scores == 'yes') / dplyr::n()) |&gt;\n#   dplyr::pull(goal_rate)\nREF_ESTIMATE &lt;- 0.12\n\nxg_brier_skill_score.data.frame &lt;- function(...) {\n  brier_skill_score(\n    ref_estimate = REF_ESTIMATE,\n    name = 'xg_brier_skill_score',\n    ...\n  )\n}\n\nxg_brier_skill_score &lt;- yardstick::new_prob_metric(\n  xg_brier_skill_score,\n  direction = 'maximize'\n)\n\n\n\n\nTuning the xG models\n## Useful reference: https://jlaw.netlify.app/2022/01/24/predicting-when-kickers-get-iced-with-tidymodels/\nTREES &lt;- 500\nLEARN_RATE &lt;- 0.01\nspec &lt;- parsnip::boost_tree(\n  trees = !!TREES,\n  learn_rate = !!LEARN_RATE,\n  tree_depth = tune::tune(),\n  min_n = tune::tune(), \n  loss_reduction = tune::tune(),\n  sample_size = tune::tune(), \n  mtry = tune::tune(),\n  stop_iter = tune::tune()\n) |&gt;\n  parsnip::set_engine('xgboost') |&gt; \n  parsnip::set_mode('classification')\n\ngrid &lt;- dials::grid_latin_hypercube(\n  dials::tree_depth(),\n  dials::min_n(range = c(5L, 40L)),\n  dials::loss_reduction(),\n  sample_size = dials::sample_prop(),\n  dials::finalize(dials::mtry(), train),\n  dials::stop_iter(range = c(10L, 50L)),\n  size = 50\n)\n\nwf_sets &lt;- workflowsets::workflow_set(\n  preproc = list(\n    base = rec_base, \n    elo = rec_elo\n  ),\n  models = list(\n    model = spec\n  ),\n  cross = FALSE\n)\n\ncontrol &lt;- finetune::control_race(\n  save_pred = TRUE,\n  parallel_over = 'everything',\n  save_workflow = TRUE,\n  verbose = TRUE\n)\n\nset.seed(42)\ntrain_folds &lt;- rsample::vfold_cv(train, strata = scores, v = 5)\n\ntuned_results &lt;- workflowsets::workflow_map(\n  wf_sets,\n  fn = 'tune_race_anova',\n  grid = grid,\n  control = control,\n  metrics = yardstick::metric_set(xg_brier_skill_score),\n  resamples = train_folds,\n  seed = 42\n)\n\n\n\n\nChoosing best hyper-parameters\nMODEL_TYPES &lt;- c(\n  'base_model' = 'Base',\n  'elo_model' = 'Elo-augmented'\n)\nselect_best_model &lt;- function(tuned_results, model_type) {\n  tuned_results |&gt;\n    workflowsets::extract_workflow_set_result(model_type) |&gt; \n    tune::select_best(metric = 'xg_brier_skill_score') |&gt;\n    dplyr::transmute(\n      model_type = MODEL_TYPES[model_type],\n      mtry,\n      min_n,\n      tree_depth, \n      loss_reduction,\n      sample_size,\n      stop_iter\n    )\n}\n\nbest_base_set &lt;- select_best_model(tuned_results, 'base_model')\nbest_elo_set &lt;- select_best_model(tuned_results, 'elo_model')\n\ndplyr::bind_rows(\n  best_base_set,\n  best_elo_set\n) |&gt; \n  dplyr::transmute(\n    model_type,\n    mtry,\n    min_n,\n    tree_depth, \n    loss_reduction,\n    sample_size,\n    stop_iter\n  ) |&gt; \n  knitr::kable()\n\n\n\n\n\n\n\n\nModel hyperparameters\n\n\n\n\n\nFor reproducibility, the chosen hyperparameters are as follows.5 By coincidence, the same hyper-parameters are chosen. (Only 50 combinations were evaluated.)\n\n\n\n\n\n\n\n\n\n\n\n\nmodel_type\nmtry\nmin_n\ntree_depth\nloss_reduction\nsample_size\nstop_iter\n\n\n\n\nBase\n7\n17\n6\n0\n0.4641502\n50\n\n\nElo-augmented\n7\n17\n6\n0\n0.4641502\n50\n\n\n\nThe number of trees and learning_rate were pre-defined to be 500 and 0.01 respectively.\n\n\n\n\n\nFit models on the entire training set after choosing the hyperparameters\nfinalize_tuned_results &lt;- function(tuned_results, model_type) {\n  best_set &lt;- select_best_model(tuned_results, model_type)\n  tuned_results |&gt;\n    hardhat::extract_workflow(model_type) |&gt;\n    tune::finalize_workflow(best_base_set) |&gt; \n    tune::last_fit(\n      split,\n      metrics = yardstick::metric_set(xg_brier_skill_score)\n    )\n}\nlast_base_fit &lt;- finalize_tuned_results(tuned_results, 'base_model')\nlast_elo_fit &lt;- finalize_tuned_results(tuned_results, 'elo_model')\n\n\n\n\nFeature Importance\nWe should verify to see that the fitted xG models are behaving as expected. One way of doing so is to look at the importance of the features for model predictions.\nIn bespoke models (like the Elo-augmented model), feature importance can be enlightening, as it tells us which features are contributing most to the predicted outcomes. For the base model, I know that shot distance should be the most important feature (by far), as this is found to be the most important features in other similar ‚Äúbasic‚Äù public xG models, such as this one and this one.\nI like to use SHAP values for feature importance since they have nice ‚Äúlocal‚Äù explanations and are found to be consistent in their ‚Äúglobal‚Äù structure.\n\n\n\n\n\n\nSHAP values\n\n\n\n\n\nSHAP values quantify the impact of each feature in a predictive model on the model‚Äôs output. For a binary classification task like ours, SHAP values are expressed on a 0-1 scale.\nA SHAP value of 0.9 for a specific feature in a binary classification model doesn‚Äôt mean that the feature contributes 90% to the prediction, nor does it directly imply that the predicted probability is 90%. Instead, it indicates that this particular feature contributes a value of 0.9 to the shift from the baseline prediction (usually the average of the training data‚Äôs output) to the specific model output for that observation.\n\n\n\n\nIndeed, we find that shot distance is the most important feature with the base xG model.\nNow, if we make the same plot and add the aggregate SHAP values for our Elo-augmented xG model, we see that Elo and Elo difference terms are in the middle of the pack in terms of feature importance. That‚Äôs pretty interesting. That indicates that these features aren‚Äôt super influential, on average.\n\nFurther, we can verify that the Elo-augmented model predicts xG in a manner that matches intuition using SHAP dependence plots. We should see that the SHAP values are higher when Elo is higher and when Elo difference is higher.\n\nIndeed, this is generally what we observe, although the relationships aren‚Äôt quite monotonic, as I would have expected.6\n\n\nModel Evaluation\nSo aggregate SHAP values indicate that our two Elo features aren‚Äôt playing a big role in contributing to model predictions, although the features do indeed contribute in the way that we‚Äôd expect. But SHAP values are just telling us about how and why a model makes certain predictions. What about the performance of the Elo-augmented model compared to the base xG model?\n\n\n\nModel Type\nBrier Skill Score\n\n\n\n\nBase\n0.1058\n\n\nElo-augmented\n0.1056\n\n\n\nWell, at least in terms of BSS, it looks like there‚Äôs basically no difference between the models.\nBut that‚Äôs just one, holistic measure. How well-calibrated is the Elo-augmented model compared to the traditional xG model? Does the augmented model demonstrate better performance across the whole range of shot conversion probabilities?\n\nVisually, I wouldn‚Äôt say there‚Äôs any significant difference between the calibration curves. A plot of calibration given various buckets for Elo difference would lead to the same conclusion‚Äìthe Elo-augmented model isn‚Äôt notably more calibrated."
  },
  {
    "objectID": "posts/xg-team-quality/index.html#discussion",
    "href": "posts/xg-team-quality/index.html#discussion",
    "title": "Should we account for team quality in an xG model?",
    "section": "Discussion",
    "text": "Discussion\nSo we‚Äôve seen that there is no clear evidence that accounting for team quality improves an xG model. Sure, it can provide value to an xG model‚Äìas shown with the SHAP plots for the Elo-augmented model‚Äìbut it‚Äôs effectively just providing a different kind of information that could be otherwise captured by other traditional xG features.\nSo naturally one may ask‚Äìwhat about player quality? Or goalkeeper quality?\nWhile I do think those would be a little more useful in terms of making a better xG model, the better question is whether we should be accounting for these kinds of ‚Äúquality‚Äù features at all. say ‚Äúno‚Äù‚Äìalthough I do think it‚Äôs fine for NFL EP models‚Äìdue to the manner in which xG models are typically applied.\n\nApplication of xG in soccer vs.¬†EP in American football\nUnlike a fourth-down model that can be ‚Äúactively‚Äù used by NFL coaches to make real-time decisions, an xG model in soccer serves a more ‚Äúpassive‚Äù role. Soccer players lack the luxury of time that NFL coaches have to calculate xG on the fly when deciding whether to take a shot. Instead, xG feels better suited as a retrospective, descriptive measure of actions that have already occurred.\nFurther, one can make the argument that incorporating team or player quality into an xG model can make interpretation more difficult, at least in the manner that we typically use xG to contextualize a game or a season. (xGods, please forgive me for looking at single-game xG.) As an extreme example, let‚Äôs say that we add an ‚ÄúIs Messi?‚Äù feature to an xG model‚Äìnot unlike what Ryan suggested with Justin Tucker in his presentation‚Äìto achieve a slightly more accurate xG model. That‚Äôs all fine in theory, but then your game- and season-level xG insights would need to change.\nI may look at an Inter Miami game where they ‚Äúlost‚Äù on actual goals, let‚Äôs say 2 to 1, and ‚Äúlost‚Äù on xG, let‚Äôs say 0.9 to 1.7, and think ‚ÄúOh, that‚Äôs a fair result‚Äù. But if Messi took 4 shots and we‚Äôre using an expected goals model with the ‚ÄúIs Messi?‚Äù feature, the Inter Miami xG might appear to be 2.2, perhaps leading to a different take on the game‚Äì‚Äúthat was Inter Miami‚Äôs game, but they failed to convert on some key chances‚Äù.\nOverall, I‚Äôd say that it feels circular to build in ‚Äúquality‚Äù features into a model used to evaluate shot quality, and, downstream, player and team quality, at least in the current paradigm of soccer analytics.\n\n\nIn favor of team and/or player quality features\nOn the other hand, I do see the argument in favor of identity features in xG models, e.g.¬†a RAPM-esque xG model. Such indicator variables allow us to estimate player and team impact directly, as well as the uncertainty for those individual estimates. Assuming the model estimation procedure is sound, a direct estimate of player or team skill is better than relying on the ‚Äúresidual‚Äù of goals minus xG, as is typically done.\nFurther, I‚Äôve been assuming that looking at xGD will always be the primary way that xG is used to evaluate players. If we can reliably estimate player skill directly with an xG model and can tease out how that effect changes over time, then perhaps we should prefer those kinds of estimates."
  },
  {
    "objectID": "posts/xg-team-quality/index.html#conclusion",
    "href": "posts/xg-team-quality/index.html#conclusion",
    "title": "Should we account for team quality in an xG model?",
    "section": "Conclusion",
    "text": "Conclusion\nInspired by observed biases in American football‚Äôs expected points models, I trained a model to evaluate whether accounting for team strength directly might improve predictiveness and calibration of an expected goals model for soccer. Despite SHAP plots indicating some value in Elo-augmented models, the overall performance metrics and calibration plots showed negligible differences compared to the base xG model.\nOn the surface, this is a little at odds with Lars Maurath‚Äôs ex-post finding that team quality is useful for explaining xGD, which matches my intuition. However, as Lars points out, team quality is highly correlated with shot volume, and team quality may just be a mask for the effect of highly skilled players with superior finishing capabilities. So, perhaps with better feature engineering‚Äìe.g.¬†shots per 90 in the prior N matches, weekly wages of the shot-taker or the shot-taking team‚ÄìI could prove that directly accounting for team strength can improve an xG model.\nGiven all my questions and speculations about the role of player quality with respect to xG calibration, perhaps it‚Äôs not surprising that the most natural area of future research would be to evaluate how measures of player skill might improve an xG model. There is some prior art on this, so it would be wise to identify how one could contribute something novel to that work."
  },
  {
    "objectID": "posts/xg-team-quality/index.html#footnotes",
    "href": "posts/xg-team-quality/index.html#footnotes",
    "title": "Should we account for team quality in an xG model?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt was also fairly easy to retrieve and is what Lars used to gauge ‚Äútop‚Äù teams in a quantitative way.‚Ü©Ô∏é\nOne might get a slightly more performant by adding the x and y coordinates of the shot‚Äìto implicitly account for right-footed bias, for example‚Äìbut I actually prefer not to add those in the model. Such terms can result in slight over-fitting, in the presence of other features that provide information about the location of the shot, such as distance and angle. (This is the classical ‚Äúbias-variance‚Äù trade-off.)‚Ü©Ô∏é\nxgboost is the state-of-the-art framework for tabular machine learning tasks and is used by companies like Opta for their xG models.‚Ü©Ô∏é\n12% is approximately the observed shot conversion rate in the whole data set.‚Ü©Ô∏é\nThis post isn‚Äôt meant to be so much about the why‚Äôs and how‚Äôs of model tuning and training, so I‚Äôve spared commentary on the code.‚Ü©Ô∏é\nOf course, I could have forced the xgboost model with the Elo terms to have monotonic behavior for those features. I intentionally didn‚Äôt do that here so as to not confirm my own biases‚Äìthat higher xG should generally correspond with higher Elo.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/soccer-meta-analytics/index.html#introduction",
    "href": "posts/soccer-meta-analytics/index.html#introduction",
    "title": "Meta-Analytics for Soccer",
    "section": "",
    "text": "This blog post demonstrates how to calculate the discrimination and stability ‚Äúmeta-metrics‚Äù proposed by Franks et al.¬†(2017) for an array of soccer stats. Before the computations (‚ÄúInference‚Äù), I start by defining these meta-metrics (‚ÄúMethods‚Äù), although I gloss over a lot of details for the sake of brevity. At the end, in ‚ÄúResults and Discussion‚Äù, I briefly discuss how my results compare to those for Franks et al., who analyzed basketball and hockey stats.\n\n\nIn the realm of sports, data analysis has grown significantly, introducing numerous metrics to guide coaching, management, and, of course, fans. However, this proliferation has also led to confusion, with overlapping and sometimes conflicting metrics. To address this, Franks et al.1 propose three ‚Äúmeta-metrics‚Äù to help determine which metrics offer unique and dependable information for decision-makers.\nBy examining sources of variation in sports metrics (e.g.¬†intrinsic player skill), Franks et al.¬†introduce three meta-metrics to assess player performance stats:\n\ndiscrimination: How good is the metric at telling players apart?\nstability: How likely is it that a player maintains the same performance for the metric in the next season?\nindependence: Does the stat tell us something different about players that other stats don‚Äôt tell us?"
  },
  {
    "objectID": "posts/soccer-meta-analytics/index.html#footnotes",
    "href": "posts/soccer-meta-analytics/index.html#footnotes",
    "title": "Meta-Analytics for Soccer",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf it means anything, the Meta-analytics paper has 40 citations at the time of writing, suggesting it has had a non-trivial influence on subsequent sports analytics research.‚Ü©Ô∏é\nThis isn‚Äôt meant to be an explainer of what these stats are or why they were chosen. Nonetheless, it‚Äôs worth pointing out the different types of stats, as it is relevant to the calculation relationship between discrimination and stability, as we shall see.‚Ü©Ô∏é\nThe fact that they‚Äôve published their code made the replication much less of a daunting task, so huge kudos to the authors. (Note that there were a few tweaks that I needed to make to get all of the code working properly with R &gt;4.0.)‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/probability-calibration/index.html#introduction",
    "href": "posts/probability-calibration/index.html#introduction",
    "title": "Calibrating Binary Probabilities",
    "section": "",
    "text": "Ever grappled with a classification model that consistently over-predicts or under-predicts an event? Your first thought might be to re-evaluate the model‚Äôs features or framework. But what if tweaking the model isn‚Äôt an option, either due to a lack of resources or access restrictions? The good news is, there‚Äôs another way‚Äìit‚Äôs called calibration.\nCalibration falls in the ‚Äúpost-processing‚Äù step of predictive modeling.1 We modify the output of a model using nothing but the model predictions and labels of the output. We do that by, you guess it, fitting another model, often called a ‚Äúcalibrator‚Äù.\n\n\nthe pre-processing stage (e.g., feature engineering, normalization, etc.)\nmodel fitting (actually training the model)\npost-processing (such as optimizing a probability threshold)\n\n\nOne of my favorite (and relatively new) packages in the {tidymodels} ecosystem is the {probably} package. It provides functions that make it fairly straightforward to do calibration, even for those who are new to the concept. So let‚Äôs use {probably} to demonstrate the power of calibration."
  },
  {
    "objectID": "posts/probability-calibration/index.html#footnotes",
    "href": "posts/probability-calibration/index.html#footnotes",
    "title": "Calibrating Binary Probabilities",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe {tidymodels} guide breaks down the traditional modeling workflow into three steps:‚Ü©Ô∏é\nSticking with the basics, I use the defaults for num_breaks (10) and conf_level (0.9).‚Ü©Ô∏é\nWe don‚Äôt have a skew problem in this context, but it‚Äôs good to note when a Beta calibration might provide meaningful benefits over other calibration methods.‚Ü©Ô∏é\nI‚Äôve discussed BSS in a prior post on expected goals model calibration and another post on expected goals match-implied win probabilities.‚Ü©Ô∏é\nIn this case, I choose to use the observed match win rate as the reference model.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/fantasy-football-schedule-problem/index.html#introduction",
    "href": "posts/fantasy-football-schedule-problem/index.html#introduction",
    "title": "Fantasy Football and the Classical Scheduling Problem",
    "section": "",
    "text": "Every year I play in several fantasy football (American) leagues. For those who are unaware, it‚Äôs a game that occurs every year in sync with the National Football League (NFL) where participants play in weekly head-to-head games as general managers of virtual football teams. (Yes, it‚Äôs very silly.) The winner at the end of the season is often not the player with the team that scores the most points; often a fortunate sequence of matchups dictates who comes out on top.\nI didn‚Äôt fare so well this year in one of my leagues, but my disappointing placement was not due to my team struggling to score points; rather, I was extremely unlucky. I finished the season in 7th place despite scoring the most points!\nThis inspired me to quantify just how unlikely I was. The most common way to calculate the likelihood of a given team‚Äôs ranking in a league with is with a Monte Carlo simulation based on some parameterized model of scoring to generate probabilities for the final standings. FiveThirtyEight uses such a model for their soccer models, for example. For a setting in which team scores are independent of one another, such as fantasy football, another approach is to simply calculate what each team‚Äôs record would be if they had played every other team each week. (So, if your league has 10 teams and each plays each other once, each team would have a hypothetical count of 90 games played.) However, I was particularly interested in answering the question: ‚ÄúIn how many different schedules would I have finished where I did?‚Äù"
  },
  {
    "objectID": "posts/fantasy-football-schedule-problem/index.html#footnotes",
    "href": "posts/fantasy-football-schedule-problem/index.html#footnotes",
    "title": "Fantasy Football and the Classical Scheduling Problem",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is almost directly taken from this Stack Exchange question.‚Ü©Ô∏é\nIn fantasy football, teams often play each other more than once in a year (depending on your league size), so I‚Äôve somewhat simplified the problem for the purpose of this post. More work could be done to figure out the number of possibilities when more than one game has to be scheduled for each pair of teams.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/xg-ratio-empirical-bayes/index.html#introduction",
    "href": "posts/xg-ratio-empirical-bayes/index.html#introduction",
    "title": "Measuring Shooting Overperformance in Soccer",
    "section": "Introduction",
    "text": "Introduction\nThis blog post is my attempt to replicate the results in Laurie Shaw‚Äôs 2018 blog post ‚ÄúExceeding Expected Goals‚Äù. Specifically, I want to shed light on how to implement Gamma-Poisson empirical Bayes (EB) estimation. If you don‚Äôt care at all about the theory behind EB and its application to this context, then go ahead and skip ahead to the ‚ÄúImplementation‚Äù section.\n\nWhat Is Empirical Bayes (EB) estimation?\nEmpirical Bayes (EB) estimation. Wow, just typing that out makes me feel smart. But what is it, really? In short, I‚Äôd describe it as a mix of Bayesian and Frequentist inference. We lean into the observed frequencies of the data (Frequentist) while simultaneously refining our initial data assumptions through Bayesian updating. In practice, one might use EB as a (relatively) simple alternative to a full Bayesian analysis, which can feel daunting.\nIn regular Bayesian analysis, you start with your initial ‚Äúguess‚Äù (prior distribution) about something, and as you gather data, you tweak that ‚Äúguess‚Äù using Bayes‚Äô theorem to get a final view (posterior distribution). We combine what we thought about the data beforehand with how likely the data matches (likelihood).\nEmpirical Bayes puts a twist on this. Instead of having a prior guess, you figure out that initial guess from the same data you‚Äôre analyzing. This can make things simpler, especially when you‚Äôre dealing with tons of guesses but not much initial info.\n\n\nA canonical example of EB estimation (Beta-Binomial)\nDavid Robinson wrote a wonderful blog post about empirical Bayes estimation for estimating batting averages in baseball, notably ‚Äúshrinking‚Äù the battering averages of those with relatively few at bats closer to some ‚Äúprior‚Äù estimate derived from a choice of hyperparameters. For context, batting average, \\(BA\\), is defined as a player‚Äôs count of hits, \\(H\\), divided by the count of their at bats, \\(AB\\).\n\\[\nBA = \\frac{H}{AB}\n\\tag{1}\\]\n\n\n\n\n\n\nNote\n\n\n\nI‚Äôd David‚Äôs post must read material before going through this blog post.\n\n\nIn his post, David uses a Beta prior and a binomial posterior together, i.e.¬†a Beta-binomial Bayesian model)12, since this tandem is suitable for proportions and probabilities. The gist of his approach: we add some fixed number of hits, \\(\\alpha_0\\), and a fixed number of at bats, \\(\\beta_0\\), to the numerator and denominator of the battering average equation as so.\n\\[\n\\frac{H + \\alpha_0}{AB + \\alpha_0 + \\beta_0}\n\\tag{2}\\]\nSpecifically, the ‚Äúprior‚Äù estimate of batting average is found by isolating the \\(\\alpha_0\\) and \\(\\beta_0\\) elements:\n\\[\n\\frac{\\alpha_0}{\\alpha_0 + \\beta_0}\n\\tag{3}\\]\nIf, for example, alpha0 = 70 and beta0 = 163, then the prior estimate of batting average is effectively 70 / (70 + 163) = 0.3. Note that alpha0 and beta0 are learned from the data using maximum likelihood estimation (MLE), although other approaches, such as ‚Äúmethod of moments‚Äù could be used. (Heck, you could even defensibly choose these ‚Äúhyperparameters‚Äù yourself, without any fancy statistics, if you feel that you have enough knowledge of the data.)\n\n\nGamma-Poisson EB estimation\nNow, for my replication of Shaw‚Äôs analysis, we‚Äôre going to be focusing on the ratio of a player‚Äôs goals, \\(G\\), divided by their expected goals), \\(xG\\), summed up over a fixed period. Shaw refers to this as ‚Äúoutperformance‚Äù \\(O\\) for a player \\(p\\):\n\\[\nO_p = \\frac{G_p}{xG_p}\n\\tag{4}\\]\nWhile one might be tempted to use Beta-Binomial EB since this setup seems similar to batting average in Equation¬†1, Shaw used a Gamma-Poisson EB adjustment, and justifiably so. Gamma-Poisson makes more sense when the underlying data consists of counts and what you‚Äôre trying to estimate is a rate or ratio, not a proportion bounded between 0 and 1. Note that a \\(O_p\\) ratio of 1 indicates that a player is scoring as many goals as expected; a ratio greater than 1 indicates overperformance; and a ratio less than 1 indicates underperformance. On, the other hand, batting average is bounded between 0 and 1.\nNow, despite the naming of conjugate prior pairs‚Äìe.g.¬†‚ÄúBeta-Binomial‚Äù and ‚ÄúGamma-Poisson‚Äù, where the prior distribution is represented by the first distribution and the likelihood distribution is indicated by the second‚Äìlet‚Äôs not forget that there is a third distribution to be noted: the posterior. In the case of the Gamma-Poisson model, the unnormalized posterior distribution of the ‚Äúkernel‚Äù (i.e.¬†the prior and likelihood pair) is a Gamma distribution. (This is always the case with Gamma-Prior kernels.)\nIn practice, this means that we‚Äôll be using the Gamma distribution for both estimating hyperparameters and posterior sampling. Perhaps surprising to the reader, you won‚Äôt need any Poisson functions in the code implementation. Rather, the Poisson distribution is pertinent to implementation only to the extent that the Gamma distribution happens to be the most reasonable distribution to pair with it.\nI‚Äôve woefully explained away a lot of details here, but hopefully this all makes sense to those with a basic understanding of the Gamma and Poisson distributions themselves."
  },
  {
    "objectID": "posts/xg-ratio-empirical-bayes/index.html#footnotes",
    "href": "posts/xg-ratio-empirical-bayes/index.html#footnotes",
    "title": "Measuring Shooting Overperformance in Soccer",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis conjugate distribution table might be handy for those curious to know which distributions are typically paired together for empirical Bayes estimation.‚Ü©Ô∏é\nIf you‚Äôve seen my work, you might have noticed that I‚Äôve used Beta-Binomial EB a few times for public projects in the past:\n\nto estimate the proportion of direct free kick shots on target (soccer), grouped by league\nto adjust Whataburger Yelp reviews for small sample sizes\n\n‚Ü©Ô∏é\nNote that understat maintains its own xG model, so the xG on understat won‚Äôt exactly match what you might get from Opta or StatsBomb.‚Ü©Ô∏é\nFBRef only provides expected goals dating back to the 2017/18 season, so unfortunately it‚Äôs not viable for this analysis.‚Ü©Ô∏é\nThe other major reason why I may not be able to match his results is if I‚Äôve implemented the Gamma-Poisson adjustment in a different (hopefully, not incorrect üòÖ) manner.‚Ü©Ô∏é\nIn the wild, you‚Äôll see alpha and beta used to describe the hyperparameters. shape and rate are different ways of framing these parameters.‚Ü©Ô∏é\nNote that this process of selecting priors is the ‚Äútwist‚Äù I mentioned in the introduction that really separates empirical Bayes estimation from a traditional, full Bayesian approach. In the latter, one chooses priors for an analysis without using the data to be included in the analysis.‚Ü©Ô∏é\nThis post isn‚Äôt meant to be about uncertainty and credible intervals for EB adjusted means, so I won‚Äôt go into it here. Loosely, one can read the interval as quantifying the bound about which we can be confident that the true estimate lands. Uncertainty intervals that overlap with 1 broadly suggest that, even though the mean estimate may look to be much greater or less than 1, we cannot say the difference is significant from the ‚Äúaverage‚Äù (of 1).‚Ü©Ô∏é\nA more strict and, arguably, correct measure is a 90 or 95% credible interval to ascertain ‚Äúsignificance‚Äù. Nonetheless, to maintain consistency with Shaw, I‚Äôm showing standard deviation.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/epl-xpts-simulation-1/index.html#introduction",
    "href": "posts/epl-xpts-simulation-1/index.html#introduction",
    "title": "What exactly is an ‚Äúexpected point‚Äù? (part 1)",
    "section": "",
    "text": "Expected goals (xG) in soccer have gone mainstream and are no longer cool to talk about.\n\n\nWhat exactly is an ‚Äù expected goal ‚Äú? Who decides the criteria ? Is there a list of‚Äù expected goal scorers ‚Äù ? Or even ‚Äù unexpected ones ‚Äù ?\n\n‚Äî Ian Darke (@IanDarke) December 24, 2020\n\n\nSo let‚Äôs talk about expected points (xPts). The one sentence explainer for xPts: it‚Äôs a number between 0 and 3 assigned to each team in a match that we estimate from the xG of each shot in the match. Teams that accumulate more xG than their opponents in the match are more likely to have xPts closer to 3, i.e.¬†the points awarded for a win, and those that accumulate less than their opponents are more likely to earn xPts closer to 0. xPts is convenient for translating a team‚Äôs xG (relative to it‚Äôs opponents) to the team‚Äôs expected placement in the standings.\nWhile several outlets have described computing expected points with simulation1, simulation is actually not necessary if you have the xG for every shot taken in a match.2 For example, let‚Äôs say team A shoots six times with an xG of 0.1 for each shot, and team B shoots three shots with xG‚Äôs of 0.1, 0.2, and 0.3 respectively. Given these goal probabilities, we can analytically compute xPts as follows.\nFirst, we find the probability of scoring 0, 1, 2, etc. goals (up to the number of shots taken).3\n\n\nCode\nlibrary(poibin)\nxg_a &lt;- rep(0.1, 6)\nxg_b &lt;- c(0.1, 0.2, 0.3)\n\nprobs_a &lt;- dpoibin(seq.int(0, length(xg_a)), xg_a)\nround(probs_a, 2)\n#&gt; [1] 0.53 0.35 0.10 0.01 0.00 0.00 0.00\nprobs_b &lt;- dpoibin(seq.int(0, length(xg_b)), xg_b)\nround(probs_b, 2)\n#&gt; [1] 0.50 0.40 0.09 0.01\n\n\n\nSecond, we convert the goal probabilities to singular probabilities for each team winning the match, as well as the probability of a draw.4\n\n\nCode\nlibrary(gdata)\nouter_prod &lt;- outer(probs_a, probs_b)\np_a &lt;- sum(lowerTriangle(outer_prod))\np_b &lt;- sum(upperTriangle(outer_prod))\np_draw &lt;- sum(diag(outer_prod))\nround(c(p_a, p_b, p_draw), 2)\n#&gt; [1] 0.28 0.30 0.42\n\n\nFinally, given the match outcome probabilities, the xPts calculation is straightforward.\n\n\nCode\nxpts_a &lt;- 3 * p_a + 1 * p_draw\nxpts_b &lt;- 3 * p_b + 1 * p_draw\nround(c(xpts_a, xpts_b), 2)\n#&gt; [1] 1.27 1.31\n\n\nFor this example, we arrive at the interesting result that, despite the two teams total xG being equal (=0.6), team B has a slightly higher probability of winning. There have been plenty of explanations on this ‚Äúquality vs.¬†quantity‚Äù phenomenon, so I won‚Äôt go into it in detail. Nonetheless, this simple example illustrates why it can be useful to translate xG into another form‚Äîdoing so can provide a better perspective on match results and, consequently, team placement in the standings.\n\n\nSo we‚Äôve gone over what expected points are and why they‚Äôre important. Now we set out to do the following.\n\nCalculate xPts from shot xG for multiple seasons of data. We‚Äôll limit the scope to the 2020/21 and 2021/22 seasons for the English Premier League.5\nCompare the calibration of the understat and fotmob match outcome probabilities. {worldfootballR} makes it easy for us to get xG from both understat and fotmob, and it should be interesting to compare the the predictive performance of the two models.\nCompare predictions of actual season-long points using xPts that we derive from understat and fotmob xG. In particular, we‚Äôll be interested to see if our conclusions regarding the better source for xG here matches the conclusions for (2)."
  },
  {
    "objectID": "posts/epl-xpts-simulation-1/index.html#footnotes",
    "href": "posts/epl-xpts-simulation-1/index.html#footnotes",
    "title": "What exactly is an ‚Äúexpected point‚Äù? (part 1)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDanny Page‚Äôs interactive web app also uses simulation.‚Ü©Ô∏é\nNow, if you desire the statistical properties that simulation offers, such as an estimation of error, that‚Äôs understandable; however, in write-ups that I‚Äôve seen, such is not mentioned explicitly. Additionally, if one chooses to go down the simulation route because they believe that it helps to suppress flaws with the xG model, that‚Äôs also understandable. On the other hand, the analytical approach I present should present nearly identical results to that which one would find with simulation, and it offers the advantage of being much faster.‚Ü©Ô∏é\nPlotting code is omitted throughout the post since it‚Äôs not particularly instructive.‚Ü©Ô∏é\nHow does this work? Under the assumption that xG comes from a Poisson binomial distribution, we look at all combinations of makes and misses of the shots and compare the relative proportion of instances in which one team‚Äôs number of success, i.e.¬†goals, is greater than, equal to, or less than their opponent‚Äôs.‚Ü©Ô∏é\nWe‚Äôve limited the scope for several reasons: (1) fotmob only has complete xG data for the 2020/21 and 2021/22 seasons as of writing, (2) I didn‚Äôt want to have to map team names across the two data sources for a ton of teams; and (3) of all league, I‚Äôm most interested in the EPL üòÑ.‚Ü©Ô∏é\nNote that there are three additional shots in the fotmob data. There‚Äôs no simple solution to resolving this data discrepancy since we don‚Äôt have matching shot identifiers in the two data sets ü§∑.‚Ü©Ô∏é\nUsing the adjective ‚Äúpredictive‚Äù is a little misleading, since we‚Äôre not actually making predictions out-of-sample. Rather, we‚Äôre using models based on xG to evaluate which xG data source better explains the observed results.‚Ü©Ô∏é\nHome field advantage is treated as a feature instead of defined directly via columns, i.e.¬†home_team, home_score, etc., which is good practice in general.‚Ü©Ô∏é\nDraws occur for 22.5% of matches in the data set, and wins and losses occur in 38.8% of matches each.‚Ü©Ô∏é\nPersonally, I tend to rely on BSS wherever I can. Not only is it more interpretable‚Äîit‚Äôs a number between 0 and 1, while MSE can take on any value, depending on the context‚ÄîI like that it forces one to compare to a baseline, which is a good principle in general.‚Ü©Ô∏é\nNote that aggregating match-level probabilities to the season-level is not a statistically valid way to use the probabilities, which are intended to be treated independently.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/fantasy-football-performance/index.html",
    "href": "posts/fantasy-football-performance/index.html",
    "title": "Measuring manager performance in fantasy football",
    "section": "",
    "text": "If you‚Äôve played (American) fantasy football, you‚Äôve probably felt that you have been unlucky and unjustly lost matches at some point. ‚ÄúI had the second most points of any team in my league this week, and I only lost because I happened to play the team that scored the most points!‚Äù If you‚Äôre like me, the first thing that we do is go to our league standings and look to see how we stack up against the rest of the teams in points for and against. If you‚Äôve scored the third most points but your current placing is eighth because you‚Äôve happened to have the most points scored against you, then of course you have a right to feel slighted.\nBut are there other ways you can go about justifying your dissatisfaction? I can think of at least three:\n\nAll-play record: What would your position in the league table be if every team played all other teams in head-to-head matchups every week? How much did your schedule impact your record?\nActual points scored compared to projection: Are you scoring less than your projected total? Is your team under-performing more than other teams compared to their projected scores?\nPoints ‚Äúleft on the bench‚Äù: How many points could you have had if you had started the best possible lineup (with the benefit of hindsight)?\n\nIn this post I‚Äôll look at each of these concepts for my own fantasy football league. Evaluating how actual scores and placings may have been impacted by schedule, projections, and lineup choices should help us reason more tangibly about really contributes to ‚Äúluck‚Äù."
  },
  {
    "objectID": "posts/fantasy-football-performance/index.html#introduction",
    "href": "posts/fantasy-football-performance/index.html#introduction",
    "title": "Measuring manager performance in fantasy football",
    "section": "",
    "text": "If you‚Äôve played (American) fantasy football, you‚Äôve probably felt that you have been unlucky and unjustly lost matches at some point. ‚ÄúI had the second most points of any team in my league this week, and I only lost because I happened to play the team that scored the most points!‚Äù If you‚Äôre like me, the first thing that we do is go to our league standings and look to see how we stack up against the rest of the teams in points for and against. If you‚Äôve scored the third most points but your current placing is eighth because you‚Äôve happened to have the most points scored against you, then of course you have a right to feel slighted.\nBut are there other ways you can go about justifying your dissatisfaction? I can think of at least three:\n\nAll-play record: What would your position in the league table be if every team played all other teams in head-to-head matchups every week? How much did your schedule impact your record?\nActual points scored compared to projection: Are you scoring less than your projected total? Is your team under-performing more than other teams compared to their projected scores?\nPoints ‚Äúleft on the bench‚Äù: How many points could you have had if you had started the best possible lineup (with the benefit of hindsight)?\n\nIn this post I‚Äôll look at each of these concepts for my own fantasy football league. Evaluating how actual scores and placings may have been impacted by schedule, projections, and lineup choices should help us reason more tangibly about really contributes to ‚Äúluck‚Äù."
  },
  {
    "objectID": "posts/fantasy-football-performance/index.html#analysis-and-results",
    "href": "posts/fantasy-football-performance/index.html#analysis-and-results",
    "title": "Measuring manager performance in fantasy football",
    "section": "Analysis and Results",
    "text": "Analysis and Results\nThe data comes from the 2018 - 2023 seasons of my primary fantasy football league, hosted on ESPN.1 We‚Äôve had different rule sets and schedule formats (i.e.¬†playoffs started in week 14 prior to the 2021 season in this league), but for the most part, measures of performance should be mostly agnostic to this.\nData was scraped with the {ffscrapr} package maintained by Tan Ho.\n\nAll-play records\nI felt like I was particularly unlucky with my schedule this past year, and indeed this bears out in the numbers.2 Juan and Drake were also very unfortunate.\n\n\n  \n  \n\n\n\n2023 All-Play Records\n\n\nTony was the most unlucky in the league this past season.\n\n\nTeam\nActual\nAll Play\nWin %\nŒî\n\n\nW\nL\nW\nL\nActual\nAll Play\n\n\n\n\nTony\n4\n10\n56\n70\n29%\n44%\n‚àí16%\n\n\nJuan\n6\n8\n73\n53\n43%\n58%\n‚àí15%\n\n\nDrake\n4\n10\n55\n71\n29%\n44%\n‚àí15%\n\n\nBrandon\n6\n8\n60\n66\n43%\n48%\n‚àí5%\n\n\nAndrew L.\n7\n7\n63\n63\n50%\n50%\n0%\n\n\nEnrique\n9\n5\n76\n50\n64%\n60%\n4%\n\n\nAlan\n7\n7\n55\n71\n50%\n44%\n6%\n\n\nAndrew E.\n9\n5\n72\n54\n64%\n57%\n7%\n\n\nLuis\n8\n6\n57\n69\n57%\n45%\n12%\n\n\nManny\n10\n4\n63\n63\n71%\n50%\n21%\n\n\n\n\n\nA 16% difference between my actual record and my all-play record feels non-trivial. In fact, in comparing that level of under-performance to the prior seasons (some of which we played with a ‚Äúsuperflex‚Äù, contributing to more variance), I find that there were only 6 less fortunate outcomes, including a -29% difference for myself in 2020!\n\n\n  \n  \n\n\n\nActual Win % - All Play Win %\n\n\nJuan and Tony have had the most unfair schedules since 2018.\n\n\nTeam\n2018\n2019\n2020\n2021\n2022\n2023\nTotal\n\n\n\n\nJuan\n‚àí12%\n15%\n‚àí4%\n‚àí25%\n7%\n‚àí15%\n‚àí6%\n\n\nTony\n2%\n6%\n‚àí29%\n‚àí5%\n5%\n‚àí16%\n‚àí6%\n\n\nEnrique\n6%\n‚àí19%\n‚àí17%\n‚àí2%\n6%\n4%\n‚àí3%\n\n\nBrandon\n‚àí2%\n‚àí9%\n6%\n‚àí11%\n3%\n‚àí5%\n‚àí3%\n\n\nDrake\n2%\n6%\n8%\n12%\n‚àí4%\n‚àí15%\n1%\n\n\nAlan\n‚àí14%\n20%\n6%\n6%\n‚àí15%\n6%\n1%\n\n\nLuis\n17%\n‚àí14%\n13%\n3%\n‚àí20%\n12%\n2%\n\n\nManny\n4%\n3%\n1%\n‚àí2%\n‚àí13%\n21%\n2%\n\n\nAndrew E.\n10%\n‚àí21%\n12%\n13%\n10%\n7%\n6%\n\n\nAndrew L.\n‚àí13%\n12%\n3%\n11%\n21%\n0%\n6%\n\n\n\n\n\nAndrew E. and Andrew L. have been fairly fortunate, only facing 1 season in the past 6 where they ended up with a record that was worse than their all-play records.\nThe top and bottom 2.5th percentiles of season-long actual win % minus all play win % are about ¬±22%. This translates to about ¬±3 wins in leagues with a 14-game regular season. So the ‚Äúback-of-the-napkin‚Äù takeaway here is that you could be winning or losing 3 more games due to your schedule, in the most extreme cases.\nOverall, it‚Äôs interesting to see that the under- and over-performance doesn‚Äôt quite level out even over the course of 6 seasons. Surely we‚Äôd see this even out over more and more seasons, but it‚Äôs noteworthy that even 6 seasons may not be sufficient to achieve long-run schedule schedule equality.\n\n\nActual vs.¬†projected points\nAnother way to look at whether a team was ‚Äúlucky‚Äù or not is to compare the points that they scored versus what they were projected to score. 3 Further, we should look to see whether one‚Äôs opponents are scoring more than they‚Äôre projected to score, as this represents another source of variance in scoring. In total, the sum of the differences between one‚Äôs own actual and projected scores and the difference with the opponents‚Äô actual and projected scores presents another way of measuring misfortune.4\n\n\n  \n  \n\n\n\n\n\n\n\n\n\n2023 Avg. Weekly Actual - Projected Points\n\n\nBrandon was the most misfortunate with performance compared to\nprojection this past season.\n\n\nTeam\nTeam\nOpponent\nTeam -\nOpponent\n\n\n\n\nBrandon\n‚àí11.1\n4.7\n‚àí15.8\n\n\nDrake\n‚àí5.7\n6.4\n‚àí12.1\n\n\nTony\n‚àí2.8\n8.2\n‚àí11.0\n\n\nJuan\n2.0\n7.0\n‚àí5.0\n\n\nAlan\n‚àí1.2\n‚àí2.1\n0.9\n\n\nAndrew L.\n1.3\n‚àí1.5\n2.8\n\n\nLuis\n‚àí2.2\n‚àí8.2\n6.0\n\n\nEnrique\n3.0\n‚àí6.4\n9.4\n\n\nManny\n‚àí3.5\n‚àí14.7\n11.2\n\n\nAndrew E.\n5.2\n‚àí8.4\n13.6\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\nAvg. Weekly Actual - Projected Points\nMinus Opponent's Actual - Projected Points\n\n\nDrake has under-performed projections the most on average since 2018.\n\n\nTeam\n2019\n2020\n2021\n2022\n2023\nTotal\n\n\n\n\nDrake\n‚àí13.2\n5.0\n‚àí5.8\n‚àí0.3\n‚àí12.1\n‚àí5.4\n\n\nBrandon\n‚àí14.3\n2.1\n7.0\n‚àí4.2\n‚àí15.8\n‚àí5.0\n\n\nJuan\n3.5\n‚àí17.3\n‚àí6.5\n6.9\n‚àí5.0\n‚àí3.5\n\n\nLuis\n‚àí8.3\n‚àí13.8\n5.5\n‚àí5.5\n6.0\n‚àí2.8\n\n\nAndrew E.\n‚àí2.1\n‚àí8.1\n‚àí7.5\n2.1\n13.6\n‚àí0.1\n\n\nEnrique\n5.0\n1.6\n‚àí2.5\n‚àí0.8\n9.4\n2.5\n\n\nTony\n17.7\n1.7\n5.3\n1.5\n‚àí11.0\n2.6\n\n\nManny\n‚àí1.7\n‚àí0.5\n‚àí0.7\n5.6\n11.2\n3.0\n\n\nAndrew L.\n3.3\n21.6\n‚àí4.0\n‚àí3.0\n2.8\n3.6\n\n\nAlan\n10.1\n7.7\n9.1\n‚àí2.3\n0.9\n4.9\n\n\n\n2018 season excluded due to miscalibrated projected points.\n\n\n\n\n\n\nObservations:\n\nI benefited from the second most fortunate season in 2019. I averaged 13.0 points per game above my projected score, and my opponents scored 4.7 less than their projected scores, adding up to +17.7.\nDrake has under-performed and Alan has over-performed in 4 of the past 5 seasons. Interestingly, I have also over-performed in 4 of the past 5 seasons, but was fairly unfortunate this past season.\nThe variance in weekly performance against expectation (as defined by projected points) is not directly represented by this table, which shows averages. The standard deviation of the weekly difference in the actual points minus projected points for both teams in a matchup is about 32.5!\n\n\n\nLost Opportunity\nHow many points did teams ‚Äúleave on the bench‚Äù? This is not something managers typically think about when looking to see how unlucky they might have been (partially because it‚Äôs not the easiest thing to calculate), but it‚Äôs worth looking at, as it gives us some sense of who is best at minimizing their own downside (by starting the best players).5\n\n\n  \n  \n\n\n\n\n\n\n\n\n\n2023 Avg. Weekly \"Lost Opportunity\"\nwith Sub-Optimal Starter Choices\n\n\nBrandon lost out on the most points this season by not starting\nbench players that scored more than his starters.\n\n\nTeam\n# of sub-optimal\nstarters\nScore improvement\nwith best lineup\nLeague avg.\nscore improvement -\nscore improvement\n\n\n\n\nBrandon\n2.6\n27.7\n‚àí7.8\n\n\nAndrew E.\n2.3\n24.1\n‚àí4.2\n\n\nJuan\n2.8\n22.3\n‚àí2.4\n\n\nManny\n2.6\n22.0\n‚àí2.1\n\n\nAlan\n2.5\n21.9\n‚àí2.0\n\n\nTony\n2.3\n18.8\n1.1\n\n\nAndrew L.\n2.0\n17.8\n2.1\n\n\nLuis\n1.9\n16.9\n3.0\n\n\nDrake\n1.9\n16.2\n3.7\n\n\nEnrique\n1.9\n11.4\n8.5\n\n\n\n\n\nThe magnitude of the points will vary by league, according to the league‚Äôs rules for roster slots and points. If your league is anything like mine, then you can expect to lose out on about 20 points due to about 2 sub-optimal starting roster choices each week.\nNow, looking at the ‚Äúlost opportunity‚Äù points compared to the league average for the past 6 seasons, we see that things tend to average out close to zero.\n\n\n  \n  \n\n\n\n\"Lost Opportunity\" Points with Sub-Optimal Starter Choices\n\n\nOver several seasons, \"lost opportunity\" points level out.\n\n\nTeam\n2018\n2019\n2020\n2021\n2022\n2023\nTotal\n\n\n\n\nEnrique\n8.1\n6.7\n‚àí0.6\n7.3\n‚àí3.8\n8.5\n4.3\n\n\nDrake\n4.4\n‚àí2.5\n12.4\n‚àí4.1\n9.1\n3.7\n3.8\n\n\nJuan\n3.8\n2.8\n2.8\n3.6\n0.4\n‚àí2.4\n1.7\n\n\nAlan\n‚àí8.4\n3.1\n‚àí2.4\n‚àí0.5\n9.8\n‚àí2.0\n0.1\n\n\nAndrew L.\n‚àí13.9\n‚àí1.2\n5.7\n3.1\n1.9\n2.1\n‚àí0.2\n\n\nBrandon\n4.2\n‚àí5.6\n‚àí8.4\n15.0\n‚àí1.4\n‚àí7.8\n‚àí0.5\n\n\nLuis\n‚àí3.7\n2.5\n‚àí5.2\n‚àí11.4\n8.1\n3.0\n‚àí1.0\n\n\nTony\n‚àí4.1\n‚àí4.5\n‚àí4.5\n2.8\n‚àí3.0\n1.1\n‚àí1.8\n\n\nManny\n5.4\n4.0\n‚àí0.5\n‚àí11.8\n‚àí6.6\n‚àí2.1\n‚àí2.3\n\n\nAndrew E.\n4.3\n‚àí5.3\n0.7\n‚àí4.0\n‚àí14.5\n‚àí4.2\n‚àí4.1\n\n\n\nValues represent league average \"lost opportunity\" minus team's \"lost opportunity\".\n\n\n\n\n\n\nNotes:\n\nThe weighted average lost opportunity points compared to the league average only varies by about ¬±4 points in the extremes. We‚Äôll likely see that range decrease as more seasons are played, as there seems to be a lot of ‚Äúnoise‚Äù here.\nJuan is the only team to perform better than the league average in 5 seasons. The rest of the teams had lost opportunity points greater than the league average in either 2, 3, or 4 seasons."
  },
  {
    "objectID": "posts/fantasy-football-performance/index.html#discussion",
    "href": "posts/fantasy-football-performance/index.html#discussion",
    "title": "Measuring manager performance in fantasy football",
    "section": "Discussion",
    "text": "Discussion\nTo overcome the variance that comes with fantasy football, some leagues play with a ‚Äúmedian scoring‚Äù format. In this structure, you earn a win or loss with your head-to-head matchup, as usual, but can earn another win every week if you score more than the median team score for the week. This has the effect of reducing ‚Äúluck‚Äù while still allowing for some of the randomness with which most players have developed a love-hate relationship.\nIf we applied this scoring format to my league, here‚Äôs how the placements would have changed over the past 6 years.\n\nAnd here‚Äôs a tabular summary. (Total placement Œî represents the sum of placement changes. changes is either 1 or 0 for each season, depending on whether the placement for the team changed in a positive or negative manner.)\n\n\n\nTeam\nTotal placement Œî\nPositive changes\nNegative changes\n\n\n\n\nEnrique\n5\n4\n0\n\n\nJuan\n5\n3\n1\n\n\nTony\n3\n1\n1\n\n\nManny\n1\n3\n2\n\n\nAndrew E.\n0\n1\n2\n\n\nBrandon\n0\n2\n3\n\n\nDrake\n-1\n0\n1\n\n\nLuis\n-1\n2\n3\n\n\nAlan\n-4\n1\n3\n\n\nAndrew L.\n-8\n0\n4\n\n\n\nSome observations:\n\nEnrique would have benefited the most‚ÄìEnrique would have moved up the rankings in 4 out of the 6 seasons (and not changed in the other 2 seasons), for a total sum of +5 placements.\nOn the other end of the spectrum, Andrew L. would have placed worse in 4 of the 6 seasons with the median scoring system, for a total ranking change of -8.\n\n\n\n\nSeason\nTeams dropping out of Top 4\nTeams rising out of Bottom 4\n\n\n\n\n2018\n0\n0\n\n\n2019\n1\n1\n\n\n2020\n1\n1\n\n\n2021\n0\n1\n\n\n2022\n1\n1\n\n\n2023\n1\n1\n\n\n\nGiven the high importance of not finishing in the bottom 4‚Äìto avoid being in the loser‚Äôs consolation playoffs and getting last place‚Äìit‚Äôs notable that 1 team would have moved out of the bottom 4 every season with the median scoring system, with the exception of 2018."
  },
  {
    "objectID": "posts/fantasy-football-performance/index.html#conclusion",
    "href": "posts/fantasy-football-performance/index.html#conclusion",
    "title": "Measuring manager performance in fantasy football",
    "section": "Conclusion",
    "text": "Conclusion\nLooking at how the median scoring system would benefit me personally‚ÄìI would never finish bottom 4 and be at risk for our last-place penalty‚ÄìI think my personal takeaway is that I should push for a median scoring format in the future. There‚Äôs still going to be variance with outcomes‚Äìdue to scheduling, under-performance compared to projection, and sub-optimal lineup choices‚Äìbut the more deserving teams (like my own) are less likely to get a short end of the stick."
  },
  {
    "objectID": "posts/fantasy-football-performance/index.html#footnotes",
    "href": "posts/fantasy-football-performance/index.html#footnotes",
    "title": "Measuring manager performance in fantasy football",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nJust based on my own league‚Äôs numbers, it seems that ESPN improved their projections after the 2018 season, and are now fairly well calibrated, i.e.¬†on average the error is close to 0.‚Ü©Ô∏é\nFor those looking to calculate this kind of table for themselves, ‚ÄúThe Fantasy Football Hub‚Äù has a nice calculator, plus some extra goodies, e.g.¬†‚ÄúWhat would my record have been if I had league member X‚Äôs schedule?‚Äù.‚Ü©Ô∏é\nIn most cases, I‚Äôd assume that a negative difference between actual and projected scores reflects misfortune (e.g.¬†injury, unexpected lack of playing time), although one can argue that it reflects poor management by the user, i.e.¬†‚ÄúYou should have known not to start that running back against the best rushing defense!‚Äù. However, projected points should theoretically capture matchup difficulty, so this seems like a weak argument in general.‚Ü©Ô∏é\nI would generally expect this measure to be correlated with the rankings of teams by actual win % minus all-play win %, assuming the projected scores are reasonably well-calibrated.‚Ü©Ô∏é\nThese numbers are biased by the quality of the players on a team‚Äôs roster. If a team has a lot of injured players, then they‚Äôre less likely to start sub-optimal players. Further, if they have a lot of players that just don‚Äôt tend to get a lot of points (e.g.¬†third string WRs), then they may appear to be starting players optimally more often, simply because they don‚Äôt have great replacement-level players.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/fbref-gamestate-expected-goal-difference/index.html#introduction",
    "href": "posts/fbref-gamestate-expected-goal-difference/index.html#introduction",
    "title": "Game state with FBref data",
    "section": "",
    "text": "Soccer is a game defined by its discrete, low-scoring nature, where the dynamics of a match are often dictated by the ‚Äúgame state‚Äù. Whether a team is leading, trailing, or level with their opponent at a specific moment can often influence how aggressively a team plays. Contextualizing statistics like expected goals (xG) can shed new light on how we evaluate team performance.\nConsider this scenario: a team enters a match as the underdog. They play aggressively early on, getting a few shots on target and eventually scoring, taking a 1-0 lead going into halftime. After the half, they decide to switch into a more defensive scheme, pulling everyone back towards their 18-yard box when the opponent has the ball. They end the match with no additional shots or xG accumulated, but they win the game. While their secondhalf statistics look poor because they ‚Äúparked the bus‚Äù, they arguably increased their odds of winning. If we consider the game state when looking at the winning team‚Äôs statistics, we can reason about why their xG looks poor.\nSo, game state is useful, right? Yet, game state analysis remains somewhat under-utilized in soccer analytics, in my opinion. Why is that? Well, it‚Äôs not without its challenges. Contextualizing numbers according to game state can introduce biases, leading us to over-attribute outcomes to tactical choices. Moreover, the calculations involved can be far from trivial.\nSo that‚Äôs what this post is for. I‚Äôll walk through how to calculate expected goals difference (xGD)1‚Äìthe difference between your team‚Äôs expected goals and your opponent‚Äôs‚Äìwith respect to the game state, using data from FBref."
  },
  {
    "objectID": "posts/fbref-gamestate-expected-goal-difference/index.html#footnotes",
    "href": "posts/fbref-gamestate-expected-goal-difference/index.html#footnotes",
    "title": "Game state with FBref data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI like xGD because it captures a lot of information about how your team is playing relative to your opponent. Your team could be putting up a lot of shots, each with a decent amount of xG (i.e.¬†‚Äúshot quality‚Äù), but if you‚Äôre conceding even more shots than you‚Äôve taken and/or the quality of those shots are better than yours, then you‚Äôre really not performing all that well. This would be reflected with a negative xGD. Respected analysts like Michael Caley also seem to like using xGD for diagnosing performance.‚Ü©Ô∏é\nAll code is intentionally hidden by default in this post, but can easily be viewed via toggles. There‚Äôs a decent amount of it, and it can be distracting upon first read.‚Ü©Ô∏é\nKeep in mind that the xG associated with a shot that is scored and changes the game state should be associated with the pre-shot game state, not the post-shot game state.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/soccer-league-strength/index.html#introduction",
    "href": "posts/soccer-league-strength/index.html#introduction",
    "title": "Quantifying Relative Soccer League Strength",
    "section": "",
    "text": "Arguing about domestic league strength is something that soccer fans seems to never tire of. (‚ÄúCould Messi do it on a cold rainy night in Stoke?‚Äù) Many of these conversations are anecdotal, leading to ‚Äúhot takes‚Äù that are unfalsifiable. While we‚Äôll probably never move away from these kinds of discussions, we can at least try to inform them with a quantitative approach.\nPerhaps the obvious way to do so is to take match results from international tournaments (e.g.¬†Champions League, Europa). But such an approach can be flawed‚Äîthere‚Äôs not a large sample, and match results may not be reflective of ‚Äútrue‚Äù team strength (e.g.¬†one team may win on xG by a large margin, but lose the game.)"
  },
  {
    "objectID": "posts/soccer-league-strength/index.html#footnotes",
    "href": "posts/soccer-league-strength/index.html#footnotes",
    "title": "Quantifying Relative Soccer League Strength",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nValuing Actions by Estimating Probabilities (VAEP) based on the atomic SPADL format.‚Ü©Ô∏é\nI had a Twitter thread in May 2021 describing how one could use VAEP ratings.‚Ü©Ô∏é\nIt‚Äôs closer to xG+xA, although the authors might disagree with that as well. It‚Äôs really best treated separately, which perhaps explains why the authors often using ‚Äúrating‚Äù and ‚Äúcontribution‚Äù when referring to VAEP.‚Ü©Ô∏é\nEach row represents one player. Each row only has one +1 and one -1, and 0s for other features.‚Ü©Ô∏é\nWe‚Äôre including all positions and ages in this regression, even though these groupings have varying standard deviations for transformation of the response variable. (All have 0 mean, as one might expect with a feature representing the difference between values with the same distribution.)‚Ü©Ô∏é\nThis NA occurs even when setting the intercept to 0, which is typically the way to get around this kind of issue with lm in R. When changing the order of columns in the regression and forcing the Netherlands coefficient to be non-NA, its estimate is lower than that of Bundesliga 2 (and a different league‚Äôs estimate is NA).‚Ü©Ô∏é\n‚ÄúWeighted-average‚Äù: Diff. (VAEP/90) = (Diff. * sum(SD * (N * sum(N)))) - sum(Mean * (N * sum(N)))‚Ü©Ô∏é\nThis is also a weakness of my approach, but arguably ratios exacerbate this.‚Ü©Ô∏é\nHere I was treating the Champions League and Europa as their own ‚Äúleagues‚Äù, purely out of curiosity.‚Ü©Ô∏é\nArguably you‚Äôll have this kind of issue no matter what, due to injuries.‚Ü©Ô∏é\n‚ÄúUn-transformation‚Äù becomes more difficult since we have to account for the ridge penalty.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/ball-progression-epv/index.html",
    "href": "posts/ball-progression-epv/index.html",
    "title": "Ball Progression is All You Need",
    "section": "",
    "text": "I‚Äôve written a lot about expected goals (xG) in soccer, but I haven‚Äôt yet talked much about possession value (PV) models1, another big topic in soccer analytics. What are they? Well, every PV model is different, but they all generally try to assign value to every on-ball action on the pitch. Such a model can help inform decisions about how to improve player and team performance.\nI heard someone recently say something like ‚ÄúPV models in soccer basically come down to ball progression‚Äù. That‚Äôs an interesting thought, and I add a hunch that it probably isn‚Äôt too wrong.\nOne way of getting at that idea is to look at how your PV model treats incomplete passes. Does it say that all long passes are ‚Äúgood‚Äù? What is the importance of the starting and end points of the pass? How does PV for an unsuccessful pass compare to a successful one, holding all else equal?\nI attempt to answer some of these questions with a VAEP model‚Äìan open-source PV model.23"
  },
  {
    "objectID": "posts/ball-progression-epv/index.html#completed-passes",
    "href": "posts/ball-progression-epv/index.html#completed-passes",
    "title": "Ball Progression is All You Need",
    "section": "Completed Passes",
    "text": "Completed Passes\nWe‚Äôll want to eventually look at the PV of incomplete passes, but it‚Äôs probably easier to start with completed passes, as we have a pretty strong intuition about them‚Äìpass the ball successfully closer to the goal, and you‚Äôre most likely helping your team (i.e.¬†positive PV).\n\nFrom One Spot, To Anywhere on the Pitch\nIn the interactive 8x12 pitch below, the blue cell illustrates where a pass is made, and the colored cells illustrate the average PV associated with all historicalLy successful passes made to that area. Hovering over a cell shows the PV value above the pitch as well.4\nOverall, I‚Äôd say that this illustration matches intuition‚Äìforward completed passes into the final third should be assigned a non-trivial positive value.\n\n\nCode\n{\n  const chart = d3.create(\"div\").style(\"background-color\", \"8f8f8f\")\n  const title = chart.append(\"div\").attr(\"id\", \"heatmap-title-complete-empirical\")\n  title.append(\"p\").html(`PV: &lt;span id='pv-value-complete-empirical'&gt;0&lt;/span&gt;`)\n  chart.append(\"div\").attr(\"id\", \"heatmap-complete-empirical\")\n\n  const legendSwatchContainer = chart.append(\"div\")\n    .attr(\"id\", \"heatmap-legend-complete-empirical\")\n    .style(\"display\", \"flex\")\n    .style(\"flex-direction\", \"column\")\n    .style(\"align-items\", \"center\")\n    .style(\"width\", \"100%\");\n\n  const legendRange = [\n    1.1 * d3.min(colorScaleCompleteRange),\n    1.1 * d3.max(colorScaleCompleteRange)\n  ];\n  const stepSize = (legendRange[1] - legendRange[0]) / (swatchParams.num - 1);\n  const legendSwatches = d3.range(legendRange[0], legendRange[1] + stepSize, stepSize);\n  legendSwatches[legendSwatches.length - 1] = legendRange[1];\n\n  const totalLegendWidth = swatchParams.width * swatchParams.num;\n\n  const swatchRow = legendSwatchContainer.append(\"div\")\n    .style(\"display\", \"flex\")\n    .style(\"justify-content\", \"center\")\n    .style(\"width\", \"100%\");\n\n  swatchRow.selectAll(\"div\")\n    .data(legendSwatches)\n    .enter()\n    .append(\"div\")\n    .style(\"width\", `${swatchParams.width}px`)\n    .style(\"height\", `${swatchParams.height}px`)\n    .style(\"background-color\", d =&gt; colorScaleComplete(d));\n\n  const labelRow = legendSwatchContainer.append(\"div\")\n    .style(\"display\", \"flex\")\n    .style(\"justify-content\", \"center\")\n    .style(\"width\", `${totalLegendWidth}px`);\n\n  labelRow.selectAll(\"span\")\n    .data(colorScaleCompleteRange)\n    .enter()\n    .append(\"span\")\n    .text(d =&gt; {\n      if (d === d3.min(colorScaleCompleteRange)) {\n        return d + \" &lt;=\";\n      } else if (d === d3.max(colorScaleCompleteRange)) {\n        return \"&gt;= \" + d;\n      }\n      return d;\n    })\n    .style(\"flex\", d =&gt; d === 0 ? \"1\" : null)\n    .style(\"text-align\", \"center\")\n\n  return chart.node();\n}\n\n\n\n\n\n\nFigure¬†1: A heatmap showing the average possession value (PV) of historically completed passes from the center spot (annotated in blue) to all areas on the pitch. The relative frequency of successful passes from the center spot to each other cell is shown as a percentage. The exact PV value associated with a complete pass ending at the hover point can be viewed above the pitch. Black cells represent areas to which successful passes from the center spot have never been made.\n\n\n\n\n\nCode\n{\n  const heatmap_complete_empirical = d3_soccer.heatmap(pitch)\n    .colorScale(colorScaleComplete)\n    .enableInteraction(true)\n    .onSelect((x, y, v) =&gt; {\n      const cappedValue = Math.min(Math.max(v, -1), 1);\n      d3.select(\"#pv-value-complete-empirical\").text(cappedValue.toFixed(3));\n    })\n    .parent_el(\"#heatmap-complete-empirical\")\n    .interpolate(false);\n\n  d3.select(\"#heatmap-complete-empirical\")\n    .html(\"\")\n    .datum(complete_empirical_pv_data)\n    .call(heatmap_complete_empirical);\n\n  const svg = d3.select(\"#heatmap-complete-empirical\").select(\"svg\");\n\n  const cells = svg.selectAll(\".cell\");\n\n  cells.each(function(d, i) {\n    const cell = d3.select(this);\n    const bbox = this.getBBox();\n\n    d3.select(this.parentNode)\n      .append(\"text\")\n      .attr(\"x\", bbox.x + bbox.width / 2)\n      .attr(\"y\", bbox.y + bbox.height / 2)\n      .attr(\"text-anchor\", \"middle\")\n      .attr(\"alignment-baseline\", \"central\")\n      .style(\"font-size\", \"3px\")\n      .style(\"pointer-events\", \"none\")\n      .text((d.prop * 100).toFixed(1) + \"%\");\n  });\n\n  svg.append(\"rect\")\n    .attr(\"x\", passStartParams.x)\n    .attr(\"y\", passStartParams.y)\n    .attr(\"width\", cellParams.width)\n    .attr(\"height\", cellParams.height)\n    .style(\"stroke\", \"blue\")\n    .style(\"fill\", \"none\")\n    .style(\"stroke-width\", \"1px\");\n}\n\n\n\n\n\n\n\nNote that the gradient in the pitch above is for PV, not for the relative frequency of completed passes from the center spot, which is instead shown as overlayed text. While passes into the box from the center spot have really strong positive PV, they‚Äôre uncommon because defenders are generally looking to stop those kinds of threatening passes.\nThe gradient in the plot below illustrates the relative frequency of successful passes from the center spot directly.\n\n\nCode\n{\n  const chart = d3.create(\"div\").style(\"background-color\", \"8f8f8f\")\n  const title = chart.append(\"div\").attr(\"id\", \"heatmap-title-complete-empirical-prop\")\n  chart.append(\"div\").attr(\"id\", \"heatmap-complete-empirical-prop\")\n\n  return chart.node();\n}\n\n\n\n\n\n\nFigure¬†2: A heatmap where the gradient and text illustrate the relative frequency of historically successful passes from the center spot (annotated in blue) to all areas on the pitch. Black cells represent areas to which successful passes from the center spot have never been made.\n\n\n\n\n\nCode\n{\n  const colorScaleSeq = d3.scaleSequential(d3.interpolateRgb(\"white\", \"gold\"))\n    .domain([0, 0.1])\n    .clamp(true)\n  const heatmap_complete_empirical_prop = d3_soccer.heatmap(pitch)\n    .colorScale(colorScaleSeq)\n    .enableInteraction(false)\n    .parent_el(\"#heatmap-complete-empirical-prop\")\n    .interpolate(false);\n\n  d3.select(\"#heatmap-complete-empirical-prop\")\n    .html(\"\")\n    .datum(complete_empirical_prop_data)\n    .call(heatmap_complete_empirical_prop);\n\n  const svg = d3.select(\"#heatmap-complete-empirical-prop\").select(\"svg\");\n\n  const cells = svg.selectAll(\".cell\");\n\n  cells.each(function(d, i) {\n    const cell = d3.select(this);\n    const bbox = this.getBBox();\n\n    d3.select(this.parentNode)\n      .append(\"text\")\n      .attr(\"x\", bbox.x + bbox.width / 2)\n      .attr(\"y\", bbox.y + bbox.height / 2)\n      .attr(\"text-anchor\", \"middle\")\n      .attr(\"alignment-baseline\", \"central\")\n      .style(\"font-size\", \"3px\")\n      .style(\"pointer-events\", \"none\")\n      .text((d.prop * 100).toFixed(1) + \"%\");\n  });\n\n  svg.append(\"rect\")\n    .attr(\"x\", passStartParams.x)\n    .attr(\"y\", passStartParams.y)\n    .attr(\"width\", cellParams.width)\n    .attr(\"height\", cellParams.height)\n    .style(\"stroke\", \"blue\")\n    .style(\"fill\", \"none\")\n    .style(\"stroke-width\", \"1px\");\n}\n\n\n\n\n\n\n\n\n\nFrom Anywhere on the Pitch, To Anywhere on the Pitch\nNow, to give the full picture, the interactive pitch below dynamically updates to show the average PV values associated with a pass starting from any cell that you hover over. The minimum and maximum PV achieved with a successful pass from the hovered spot are shown in the text above the pitch.\n\n\nCode\n{\n  const chart = d3.create(\"div\").style(\"background-color\", \"8f8f8f\")\n  const title = chart.append(\"div\").attr(\"id\", \"heatmap-title-complete-empirical-nested\")\n  title.append(\"p\").html(`min PV: &lt;span id='pv-min-complete-empirical-nested'&gt;0&lt;/span&gt;, max PV: &lt;span id='pv-max-complete-empirical-nested'&gt;0&lt;/span&gt;`)\n  chart.append(\"div\").attr(\"id\", \"heatmap-complete-empirical-nested\")\n  \n  const legendSwatchContainer = chart.append(\"div\")\n    .attr(\"id\", \"heatmap-legend-complete-empirical-nested\")\n    .style(\"display\", \"flex\")\n    .style(\"flex-direction\", \"column\")\n    .style(\"align-items\", \"center\")\n    .style(\"width\", \"100%\");\n  \n  const legendRange = [\n    1.1 * d3.min(colorScaleCompleteRange),\n    1.1 * d3.max(colorScaleCompleteRange)\n  ];\n  const stepSize = (legendRange[1] - legendRange[0]) / (swatchParams.num - 1);\n  const legendSwatches = d3.range(legendRange[0], legendRange[1] + stepSize, stepSize);\n  legendSwatches[legendSwatches.length - 1] = legendRange[1];\n  \n  const totalLegendWidth = swatchParams.width * swatchParams.num;\n  \n  const swatchRow = legendSwatchContainer.append(\"div\")\n    .style(\"display\", \"flex\")\n    .style(\"justify-content\", \"center\")\n    .style(\"width\", \"100%\");\n  \n  swatchRow.selectAll(\"div\")\n    .data(legendSwatches)\n    .enter()\n    .append(\"div\")\n    .style(\"width\", `${swatchParams.width}px`)\n    .style(\"height\", `${swatchParams.height}px`)\n    .style(\"background-color\", d =&gt; colorScaleComplete(d));\n  \n  const labelRow = legendSwatchContainer.append(\"div\")\n    .style(\"display\", \"flex\")\n    .style(\"justify-content\", \"center\")\n    .style(\"width\", `${totalLegendWidth}px`);\n  \n  \n  labelRow.selectAll(\"span\")\n    .data(colorScaleCompleteRange)\n    .enter()\n    .append(\"span\")\n    .text(d =&gt; {\n      if (d === d3.min(colorScaleCompleteRange)) {\n        return d + \" &lt;=\";\n      } else if (d === d3.max(colorScaleCompleteRange)) {\n        return \"&gt;= \" + d;\n      }\n      return d;\n    })\n    .style(\"flex\", d =&gt; d === 0 ? \"1\" : null)\n    .style(\"text-align\", \"center\")\n  \n  return chart.node();\n\n}\n\n\n\n\n\n\nFigure¬†3: A heatmap showing the average possession value (PV) of historically completed pass from the hover spot to all areas on the pitch. The relative frequency of successful passes from the hover spot to each other cell is shown as a percentage. The highest and lowest PV values across all end points associated with a completed pass from the hover point are shown above the pitch. Black cells represent areas to which successful passes from the hover spot have never been made.\n\n\n\n\n\nCode\n{  \n  const heatmap_complete_empirical_nested = d3_soccer.heatmap(pitch)\n    .colorScale(d3.scaleLinear().domain([-1, 1]).range([\"white\", \"white\"]))\n    .enableInteraction(true)\n    .onSelect((x, y, v) =&gt; {\n      const rawMinValue = d3.min(v, d =&gt; d.value);\n      const rawMaxValue = d3.max(v, d =&gt; d.value);\n      const minValue = Math.max(rawMinValue, -1);\n      const maxValue = Math.min(rawMaxValue, 1);\n  \n      d3.select(\"#pv-min-complete-empirical-nested\").text(minValue.toFixed(3));\n      d3.select(\"#pv-max-complete-empirical-nested\").text(maxValue.toFixed(3));\n      const cells = d3\n        .select(\"#heatmap-complete-empirical-nested\")\n        .selectAll(\"rect.cell\")\n        .data(v)\n  \n      cells.enter()\n        .merge(cells)\n        .attr(\"x\", d =&gt; d.x)\n        .attr(\"y\", d =&gt; d.y)\n        .attr(\"width\", d =&gt; d.width)\n        .attr(\"height\", d =&gt; d.height)\n        .style(\"fill\", d =&gt; colorScaleComplete(+d.value));\n\n      d3.select(\"#heatmap-complete-empirical-nested\")\n        .selectAll(\"text\")\n        .remove();\n        \n      cells.each(function(d, i) {\n        const cell = d3.select(this.parentNode);\n        const bbox = this.getBBox();\n        cell.append(\"text\")\n          .attr(\"x\", bbox.x + bbox.width / 2)\n          .attr(\"y\", bbox.y + bbox.height / 2)\n          .attr(\"text-anchor\", \"middle\")\n          .attr(\"alignment-baseline\", \"central\")\n          .style(\"font-size\", \"3px\")\n          .style(\"pointer-events\", \"none\")\n          .text((d.prop * 100).toFixed(1) + \"%\");\n      });\n      \n      cells.exit().remove();\n\n      d3.select(\"#heatmap-complete-empirical-nested\")\n        .selectAll(\"rect.cell\")\n        .data(complete_empirical_nested_pv_data)\n\n    })\n    .parent_el(\"#heatmap-complete-empirical-nested\")\n    .interpolate(false);\n  \n  d3.select(\"#heatmap-complete-empirical-nested\")\n    .html(\"\")\n    .datum(complete_empirical_nested_pv_data)\n    .call(heatmap_complete_empirical_nested);\n}\n\n\n\n\n\n\n\nThere are several takeaways one might have from this view, but the big one that I have is this: As you move your mouse (i.e.¬†the starting point of the pass) from the defender‚Äôs box to the opponent‚Äôs box, the consolidated green box of +0.025 PV doesn‚Äôt change much. It stays basically at around the final quarter of the pitch. So you can‚Äôt just complete a 30-yard pass from the top of your own box progressing the ball towards the middle of the pitch and expect to get anywhere near the same PV as completing a 30-yard pass from the center of the pitch to near the opponent‚Äôs 18-yard box. The end point really matters.\nThis conclusion gets at our primary question‚Äì‚ÄúAre all long passes good?‚Äù‚Äìto which the answer so far is ‚Äúnot quite‚Äù (in the sense that ‚Äúgood‚Äù is more than just ‚Äúpositive PV‚Äù for completed passes). A long completed pass in your own half doesn‚Äôt boast a huge positive PV, unless it ends up near the opponent‚Äôs box.\nTo get a more complete perspective, we‚Äôll plot out the PV for incomplete passes to see what the answer is there."
  },
  {
    "objectID": "posts/ball-progression-epv/index.html#incomplete-passes",
    "href": "posts/ball-progression-epv/index.html#incomplete-passes",
    "title": "Ball Progression is All You Need",
    "section": "Incomplete Passes",
    "text": "Incomplete Passes\n\nFrom One Spot, To Anywhere on the Pitch\nLet‚Äôs start with an example again, looking at PV for unsuccessful passes from the center spot.\n\n\nCode\n{\n  const chart = d3.create(\"div\").style(\"background-color\", \"8f8f8f\")\n  const title = chart.append(\"div\").attr(\"id\", \"heatmap-title-incomplete-empirical\")\n  title.append(\"p\").html(`PV: &lt;span id='pv-value-incomplete-empirical'&gt;0&lt;/span&gt;`)\n  chart.append(\"div\").attr(\"id\", \"heatmap-incomplete-empirical\")\n\n  const legendSwatchContainer = chart.append(\"div\")\n    .attr(\"id\", \"heatmap-legend-incomplete-empirical\")\n    .style(\"display\", \"flex\")\n    .style(\"flex-direction\", \"column\")\n    .style(\"align-items\", \"center\")\n    .style(\"width\", \"100%\");\n\n  const legendRange = [\n    1.1 * d3.min(colorScaleIncompleteRange),\n    1.1 * d3.max(colorScaleIncompleteRange)\n  ];\n  const stepSize = (legendRange[1] - legendRange[0]) / (swatchParams.num - 1);\n  const legendSwatches = d3.range(legendRange[0], legendRange[1] + stepSize, stepSize);\n  legendSwatches[legendSwatches.length - 1] = legendRange[1];\n\n  const totalLegendWidth = swatchParams.width * swatchParams.num;\n\n  const swatchRow = legendSwatchContainer.append(\"div\")\n    .style(\"display\", \"flex\")\n    .style(\"justify-content\", \"center\")\n    .style(\"width\", \"100%\");\n\n  swatchRow.selectAll(\"div\")\n    .data(legendSwatches)\n    .enter()\n    .append(\"div\")\n    .style(\"width\", `${swatchParams.width}px`)\n    .style(\"height\", `${swatchParams.height}px`)\n    .style(\"background-color\", d =&gt; colorScaleIncomplete(d));\n\n  const labelRow = legendSwatchContainer.append(\"div\")\n    .style(\"display\", \"flex\")\n    .style(\"justify-content\", \"center\")\n    .style(\"width\", `${totalLegendWidth}px`);\n\n  labelRow.selectAll(\"span\")\n    .data(colorScaleIncompleteRange)\n    .enter()\n    .append(\"span\")\n    .text(d =&gt; {\n      if (d === d3.min(colorScaleIncompleteRange)) {\n        return d + \" &lt;=\";\n      } else if (d === d3.max(colorScaleIncompleteRange)) {\n        return \"&gt;= \" + d;\n      }\n      return d;\n    })\n    .style(\"flex\", d =&gt; d === 0 ? \"1\" : null)\n    .style(\"text-align\", \"center\")\n\n  return chart.node();\n}\n\n\n\n\n\n\nFigure¬†4: A heatmap showing the average possession value (PV) of historically incomplete passes from the center spot (annotated in blue) to all areas of the pitch. The relative frequency of unsuccessful passes from the center spot to each other cell is shown as a percentage. The exact PV value associated with an incomplete pass ending at the hover point can be viewed above the pitch. Black cells represent areas to which unsuccessful passes from the center spot have never been made.\n\n\n\n\n\nCode\n{\n  const heatmap_incomplete_empirical = d3_soccer.heatmap(pitch)\n    .colorScale(colorScaleIncomplete)\n    .enableInteraction(true)\n    .onSelect((x, y, v) =&gt; {\n      const cappedValue = Math.min(Math.max(v, -1), 1);\n      d3.select(\"#pv-value-incomplete-empirical\").text(cappedValue.toFixed(3));\n    })\n    .parent_el(\"#heatmap-incomplete-empirical\")\n    .interpolate(false);\n\n  d3.select(\"#heatmap-incomplete-empirical\")\n    .html(\"\")\n    .datum(incomplete_empirical_pv_data)\n    .call(heatmap_incomplete_empirical);\n\n  const svg = d3.select(\"#heatmap-incomplete-empirical\").select(\"svg\");\n\n  const cells = svg.selectAll(\".cell\");\n\n  cells.each(function(d, i) {\n    const cell = d3.select(this);\n    const bbox = this.getBBox();\n\n    d3.select(this.parentNode)\n      .append(\"text\")\n      .attr(\"x\", bbox.x + bbox.width / 2)\n      .attr(\"y\", bbox.y + bbox.height / 2)\n      .attr(\"text-anchor\", \"middle\")\n      .attr(\"alignment-baseline\", \"central\")\n      .style(\"font-size\", \"3px\")\n      .style(\"pointer-events\", \"none\")\n      .text((d.prop * 100).toFixed(1) + \"%\");\n  });\n\n  svg.append(\"rect\")\n    .attr(\"x\", passStartParams.x)\n    .attr(\"y\", passStartParams.y)\n    .attr(\"width\", cellParams.width)\n    .attr(\"height\", cellParams.height)\n    .style(\"stroke\", \"blue\")\n    .style(\"fill\", \"none\")\n    .style(\"stroke-width\", \"1px\");\n}\n\n\n\n\n\n\n\nI think this grid is fairly intuitive.5 Incomplete passes backward have fairly negative PVs, as those are turnovers probably setting up the opponent for good scoring opportunities. Incomplete passes forward mostly have neutral PVs, with some spots on the pitch having slightly positive PVs. Notably, a positive PV for an incomplete pass is a non-trivial revelation.\nSome of the positive PV cells include the area at the top of the 18-yard box, i.e.¬†‚Äúzone 14‚Äù. You can make the argument that the ‚Äúrisk‚Äù of losing possession to passes to zone 14 is justified from the potential to take a shot. Further, a loss of possession in this area can be advantageous, as it leaves the opponent likely in a vulnerable position.\n\n\nFrom Anywhere on the Pitch, To Anywhere on the Pitch\nNow let‚Äôs scale up our pass PV grid to all incomplete passes. As with the dynamic successful pass heatmap, hovering over a cell will show PV associated with unsuccessful passes from that point on the pitch.\n\n\nCode\n{\n  const chart = d3.create(\"div\").style(\"background-color\", \"8f8f8f\")\n  const title = chart.append(\"div\").attr(\"id\", \"heatmap-title-incomplete-empirical-nested\")\n  title.append(\"p\").html(`min PV: &lt;span id='pv-min-incomplete-empirical-nested'&gt;0&lt;/span&gt;, max PV: &lt;span id='pv-max-incomplete-empirical-nested'&gt;0&lt;/span&gt;`)\n  chart.append(\"div\").attr(\"id\", \"heatmap-incomplete-empirical-nested\")\n  \n  const legendSwatchContainer = chart.append(\"div\")\n    .attr(\"id\", \"heatmap-legend-incomplete-empirical-nested\")\n    .style(\"display\", \"flex\")\n    .style(\"flex-direction\", \"column\")\n    .style(\"align-items\", \"center\")\n    .style(\"width\", \"100%\");\n  \n  const legendRange = [\n    1.1 * d3.min(colorScaleIncompleteRange),\n    1.1 * d3.max(colorScaleIncompleteRange)\n  ];\n  const stepSize = (legendRange[1] - legendRange[0]) / (swatchParams.num - 1);\n  const legendSwatches = d3.range(legendRange[0], legendRange[1] + stepSize, stepSize);\n  legendSwatches[legendSwatches.length - 1] = legendRange[1];\n  \n  const totalLegendWidth = swatchParams.width * swatchParams.num;\n  \n  const swatchRow = legendSwatchContainer.append(\"div\")\n    .style(\"display\", \"flex\")\n    .style(\"justify-content\", \"center\")\n    .style(\"width\", \"100%\");\n  \n  swatchRow.selectAll(\"div\")\n    .data(legendSwatches)\n    .enter()\n    .append(\"div\")\n    .style(\"width\", `${swatchParams.width}px`)\n    .style(\"height\", `${swatchParams.height}px`)\n    .style(\"background-color\", d =&gt; colorScaleIncomplete(d));\n  \n  const labelRow = legendSwatchContainer.append(\"div\")\n    .style(\"display\", \"flex\")\n    .style(\"justify-content\", \"center\")\n    .style(\"width\", `${totalLegendWidth}px`);\n  \n  labelRow.selectAll(\"span\")\n    .data(colorScaleIncompleteRange)\n    .enter()\n    .append(\"span\")\n    .text(d =&gt; {\n      if (d === d3.min(colorScaleIncompleteRange)) {\n        return d + \" &lt;=\";\n      } else if (d === d3.max(colorScaleIncompleteRange)) {\n        return \"&gt;= \" + d;\n      }\n      return d;\n    })\n    .style(\"flex\", d =&gt; d === 0 ? \"1\" : null)\n    .style(\"text-align\", \"center\")\n  \n  return chart.node();\n}\n\n\n\n\n\n\nFigure¬†5: A heatmap showing the average possession value (PV) of historically incomplete pass from the hover spot to all areas on the pitch. The relative frequency of successful passes from the center spot to each other cell is shown as a percentage. The highest and lowest PV values across all end points associated with an incomplete pass from the hover point are shown above the pitch. Black cells represent areas to which unsuccessful passes from the hover spot have never been made.\n\n\n\n\n\nCode\n{  \n  const heatmap_incomplete_empirical_nested = d3_soccer.heatmap(pitch)\n    .colorScale(d3.scaleLinear().domain([-1, 1]).range([\"white\", \"white\"]))\n    .enableInteraction(true)\n    .onSelect((x, y, v) =&gt; {\n      const rawMinValue = d3.min(v, d =&gt; d.value);\n      const rawMaxValue = d3.max(v, d =&gt; d.value);\n      const minValue = Math.max(rawMinValue, -1);\n      const maxValue = Math.min(rawMaxValue, 1);\n  \n      d3.select('#pv-min-incomplete-empirical-nested').text(minValue.toFixed(3));\n      d3.select('#pv-max-incomplete-empirical-nested').text(maxValue.toFixed(3));\n      const cells = d3\n        .select(\"#heatmap-incomplete-empirical-nested\")\n        .selectAll(\"rect.cell\")\n        .data(v);\n  \n      cells.enter()\n        .merge(cells)\n        .attr(\"x\", d =&gt; d.x)\n        .attr(\"y\", d =&gt; d.y)\n        .attr(\"width\", d =&gt; d.width)\n        .attr(\"height\", d =&gt; d.height)\n        .style(\"fill\", d =&gt; colorScaleIncomplete(+d.value));\n        \n      d3.select(\"#heatmap-incomplete-empirical-nested\")\n        .selectAll(\"text\")\n        .remove();\n        \n      cells.each(function(d, i) {\n        const cell = d3.select(this.parentNode);\n        const bbox = this.getBBox();\n        cell.append(\"text\")\n          .attr(\"x\", bbox.x + bbox.width / 2)\n          .attr(\"y\", bbox.y + bbox.height / 2)\n          .attr(\"text-anchor\", \"middle\")\n          .attr(\"alignment-baseline\", \"central\")\n          .style(\"font-size\", \"3px\")\n          .style(\"pointer-events\", \"none\")\n          .text((d.prop * 100).toFixed(1) + \"%\");\n      });\n      \n      cells.exit().remove();\n  \n      d3.select(\"#heatmap-incomplete-empirical-nested\")\n        .selectAll(\"rect.cell\")\n        .data(incomplete_empirical_nested_pv_data);\n    })\n    .parent_el(\"#heatmap-incomplete-empirical-nested\")\n    .interpolate(false);\n  \n  d3.select(\"#heatmap-incomplete-empirical-nested\")\n    .html(\"\")\n    .datum(incomplete_empirical_nested_pv_data)\n    .call(heatmap_incomplete_empirical_nested);\n}\n\n\n\n\n\n\n\nHovering my mouse over various areas in the middle third of the pitch, I consistently see slightly positive values near the top of the 18-yard box. This is not all that dissimilar from the trend observed with the successful pass pitch, where the passes into the final quarter of the pitch had strong positive PV from basically anywhere. And, like the interactive pitch for completed passes, a 30-yard incomplete pass forward from one‚Äôs own 18-yard box doesn‚Äôt have the same PV as a 30-yard incomplete pass forward from the half line to the opponent‚Äôs 18-yard box. Not all long incomplete passes are judged equally."
  },
  {
    "objectID": "posts/ball-progression-epv/index.html#caveats",
    "href": "posts/ball-progression-epv/index.html#caveats",
    "title": "Ball Progression is All You Need",
    "section": "Caveats",
    "text": "Caveats\n\nThe choice of model surely plays a role in the inference we‚Äôll make. Even atomic VAEP, the cooler younger brother to the baseline VAEP model, may yield different answers due to the way that it treats passes.6\nAlong the same lines, the gradients in the pitches are only as ‚Äúgood‚Äù as the quality of the model. While the cells show average PV values over many passes7, the PV value may not match intuition if the model doesn‚Äôt account for all relevant factors. If headed passes weren‚Äôt treated differently from footed passes, the gradients would likely show a lot more noise due to the randomness at which headed passes are successfully made.\nThe endpoint of incomplete passes is subject to a fundamental source of noise‚Äìinterception locations. I‚Äôve implicitly assumed that unsuccessful passes are intercepted very near the intended target, but this is not always the case. Interceptions where, for example, the defender blocks a long through ball near where the pass is made, can skew the model training, exaggerating the value of short incomplete passes."
  },
  {
    "objectID": "posts/ball-progression-epv/index.html#footnotes",
    "href": "posts/ball-progression-epv/index.html#footnotes",
    "title": "Ball Progression is All You Need",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nExcept in this post, where I only briefly mention that I use a PV model.‚Ü©Ô∏é\nMy model is trained on 2013/14 - 2023/24 English Premier League data.‚Ü©Ô∏é\nWhile all PV models are similar conceptually, it‚Äôs important to identify how they differ in their target variables. VAEP specifically tries to quantify the difference in the probability of scoring and conceding in the next 10 actions. In contrast, expected threat (xT)‚Äìperhaps the most well-known PV model‚Äìtries to quantify only the probability of scoring in the next 5 actions, not accounting for the conceding probability, which can undermine the ‚Äúrisk‚Äù associated with incomplete passes.‚Ü©Ô∏é\nDo not be alarmed by the small values! Values between 0.02 and 0.02 are very common for PV models. After all, the values represent goal probabilities over sequence of actions, and goals don‚Äôt happen all that frequently in soccer.‚Ü©Ô∏é\nWe observe lots of missingness near the defender‚Äôs box. Such incomplete passes backward would be very illogical no matter the game situation, so it‚Äôs not surprising to see that such passes are not observed in our data set.‚Ü©Ô∏é\nAtomic VAEP splits passes into two actions‚Äìthe pass itself and the reception (or lack of).‚Ü©Ô∏é\nThere are over 3.4M total passes in the data set.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/ball-progression-epv/index.html#vaep",
    "href": "posts/ball-progression-epv/index.html#vaep",
    "title": "Ball Progression is All You Need",
    "section": "VAEP",
    "text": "VAEP\nFor those really interested in the details, the PV I‚Äôve been showing is actually the goal probabilities underlying the VAEP framework, but not actually VAEP. In other words, I‚Äôve been showing\n\\[\nP_{\\text{goal}}(S_i, x) = P_{\\text{scores}}(S_i, x) + (-P_{\\text{concedes}}(S_i, x))\n\\tag{1}\\]\nwhere \\(S_i\\) is the \\(i\\)th game state and \\(x\\) is the team, either home or visiting. But VAEP is actually\n\\[\nV(a_i, x) = \\Delta P_{\\text{scores}}(a_i, x) + (-\\Delta P_{\\text{concedes}}(a_i, x))\n\\tag{2}\\]\nwhere\n\\[\n\\Delta P_{\\text{scores}}(a_i, x) = P_{\\text{scores}}(S_i, x) - P_{\\text{scores}}(S_{i-1}, x)\n\\tag{3}\\]\nfor action \\(a_i\\) moving the game from state \\(S_{i-1}\\) to \\(S_i\\), and where \\(\\Delta P_{\\text{concedes}}(a_i, x)\\) is defined similarly.\nVAEP directly reflects the value added by an action relative to the prior action. For those who have worked with expected threat (xT) before, this is analogous to the ‚ÄúxT created‚Äù metric, as described by Singh.\n\n‚Ä¶ [T]he point of xT was to come up with a metric that can quantify threat at any location on the pitch‚Ä¶ [W]e can value individual player actions in buildup play by computing the difference in xT between the start and end locations. In other words, we will say that an action that moves the ball from location \\((x,y)\\) to location \\((z,w)\\) has value \\(\\texttt{xT}_{z,w} - \\texttt{xT}_{x,y}\\).\n\n\nComplete Passes\nAssuming the reader is comfortable with the plotting style and notations before, we now skip to re-creating the dynamic completed pass pitch.\n\n\nCode\n{\n  const chart = d3.create(\"div\").style(\"background-color\", \"8f8f8f\")\n  const title = chart.append(\"div\").attr(\"id\", \"heatmap-vaep-title-complete-empirical-nested\")\n  title.append(\"p\").html(`min VAEP: &lt;span id='vaep-min-complete-empirical-nested'&gt;0&lt;/span&gt;, max VAEP: &lt;span id='vaep-max-complete-empirical-nested'&gt;0&lt;/span&gt;`)\n  chart.append(\"div\").attr(\"id\", \"heatmap-vaep-complete-empirical-nested\")\n  \n  const legendSwatchContainer = chart.append(\"div\")\n    .attr(\"id\", \"heatmap-vaep-legend-complete-empirical-nested\")\n    .style(\"display\", \"flex\")\n    .style(\"flex-direction\", \"column\")\n    .style(\"align-items\", \"center\")\n    .style(\"width\", \"100%\");\n  \n  const legendRange = [\n    1.1 * d3.min(colorScaleCompleteRange),\n    1.1 * d3.max(colorScaleCompleteRange)\n  ];\n  const stepSize = (legendRange[1] - legendRange[0]) / (swatchParams.num - 1);\n  const legendSwatches = d3.range(legendRange[0], legendRange[1] + stepSize, stepSize);\n  legendSwatches[legendSwatches.length - 1] = legendRange[1];\n  \n  const totalLegendWidth = swatchParams.width * swatchParams.num;\n  \n  const swatchRow = legendSwatchContainer.append(\"div\")\n    .style(\"display\", \"flex\")\n    .style(\"justify-content\", \"center\")\n    .style(\"width\", \"100%\");\n  \n  swatchRow.selectAll(\"div\")\n    .data(legendSwatches)\n    .enter()\n    .append(\"div\")\n    .style(\"width\", `${swatchParams.width}px`)\n    .style(\"height\", `${swatchParams.height}px`)\n    .style(\"background-color\", d =&gt; colorScaleComplete(d));\n  \n  const labelRow = legendSwatchContainer.append(\"div\")\n    .style(\"display\", \"flex\")\n    .style(\"justify-content\", \"center\")\n    .style(\"width\", `${totalLegendWidth}px`);\n  \n  \n  labelRow.selectAll(\"span\")\n    .data(colorScaleCompleteRange)\n    .enter()\n    .append(\"span\")\n    .text(d =&gt; {\n      if (d === d3.min(colorScaleCompleteRange)) {\n        return d + \" &lt;=\";\n      } else if (d === d3.max(colorScaleCompleteRange)) {\n        return \"&gt;= \" + d;\n      }\n      return d;\n    })\n    .style(\"flex\", d =&gt; d === 0 ? \"1\" : null)\n    .style(\"text-align\", \"center\")\n  \n  return chart.node();\n\n}\n\n\n\n\n\n\nFigure¬†6: A heatmap showing the average VAEP of historically completed pass from the hover spot to all areas on the pitch. The relative frequency of successful passes from the hover spot to each other cell is shown as a percentage. The highest and lowest VAEP values across all end points associated with a completed pass from the hover point are shown above the pitch. Black cells represent areas to which successful passes from the hover spot have never been made.\n\n\n\n\n\nCode\n{  \n  const heatmap_vaep_complete_empirical_nested = d3_soccer.heatmap(pitch)\n    .colorScale(d3.scaleLinear().domain([-1, 1]).range([\"white\", \"white\"]))\n    .enableInteraction(true)\n    .onSelect((x, y, v) =&gt; {\n      const rawMinValue = d3.min(v, d =&gt; d.value);\n      const rawMaxValue = d3.max(v, d =&gt; d.value);\n      const minValue = Math.max(rawMinValue, -1);\n      const maxValue = Math.min(rawMaxValue, 1);\n  \n      d3.select(\"#vaep-min-complete-empirical-nested\").text(minValue.toFixed(3));\n      d3.select(\"#vaep-max-complete-empirical-nested\").text(maxValue.toFixed(3));\n      const cells = d3\n        .select(\"#heatmap-vaep-complete-empirical-nested\")\n        .selectAll(\"rect.cell\")\n        .data(v)\n  \n      cells.enter()\n        .merge(cells)\n        .attr(\"x\", d =&gt; d.x)\n        .attr(\"y\", d =&gt; d.y)\n        .attr(\"width\", d =&gt; d.width)\n        .attr(\"height\", d =&gt; d.height)\n        .style(\"fill\", d =&gt; colorScaleComplete(+d.value));\n\n      d3.select(\"#heatmap-vaep-complete-empirical-nested\")\n        .selectAll(\"text\")\n        .remove();\n        \n      cells.each(function(d, i) {\n        const cell = d3.select(this.parentNode);\n        const bbox = this.getBBox();\n        cell.append(\"text\")\n          .attr(\"x\", bbox.x + bbox.width / 2)\n          .attr(\"y\", bbox.y + bbox.height / 2)\n          .attr(\"text-anchor\", \"middle\")\n          .attr(\"alignment-baseline\", \"central\")\n          .style(\"font-size\", \"3px\")\n          .style(\"pointer-events\", \"none\")\n          .text((d.prop * 100).toFixed(1) + \"%\");\n      });\n      \n      cells.exit().remove();\n\n      d3.select(\"#heatmap-vaep-complete-empirical-nested\")\n        .selectAll(\"rect.cell\")\n        .data(complete_empirical_nested_vaep_data)\n\n    })\n    .parent_el(\"#heatmap-vaep-complete-empirical-nested\")\n    .interpolate(false);\n  \n  d3.select(\"#heatmap-vaep-complete-empirical-nested\")\n    .html(\"\")\n    .datum(complete_empirical_nested_vaep_data)\n    .call(heatmap_vaep_complete_empirical_nested);\n}\n\n\n\n\n\n\n\nThe big takeaway for me here is that there are a lot more cells on the pitch showing negative values (now VAEP instead of ‚ÄúPV‚Äù), especially for passes backward. This makes sense, as the model should see that, on average, such passes put the ball in a less advantageous position.\nRecall that our pre-Appendix ‚ÄúPV‚Äù pitches account for the probability of conceding. Instances in which the pre-Appendix complete pass pitch shows a negative value indicate a pass start-end pair in which the probability of conceding increases more than the probability of scoring increases (or instances in which the probability of conceding decreases less than the probability of scoring decreases). Naturally, this resulted in a few negative start-to-end pass location combinations, particularly for passes sent very far backward. But now that we‚Äôre also accounting for the value of the prior action with VAEP, the pitch shows a lot more negatively valued start-end pairs, particularly for short passes backward.\n\n\nIncomplete Passes\nAnd now we re-create the dynamic pitch for incomplete passes, but showing VAEP instead of goal probability.\n\n\nCode\n{\n  const chart = d3.create(\"div\").style(\"background-color\", \"8f8f8f\")\n  const title = chart.append(\"div\").attr(\"id\", \"heatmap-vaep-title-incomplete-empirical-nested\")\n  title.append(\"p\").html(`min VAEP: &lt;span id='vaep-min-incomplete-empirical-nested'&gt;0&lt;/span&gt;, max VAEP: &lt;span id='vaep-max-incomplete-empirical-nested'&gt;0&lt;/span&gt;`)\n  chart.append(\"div\").attr(\"id\", \"heatmap-vaep-incomplete-empirical-nested\")\n  \n  const legendSwatchContainer = chart.append(\"div\")\n    .attr(\"id\", \"heatmap-vaep-legend-incomplete-empirical-nested\")\n    .style(\"display\", \"flex\")\n    .style(\"flex-direction\", \"column\")\n    .style(\"align-items\", \"center\")\n    .style(\"width\", \"100%\");\n  \n  const legendRange = [\n    1.1 * d3.min(colorScaleIncompleteRange),\n    1.1 * d3.max(colorScaleIncompleteRange)\n  ];\n  const stepSize = (legendRange[1] - legendRange[0]) / (swatchParams.num - 1);\n  const legendSwatches = d3.range(legendRange[0], legendRange[1] + stepSize, stepSize);\n  legendSwatches[legendSwatches.length - 1] = legendRange[1];\n  \n  const totalLegendWidth = swatchParams.width * swatchParams.num;\n  \n  const swatchRow = legendSwatchContainer.append(\"div\")\n    .style(\"display\", \"flex\")\n    .style(\"justify-content\", \"center\")\n    .style(\"width\", \"100%\");\n  \n  swatchRow.selectAll(\"div\")\n    .data(legendSwatches)\n    .enter()\n    .append(\"div\")\n    .style(\"width\", `${swatchParams.width}px`)\n    .style(\"height\", `${swatchParams.height}px`)\n    .style(\"background-color\", d =&gt; colorScaleIncomplete(d));\n  \n  const labelRow = legendSwatchContainer.append(\"div\")\n    .style(\"display\", \"flex\")\n    .style(\"justify-content\", \"center\")\n    .style(\"width\", `${totalLegendWidth}px`);\n  \n  labelRow.selectAll(\"span\")\n    .data(colorScaleIncompleteRange)\n    .enter()\n    .append(\"span\")\n    .text(d =&gt; {\n      if (d === d3.min(colorScaleIncompleteRange)) {\n        return d + \" &lt;=\";\n      } else if (d === d3.max(colorScaleIncompleteRange)) {\n        return \"&gt;= \" + d;\n      }\n      return d;\n    })\n    .style(\"flex\", d =&gt; d === 0 ? \"1\" : null)\n    .style(\"text-align\", \"center\")\n  \n  return chart.node();\n}\n\n\n\n\n\n\nFigure¬†7: A heatmap showing the average VAEP of historically incomplete pass from the hover spot to all areas on the pitch. The relative frequency of successful passes from the hover spot to each other cell is shown as a percentage. The highest and lowest VAEP values across all end points associated with an incomplete pass from the hover point are shown above the pitch. Black cells represent areas to which unsuccessful passes from the hover spot have never been made.\n\n\n\n\n\nCode\n{  \n  const heatmap_vaep_incomplete_empirical_nested = d3_soccer.heatmap(pitch)\n    .colorScale(d3.scaleLinear().domain([-1, 1]).range([\"white\", \"white\"]))\n    .enableInteraction(true)\n    .onSelect((x, y, v) =&gt; {\n      const rawMinValue = d3.min(v, d =&gt; d.value);\n      const rawMaxValue = d3.max(v, d =&gt; d.value);\n      const minValue = Math.max(rawMinValue, -1);\n      const maxValue = Math.min(rawMaxValue, 1);\n  \n      d3.select('#vaep-min-incomplete-empirical-nested').text(minValue.toFixed(3));\n      d3.select('#vaep-max-incomplete-empirical-nested').text(maxValue.toFixed(3));\n      const cells = d3\n        .select(\"#heatmap-vaep-incomplete-empirical-nested\")\n        .selectAll(\"rect.cell\")\n        .data(v);\n  \n      cells.enter()\n        .merge(cells)\n        .attr(\"x\", d =&gt; d.x)\n        .attr(\"y\", d =&gt; d.y)\n        .attr(\"width\", d =&gt; d.width)\n        .attr(\"height\", d =&gt; d.height)\n        .style(\"fill\", d =&gt; colorScaleIncomplete(+d.value));\n        \n      d3.select(\"#heatmap-vaep-incomplete-empirical-nested\")\n        .selectAll(\"text\")\n        .remove();\n        \n      cells.each(function(d, i) {\n        const cell = d3.select(this.parentNode);\n        const bbox = this.getBBox();\n        cell.append(\"text\")\n          .attr(\"x\", bbox.x + bbox.width / 2)\n          .attr(\"y\", bbox.y + bbox.height / 2)\n          .attr(\"text-anchor\", \"middle\")\n          .attr(\"alignment-baseline\", \"central\")\n          .style(\"font-size\", \"3px\")\n          .style(\"pointer-events\", \"none\")\n          .text((d.prop * 100).toFixed(1) + \"%\");\n      });\n      \n      cells.exit().remove();\n  \n      d3.select(\"#heatmap-vaep-incomplete-empirical-nested\")\n        .selectAll(\"rect.cell\")\n        .data(incomplete_empirical_nested_vaep_data);\n    })\n    .parent_el(\"#heatmap-vaep-incomplete-empirical-nested\")\n    .interpolate(false);\n  \n  d3.select(\"#heatmap-vaep-incomplete-empirical-nested\")\n    .html(\"\")\n    .datum(incomplete_empirical_nested_vaep_data)\n    .call(heatmap_vaep_incomplete_empirical_nested);\n}\n\n\n\n\n\n\n\nCompared to the pre-Appendix dynamic pitch for incomplete passes, this one shows a lot more negative values. In fact, there is only a very small subset of end points‚Äìthose near the penalty spot‚Äìwhere an incomplete pass can have positive VAEP, no matter the starting point. So, when accounting for the value of the prior action with VAEP, it appears that incomplete passes only have positive impact in a handful of situations.\n\n\nCode\npitch = d3_soccer.pitch()\n  .height(300)\n  .rotate(false)\n  .showDirOfPlay(true)\n  .shadeMiddleThird(false)\n  .pitchStrokeWidth(0.5)\n  .clip([[0, 0], [105, 68]]);\n\n\n\n\n\n\n\n\n\nCode\nd3 = require(\"d3@v5\")\n\n\n\n\n\n\n\n\n\nCode\nd3_soccer = require(\"d3-soccer@0.1.0\")\n\n\n\n\n\n\n\n\n\nCode\ncomplete_empirical_prop_data  = FileAttachment(\"complete_empirical_prop_data.json\").json()\n\n\n\n\n\n\n\n\n\nCode\ncomplete_empirical_pv_data  = FileAttachment(\"complete_empirical_pv_data.json\").json()\n\n\n\n\n\n\n\n\n\nCode\ncomplete_empirical_nested_pv_data = FileAttachment(\"complete_empirical_nested_pv_data.json\").json()\n\n\n\n\n\n\n\n\n\nCode\nincomplete_empirical_pv_data  = FileAttachment(\"incomplete_empirical_pv_data.json\").json()\n\n\n\n\n\n\n\n\n\nCode\nincomplete_empirical_nested_pv_data = FileAttachment(\"incomplete_empirical_nested_pv_data.json\").json()\n\n\n\n\n\n\n\n\n\nCode\ncomplete_empirical_nested_vaep_data = FileAttachment(\"complete_empirical_nested_vaep_data.json\").json()\n\n\n\n\n\n\n\n\n\nCode\nincomplete_empirical_nested_vaep_data = FileAttachment(\"incomplete_empirical_nested_vaep_data.json\").json()\n\n\n\n\n\n\n\n\n\nCode\ncolorScaleCompleteRange = [-0.025, 0, 0.025]\n\n\n\n\n\n\n\n\n\nCode\ncolorScaleIncompleteRange = [-0.025, 0, 0.025]\n\n\n\n\n\n\n\n\n\nCode\ncolorScaleComplete = d3.scaleLinear()\n  .domain(colorScaleCompleteRange)\n  .range([\"#a6611a\", \"white\", \"#018571\"]).clamp(true)\n\n\n\n\n\n\n\n\n\nCode\ncolorScaleIncomplete = d3.scaleLinear()\n  .domain(colorScaleIncompleteRange)\n  .range([\"#d01c8b\", \"white\", \"#4dac26\"]).clamp(true)\n\n\n\n\n\n\n\n\n\nCode\nswatchParams = {\n  return {\n    width: 40,\n    height: 20,\n    num: 7\n  }\n}\n\n\n\n\n\n\n\n\n\nCode\npassStartParams = {\n  return {\n    x: 43.75,\n    y: 34\n  }\n}\n\n\n\n\n\n\n\n\n\nCode\ncellParams = {\n  return {\n    width: 8.75,\n    height: 8.5\n  }\n}"
  },
  {
    "objectID": "drafts.html",
    "href": "drafts.html",
    "title": "Tony's Blog",
    "section": "",
    "text": "Good job finding this page. You must be a sicko creeping on my drafts.\n\n\n\n\n\n\n\n\n  \n\n\n\n\nShooting Performance Unlikeliness\n\n\n\n\n\nQuantifying how unlikely a player‚Äôs season-long shooting performance was\n\n\n\n\n\n\nMay 4, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "drafts/xg-likelihood/index.html",
    "href": "drafts/xg-likelihood/index.html",
    "title": "Shooting Performance Unlikeliness",
    "section": "",
    "text": "Towards the end of each soccer season, we naturally start to look back at player stats, often looking to see who has performed worse compared to their past seasons. We may have different motivations for doing so‚Äìe.g.¬†we may be trying to attribute team under-performance to individuals, we may be hypothesizing who is likely to be transferred or resigned, etc.\nIt‚Äôs not uncommon to ask ‚ÄúHow unlikely was their shooting performance this season?‚Äù when looking at a player who has scored less than goals than expected.1 For instance, if a striker only scores 9 goals on 12 expected goals (xG), their ‚Äúunderperformance‚Äù of 3 goals jumps off the page.\nThe ‚ÄúOutperformance‚Äù (\\(O_p\\)) ratio‚Äìthe ratio of a player \\(p\\)‚Äôs goals \\(G_p\\) to expected goals \\(xG_p\\)‚Äìis perhaps the most common way of evaluating a player‚Äôs shooting performance.2\n\\[\nO_p = \\frac{G_p}{xG_p}\n\\]\nAn \\(O_p\\) ratio of 1 indicates that a player is scoring as many goals as expected; a ratio greater than 1 indicates underperformance; and a ratio less than 1 indicates overperformance. Our hypothetical player underperformed with \\(O_p = \\frac{8}{12} = 0.67\\).\nIn most cases, we have prior seasons of data to use when evaluating a player‚Äôs \\(O_p\\) ratio for a given season. For example, let‚Äôs say our hypothetical player scored 14 goals on 10 xG (\\(O_p = 1.4\\)) in the season prior, and 12 goals on 8 xG (\\(O_p = 1.5\\)) before that. A \\(O_p = 0.75\\) after those seasons seems fairly unlikely, especially compared to an ‚Äúaverage‚Äù player who theoretically achieves \\(O_p = 1\\) ratio every year.\nSo how do we put a number on the unlikeliness of that \\(O_p = 0.75\\) for our hypothetical player, accounting for their prior season-long performances?\n\n\nI‚Äôll be using public data from FBref for the 2018/19 - 2023/24 seasons of the the Big 5 European soccer leagues, updated through April 25. Fake data is nice for examples, but ultimately we want to test our methods on real data. Our intuition about the results can be a useful caliber of the sensibility of our results.\n\n\nGet shot data\nraw_shots &lt;- worldfootballR::load_fb_match_shooting(\n  country = COUNTRIES,\n  tier = TIERS,\n  gender = GENDERS,\n  season_end_year = SEASON_END_YEARS\n)\n#&gt; ‚Üí Data last updated 2024-04-25 17:52:47 UTC\n\nnp_shots &lt;- raw_shots |&gt; \n  ## Drop penalties\n  dplyr::filter(\n    !dplyr::coalesce((Distance == '13' & round(as.double(xG), 2) == 0.79), FALSE)\n  ) |&gt; \n  dplyr::transmute(\n    season_end_year = Season_End_Year,\n    player_id = Player_Href |&gt; dirname() |&gt; basename(),\n    player = Player,\n    match_date = lubridate::ymd(Date),\n    match_id = MatchURL |&gt; dirname() |&gt; basename(),\n    minute = Minute,\n    g = as.integer(Outcome == 'Goal'),\n    xg = as.double(xG)\n  ) |&gt; \n  ## A handful of scored shots with empty xG\n  dplyr::filter(!is.na(xg)) |&gt; \n  dplyr::arrange(season_end_year, player_id, match_date, minute)\n\n## Use the more commonly used name when a player ID is mapped to multiple names\n##   (This \"bug\" happens because worldfootballR doesn't go back and re-scrape data\n##   when fbref makes a name update.)\nplayer_name_mapping &lt;- np_shots |&gt; \n  dplyr::count(player_id, player) |&gt; \n  dplyr::group_by(player_id) |&gt; \n  dplyr::slice_max(n, n = 1, with_ties = FALSE) |&gt; \n  dplyr::ungroup() |&gt; \n  dplyr::distinct(player_id, player)\n\nplayer_season_np_shots &lt;- np_shots |&gt; \n  dplyr::summarize(\n    .by = c(player_id, season_end_year), \n    shots = dplyr::n(),\n    dplyr::across(c(g, xg), sum)\n  ) |&gt; \n  dplyr::mutate(\n    o = g / xg\n  ) |&gt; \n  dplyr::left_join(\n    player_name_mapping,\n    by = dplyr::join_by(player_id)\n  ) |&gt; \n  dplyr::relocate(player, .after = player_id) |&gt; \n  dplyr::arrange(player_id, season_end_year)\nplayer_season_np_shots\n#&gt; # A tibble: 15,317 √ó 7\n#&gt;    player_id player          season_end_year shots     g    xg     o\n#&gt;    &lt;chr&gt;     &lt;chr&gt;                     &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1 0000acda  Marco Benassi              2018    70     5  4.01 1.25 \n#&gt;  2 0000acda  Marco Benassi              2019    59     7  5.61 1.25 \n#&gt;  3 0000acda  Marco Benassi              2020    20     1  1.01 0.990\n#&gt;  4 0000acda  Marco Benassi              2022    10     0  0.99 0    \n#&gt;  5 0000acda  Marco Benassi              2023    19     0  1.35 0    \n#&gt;  6 000b3da6  Manuel Iturra              2018     2     0  0.41 0    \n#&gt;  7 00242715  Moussa Niakhate            2018    16     0  1.43 0    \n#&gt;  8 00242715  Moussa Niakhate            2019    10     1  1.5  0.667\n#&gt;  9 00242715  Moussa Niakhate            2020    11     1  1.02 0.980\n#&gt; 10 00242715  Moussa Niakhate            2021     9     2  1.56 1.28 \n#&gt; # ‚Ñπ 15,307 more rows\n\n\nFor illustrative purposes, we‚Äôll focus on one player in particular‚ÄìJames Maddison. Maddison has had a sub-par 2023/2024 season for his own standards, underperforming his xG for the first time in since he started playing in the Premier League in 2018/19.\n\n\nMaddison‚Äôs season-by-season data\nplayer_season_np_shots |&gt; dplyr::filter(player == 'James Maddison')\n#&gt; # A tibble: 6 √ó 7\n#&gt;   player_id player         season_end_year shots     g    xg     o\n#&gt;   &lt;chr&gt;     &lt;chr&gt;                    &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 ee38d9c5  James Maddison            2019    81     6  5.85 1.03 \n#&gt; 2 ee38d9c5  James Maddison            2020    74     6  5.36 1.12 \n#&gt; 3 ee38d9c5  James Maddison            2021    75     8  3.86 2.07 \n#&gt; 4 ee38d9c5  James Maddison            2022    72    12  7.56 1.59 \n#&gt; 5 ee38d9c5  James Maddison            2023    83     9  7.12 1.26 \n#&gt; 6 ee38d9c5  James Maddison            2024    49     4  4.72 0.847\n\n\n\n\nMore variables useful for the rest of the post\nTARGET_SEASON_END_YEAR &lt;- 2024\n\nplayer_np_shots &lt;- player_season_np_shots |&gt; \n  dplyr::mutate(\n    is_target = season_end_year == TARGET_SEASON_END_YEAR\n  ) |&gt; \n  dplyr::summarize(\n    .by = c(is_target, player_id, player),\n    dplyr::across(\n      c(shots, g, xg),\n      \\(.x) sum(.x, na.rm = TRUE)\n    )\n  ) |&gt; \n  dplyr::mutate(o = g / xg) |&gt; \n  dplyr::arrange(player, player_id, is_target)\n\nwide_player_np_shots &lt;- player_np_shots |&gt;\n  dplyr::transmute(\n    player_id, \n    player,\n    which = ifelse(is_target, 'target', 'prior'), \n    shots, g, xg, o\n  ) |&gt; \n  tidyr::pivot_wider(\n    names_from = which, \n    values_from = c(shots, g, xg, o), \n    names_glue = '{which}_{.value}'\n  )\n\nall_players_to_evaluate &lt;- wide_player_np_shots |&gt; \n  tidyr::drop_na(prior_o, target_o) |&gt; \n  dplyr::filter(\n    prior_shots &gt;= 50,\n    target_shots &gt;= 10,\n    prior_g &gt; 0, \n    target_g &gt; 0\n  )\n\n\n\n\n\nI‚Äôll present 3 approaches to quantifying the ‚Äúunlikelihood‚Äù of a player ‚Äúunderperforming‚Äù relative to their prior \\(O_p\\) history.3 Note that I use ‚Äúprior‚Äù to refer to an aggregate of pre-2023/24 statistics, and ‚Äútarget‚Äù to refer to 2023/24.\nI‚Äôll discuss some of the strengths and weaknesses of each approach as we go along, then summarize the findings in the end.\n\n\nThe first approach I‚Äôll present is sort of a handcrafted ‚Äúranking‚Äù method.\n\nCalculate the proportional difference between the pre-target and target season outperformance ratios‚Äì\\(O_{p,\\text{target}'}\\) and \\(O_{p,\\text{target}'}\\) respectively‚Äìfor all players \\(P\\).\n\n\\[\n\\delta O_p = \\frac{O_{p,\\text{target}} - O_{p,\\text{target}'}}{O_{p,\\text{target}'}}\n\\]\n\nWeight \\(\\delta O^w_p\\) by the player‚Äôs \\(xG_p\\) accumulated in prior seasons.4\n\n\\[\n\\delta O^w_p = \\delta O_p * xG_p\n\\]\n\nCalculate the the underperforming unlikelihood of the outcome \\(U^-_p\\) as a percentile rank of ascending \\(\\delta O^w_p\\), i.e.¬†more negative \\(\\delta O^w_p\\) values correspond to a lower \\(U^-_p\\) percentile.5\n\nThis is pretty straightforward to calculate once you‚Äôve got the data prepared in the right format.\n\n\nApproach 1\n## `uu` for \"underperforming unlikelihood\"\nall_uu_approach1 &lt;- all_players_to_evaluate |&gt; \n  dplyr::transmute(\n    player_id,\n    player,\n    prior_o,\n    target_o,\n    prior_xg,\n    weighted_delta_o = prior_shots * (target_o - prior_o) / prior_o,\n    uu = dplyr::percent_rank(weighted_delta_o)\n  ) |&gt; \n  dplyr::arrange(uu)\n\n\n\n\nApproach 1 output for Maddison\nall_uu_approach1 |&gt; \n  dplyr::filter(player == 'James Maddison') |&gt; \n  dplyr::select(player, prior_o, target_o, prior_xg, uu) |&gt; \n  reprex_print()\n#&gt; # A tibble: 1 √ó 5\n#&gt;   player         prior_o target_o prior_xg     uu\n#&gt;   &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 James Maddison    1.38    0.847     29.8 0.0321\n\n\nThis approach finds Maddison‚Äôs 2023/24 \\(O_p\\) of 0.847 to be about a 3rd percentile outcome. Among the 593 player‚Äôs evaluated, Maddison‚Äôs 2023/24 \\(O_p\\) ranks as the 20th most unlikely.\nFor context, here‚Äôs a look at the top 10 most unlikely outcomes for the 2023/24 season.\n\n\nApproach 1 output, top 10 unlikeliest outcomes\nuu_approach1 |&gt; \n  head(10) |&gt; \n  dplyr::select(player, prior_o, target_o, prior_xg, uu)\n#&gt; # A tibble: 10 √ó 5\n#&gt;    player             prior_o target_o prior_xg      uu\n#&gt;    &lt;chr&gt;                &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n#&gt;  1 Ciro Immobile        1.23     0.383     80.6 0      \n#&gt;  2 Giovanni Simeone     1.03     0.306     56.0 0.00169\n#&gt;  3 Nabil Fekir          1.14     0.5       32.3 0.00338\n#&gt;  4 Wahbi Khazri         1.11     0.322     28   0.00507\n#&gt;  5 T√©ji Savanier        1.42     0.282     14.1 0.00676\n#&gt;  6 Adrien Thomasson     1.18     0.282     30.5 0.00845\n#&gt;  7 Timo Werner          0.951    0.543     74.6 0.0101 \n#&gt;  8 Ga√´tan Laborde       1.02     0.546     60.6 0.0118 \n#&gt;  9 Kevin Volland        1.18     0.450     55.0 0.0135 \n#&gt; 10 Robert Lewandowski   1.01     0.759    155.  0.0152 \n\n\nCiro Immobile tops the list, with several other notable attacking players who had less than stellar seasons.\nOverall, I‚Äôd say that this methodology seems to generate fairly reasonable results, but it‚Äôs hard to pinpoint why exactly this approach is defensible other than it may lead to intuitive results.\nA couple of notes about this methodology:\n\nThe weighting choice (pre-2023/24 xG) is subjective. An alternative choice of weighting could easily shuffle the result set. Nonetheless, some sort of weighting should be applied. If none is applied, then players who shoot very few shots will appear as the most unlikely, and that simply does not match intuition.\nThe unlikelihood percentile is sensitive to the pool of players with which a given player is compared. If we decide to compare a striker with a pool of defenders, then our results would be susceptible to the noisy goals-to-expected goals ratio that defenders tend to have. Further, if a large majority of players in a given season happened to score more than their xG would imply, then a player scoring at a neutral pace would be penalized by this approach. Of course, such an outcome is unlikely, but speaks to the role that selection bias might have with influencing results with this methodology.\nWe assumee that the distribution of unlikeliness is uniform across all players in a given season. In other words, we‚Äôre assuming that there has to be 1% of players with a 1st percentile underperforming outcome, 17% of players with 17th percentile underperforming outcome, etc. I think this assumption makes sense in the long run, but in any given individual season, there may not be a single player who shots terribly enough to really deserve that ‚Äúworst of the worst‚Äù, 1st percentile outcome.\n\n\n\n\nThere‚Äôs only so much you can do with player-season level data. We need to dive into shot-level data if we want to more robustly understand uncertainty of outcomes.\nHere‚Äôs a ‚Äúresampling‚Äù approach to quantifying the unlikeliness of a player‚Äôs \\(O_p\\) in the target season:\n\nSample \\(N_{p,\\text{target}}\\) shots (with replacement6) from a player‚Äôs past shots \\(S_{p,\\text{target}'}\\). Repeat this for \\(R\\) resamples.78\nCount the number of resamples \\(r^-\\) in which the outperformance ratio \\(\\hat{O}_{p,\\text{target}'}\\) of the sampled shots is less than or equal to the observed \\(O_{p,\\text{target}}\\) in the target season for the player.9 The proportion \\(U^-_p = \\frac{r^-}{R}\\) represents the unlikeness of a given player‚Äôs observed \\(O_{p,\\text{target}}\\) (or worse) in the target season.\n\nHere‚Äôs how that looks in code.\n\n\nApproach 2\nR &lt;- 1000\nresample_player_shots &lt;- function(\n    shots, \n    n_shots_to_sample, \n    n_sims = R,\n    replace = TRUE,\n    seed = 42\n) {\n  \n  withr::local_seed(seed)\n  purrr::map_dfr(\n    1:n_sims,\n    \\(.sim) {\n      sampled_shots &lt;- shots |&gt; \n        slice_sample(n = n_shots_to_sample, replace = replace)\n      \n      list(\n        sim = .sim,\n        xg = sum(sampled_shots$xg),\n        g = sum(sampled_shots$g),\n        o = sum(sampled_shots$g) / sum(sampled_shots$xg)\n      )\n    }\n  )\n}\n\nresample_one_player_o &lt;- function(shots, target_season_end_year) {\n  target_shots &lt;- shots |&gt;\n    dplyr::filter(season_end_year == target_season_end_year)\n  \n  prior_shots &lt;- shots |&gt;\n    dplyr::filter(season_end_year &lt; target_season_end_year)\n  \n  prior_shots |&gt; \n    resample_player_shots(\n      n_shots_to_sample = nrow(target_shots)\n    )\n}\n\nresample_player_o &lt;- function(shots, players, target_season_end_year = TARGET_SEASON_END_YEAR) {\n  purrr::map_dfr(\n    players,\n    \\(.player) {\n      shots |&gt; \n        dplyr::filter(player == .player) |&gt; \n        resample_one_player_o(\n          target_season_end_year = target_season_end_year\n        ) |&gt; \n        dplyr::mutate(\n          player = .player\n        )\n    }\n  )\n}\n\nmaddison_resampled_o &lt;- np_shots |&gt; \n  resample_player_o(\n    players = 'James Maddison'\n  ) |&gt; \n  dplyr::inner_join(\n    wide_player_np_shots |&gt; \n      dplyr::select(\n        player,\n        prior_o,\n        target_o\n      ),\n    by = dplyr::join_by(player)\n  ) |&gt; \n  dplyr::arrange(player)\n\nmaddison_uu_approach2 &lt;- maddison_resampled_o |&gt;\n  dplyr::summarize(\n    .by = c(player, prior_o, target_o),\n    uu = sum(o &lt;= target_o) / n()\n  ) |&gt; \n  dplyr::arrange(player)\n\n\n\n\nApproach 2 output\nmaddison_uu_approach2 |&gt; dplyr::select(player, prior_o, target_o, uu)\n#&gt; # A tibble: 1 √ó 4\n#&gt;   player         prior_o target_o    uu\n#&gt;   &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 James Maddison    1.38    0.847 0.163\n\n\nThe plot below should provide a bit of visual intuition as to what‚Äôs going on.\n\nThese results imply that Maddison‚Äôs 2023/24 \\(G / xG\\) ratio of 0.847 (or worse) occurs in 16.3% of simulations, i.e.¬†a 16th percentile outcome. That‚Äôs a bit higher than what the first approach showed.\nHow can we feel more confident about this approach? Well, in the first approach, we made the assumption that the underperforming unlikelihood percentages should be uniform across all players, hence the percentile ranking. I think that‚Äôs a good assumption, so we should see if the same bears out with this second approach.\nThe plot below shows a histogram of the underperforming unlikelihood across all players, where each player‚Äôs estimated unlikelihood is grouped into a decile.\n\n\nApproach 2, but with all players\nall_resampled_o &lt;- np_shots |&gt; \n  resample_player_o(\n    players = all_players_to_evaluate$player\n  ) |&gt; \n  dplyr::inner_join(\n    wide_player_np_shots |&gt; \n      ## to make sure we just one Rodri, Danilo, and Nicol√°s Gonz√°lez \n      dplyr::filter(player_id %in% all_players_to_evaluate$player_id) |&gt; \n      dplyr::select(\n        player,\n        prior_o,\n        target_o,\n        prior_shots,\n        target_shots\n      ),\n    by = dplyr::join_by(player)\n  ) |&gt; \n  dplyr::arrange(player, player)\n\nall_uu_approach2 &lt;- all_resampled_o |&gt;\n  dplyr::summarize(\n    .by = c(player, prior_o, target_o, prior_shots, target_shots),\n    uu = sum(o &lt;= target_o) / n()\n  ) |&gt; \n  dplyr::arrange(uu)\n\n\n\nIndeed, the histogram shows a fairly uniform distribution, with a bit of irregularity at the very edges.\nLooking at who is in the lower end of the leftmost decile, we see some of the same names‚ÄìImmobile and Savanier‚Äìamong the ten underperformers.\n\n\nApproach 2 output, top 10 underperforming players\nall_uu_approach2 |&gt; head(10) |&gt; dplyr::select(player, prior_o, target_o, uu)\n#&gt; # A tibble: 593 √ó 4\n#&gt;    player                    prior_o target_o    uu\n#&gt;    &lt;chr&gt;                       &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1 Erling Haaland               1.26    0.791 0.004\n#&gt;  2 Amine Harit                  1.27    0.262 0.01 \n#&gt;  3 Pierre-Emerick Aubameyang    1.07    0.606 0.01 \n#&gt;  4 T√©ji Savanier                1.42    0.282 0.01 \n#&gt;  5 Antonio Sanabria             1.05    0.386 0.014\n#&gt;  6 Alex Baena                   1.54    0.348 0.016\n#&gt;  7 Elye Wahi                    1.38    0.753 0.017\n#&gt;  8 Kevin Behrens                1.39    0.673 0.019\n#&gt;  9 Ciro Immobile                1.23    0.383 0.02 \n#&gt; 10 Ansu Fati                    1.31    0.430 0.024\n\n\nOne familiar face in the print out above is Manchester City‚Äôs striker Erling Haaland, whose underperformance this season has been called among fans and the media. His sub-par performance this year only ranked as a 7th percentile outcome by approach 1.\nWithholding judgement on the superiority of any methodology, we can find some solace in seeing some of the same names among the most unlikely underperformers here as we did with approach 1.\nHere are some parting thoughts on this methodology before we look at another:\n\nWe‚Äôre making an assumption that a player‚Äôs past shot profile is representative of their future shot profile. This is reasonable in many cases, but has exceptions for a player who has just changed roles and/or teams, who has been recovering from injuries, etc.\nThis approach is ‚Äúnon-parametric‚Äù. We don‚Äôt assume anything about the underlying distribution of a player‚Äôs \\(O_p\\) other than that it is stable between the prior and target seasons. (See note above.) We let the power of resampling shape the distribution of outcomes, which should look different for a striker that only takes shots near the goal and a defender that only launches shots from outside the box.\nThis approach is computationally intensive (relative to the other approaches). Running this approach with just \\(R = 1000\\) can take several minutes across all players (without palatalization).\n\n\n\n\nIf we assume that the set of goals-to-xG ratios come from a Gamma data-generating process, then we can leverage the properties of a player-level Gamma distribution to assess the unlikelihood of a players \\(O_p\\) ratio.\nTo calculate the underperforming unlikeliness \\(U^-_p\\):\n\nEstimate a Gamma distribution \\(\\Gamma_{p,\\text{target}'}\\) to model a player‚Äôs true outperformance ratio \\(O_{p}\\) across all prior shots, excluding those in the target season‚Äì\\(\\hat{O}_{p,\\text{target}'}\\).\nCalculate the probability that \\(\\hat{O}_{p,\\text{target}'}\\) is less than or equal to the player‚Äôs observed \\(O_{p,\\text{target}}\\) in the target season using the Gamma distribution‚Äôs cumulative distribution function (CDF).\n\nWhile that may sound daunting, I promise that it‚Äôs not (well, aside from a bit of ‚Äúmagic‚Äù in estimating a reasonable Gamma distribution per player).\n\n\nApproach 3\nN_SIMS &lt;- 10000\n\nSHOT_TO_SHAPE_MAPPING &lt;- list(\n  'from' = c(50, 750),\n  'to' = c(1, 25)\n)\nestimate_one_gamma_distributed_o &lt;- function(\n    shots,\n    target_season_end_year\n) {\n  player_np_shots &lt;- shots |&gt; \n    dplyr::mutate(is_target = season_end_year == target_season_end_year)\n  \n  prior_player_np_shots &lt;- player_np_shots |&gt; \n    dplyr::filter(!is_target)\n  \n  target_player_np_shots &lt;- player_np_shots |&gt; \n    dplyr::filter(is_target)\n  \n\n  agg_player_np_shots &lt;- player_np_shots |&gt;\n    dplyr::summarize(\n      .by = c(is_target),\n      shots = dplyr::n(),\n      dplyr::across(c(g, xg), \\(.x) sum(.x))\n    ) |&gt; \n    dplyr::mutate(o = g / xg)\n  \n  agg_prior_player_np_shots &lt;- agg_player_np_shots |&gt; \n    dplyr::filter(!is_target)\n  \n  agg_target_player_np_shots &lt;- agg_player_np_shots |&gt; \n    dplyr::filter(is_target)\n\n  shape &lt;- dplyr::case_when(\n    agg_prior_player_np_shots$shots &lt; SHOT_TO_SHAPE_MAPPING$from[1] ~ SHOT_TO_SHAPE_MAPPING$to[2],\n    agg_prior_player_np_shots$shots &gt; SHOT_TO_SHAPE_MAPPING$from[2] ~ SHOT_TO_SHAPE_MAPPING$to[2],\n    TRUE ~ scales::rescale(\n      agg_prior_player_np_shots$shots, \n      from = SHOT_TO_SHAPE_MAPPING$from, \n      to = SHOT_TO_SHAPE_MAPPING$to\n    )\n  )\n  list(\n    'shape' = shape,\n    'rate' = shape / agg_prior_player_np_shots$o\n  )\n}\n\nestimate_gamma_distributed_o &lt;- function(\n    shots,\n    players,\n    target_season_end_year\n) {\n  \n  purrr::map_dfr(\n    players,\n    \\(.player) {\n      params &lt;- shots |&gt; \n        dplyr::filter(player == .player) |&gt; \n        estimate_one_gamma_distributed_o(\n          target_season_end_year = target_season_end_year\n        )\n      \n      list(\n        'player' = .player,\n        'params' = list(params)\n      )\n    }\n  )\n}\n\nselect_gamma_o &lt;- np_shots |&gt; \n  estimate_gamma_distributed_o(\n    players = 'James Maddison',\n    target_season_end_year = TARGET_SEASON_END_YEAR\n  ) |&gt; \n  dplyr::inner_join(\n    wide_player_np_shots |&gt; \n      dplyr::select(\n        player,\n        prior_o,\n        target_o\n      ),\n    by = dplyr::join_by(player)\n  ) |&gt; \n  dplyr::arrange(player)\n\nuu_approach3 &lt;- select_gamma_o |&gt; \n  dplyr::mutate(\n    uu = purrr::map2_dbl(\n      target_o,\n      params,\n      \\(.target_o, .params) {\n        pgamma(\n          .target_o, \n          shape = .params$shape, \n          rate = .params$rate,\n          lower.tail = TRUE\n        )\n      }\n    ),\n    ou = 1 - uu\n  ) |&gt; \n  tidyr::unnest_wider(params)\n\n\n\n\nApproach 3 output\nuu_approach3 |&gt; dplyr::select(player, prior_o, target_o, uu)\n#&gt; # A tibble: 1 √ó 4\n#&gt;   player         prior_o target_o     uu\n#&gt;   &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 James Maddison    1.38    0.847 0.0679\n\n\nWe see that Maddison‚Äôs 2023/24 \\(O_{p,\\text{target}}\\) ratio of 0.847 (or worse) is about a 7th percentile outcome given his prior shot history. The 7th percentile outcome estimated is about on par with the 6th percentile estimated with approach 1.\nTo gain some intuition around this approach, we can plot out the Gamma distributed estimate of Maddison‚Äôs \\(O_p\\). The result is a histogram that looks not all that dissimilar to the one from before with resampled shots, just much smoother (since this is a ‚Äúparametric‚Äù approach).\n\nAs with approach 2, we should check to see what the distribution of underperforming unlikeliness looks like‚Äìwe should expect to see a somewhat uniform distribution.\n\n\nApproach 3 for all players\nall_gamma_o &lt;- np_shots |&gt; \n  estimate_gamma_distributed_o(\n    players = all_players_to_evaluate$player,\n    target_season_end_year = TARGET_SEASON_END_YEAR\n  ) |&gt; \n  dplyr::inner_join(\n    wide_player_np_shots |&gt; \n      dplyr::filter(\n        player_id %in% all_players_to_evaluate$player_id\n      ) |&gt; \n      dplyr::select(\n        player,\n        prior_o,\n        target_o\n      ),\n    by = dplyr::join_by(player)\n  ) |&gt; \n  dplyr::arrange(player)\n\nall_uu_approach3 &lt;- all_gamma_o |&gt; \n  dplyr::mutate(\n    uu = purrr::map2_dbl(\n      target_o,\n      params,\n      \\(.target_o, .params) {\n        pgamma(\n          .target_o, \n          shape = .params$shape, \n          rate = .params$rate,\n          lower.tail = TRUE\n        )\n      }\n    )\n  ) |&gt; \n  tidyr::unnest_wider(params) |&gt; \n  dplyr::arrange(uu)\n\n\n\nThis histogram has a bit more distortion, i.e.¬†more players, in the highest decile then our resampling approach, so perhaps it‚Äôs a little less calibrated.\nLooking at the top 10 strongest underperformers, 3 of the names here‚ÄìImmobile, Savanier, and Sanabria‚Äìare shared with approach 2‚Äôs top 10, and 7 are shared with approach 1‚Äôs top 10.\n\n\nApproach 3 output, top 10 underperforming players\nall_uu_approach3 |&gt; head(10) |&gt; dplyr::select(player, prior_o, target_o, uu)\n#&gt; # A tibble: 10 √ó 4\n#&gt;    player           prior_o target_o         uu\n#&gt;    &lt;chr&gt;              &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n#&gt;  1 Ciro Immobile       1.23    0.383 0.00000533\n#&gt;  2 T√©ji Savanier       1.42    0.282 0.000127  \n#&gt;  3 Giovanni Simeone    1.03    0.306 0.000248  \n#&gt;  4 Adrien Thomasson    1.18    0.282 0.000346  \n#&gt;  5 Wahbi Khazri        1.11    0.322 0.000604  \n#&gt;  6 Fabi√°n Ruiz Pe√±a    1.67    0.510 0.00271   \n#&gt;  7 Nabil Fekir         1.14    0.5   0.00308   \n#&gt;  8 Kevin Volland       1.18    0.450 0.00408   \n#&gt;  9 Antonio Sanabria    1.05    0.386 0.00877   \n#&gt; 10 Nicol√≤ Barella      1.11    0.417 0.0138\n\n\nWe can visually check the consistency of the results from this method with the prior two with scatter plots of the estimated underperforming unlikeliness from each.\n\nIf two of the approaches were perfectly in agreement, then each point, representing one of the 593 evaluated players, would fall along the 45-degree slope 1 line. With that in mind, we can see that approach 3 is in a little bit more precisely in agreement with approach 1, but approach 3 tends to assign slightly higher percentiles to players on the whole. The results from approach 2 and 3 also have a fair degree of agreement, and the results are more equally calibrated.\nStepping back from the results, what can we say about the principles of the methodology?\n\nThis parametric approach is fixated on using a Gamma distribution. While a Gamma distribution is probably the best choice of any family of distribution‚Äìsince its used to model continuous random variables that are positive and skewed‚Äìthe reliance on a distribution in the first place can feel like a limiting factor.\nWhile I don‚Äôt showcase it, the results can be very sensitive to the choice of parameters used to define each player‚Äôs Gamma distribution. Increasing the shape and/or rate parameters by a few integer values (say, from 10 to 15) tighten the distribution and make tail probabilities appear more extreme (i.e.¬†a 12th percentile outcome would become a 2nd percentile outcome). On the other hand, the Gamma distribution is fairly flexible, so the freedom to choose parameters allows one to tune uncertainty to one‚Äôs desire."
  },
  {
    "objectID": "drafts/xg-likelihood/index.html#analysis",
    "href": "drafts/xg-likelihood/index.html#analysis",
    "title": "Shooting Performance Likeliness",
    "section": "",
    "text": "If you have some background in statistics, perhaps the first approach that comes to mind is a \\(t\\)-test (using shot-weighted averages and standard deviations).\n\n\nApproach 0\nTARGET_SEASON_END_YEAR &lt;- 2024\n\nselect_t_test &lt;- select_np_season_shooting |&gt; \n  dplyr::filter(season_end_year &lt; TARGET_SEASON_END_YEAR) |&gt; \n  dplyr::summarise(\n    .by = c(player_id, player),\n    mean = weighted.mean(o, w = shots),\n    ## could also use a function like Hmisc::wtd.var for weighted variance\n    sd = sqrt(sum(shots * (o - weighted.mean(o, w = shots))^2) / sum(shots))\n  ) |&gt; \n  dplyr::inner_join(\n    select_np_season_shooting |&gt; \n      dplyr::filter(season_end_year == TARGET_SEASON_END_YEAR) |&gt; \n      dplyr::select(player_id, target_o = o),\n    by = dplyr::join_by(player_id)\n  ) |&gt; \n  dplyr::mutate(\n    z_score = (target_o - mean) / sd,\n    p_value = 2 * pnorm(-abs(z_score))\n  ) |&gt; \n  dplyr::arrange(player)\n\n\n\n\nOutput from Approach 0\nselect_t_test\n#&gt; # A tibble: 2 √ó 7\n#&gt;   player_id player          mean    sd target_o z_score p_value\n#&gt;   &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 ee38d9c5  James Maddison 1.40  0.378    0.847   -1.47  0.141 \n#&gt; 2 dc62b55d  Matheus Cunha  0.757 0.182    1.16     2.24  0.0250\n\n\nIn reality, this isn‚Äôt giving us a percentage of likelihood of the outcome. Rather, the p-value measures the probability of obtaining an outperformance as extreme as the one observed in 2023/24 (or more extreme) if the null hypothesis is true. The null hypothesis in this case would be that there is no significant difference between the player‚Äôs actual outperformance ratio in the 2023/24 ‚Äútarget‚Äù season and the distribution of outperformance ratios observed in previous seasons.\nThe t-test indicates that Cunha‚Äôs goals-to-xG ratio this year violates the null hypothesis, suggesting that this season has been significantly remarkable for him. On the other hand, the t-test indicates that there is not sufficient evidence that Maddison‚Äôs \\(O_p\\) this season is significantly worse than his \\(O_p\\) in prior seasons.\n\n\n\nWe‚Äôre not going to get very far just using a traditional approach, nor just by looking at the player-season level. We need to dive into shot-level data to more robustly understand uncertainty.\nHere‚Äôs a more sophisticated approach to quantifying the unlikeliness of a ‚Äútarget‚Äù season‚Äôs \\(G / xG\\) ratio:\n\nSample \\(N_{p,\\text{target}}\\) shots (with replacement2) from a player‚Äôs past shots \\(S_{p,\\text{target}'}\\). Repeat this for \\(R\\) resamples.3\nThen, to quantify unlikeliness of an underperforming season, count the number of resamples \\(r^-\\) in which the outperformance ratio \\(\\hat{O}_{p,\\text{target}'}\\) of the sampled shots is less than or equal to the observed \\(O_{p,\\text{target}}\\) ratio in the target season for the player.4 The proportion \\(\\Pi^- = \\frac{r^-}{R}\\) represents the unlikeness of a given player‚Äôs observed \\(O_{p,\\text{target}}\\) in the target season.\n\nHere‚Äôs how that looks in code.\n\n\nApproach 1\n## To have a variable to reference for player-level target and aggregate prior stats\nplayer_np_shots &lt;- player_season_np_shots |&gt; \n  dplyr::mutate(\n    is_target = season_end_year == TARGET_SEASON_END_YEAR\n  ) |&gt; \n  dplyr::summarize(\n    .by = c(is_target, player_id, player),\n    dplyr::across(\n      c(shots, g, xg),\n      \\(.x) sum(.x, na.rm = TRUE)\n    )\n  ) |&gt; \n  dplyr::mutate(o = g / xg) |&gt; \n  dplyr::arrange(player, player_id, is_target)\n\nwide_player_np_shots &lt;- player_np_shots |&gt;\n  dplyr::transmute(\n    player_id, \n    player,\n    which = ifelse(is_target, 'target', 'prior'), \n    shots, g, xg, o\n  ) |&gt; \n  tidyr::pivot_wider(\n    names_from = which, \n    values_from = c(shots, g, xg, o), \n    names_glue = '{which}_{.value}'\n  )\n\nresample_player_shots &lt;- function(\n    shots, \n    n_shots_to_sample, \n    n_sims = 1000,\n    replace = TRUE,\n    seed = 42\n) {\n  \n  withr::local_seed(seed)\n  purrr::map_dfr(\n    1:n_sims,\n    \\(.sim) {\n      sampled_shots &lt;- shots |&gt; \n        slice_sample(n = n_shots_to_sample, replace = replace)\n      \n      list(\n        sim = .sim,\n        xg = sum(sampled_shots$xg),\n        g = sum(sampled_shots$g),\n        o = sum(sampled_shots$g) / sum(sampled_shots$xg)\n      )\n    }\n  )\n}\n\nresample_player_o &lt;- function(shots, players, target_season_end_year) {\n  purrr::imap_dfr(\n    players,\n    \\(.player, .player_id) {\n      player_shots &lt;- shots |&gt; \n        dplyr::filter(player_id == .player_id)\n      \n      target_shots &lt;- player_shots |&gt;\n        dplyr::filter(season_end_year == target_season_end_year)\n      \n      player_shots |&gt;\n        dplyr::filter(season_end_year &lt; target_season_end_year) |&gt; \n        resample_player_shots(\n          n_shots_to_sample = nrow(target_shots)\n        ) |&gt; \n        dplyr::mutate(\n          player_id = .player_id,\n          player = .player\n        )\n    }\n  )\n}\n\nselect_resampled_o &lt;- np_shots |&gt; \n  resample_player_o(\n    players = SELECT_PLAYERS,\n    target_season_end_year = TARGET_SEASON_END_YEAR\n  ) |&gt; \n  dplyr::inner_join(\n    wide_player_np_shots |&gt; \n      dplyr::select(\n        player_id,\n        prior_o,\n        target_o\n      ),\n    by = dplyr::join_by(player_id)\n  ) |&gt; \n  dplyr::arrange(player, player_id)\n\nselect_resampled_props &lt;- select_resampled_o |&gt;\n  dplyr::summarize(\n    .by = c(player_id, player, prior_o, target_o),\n    prop_lte = sum(o &lt;= target_o) / n()\n  ) |&gt; \n  dplyr::arrange(player)\n\n\n\n\nOutput from Approach 1\nselect_resampled_props\n#&gt; # A tibble: 2 √ó 5\n#&gt;   player_id player         prior_o target_o prop_lte\n#&gt;   &lt;chr&gt;     &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 ee38d9c5  James Maddison   1.38     0.847    0.163\n#&gt; 2 dc62b55d  Matheus Cunha    0.772    1.16     0.898\n\n\nThese results imply that Maddison‚Äôs 2023/24 \\(G / xG\\) ratio of 0.847 (or worse) occurs in 16.3% of simulations (prop_lte), and that Cunha‚Äôs 2023/24 \\(O\\) ratio of 1.16 (or better) occurs in 10.3% of simulations (1 - prop_lte).5 Honestly, these numbers feel fairly reasonable, but perhaps a bit too opinionated.\nThis approach is fairly simplistic and elegant‚Äìwe just sample shots from a player‚Äôs history and count up the outcomes with respect to a target threshold. The plot below should provide a bit of visual intuition as to what‚Äôs going on.\n\n\n\nSo is this good enough? Maybe. There are some caveats I can think of:\n\nWe‚Äôre implicitly making an assumption that a player‚Äôs past shot profile is representative of their future shot profile.\nWe‚Äôre effectively treating each player‚Äôs \\(G / xG\\) ratio as constant and only trying to understand the uncertainty around it.\n\nThese things don‚Äôt necessarily mean that this methodology is bad‚Äìit just has its caveats. On the other hand, one notable advantage of this approach is that it is ‚Äúnon-parametric‚Äù‚Äìwe don‚Äôt assume anything about the underlying distribution of a player‚Äôs \\(O_p\\); we simply let the power of resampling shape the distribution of outcomes, which should look different for a striker that only takes shots near the goal and a defender that only launches shots from outside the box.\n\n\n\nOne way to check on the robustness of this approach is to see what the distribution of unlikeness \\(\\Pi^-\\) looks like across all players \\(P\\). We should expect a distribution that looks relatively uniform‚Äìin other words, about 10% of players should have a 90th percentile unlikely outcomes (i.e.¬†very likely), 10% of players should have a 10th percentile unlikely outcome (i.e.¬†very unlikely), and so on.\nHISTOGRAM HERE\nIndeed, the histogram below follows a fairly uniform distribution, with a bit of irregularity at the very edges.\nWho exactly is in the edges of the spectrum?\nWell, notably, Manchester City‚Äôs striker Erling Haaland shows up as having the most unlikely outcome for the 2023/24 season using this methodology.\n\n\nAll output from Approach 1\nall_resampled_props\n#&gt; # A tibble: 593 √ó 6\n#&gt;    player              prior_o target_o prior_shots target_shots prop_lte\n#&gt;    &lt;chr&gt;                 &lt;dbl&gt;    &lt;dbl&gt;       &lt;int&gt;        &lt;int&gt;    &lt;dbl&gt;\n#&gt;  1 Erling Haaland         1.26    0.791         313           98    0.004\n#&gt;  2 Amine Harit            1.27    0.262         107           33    0.01 \n#&gt;  3 Pierre-Emerick Aub‚Ä¶    1.07    0.606         405           91    0.01 \n#&gt;  4 T√©ji Savanier          1.42    0.282         276           72    0.01 \n#&gt;  5 Antonio Sanabria       1.05    0.386         269           47    0.014\n#&gt;  6 Alex Baena             1.54    0.348          61           57    0.016\n#&gt;  7 Elye Wahi              1.38    0.753         114           47    0.017\n#&gt;  8 Kevin Behrens          1.39    0.673          51           52    0.019\n#&gt;  9 Ciro Immobile          1.23    0.383         635           41    0.02 \n#&gt; 10 Ansu Fati              1.31    0.430         125           34    0.024\n#&gt; # ‚Ñπ 583 more rows\n#&gt; # ‚Ñπ Use `print(n = ...)` to see more rows\n\n\nHaaland‚Äôs peculiar season has been noted among fans and the media, and it‚Äôs interesting to see that this methodology has put the odds of such an outcome at just 0.4%. Perhaps that‚Äôs right, but I honestly think that‚Äôs just a little too extreme.\nLet‚Äôs see if we can improve on things with another approach.\n\n\n\n\nIf we assume that \\(O_p\\) ratios come from a Gamma data-generating process, then we can leverage the properties of a player-level Gamma distribution to assess the unlikelihood of an observed \\(O_p\\) ratio. Specifically, this approach allows us to determine the probability that a player‚Äôs \\(O_p\\) ratio falls at or below a certain quantile of the distribution, providing a robust metric for evaluating player performance against expected outcomes.\nFor estimating the unlikeliness of observed outcomes for the 2023/24 ‚Äútarget‚Äù season, we proceed in the following manner:\n\nEstimate a Player-Specific Gamma Distribution: Estimate a Gamma distribution \\(\\Gamma_{p,\\text{target}'}\\) to model a player‚Äôs true outperformance ratio \\(O_{p}\\) across all shots, excluding those in the target season‚Äì\\(\\hat{O}_{p,\\text{target}'}\\).\nAssess the Probability of the Target Season‚Äôs Outcome: Calculate the probability that \\(\\hat{O}_{p,\\text{target}'}\\) is less than or equal to the player‚Äôs observed \\(O_{p,\\text{target}}\\) in the target season.\n\nThe code that follows shares similarities to my prior analysis on Empirical Bayes (EB) shrinkage of \\(O_p\\). However, a key difference is that we now estimate the parameters of the prior distribution separately for each player, tailoring the analysis to individual performance variations. Further, we don‚Äôt focus on any posterior distribution, although we could here if we wanted to answer the question: ‚ÄúWhat is the likelihood of a player achieving their 2023/24 \\(G / xG\\) ratio again (in a future season)?‚Äù\n\n\nApproach 2\n## Similar to my code in /posts/xg-empirical-bayes\nN_SIMS &lt;- 10000\n\n## We need to choose \"bounds\" sometimes to help MASS::fidistr estimate parameter values.\n##   In practice, I found that MASS::fidistr + dgamma needed a vector of length 5\n##   before `lower` and `upper` need not be specified. But we can't gaurantee that we'll\n##   always have a vector of at least 5 values to pass to it, so we must guide it\n##   by intelligently choosing lower and upper bounds.\n## At 500 shots, we feel like we can set pretty strong lower and upper bounds\n##   for the prior parameters to estimate. Around 50 shots is when we feel like we can\n##   start moving up our bounds from (1, 1)\nchoose_prior_bound &lt;- function(x, to, from = c(50, 500)) {\n  dplyr::case_when(\n    x &lt; from[1] ~ to[1],\n    x &gt;= from[2] ~ to[2],\n    TRUE ~ scales::rescale(x, from = from, to = to)\n  )\n}\n\nchoose_prior_lower_bound &lt;- function(x) {\n  choose_prior_bound(\n    x = x,\n    to = c(1, 25)\n  )\n}\n\nchoose_prior_upper_bound &lt;- function(x) {\n  choose_prior_bound(\n    x = x,\n    to = c(10, 50)\n  )\n}\n\n## By default:\n## 1. `lower`: Don't return values less than 1 (too weak of a prior)\n## 2. `upper`: Dont' return values greater than 25 (too strong of a prior)\nestimate_gamma_prior_distr_params &lt;- function(x, lower = 1, upper = 25) {\n  prior_distr &lt;- MASS::fitdistr(\n    x + 1e-6, ## fudge factor to prevent 0s,\n    dgamma,\n    start = list(shape = 1, rate = 1),\n    lower = lower,\n    upper = upper\n  )\n  \n  list(\n    shape = unname(prior_distr$estimate[1]),\n    rate = unname(prior_distr$estimate[2])\n  )\n}\n\nestimate_gamma_distributed_o &lt;- function(\n    shots,\n    players,\n    target_season_end_year\n) {\n  select_gamma_o &lt;- purrr::imap_dfr(\n    players,\n    \\(.player, .player_id) {\n      \n      player_np_shots &lt;- shots |&gt; \n        dplyr::filter(player_id == .player_id) |&gt; \n        dplyr::mutate(is_target = season_end_year == target_season_end_year)\n      \n      prior_player_np_shots &lt;- player_np_shots |&gt; \n        dplyr::filter(!is_target)\n      \n      target_player_np_shots &lt;- player_np_shots |&gt; \n        dplyr::filter(is_target)\n      \n      n_prior_shots &lt;- nrow(prior_player_np_shots)\n      n_prior_seasons &lt;- length(unique(prior_player_np_shots))\n      \n      ## For the purpose of estimating a prior, keep the number of non-target seasons\n      ##   fixed and spread the non-target season shots evently across those seasons.\n      ##   This is to help with choosing a set of Gamma distribution parameters.\n      ## An alternative approach might be to  split the player's non-target season shots\n      ##   into equally sized intervals (`fold`) that are approximately equal to (but usually\n      ##   smaller than) their target season shot volume. Assuming a player tends to\n      ##   shoot a similar number of shots per season, the number of `folds` will equal\n      ##   the number of seasons in which they appear in the data set, and this process\n      ##   would simply be splitting their shots evenly across those seasons.\n      ##   However, this approach requires \"peeking\" at the target season data, relying\n      ##   on the number of target season shots being known.\n      n_folds &lt;- (n_prior_shots %/% n_prior_seasons) + 1L\n      shots_in_each_fold &lt;- n_prior_shots %/% n_folds\n      \n      numbered_player_np_shots &lt;- prior_player_np_shots |&gt; \n        dplyr::arrange(match_date, minute) |&gt; \n        dplyr::mutate(\n          rn = dplyr::row_number(),\n          fold = 1L + ((rn - 1L) %/% shots_in_each_fold),\n          ## add any leftover to the last fold\n          fold = ifelse(fold &gt; n_folds, n_folds, fold)\n        )\n      \n      prior_distr_shots &lt;- numbered_player_np_shots |&gt; \n        dplyr::summarize(\n          .by = c(fold),\n          shots = dplyr::n(),\n          dplyr::across(c(g, xg), \\(.x) sum(.x))\n        ) |&gt; \n        dplyr::mutate(o = g / xg)\n      \n      agg_player_np_shots &lt;- player_np_shots |&gt;\n        dplyr::summarize(\n          .by = c(is_target),\n          shots = dplyr::n(),\n          dplyr::across(c(g, xg), \\(.x) sum(.x))\n        ) |&gt; \n        dplyr::mutate(o = g / xg)\n      \n      agg_prior_player_np_shots &lt;- agg_player_np_shots |&gt; \n        dplyr::filter(!is_target)\n      \n      agg_target_player_np_shots &lt;- agg_player_np_shots |&gt; \n        dplyr::filter(is_target)\n      \n      params &lt;- estimate_gamma_prior_distr_params(\n        prior_distr_shots$o,\n        lower = choose_prior_lower_bound(agg_prior_player_np_shots$shots),\n        upper = choose_prior_upper_bound(agg_prior_player_np_shots$shots)\n      )\n      \n      list(\n        'player_id' = .player_id,\n        'player' = .player,\n        'params' = list(params)\n      )\n    }\n  )\n}\n\nselect_gamma_o &lt;- np_shots |&gt; \n  estimate_gamma_distributed_o(\n    players = SELECT_PLAYERS,\n    target_season_end_year = TARGET_SEASON_END_YEAR\n  ) |&gt; \n  dplyr::inner_join(\n    wide_player_np_shots |&gt; \n      dplyr::select(\n        player_id,\n        prior_o,\n        target_o\n      ),\n    by = dplyr::join_by(player_id)\n  ) |&gt; \n  dplyr::arrange(player, player_id)\n\nselect_gamma_props &lt;- select_gamma_o |&gt; \n  dplyr::mutate(\n    prop_lte = purrr::map2_dbl(\n      target_o,\n      params,\n      \\(.target_o, .params) {\n        pgamma(\n          .target_o, \n          shape = .params$shape, \n          rate = .params$rate,\n          lower.tail = TRUE\n        )\n      }\n    )\n  )\n\n\n\n\nOutput from Approach 2\nselect_gamma_props |&gt; \n  dplyr::select(player, prior_o, target_o, prop_lte)\n#&gt; # A tibble: 2 √ó 4\n#&gt;   player         prior_o target_o prop_lte\n#&gt;   &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 James Maddison   1.38     0.847    0.267\n#&gt; 2 Matheus Cunha    0.772    1.16     0.845\n\n\nWe see that Maddison‚Äôs 2023/24 \\(G / xG\\) ratio of 0.847 (or worse) was about 26.7% unlikely given his prior shot history (prior_prop_lte). Using the same line of reasoning, we can say that Cunha‚Äôs 2023/24 \\(O\\) ratio of 1.16 was a 15.5% percentile outcome (1 - prior_prop_lte), so an outcome expected for him once every 6 to 7 seasons.\nTo gain some intuition around this approach, we can plot out the Gamma distributed estimate of Maddison‚Äôs \\(O_p\\). The result is a histogram that looks not all that dissimilar to the one from before with resampled shots, just much smoother (since this is a ‚Äúparametric‚Äù approach).\nMADDISON PLOT HERE"
  },
  {
    "objectID": "drafts/xg-likelihood/index.html#footnotes",
    "href": "drafts/xg-likelihood/index.html#footnotes",
    "title": "Shooting Performance Unlikeliness",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI‚Äôll only be considering non-penalty goals for this post. The ability to score penalties at a high success rate is generally seen as a different skill set than the ability to score goals in open play.‚Ü©Ô∏é\nThe raw difference between goals and xG is a reasonable measure of shooting performance, but it can ‚Äúhide‚Äù shot volume. Is it fair to compare a player who take 100 shots in a year and scores 12 goals on 10 xG with a player who takes 10 shots and scores 3 goals on 1 xG? The raw difference is +2 in both cases, indicating no difference in the shooting performance for the two players. But \\(O_p\\), as defined here, would be 1.2 and 3 respectively, hinting at the former player‚Äôs small sample size. (Not even Messi can sustain an \\(O_p\\) greater than 1.5 or 2 over many shots.)‚Ü©Ô∏é\nWhile I‚Äôll only be focusing on underperformance, ‚Äúoverperformance‚Äù could be quantified in a similar (i.e.¬†symmetrical) manner with each technique.‚Ü©Ô∏é\nThe weighting here is to reflect the intuition that players who have taken a lot of shots in the past and have an uncharacteristic season are shown as more unlikely than a player with only one prior season of shots, who happens to have a very different goals-to-expected goals ratio in the latter season.‚Ü©Ô∏é\nAn overperforming unlikelihood \\(U^+_p\\) could be calculated by sorting \\(\\delta O^w_p\\) in descending order instead.‚Ü©Ô∏é\nOne could argue that without replacement is also reasonable. However, some players wouldn‚Äôt have enough shots from prior seasons to match their volume of shots in 2023/24.‚Ü©Ô∏é\nChange nothing about the shot‚Äôs xG and goal outcomes.‚Ü©Ô∏é\n\\(N_p\\) should be set equal to the number of shots a player has taken in the target season, i.e.¬†2023/24 here. \\(M\\) should be set to some fairly large number, so as to achieve stability in the results. I set it to 1,000.‚Ü©Ô∏é\nSimilarly, to estimate the unlikeness of an overperforming season, count up in how many simulations \\(r^+\\) the outperformance ratio of the resampled shots is greater than \\(O_{p,\\text{target}}\\) and calculate the proportion \\(\\U^+ = \\frac{r^+}{r}\\).‚Ü©Ô∏é\nThis is a two-sided test‚Ü©Ô∏é"
  },
  {
    "objectID": "drafts/xg-likelihood/index.html#methods-and-analysis",
    "href": "drafts/xg-likelihood/index.html#methods-and-analysis",
    "title": "Shooting Performance Unlikeliness",
    "section": "",
    "text": "I‚Äôll present 3 approaches to quantifying the ‚Äúunlikelihood‚Äù of a player ‚Äúunderperforming‚Äù relative to their prior \\(O_p\\) history.3 Note that I use ‚Äúprior‚Äù to refer to an aggregate of pre-2023/24 statistics, and ‚Äútarget‚Äù to refer to 2023/24.\nI‚Äôll discuss some of the strengths and weaknesses of each approach as we go along, then summarize the findings in the end.\n\n\nThe first approach I‚Äôll present is sort of a handcrafted ‚Äúranking‚Äù method.\n\nCalculate the proportional difference between the pre-target and target season outperformance ratios‚Äì\\(O_{p,\\text{target}'}\\) and \\(O_{p,\\text{target}'}\\) respectively‚Äìfor all players \\(P\\).\n\n\\[\n\\delta O_p = \\frac{O_{p,\\text{target}} - O_{p,\\text{target}'}}{O_{p,\\text{target}'}}\n\\]\n\nWeight \\(\\delta O^w_p\\) by the player‚Äôs \\(xG_p\\) accumulated in prior seasons.4\n\n\\[\n\\delta O^w_p = \\delta O_p * xG_p\n\\]\n\nCalculate the the underperforming unlikelihood of the outcome \\(U^-_p\\) as a percentile rank of ascending \\(\\delta O^w_p\\), i.e.¬†more negative \\(\\delta O^w_p\\) values correspond to a lower \\(U^-_p\\) percentile.5\n\nThis is pretty straightforward to calculate once you‚Äôve got the data prepared in the right format.\n\n\nApproach 1\n## `uu` for \"underperforming unlikelihood\"\nall_uu_approach1 &lt;- all_players_to_evaluate |&gt; \n  dplyr::transmute(\n    player_id,\n    player,\n    prior_o,\n    target_o,\n    prior_xg,\n    weighted_delta_o = prior_shots * (target_o - prior_o) / prior_o,\n    uu = dplyr::percent_rank(weighted_delta_o)\n  ) |&gt; \n  dplyr::arrange(uu)\n\n\n\n\nApproach 1 output for Maddison\nall_uu_approach1 |&gt; \n  dplyr::filter(player == 'James Maddison') |&gt; \n  dplyr::select(player, prior_o, target_o, prior_xg, uu) |&gt; \n  reprex_print()\n#&gt; # A tibble: 1 √ó 5\n#&gt;   player         prior_o target_o prior_xg     uu\n#&gt;   &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 James Maddison    1.38    0.847     29.8 0.0321\n\n\nThis approach finds Maddison‚Äôs 2023/24 \\(O_p\\) of 0.847 to be about a 3rd percentile outcome. Among the 593 player‚Äôs evaluated, Maddison‚Äôs 2023/24 \\(O_p\\) ranks as the 20th most unlikely.\nFor context, here‚Äôs a look at the top 10 most unlikely outcomes for the 2023/24 season.\n\n\nApproach 1 output, top 10 unlikeliest outcomes\nuu_approach1 |&gt; \n  head(10) |&gt; \n  dplyr::select(player, prior_o, target_o, prior_xg, uu)\n#&gt; # A tibble: 10 √ó 5\n#&gt;    player             prior_o target_o prior_xg      uu\n#&gt;    &lt;chr&gt;                &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n#&gt;  1 Ciro Immobile        1.23     0.383     80.6 0      \n#&gt;  2 Giovanni Simeone     1.03     0.306     56.0 0.00169\n#&gt;  3 Nabil Fekir          1.14     0.5       32.3 0.00338\n#&gt;  4 Wahbi Khazri         1.11     0.322     28   0.00507\n#&gt;  5 T√©ji Savanier        1.42     0.282     14.1 0.00676\n#&gt;  6 Adrien Thomasson     1.18     0.282     30.5 0.00845\n#&gt;  7 Timo Werner          0.951    0.543     74.6 0.0101 \n#&gt;  8 Ga√´tan Laborde       1.02     0.546     60.6 0.0118 \n#&gt;  9 Kevin Volland        1.18     0.450     55.0 0.0135 \n#&gt; 10 Robert Lewandowski   1.01     0.759    155.  0.0152 \n\n\nCiro Immobile tops the list, with several other notable attacking players who had less than stellar seasons.\nOverall, I‚Äôd say that this methodology seems to generate fairly reasonable results, but it‚Äôs hard to pinpoint why exactly this approach is defensible other than it may lead to intuitive results.\nA couple of notes about this methodology:\n\nThe weighting choice (pre-2023/24 xG) is subjective. An alternative choice of weighting could easily shuffle the result set. Nonetheless, some sort of weighting should be applied. If none is applied, then players who shoot very few shots will appear as the most unlikely, and that simply does not match intuition.\nThe unlikelihood percentile is sensitive to the pool of players with which a given player is compared. If we decide to compare a striker with a pool of defenders, then our results would be susceptible to the noisy goals-to-expected goals ratio that defenders tend to have. Further, if a large majority of players in a given season happened to score more than their xG would imply, then a player scoring at a neutral pace would be penalized by this approach. Of course, such an outcome is unlikely, but speaks to the role that selection bias might have with influencing results with this methodology.\nWe assumee that the distribution of unlikeliness is uniform across all players in a given season. In other words, we‚Äôre assuming that there has to be 1% of players with a 1st percentile underperforming outcome, 17% of players with 17th percentile underperforming outcome, etc. I think this assumption makes sense in the long run, but in any given individual season, there may not be a single player who shots terribly enough to really deserve that ‚Äúworst of the worst‚Äù, 1st percentile outcome.\n\n\n\n\nThere‚Äôs only so much you can do with player-season level data. We need to dive into shot-level data if we want to more robustly understand uncertainty of outcomes.\nHere‚Äôs a ‚Äúresampling‚Äù approach to quantifying the unlikeliness of a player‚Äôs \\(O_p\\) in the target season:\n\nSample \\(N_{p,\\text{target}}\\) shots (with replacement6) from a player‚Äôs past shots \\(S_{p,\\text{target}'}\\). Repeat this for \\(R\\) resamples.78\nCount the number of resamples \\(r^-\\) in which the outperformance ratio \\(\\hat{O}_{p,\\text{target}'}\\) of the sampled shots is less than or equal to the observed \\(O_{p,\\text{target}}\\) in the target season for the player.9 The proportion \\(U^-_p = \\frac{r^-}{R}\\) represents the unlikeness of a given player‚Äôs observed \\(O_{p,\\text{target}}\\) (or worse) in the target season.\n\nHere‚Äôs how that looks in code.\n\n\nApproach 2\nR &lt;- 1000\nresample_player_shots &lt;- function(\n    shots, \n    n_shots_to_sample, \n    n_sims = R,\n    replace = TRUE,\n    seed = 42\n) {\n  \n  withr::local_seed(seed)\n  purrr::map_dfr(\n    1:n_sims,\n    \\(.sim) {\n      sampled_shots &lt;- shots |&gt; \n        slice_sample(n = n_shots_to_sample, replace = replace)\n      \n      list(\n        sim = .sim,\n        xg = sum(sampled_shots$xg),\n        g = sum(sampled_shots$g),\n        o = sum(sampled_shots$g) / sum(sampled_shots$xg)\n      )\n    }\n  )\n}\n\nresample_one_player_o &lt;- function(shots, target_season_end_year) {\n  target_shots &lt;- shots |&gt;\n    dplyr::filter(season_end_year == target_season_end_year)\n  \n  prior_shots &lt;- shots |&gt;\n    dplyr::filter(season_end_year &lt; target_season_end_year)\n  \n  prior_shots |&gt; \n    resample_player_shots(\n      n_shots_to_sample = nrow(target_shots)\n    )\n}\n\nresample_player_o &lt;- function(shots, players, target_season_end_year = TARGET_SEASON_END_YEAR) {\n  purrr::map_dfr(\n    players,\n    \\(.player) {\n      shots |&gt; \n        dplyr::filter(player == .player) |&gt; \n        resample_one_player_o(\n          target_season_end_year = target_season_end_year\n        ) |&gt; \n        dplyr::mutate(\n          player = .player\n        )\n    }\n  )\n}\n\nmaddison_resampled_o &lt;- np_shots |&gt; \n  resample_player_o(\n    players = 'James Maddison'\n  ) |&gt; \n  dplyr::inner_join(\n    wide_player_np_shots |&gt; \n      dplyr::select(\n        player,\n        prior_o,\n        target_o\n      ),\n    by = dplyr::join_by(player)\n  ) |&gt; \n  dplyr::arrange(player)\n\nmaddison_uu_approach2 &lt;- maddison_resampled_o |&gt;\n  dplyr::summarize(\n    .by = c(player, prior_o, target_o),\n    uu = sum(o &lt;= target_o) / n()\n  ) |&gt; \n  dplyr::arrange(player)\n\n\n\n\nApproach 2 output\nmaddison_uu_approach2 |&gt; dplyr::select(player, prior_o, target_o, uu)\n#&gt; # A tibble: 1 √ó 4\n#&gt;   player         prior_o target_o    uu\n#&gt;   &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 James Maddison    1.38    0.847 0.163\n\n\nThe plot below should provide a bit of visual intuition as to what‚Äôs going on.\n\nThese results imply that Maddison‚Äôs 2023/24 \\(G / xG\\) ratio of 0.847 (or worse) occurs in 16.3% of simulations, i.e.¬†a 16th percentile outcome. That‚Äôs a bit higher than what the first approach showed.\nHow can we feel more confident about this approach? Well, in the first approach, we made the assumption that the underperforming unlikelihood percentages should be uniform across all players, hence the percentile ranking. I think that‚Äôs a good assumption, so we should see if the same bears out with this second approach.\nThe plot below shows a histogram of the underperforming unlikelihood across all players, where each player‚Äôs estimated unlikelihood is grouped into a decile.\n\n\nApproach 2, but with all players\nall_resampled_o &lt;- np_shots |&gt; \n  resample_player_o(\n    players = all_players_to_evaluate$player\n  ) |&gt; \n  dplyr::inner_join(\n    wide_player_np_shots |&gt; \n      ## to make sure we just one Rodri, Danilo, and Nicol√°s Gonz√°lez \n      dplyr::filter(player_id %in% all_players_to_evaluate$player_id) |&gt; \n      dplyr::select(\n        player,\n        prior_o,\n        target_o,\n        prior_shots,\n        target_shots\n      ),\n    by = dplyr::join_by(player)\n  ) |&gt; \n  dplyr::arrange(player, player)\n\nall_uu_approach2 &lt;- all_resampled_o |&gt;\n  dplyr::summarize(\n    .by = c(player, prior_o, target_o, prior_shots, target_shots),\n    uu = sum(o &lt;= target_o) / n()\n  ) |&gt; \n  dplyr::arrange(uu)\n\n\n\nIndeed, the histogram shows a fairly uniform distribution, with a bit of irregularity at the very edges.\nLooking at who is in the lower end of the leftmost decile, we see some of the same names‚ÄìImmobile and Savanier‚Äìamong the ten underperformers.\n\n\nApproach 2 output, top 10 underperforming players\nall_uu_approach2 |&gt; head(10) |&gt; dplyr::select(player, prior_o, target_o, uu)\n#&gt; # A tibble: 593 √ó 4\n#&gt;    player                    prior_o target_o    uu\n#&gt;    &lt;chr&gt;                       &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1 Erling Haaland               1.26    0.791 0.004\n#&gt;  2 Amine Harit                  1.27    0.262 0.01 \n#&gt;  3 Pierre-Emerick Aubameyang    1.07    0.606 0.01 \n#&gt;  4 T√©ji Savanier                1.42    0.282 0.01 \n#&gt;  5 Antonio Sanabria             1.05    0.386 0.014\n#&gt;  6 Alex Baena                   1.54    0.348 0.016\n#&gt;  7 Elye Wahi                    1.38    0.753 0.017\n#&gt;  8 Kevin Behrens                1.39    0.673 0.019\n#&gt;  9 Ciro Immobile                1.23    0.383 0.02 \n#&gt; 10 Ansu Fati                    1.31    0.430 0.024\n\n\nOne familiar face in the print out above is Manchester City‚Äôs striker Erling Haaland, whose underperformance this season has been called among fans and the media. His sub-par performance this year only ranked as a 7th percentile outcome by approach 1.\nWithholding judgement on the superiority of any methodology, we can find some solace in seeing some of the same names among the most unlikely underperformers here as we did with approach 1.\nHere are some parting thoughts on this methodology before we look at another:\n\nWe‚Äôre making an assumption that a player‚Äôs past shot profile is representative of their future shot profile. This is reasonable in many cases, but has exceptions for a player who has just changed roles and/or teams, who has been recovering from injuries, etc.\nThis approach is ‚Äúnon-parametric‚Äù. We don‚Äôt assume anything about the underlying distribution of a player‚Äôs \\(O_p\\) other than that it is stable between the prior and target seasons. (See note above.) We let the power of resampling shape the distribution of outcomes, which should look different for a striker that only takes shots near the goal and a defender that only launches shots from outside the box.\nThis approach is computationally intensive (relative to the other approaches). Running this approach with just \\(R = 1000\\) can take several minutes across all players (without palatalization).\n\n\n\n\nIf we assume that the set of goals-to-xG ratios come from a Gamma data-generating process, then we can leverage the properties of a player-level Gamma distribution to assess the unlikelihood of a players \\(O_p\\) ratio.\nTo calculate the underperforming unlikeliness \\(U^-_p\\):\n\nEstimate a Gamma distribution \\(\\Gamma_{p,\\text{target}'}\\) to model a player‚Äôs true outperformance ratio \\(O_{p}\\) across all prior shots, excluding those in the target season‚Äì\\(\\hat{O}_{p,\\text{target}'}\\).\nCalculate the probability that \\(\\hat{O}_{p,\\text{target}'}\\) is less than or equal to the player‚Äôs observed \\(O_{p,\\text{target}}\\) in the target season using the Gamma distribution‚Äôs cumulative distribution function (CDF).\n\nWhile that may sound daunting, I promise that it‚Äôs not (well, aside from a bit of ‚Äúmagic‚Äù in estimating a reasonable Gamma distribution per player).\n\n\nApproach 3\nN_SIMS &lt;- 10000\n\nSHOT_TO_SHAPE_MAPPING &lt;- list(\n  'from' = c(50, 750),\n  'to' = c(1, 25)\n)\nestimate_one_gamma_distributed_o &lt;- function(\n    shots,\n    target_season_end_year\n) {\n  player_np_shots &lt;- shots |&gt; \n    dplyr::mutate(is_target = season_end_year == target_season_end_year)\n  \n  prior_player_np_shots &lt;- player_np_shots |&gt; \n    dplyr::filter(!is_target)\n  \n  target_player_np_shots &lt;- player_np_shots |&gt; \n    dplyr::filter(is_target)\n  \n\n  agg_player_np_shots &lt;- player_np_shots |&gt;\n    dplyr::summarize(\n      .by = c(is_target),\n      shots = dplyr::n(),\n      dplyr::across(c(g, xg), \\(.x) sum(.x))\n    ) |&gt; \n    dplyr::mutate(o = g / xg)\n  \n  agg_prior_player_np_shots &lt;- agg_player_np_shots |&gt; \n    dplyr::filter(!is_target)\n  \n  agg_target_player_np_shots &lt;- agg_player_np_shots |&gt; \n    dplyr::filter(is_target)\n\n  shape &lt;- dplyr::case_when(\n    agg_prior_player_np_shots$shots &lt; SHOT_TO_SHAPE_MAPPING$from[1] ~ SHOT_TO_SHAPE_MAPPING$to[2],\n    agg_prior_player_np_shots$shots &gt; SHOT_TO_SHAPE_MAPPING$from[2] ~ SHOT_TO_SHAPE_MAPPING$to[2],\n    TRUE ~ scales::rescale(\n      agg_prior_player_np_shots$shots, \n      from = SHOT_TO_SHAPE_MAPPING$from, \n      to = SHOT_TO_SHAPE_MAPPING$to\n    )\n  )\n  list(\n    'shape' = shape,\n    'rate' = shape / agg_prior_player_np_shots$o\n  )\n}\n\nestimate_gamma_distributed_o &lt;- function(\n    shots,\n    players,\n    target_season_end_year\n) {\n  \n  purrr::map_dfr(\n    players,\n    \\(.player) {\n      params &lt;- shots |&gt; \n        dplyr::filter(player == .player) |&gt; \n        estimate_one_gamma_distributed_o(\n          target_season_end_year = target_season_end_year\n        )\n      \n      list(\n        'player' = .player,\n        'params' = list(params)\n      )\n    }\n  )\n}\n\nselect_gamma_o &lt;- np_shots |&gt; \n  estimate_gamma_distributed_o(\n    players = 'James Maddison',\n    target_season_end_year = TARGET_SEASON_END_YEAR\n  ) |&gt; \n  dplyr::inner_join(\n    wide_player_np_shots |&gt; \n      dplyr::select(\n        player,\n        prior_o,\n        target_o\n      ),\n    by = dplyr::join_by(player)\n  ) |&gt; \n  dplyr::arrange(player)\n\nuu_approach3 &lt;- select_gamma_o |&gt; \n  dplyr::mutate(\n    uu = purrr::map2_dbl(\n      target_o,\n      params,\n      \\(.target_o, .params) {\n        pgamma(\n          .target_o, \n          shape = .params$shape, \n          rate = .params$rate,\n          lower.tail = TRUE\n        )\n      }\n    ),\n    ou = 1 - uu\n  ) |&gt; \n  tidyr::unnest_wider(params)\n\n\n\n\nApproach 3 output\nuu_approach3 |&gt; dplyr::select(player, prior_o, target_o, uu)\n#&gt; # A tibble: 1 √ó 4\n#&gt;   player         prior_o target_o     uu\n#&gt;   &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 James Maddison    1.38    0.847 0.0679\n\n\nWe see that Maddison‚Äôs 2023/24 \\(O_{p,\\text{target}}\\) ratio of 0.847 (or worse) is about a 7th percentile outcome given his prior shot history. The 7th percentile outcome estimated is about on par with the 6th percentile estimated with approach 1.\nTo gain some intuition around this approach, we can plot out the Gamma distributed estimate of Maddison‚Äôs \\(O_p\\). The result is a histogram that looks not all that dissimilar to the one from before with resampled shots, just much smoother (since this is a ‚Äúparametric‚Äù approach).\n\nAs with approach 2, we should check to see what the distribution of underperforming unlikeliness looks like‚Äìwe should expect to see a somewhat uniform distribution.\n\n\nApproach 3 for all players\nall_gamma_o &lt;- np_shots |&gt; \n  estimate_gamma_distributed_o(\n    players = all_players_to_evaluate$player,\n    target_season_end_year = TARGET_SEASON_END_YEAR\n  ) |&gt; \n  dplyr::inner_join(\n    wide_player_np_shots |&gt; \n      dplyr::filter(\n        player_id %in% all_players_to_evaluate$player_id\n      ) |&gt; \n      dplyr::select(\n        player,\n        prior_o,\n        target_o\n      ),\n    by = dplyr::join_by(player)\n  ) |&gt; \n  dplyr::arrange(player)\n\nall_uu_approach3 &lt;- all_gamma_o |&gt; \n  dplyr::mutate(\n    uu = purrr::map2_dbl(\n      target_o,\n      params,\n      \\(.target_o, .params) {\n        pgamma(\n          .target_o, \n          shape = .params$shape, \n          rate = .params$rate,\n          lower.tail = TRUE\n        )\n      }\n    )\n  ) |&gt; \n  tidyr::unnest_wider(params) |&gt; \n  dplyr::arrange(uu)\n\n\n\nThis histogram has a bit more distortion, i.e.¬†more players, in the highest decile then our resampling approach, so perhaps it‚Äôs a little less calibrated.\nLooking at the top 10 strongest underperformers, 3 of the names here‚ÄìImmobile, Savanier, and Sanabria‚Äìare shared with approach 2‚Äôs top 10, and 7 are shared with approach 1‚Äôs top 10.\n\n\nApproach 3 output, top 10 underperforming players\nall_uu_approach3 |&gt; head(10) |&gt; dplyr::select(player, prior_o, target_o, uu)\n#&gt; # A tibble: 10 √ó 4\n#&gt;    player           prior_o target_o         uu\n#&gt;    &lt;chr&gt;              &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n#&gt;  1 Ciro Immobile       1.23    0.383 0.00000533\n#&gt;  2 T√©ji Savanier       1.42    0.282 0.000127  \n#&gt;  3 Giovanni Simeone    1.03    0.306 0.000248  \n#&gt;  4 Adrien Thomasson    1.18    0.282 0.000346  \n#&gt;  5 Wahbi Khazri        1.11    0.322 0.000604  \n#&gt;  6 Fabi√°n Ruiz Pe√±a    1.67    0.510 0.00271   \n#&gt;  7 Nabil Fekir         1.14    0.5   0.00308   \n#&gt;  8 Kevin Volland       1.18    0.450 0.00408   \n#&gt;  9 Antonio Sanabria    1.05    0.386 0.00877   \n#&gt; 10 Nicol√≤ Barella      1.11    0.417 0.0138\n\n\nWe can visually check the consistency of the results from this method with the prior two with scatter plots of the estimated underperforming unlikeliness from each.\n\nIf two of the approaches were perfectly in agreement, then each point, representing one of the 593 evaluated players, would fall along the 45-degree slope 1 line. With that in mind, we can see that approach 3 is in a little bit more precisely in agreement with approach 1, but approach 3 tends to assign slightly higher percentiles to players on the whole. The results from approach 2 and 3 also have a fair degree of agreement, and the results are more equally calibrated.\nStepping back from the results, what can we say about the principles of the methodology?\n\nThis parametric approach is fixated on using a Gamma distribution. While a Gamma distribution is probably the best choice of any family of distribution‚Äìsince its used to model continuous random variables that are positive and skewed‚Äìthe reliance on a distribution in the first place can feel like a limiting factor.\nWhile I don‚Äôt showcase it, the results can be very sensitive to the choice of parameters used to define each player‚Äôs Gamma distribution. Increasing the shape and/or rate parameters by a few integer values (say, from 10 to 15) tighten the distribution and make tail probabilities appear more extreme (i.e.¬†a 12th percentile outcome would become a 2nd percentile outcome). On the other hand, the Gamma distribution is fairly flexible, so the freedom to choose parameters allows one to tune uncertainty to one‚Äôs desire."
  },
  {
    "objectID": "projects.html#packages",
    "href": "projects.html#packages",
    "title": "Tony's Blog",
    "section": "",
    "text": "Co-maintain {worldfootballR}, an R package for extracting world football (soccer) data from several sites. I work mostly with the Fotmob functions. (Removed as of August 2023.)\nContributed functions for scraping data from ESPN to {ffscrapr}, an R API client for several fantasy football league platforms.\nWrote {valorantr}, an R package for pro Valorant data from rib.gg. (Archived due to API changes)"
  },
  {
    "objectID": "posts/xg-likelihood/index.html",
    "href": "posts/xg-likelihood/index.html",
    "title": "Estimating Shooting Performance Unlikeliness",
    "section": "",
    "text": "Towards the end of each soccer season, we naturally start to look back at player stats, often looking to see who has performed worse compared to their past seasons. We may have different motivations for doing so‚Äìwe may be trying to attribute team under-performance to individuals, we may be hypothesizing who is likely to be transferred, etc.\nIt‚Äôs not uncommon to ask ‚ÄúHow unlikely was their shooting performance this season?‚Äù when looking at a player who has scored fewer goals than expected.1 For instance, if a striker only scores 8 goals on 12 expected goals (xG), their ‚Äúunderperformance‚Äù of 4 goals is stark, especially if they had scored more goals than their xG in prior seasons.\nThe ratio of a player \\(p\\)‚Äôs goals \\(G_p\\) to expected goals \\(xG_p\\)‚Äìthe ‚Äúperformance‚Äù (\\(PR_p\\)) ratio‚Äìis a common, albeit flawed, way of evaluating a player‚Äôs shooting performance.2\n\\[\nPR_p = \\frac{G_p}{xG_p}\n\\]\nAn \\(PR_p\\) of 1 indicates that a player is scoring as many goals as expected; a ratio greater than 1 indicates overperformance; and a ratio less than 1 indicates underperformance. Our hypothetical player underperformed with \\(PR_p = \\frac{8}{12} = 0.67\\).\nIn most cases, we have prior seasons of data to use when evaluating a player‚Äôs \\(PR_p\\) for a given season. For example, let‚Äôs say our hypothetical player scored 14 goals on 10 xG (\\(PR_p = 1.4\\)) in the season prior, and 12 goals on 8 xG (\\(PR_p = 1.5\\)) before that. A \\(PR_p = 0.67\\) after those seasons seems fairly unlikely, especially compared to an ‚Äúaverage‚Äù player who has a \\(PR_p = 1\\) every year.\nSo how do we put a number on the unlikeliness of the \\(PR_p = 0.67\\) for our hypothetical player, accounting for their prior season-long performances?\n\n\nI‚Äôll be using public data from FBref for the 2018/19 - 2023/24 seasons of the the Big Five European soccer leagues, updated through May 7. Fake data is nice for examples, but ultimately we want to test our methods on real data. Our intuition about the results can be a useful caliber of the sensibility of our results.\n\n\nGet shot data\nraw_shots &lt;- worldfootballR::load_fb_match_shooting(\n  country = COUNTRIES,\n  tier = TIERS,\n  gender = GENDERS,\n  season_end_year = SEASON_END_YEARS\n)\n#&gt; ‚Üí Data last updated 2024-05-07 17:52:59 UTC\n\nnp_shots &lt;- raw_shots |&gt; \n  ## Drop penalties\n  dplyr::filter(\n    !dplyr::coalesce((Distance == '13' & round(as.double(xG), 2) == 0.79), FALSE)\n  ) |&gt; \n  dplyr::transmute(\n    season_end_year = Season_End_Year,\n    team = Squad,\n    player_id = Player_Href |&gt; dirname() |&gt; basename(),\n    player = Player,\n    match_date = lubridate::ymd(Date),\n    match_id = MatchURL |&gt; dirname() |&gt; basename(),\n    minute = Minute,\n    g = as.integer(Outcome == 'Goal'),\n    xg = as.double(xG)\n  ) |&gt; \n  ## A handful of scored shots with empty xG\n  dplyr::filter(!is.na(xg)) |&gt; \n  dplyr::arrange(season_end_year, player_id, match_date, minute)\n\n## Use the more commonly used name when a player ID is mapped to multiple names\n##   (This \"bug\" happens because worldfootballR doesn't go back and re-scrape data\n##   when fbref makes a name update.)\nplayer_name_mapping &lt;- np_shots |&gt; \n  dplyr::count(player_id, player) |&gt; \n  dplyr::group_by(player_id) |&gt; \n  dplyr::slice_max(n, n = 1, with_ties = FALSE) |&gt; \n  dplyr::ungroup() |&gt; \n  dplyr::distinct(player_id, player)\n\nplayer_season_np_shots &lt;- np_shots |&gt; \n  dplyr::summarize(\n    .by = c(player_id, season_end_year), \n    shots = dplyr::n(),\n    dplyr::across(c(g, xg), sum)\n  ) |&gt; \n  dplyr::mutate(\n    pr = g / xg\n  ) |&gt; \n  dplyr::left_join(\n    player_name_mapping,\n    by = dplyr::join_by(player_id)\n  ) |&gt; \n  dplyr::relocate(player, .after = player_id) |&gt; \n  dplyr::arrange(player_id, season_end_year)\nplayer_season_np_shots\n#&gt; # A tibble: 15,327 √ó 7\n#&gt;    player_id player          season_end_year shots     g    xg    pr\n#&gt;    &lt;chr&gt;     &lt;chr&gt;                     &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1 0000acda  Marco Benassi              2018    70     5  4.01 1.25 \n#&gt;  2 0000acda  Marco Benassi              2019    59     7  5.61 1.25 \n#&gt;  3 0000acda  Marco Benassi              2020    20     1  1.01 0.990\n#&gt;  4 0000acda  Marco Benassi              2022    10     0  0.99 0    \n#&gt;  5 0000acda  Marco Benassi              2023    19     0  1.35 0    \n#&gt;  6 000b3da6  Manuel Iturra              2018     2     0  0.41 0    \n#&gt;  7 00242715  Moussa Niakhate            2018    16     0  1.43 0    \n#&gt;  8 00242715  Moussa Niakhate            2019    10     1  1.5  0.667\n#&gt;  9 00242715  Moussa Niakhate            2020    11     1  1.02 0.980\n#&gt; 10 00242715  Moussa Niakhate            2021     9     2  1.56 1.28 \n#&gt; # ‚Ñπ 15,307 more rows\n\n\nFor illustrative purposes, we‚Äôll focus on one player in particular‚ÄìJames Maddison. Maddison has had a sub-par 2023/2024 season by his own standards, underperforming his xG for the first time since he started playing in the Premier League in 2018/19.\n\n\nMaddison‚Äôs season-by-season data\nplayer_season_np_shots |&gt; dplyr::filter(player == 'James Maddison')\n#&gt; # A tibble: 6 √ó 7\n#&gt;   player_id player         season_end_year shots     g    xg    pr\n#&gt;   &lt;chr&gt;     &lt;chr&gt;                    &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 ee38d9c5  James Maddison            2019    81     6  5.85 1.03 \n#&gt; 2 ee38d9c5  James Maddison            2020    74     6  5.36 1.12 \n#&gt; 3 ee38d9c5  James Maddison            2021    75     8  3.86 2.07 \n#&gt; 4 ee38d9c5  James Maddison            2022    72    12  7.56 1.59 \n#&gt; 5 ee38d9c5  James Maddison            2023    83     9  7.12 1.26 \n#&gt; 6 ee38d9c5  James Maddison            2024    55     4  5.02 0.797\n\n\n\n\nMore variables useful for the rest of the post\nTARGET_SEASON_END_YEAR &lt;- 2024\n\nplayer_np_shots &lt;- player_season_np_shots |&gt; \n  dplyr::mutate(\n    is_target = season_end_year == TARGET_SEASON_END_YEAR\n  ) |&gt; \n  dplyr::summarize(\n    .by = c(is_target, player_id, player),\n    dplyr::across(\n      c(shots, g, xg),\n      \\(.x) sum(.x, na.rm = TRUE)\n    )\n  ) |&gt; \n  dplyr::mutate(pr = g / xg) |&gt; \n  dplyr::arrange(player, player_id, is_target)\n\nwide_player_np_shots &lt;- player_np_shots |&gt;\n  dplyr::transmute(\n    player_id, \n    player,\n    which = ifelse(is_target, 'target', 'prior'), \n    shots, g, xg, pr\n  ) |&gt; \n  tidyr::pivot_wider(\n    names_from = which, \n    values_from = c(shots, g, xg, pr), \n    names_glue = '{which}_{.value}'\n  )\n\nall_players_to_evaluate &lt;- wide_player_np_shots |&gt; \n  tidyr::drop_na(prior_pr, target_pr) |&gt; \n  dplyr::filter(\n    prior_shots &gt;= 50,\n    target_shots &gt;= 10,\n    prior_g &gt; 0, \n    target_g &gt; 0\n  )\n\n\n\n\n\nI‚Äôll present 3 approaches to contextualizing the likelihood of a player underperforming relative to their prior \\(G / xG\\) ratio, which I‚Äôll broadly call the ‚Äúperformance ratio percentile‚Äù, \\(PRP_p\\).\n\nWeighted percentile ranking: Identify where a player‚Äôs performance relative to their own past ranks among the whole spectrum of player performances.\nResampling from prior shot history: Quantify the likelihood of the observed outcome for a given player by resampling shots from their past.\nEvaluating a player-specific cumulative distribution function (CDF): Fit a distribution to represent a player‚Äôs past set of season-long outcomes, then identify where the target season‚Äôs outcome lies on that distribution.\n\nI‚Äôll discuss some of the strengths and weaknesses of each approach as we go along, then summarize the findings in the end.\nNote that I use ‚Äúprior‚Äù, or \\(\\text{target}'\\), to refer to an aggregate of pre-2023/24 statistics, and ‚Äútarget‚Äù to refer to 2023/24. Here‚Äôs what the distribution of \\(PR_{p,\\text{target}'}\\) and \\(PR_{p,\\text{target}}\\) looks like. The latter‚Äôs distribution has a bit more noise‚Äìnote the lump of players with ratios greater than 2‚Äìdue to smaller sample sizes.\n\n\n\nThe first approach I‚Äôll present is a handcrafted ‚Äúranking‚Äù method.\n\nCalculate the proportional difference between the pre-target and target season performance ratios for all players \\(P\\).\n\n\\[\n\\delta PR_p = \\frac{PR_{p,\\text{target}} - PR_{p,\\text{target}'}}{PR_{p,\\text{target}'}}\n\\]\n\nWeight \\(\\delta PR^w_p\\) by the player‚Äôs \\(xG_{p,\\text{target}'}\\) accumulated in prior seasons.3\n\n\\[\n\\delta PR^w_p = \\delta PR_p * xG_{p,\\text{target}'}\n\\]\n\nCalculate the the performance percentile \\(PRP_p\\) as a percentile rank of ascending \\(\\delta PR^w_p\\), i.e.¬†more negative \\(\\delta PR^w_p\\) values correspond to a lower \\(PRP_p\\) percentile.4\n\nWith the data prepped in the correct manner, this is straightforward to calculate.\n\n\nApproach 1 implementation\n## `prp` for \"performance ratio percentile\"\nall_prp_approach1 &lt;- all_players_to_evaluate |&gt; \n  dplyr::transmute(\n    player,\n    prior_pr,\n    target_pr,\n    prior_xg,\n    weighted_delta_pr = prior_shots * (target_pr - prior_pr) / prior_pr,\n    prp = dplyr::percent_rank(weighted_delta_pr)\n  ) |&gt; \n  dplyr::arrange(prp)\n\nmaddison_prp_approach1 &lt;- all_prp_approach1 |&gt; \n  dplyr::filter(player == 'James Maddison')\n\n\n\n\nApproach 1 output for Maddison\nmaddison_prp_approach1 |&gt; dplyr::select(player, prior_pr, target_pr, prp)\n#&gt; # A tibble: 1 √ó 4\n#&gt;   player         prior_pr target_pr    prp\n#&gt;   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 James Maddison     1.38     0.797 0.0233\n\n\nThis approach finds Maddison‚Äôs 2023/24 \\(PR_p\\) of 0.797 to be about a 2nd percentile outcome. Among the 602 players evaluated, Maddison‚Äôs 2023/24 \\(PR_p\\) ranks as the 15th lowest.\nFor context, here‚Äôs a look at the 10 players who underperformed the most in the 2023/24 season.\n\n\nApproach 1 output, top 10 underperforming players\nall_prp_approach1 |&gt; head(10) |&gt; dplyr::select(player, prior_pr, target_pr, prp)\n#&gt; # A tibble: 10 √ó 4\n#&gt;    player              prior_pr target_pr     prp\n#&gt;    &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n#&gt;  1 Ciro Immobile          1.23      0.503 0      \n#&gt;  2 Giovanni Simeone       1.03      0.306 0.00166\n#&gt;  3 Nabil Fekir            1.14      0.490 0.00333\n#&gt;  4 Wahbi Khazri           1.11      0.322 0.00499\n#&gt;  5 Kevin Volland          1.18      0.388 0.00666\n#&gt;  6 Adrien Thomasson       1.18      0.282 0.00832\n#&gt;  7 Timo Werner            0.951     0.543 0.00998\n#&gt;  8 Ga√´tan Laborde         1.02      0.546 0.0116 \n#&gt;  9 Fabi√°n Ruiz Pe√±a       1.67      0.510 0.0133 \n#&gt; 10 Benjamin Bourigeaud    1.12      0.503 0.0150\n\n\nCiro Immobile tops the list, with several other notable attacking players who had less than stellar seasons.\n\n\nOverall, I‚Äôd say that this methodology is straightforward and seems to generate fairly reasonable results. However, it certainly has its flaws.\n\nSubjectivity in weighting: The choice to weight the difference in performance ratios by pre-2023/24 xG is inherently subjective. While it‚Äôs important to have some form of weighting‚Äìso as to avoid disproportionately emphasizing players with a shorter history of past shots or who shoot relatively few shots in the target season‚Äìalternative weighting strategies could lead to significantly different rankings.\nSensitivity to player pool: The percentile ranking of a player‚Äôs \\(PR_{p,\\text{target}}\\) is highly sensitive to the comparison group. For instance, comparing forwards to defenders could skew results due to generally higher variability in defenders‚Äô goal-to-expected goals ratios. Moreover, if we chose to evaluate a set of players from lower-tier leagues who generally score fewer goals than their expected goals, even players who can simply maintain a \\(PR_p = 1\\) might appear more favorably than they would when compared to Big Five league players. This potential for selection bias underlines the importance of carefully choosing the comparison set of players.\n\n\n\n\n\nThere‚Äôs only so much you can do with player-season-level data; shot-level data can help us more robustly understand and quantify the uncertainty of shooting outcomes.\nHere‚Äôs a ‚Äúresampling‚Äù approach to quantify the performance ratio percentile \\(PRP_p\\) of a player in the target season:\n\nSample \\(N_{p,\\text{target}}\\) shots from a player‚Äôs past shots \\(S_{p,\\text{target}'}\\). Repeat this for \\(R\\) resamples.5\nCount the number of resamples \\(r\\) in which the performance ratio \\(\\hat{PR}_{p,\\text{target}'}\\) of the sampled shots is less than or equal to the observed \\(PR_{p,\\text{target}}\\) in the target season for the player. The proportion \\(PRP_p = \\frac{r}{R}\\) represents the unlikeness of a given player‚Äôs observed \\(PR_{p,\\text{target}}\\) (or worse) in the target season.\n\nHere‚Äôs how that looks in code.\n\n\nApproach 2 implementation\nR &lt;- 1000\nresample_player_shots &lt;- function(\n    shots, \n    n_shots_to_sample, \n    n_sims = R,\n    replace = TRUE,\n    seed = 42\n) {\n  \n  withr::local_seed(seed)\n  purrr::map_dfr(\n    1:n_sims,\n    \\(.sim) {\n      sampled_shots &lt;- shots |&gt; \n        slice_sample(n = n_shots_to_sample, replace = replace)\n      \n      list(\n        sim = .sim,\n        xg = sum(sampled_shots$xg),\n        g = sum(sampled_shots$g),\n        pr = sum(sampled_shots$g) / sum(sampled_shots$xg)\n      )\n    }\n  )\n}\n\nresample_one_player_pr &lt;- function(shots, target_season_end_year) {\n  target_shots &lt;- shots |&gt;\n    dplyr::filter(season_end_year == target_season_end_year)\n  \n  prior_shots &lt;- shots |&gt;\n    dplyr::filter(season_end_year &lt; target_season_end_year)\n  \n  prior_shots |&gt; \n    resample_player_shots(\n      n_shots_to_sample = nrow(target_shots)\n    )\n}\n\nresample_player_pr &lt;- function(shots, players, target_season_end_year = TARGET_SEASON_END_YEAR) {\n  purrr::map_dfr(\n    players,\n    \\(.player) {\n      shots |&gt; \n        dplyr::filter(player == .player) |&gt; \n        resample_one_player_pr(\n          target_season_end_year = target_season_end_year\n        ) |&gt; \n        dplyr::mutate(\n          player = .player\n        )\n    }\n  )\n}\n\nmaddison_resampled_pr &lt;- np_shots |&gt; \n  resample_player_pr(\n    players = 'James Maddison'\n  ) |&gt; \n  dplyr::inner_join(\n    wide_player_np_shots |&gt; \n      dplyr::select(\n        player,\n        prior_pr,\n        target_pr\n      ),\n    by = dplyr::join_by(player)\n  ) |&gt; \n  dplyr::arrange(player)\n\nmaddison_prp_approach2 &lt;- maddison_resampled_pr |&gt;\n  dplyr::summarize(\n    .by = c(player, prior_pr, target_pr),\n    prp = sum(pr &lt;= target_pr) / n()\n  ) |&gt; \n  dplyr::arrange(player)\n\n\n\n\nApproach 2 output for Maddison\nmaddison_prp_approach2 |&gt; dplyr::select(player, prior_pr, target_pr, prp)\n#&gt; # A tibble: 1 √ó 4\n#&gt;   player         prior_pr target_pr   prp\n#&gt;   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 James Maddison     1.38     0.797 0.109\n\n\nThe plot below should provide a bit of visual intuition as to what‚Äôs going on.\n\nThese results imply that Maddison‚Äôs 2023/24 \\(G / xG\\) ratio of 0.797 (or worse) occurs in 11% of simulations, i.e.¬†an 11th percentile outcome. That‚Äôs a bit higher than what the first approach showed.\nHow can we feel confident about this approach? Well, in the first approach, we implicitly assumed that the performance ratio percentiles should be uniform across all players, hence the percentile ranking. We should see if the same bears out with this second approach.\nThe plot below shows a histogram of the performance ratio percentile across all players, where each player‚Äôs estimated performance ratio percentile is grouped into a decile.\n\n\nApproach 2 implementation for all players\nall_resampled_pr &lt;- np_shots |&gt; \n  resample_player_pr(\n    players = all_players_to_evaluate$player\n  ) |&gt; \n  dplyr::inner_join(\n    wide_player_np_shots |&gt; \n      ## to make sure we just one Rodri, Danilo, and Nicol√°s Gonz√°lez \n      dplyr::filter(player_id %in% all_players_to_evaluate$player_id) |&gt; \n      dplyr::select(\n        player,\n        prior_pr,\n        target_pr,\n        prior_shots,\n        target_shots\n      ),\n    by = dplyr::join_by(player)\n  ) |&gt; \n  dplyr::arrange(player, player)\n\nall_prp_approach2 &lt;- all_resampled_pr |&gt;\n  dplyr::summarize(\n    .by = c(player, prior_pr, target_pr, prior_shots, target_shots),\n    prp = sum(pr &lt;= target_pr) / dplyr::n()\n  ) |&gt; \n  dplyr::arrange(prp)\n\n\n\nIndeed, the histogram shows a fairly uniform distribution, with a bit of irregularity at the upper end.\nLooking at who is in the lower end of the leftmost decile, we see some of the same names‚ÄìImmobile and Savanier‚Äìamong the ten underperformers. Withholding judgment on the superiority of any methodology, we can find some solace in seeing some of the same names among the most unlikely underperformers here as we did with approach 1.\n\n\nApproach 2 output, top 10 underperforming players\nall_prp_approach2 |&gt; head(10) |&gt; dplyr::select(player, prior_pr, target_pr, prp)\n#&gt; # A tibble: 10 √ó 4\n#&gt;    player                    prior_pr target_pr   prp\n#&gt;    &lt;chr&gt;                        &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1 Pierre-Emerick Aubameyang     1.07     0.636 0.009\n#&gt;  2 Alex Baena                    1.54     0.326 0.01 \n#&gt;  3 Amine Harit                   1.27     0.262 0.01 \n#&gt;  4 Erling Haaland                1.26     0.897 0.013\n#&gt;  5 Kevin Volland                 1.18     0.388 0.015\n#&gt;  6 Antonio Sanabria              1.05     0.380 0.018\n#&gt;  7 Kevin Behrens                 1.39     0.673 0.019\n#&gt;  8 Elye Wahi                     1.38     0.770 0.021\n#&gt;  9 Ansu Fati                     1.31     0.430 0.024\n#&gt; 10 M'Bala Nzola                  1.10     0.274 0.025\n\n\nOne familiar face in the printout above is Manchester City‚Äôs striker Erling Haaland, whose underperformance this season has been called out among fans and the media. His sub-par performance this year ranked as a 9th percentile outcome by approach 1, which is very low, but not quite as low as what this approach finds (1st percentile).\n\n\n\nAssumption of shot profile consistency: We assume that a player‚Äôs past shot behavior accurately predicts their future performance. This generally holds unless a player changes their role or team, or is recovering from an injury. But there are other exceptions as well. For example, Haaland has taken a lot more headed shots this season, despite playing effectively the same role on mostly the same team from last season The change in Haaland‚Äôs shot profile this year conflicts with the assumption of a consistent shot profile, perhaps explaining why this resampling approach finds Haaland‚Äôs shooting performance to be more unlikely the percentile ranking approach.\nNon-parametric nature: This method does not assume any specific distribution for a player‚Äôs performance ratios; instead, it relies on the stability of a player‚Äôs performance over time. The resampling process itself shapes the outcome distribution, which can vary significantly between players with different shooting behaviors, such as a forward versus a defender.\nComputational demands: The resampling approach requires relatively more computational resources than the prior approach, especially without parallel processing. Even a relatively small number of resamples, such as \\(R=1000\\), can take a few seconds per player to compute.\n\n\n\n\n\nIf we assume that the set of goals-to-xG ratios come from a Gamma data-generating process, then we can leverage the properties of a player-level Gamma distribution to assess the likelihood of a player‚Äôs observed goals-to-xG ratio.\nTo calculate the performance ratio percentile \\(PRP_p\\):\n\nEstimate a Gamma distribution \\(\\Gamma_{p,\\text{target}'}\\) to model a player‚Äôs true outperformance ratio \\(PR_{p,\\text{target}'}\\) across all prior shots, excluding those in the target season‚Äì\\(\\hat{PR}_{p,\\text{target}'}\\).\nCalculate the probability that \\(\\hat{PR}_{p,\\text{target}'}\\) is less than or equal to the player‚Äôs observed \\(PR_{p,\\text{target}}\\) in the target season using the Gamma distribution‚Äôs cumulative distribution function (CDF).\n\nWhile that may sound daunting, I promise that it‚Äôs not.\n\n\nApproach 3 implementation\nSHOT_TO_SHAPE_MAPPING &lt;- list(\n  'from' = c(50, 750),\n  'to' = c(1, 25)\n)\n## Fix the gamma distribution's \"median\" to be shaped around the player's past\n##   historical G/xG ratio (with minimum shape value of 1, so as to prevent a \n##   monotonically decreasing distribution function).\n## Fit larger shape and rate parameters when the player has a lot of prior shots, \n##   so as to create a tighter Gamma distibution.\nestimate_one_gamma_distributed_pr  &lt;- function(\n    shots,\n    target_season_end_year\n) {\n  player_np_shots &lt;- shots |&gt; \n    dplyr::mutate(is_target = season_end_year == target_season_end_year)\n  \n  prior_player_np_shots &lt;- player_np_shots |&gt; \n    dplyr::filter(!is_target)\n  \n  target_player_np_shots &lt;- player_np_shots |&gt; \n    dplyr::filter(is_target)\n  \n\n  agg_player_np_shots &lt;- player_np_shots |&gt;\n    dplyr::summarize(\n      .by = c(is_target),\n      shots = dplyr::n(),\n      dplyr::across(c(g, xg), \\(.x) sum(.x))\n    ) |&gt; \n    dplyr::mutate(pr = g / xg)\n  \n  agg_prior_player_np_shots &lt;- agg_player_np_shots |&gt; \n    dplyr::filter(!is_target)\n  \n  agg_target_player_np_shots &lt;- agg_player_np_shots |&gt; \n    dplyr::filter(is_target)\n\n  shape &lt;- dplyr::case_when(\n    agg_prior_player_np_shots$shots &lt; SHOT_TO_SHAPE_MAPPING$from[1] ~ SHOT_TO_SHAPE_MAPPING$to[2],\n    agg_prior_player_np_shots$shots &gt; SHOT_TO_SHAPE_MAPPING$from[2] ~ SHOT_TO_SHAPE_MAPPING$to[2],\n    TRUE ~ scales::rescale(\n      agg_prior_player_np_shots$shots, \n      from = SHOT_TO_SHAPE_MAPPING$from, \n      to = SHOT_TO_SHAPE_MAPPING$to\n    )\n  )\n  list(\n    'shape' = shape,\n    'rate' = shape / agg_prior_player_np_shots$pr\n  )\n}\n\nestimate_gamma_distributed_pr &lt;- function(\n    shots,\n    players,\n    target_season_end_year\n) {\n  \n  purrr::map_dfr(\n    players,\n    \\(.player) {\n      params &lt;- shots |&gt; \n        dplyr::filter(player == .player) |&gt; \n        estimate_one_gamma_distributed_pr(\n          target_season_end_year = target_season_end_year\n        )\n      \n      list(\n        'player' = .player,\n        'params' = list(params)\n      )\n    }\n  )\n}\n\nmaddison_gamma_pr &lt;- np_shots |&gt; \n  estimate_gamma_distributed_pr(\n    players = 'James Maddison',\n    target_season_end_year = TARGET_SEASON_END_YEAR\n  ) |&gt; \n  dplyr::inner_join(\n    wide_player_np_shots |&gt; \n      dplyr::select(\n        player,\n        prior_pr,\n        target_pr\n      ),\n    by = dplyr::join_by(player)\n  ) |&gt; \n  dplyr::arrange(player)\n\nmaddison_prp_approach3 &lt;- maddison_gamma_pr |&gt; \n  dplyr::mutate(\n    prp = purrr::map2_dbl(\n      target_pr,\n      params,\n      \\(.target_pr, .params) {\n        pgamma(\n          .target_pr, \n          shape = .params$shape, \n          rate = .params$rate,\n          lower.tail = TRUE\n        )\n      }\n    )\n  ) |&gt; \n  tidyr::unnest_wider(params)\n\n\n\n\nApproach 3 output for Maddison\nmaddison_prp_approach3 |&gt; dplyr::select(player, prior_pr, target_pr, prp)\n#&gt; # A tibble: 1 √ó 4\n#&gt;   player         prior_pr target_pr    prp\n#&gt;   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 James Maddison     1.38     0.797 0.0469\n\n\nWe see that Maddison‚Äôs 2023/24 \\(PR_{p,\\text{target}}\\) ratio of 0.797 (or worse) is about a 5th percentile outcome given his prior shot history.\nTo gain some intuition around this approach, we can plot out the Gamma distributed estimate of Maddison‚Äôs \\(PR_p\\). The result is a histogram that looks not all that dissimilar to the one from before with resampled shots, just much smoother (since this is a ‚Äúparametric‚Äù approach).\n\nAs with approach 2, we should check to see what the distribution of performance ratio percentiles looks like‚Äìwe should expect to see a somewhat uniform distribution.\n\n\nApproach 3 for all players\nall_gamma_pr &lt;- np_shots |&gt; \n  estimate_gamma_distributed_pr(\n    players = all_players_to_evaluate$player,\n    target_season_end_year = TARGET_SEASON_END_YEAR\n  ) |&gt; \n  dplyr::inner_join(\n    wide_player_np_shots |&gt; \n      dplyr::filter(\n        player_id %in% all_players_to_evaluate$player_id\n      ) |&gt; \n      dplyr::select(\n        player,\n        prior_pr,\n        target_pr\n      ),\n    by = dplyr::join_by(player)\n  ) |&gt; \n  dplyr::arrange(player)\n\nall_prp_approach3 &lt;- all_gamma_pr |&gt; \n  dplyr::mutate(\n    prp = purrr::map2_dbl(\n      target_pr,\n      params,\n      \\(.target_pr, .params) {\n        pgamma(\n          .target_pr, \n          shape = .params$shape, \n          rate = .params$rate,\n          lower.tail = TRUE\n        )\n      }\n    )\n  ) |&gt; \n  tidyr::unnest_wider(params) |&gt; \n  dplyr::arrange(prp)\n\n\n\nThis histogram has a bit more distortion than our resampling approach, so perhaps it‚Äôs a little less calibrated.\nLooking at the top 10 strongest underperformers, 2 of the names here‚ÄìVolland Sanabria‚Äìare shared with approach 2‚Äôs top 10, and 7 are shared with approach 1‚Äôs top 10.\n\n\nApproach 3 output, top 10 underperforming players\nall_prp_approach3 |&gt; head(10) |&gt; dplyr::select(player, prior_pr, target_pr, prp)\n#&gt; # A tibble: 10 √ó 4\n#&gt;    player           prior_pr target_pr      prp\n#&gt;    &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n#&gt;  1 Ciro Immobile        1.23     0.503 0.000238\n#&gt;  2 Giovanni Simeone     1.03     0.306 0.000248\n#&gt;  3 Adrien Thomasson     1.18     0.282 0.000346\n#&gt;  4 Wahbi Khazri         1.11     0.322 0.000604\n#&gt;  5 Kevin Volland        1.18     0.388 0.00132 \n#&gt;  6 Nabil Fekir          1.14     0.490 0.00256 \n#&gt;  7 Fabi√°n Ruiz Pe√±a     1.67     0.510 0.00271 \n#&gt;  8 Antonio Sanabria     1.05     0.380 0.00796 \n#&gt;  9 T√©ji Savanier        1.42     0.548 0.0103  \n#&gt; 10 Jordan Veretout      1.16     0.360 0.0105\n\n\nWe can visually check the consistency of the results from this method with the prior two with scatter plots of the estimated performance ratio percentile from each.\n\nIf two of the approaches were perfectly in agreement, then each point‚Äìrepresenting one of the 602 evaluated players‚Äìwould fall along the 45-degree slope 1 line.\nWith that in mind, we can see that approach 3 more precisely agrees with approach 1, although approach 3 tends to assign slightly higher percentiles to players on the whole. The results from approaches 2 and 3 also have a fair degree of agreement, and the results are more uniformly calibrated.\n\n\n\nParametric nature: The reliance on a Gamma distribution for modeling a player‚Äôs performance is both a strength and a limitation. The Gamma distribution is apt for positive, skewed continuous variables, making it suitable for modeling goals-to-xG ratios. And we can leverage the properties of the distribution to calculate a likelihood percentile directly. However, the dependency on a distribution is sort of an ‚Äúall-or-nothing‚Äù endeavor‚Äìif we don‚Äôt estimate an appropriate distribution, then we can under- or over-estimate the likelihood of individual player outcomes.\nSensitivity to distribution parameters: The outcomes of this methodology are highly sensitive to the parameters defining each player‚Äôs Gamma distribution. Small adjustments in shape or rate parameters can significantly alter the distribution, causing substantial shifts in the percentile outcomes of player performances. This sensitivity underscores the need for careful parameter selection and calibration."
  },
  {
    "objectID": "posts/xg-likelihood/index.html#methods-and-analysis",
    "href": "posts/xg-likelihood/index.html#methods-and-analysis",
    "title": "Estimating Shooting Performance Unlikeliness",
    "section": "",
    "text": "I‚Äôll present 3 approaches to contextualizing the likelihood of a player underperforming relative to their prior \\(G / xG\\) ratio, which I‚Äôll broadly call the ‚Äúperformance ratio percentile‚Äù, \\(PRP_p\\).\n\nWeighted percentile ranking: Identify where a player‚Äôs performance relative to their own past ranks among the whole spectrum of player performances.\nResampling from prior shot history: Quantify the likelihood of the observed outcome for a given player by resampling shots from their past.\nEvaluating a player-specific cumulative distribution function (CDF): Fit a distribution to represent a player‚Äôs past set of season-long outcomes, then identify where the target season‚Äôs outcome lies on that distribution.\n\nI‚Äôll discuss some of the strengths and weaknesses of each approach as we go along, then summarize the findings in the end.\nNote that I use ‚Äúprior‚Äù, or \\(\\text{target}'\\), to refer to an aggregate of pre-2023/24 statistics, and ‚Äútarget‚Äù to refer to 2023/24. Here‚Äôs what the distribution of \\(PR_{p,\\text{target}'}\\) and \\(PR_{p,\\text{target}}\\) looks like. The latter‚Äôs distribution has a bit more noise‚Äìnote the lump of players with ratios greater than 2‚Äìdue to smaller sample sizes.\n\n\n\nThe first approach I‚Äôll present is a handcrafted ‚Äúranking‚Äù method.\n\nCalculate the proportional difference between the pre-target and target season performance ratios for all players \\(P\\).\n\n\\[\n\\delta PR_p = \\frac{PR_{p,\\text{target}} - PR_{p,\\text{target}'}}{PR_{p,\\text{target}'}}\n\\]\n\nWeight \\(\\delta PR^w_p\\) by the player‚Äôs \\(xG_{p,\\text{target}'}\\) accumulated in prior seasons.3\n\n\\[\n\\delta PR^w_p = \\delta PR_p * xG_{p,\\text{target}'}\n\\]\n\nCalculate the the performance percentile \\(PRP_p\\) as a percentile rank of ascending \\(\\delta PR^w_p\\), i.e.¬†more negative \\(\\delta PR^w_p\\) values correspond to a lower \\(PRP_p\\) percentile.4\n\nWith the data prepped in the correct manner, this is straightforward to calculate.\n\n\nApproach 1 implementation\n## `prp` for \"performance ratio percentile\"\nall_prp_approach1 &lt;- all_players_to_evaluate |&gt; \n  dplyr::transmute(\n    player,\n    prior_pr,\n    target_pr,\n    prior_xg,\n    weighted_delta_pr = prior_shots * (target_pr - prior_pr) / prior_pr,\n    prp = dplyr::percent_rank(weighted_delta_pr)\n  ) |&gt; \n  dplyr::arrange(prp)\n\nmaddison_prp_approach1 &lt;- all_prp_approach1 |&gt; \n  dplyr::filter(player == 'James Maddison')\n\n\n\n\nApproach 1 output for Maddison\nmaddison_prp_approach1 |&gt; dplyr::select(player, prior_pr, target_pr, prp)\n#&gt; # A tibble: 1 √ó 4\n#&gt;   player         prior_pr target_pr    prp\n#&gt;   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 James Maddison     1.38     0.797 0.0233\n\n\nThis approach finds Maddison‚Äôs 2023/24 \\(PR_p\\) of 0.797 to be about a 2nd percentile outcome. Among the 602 players evaluated, Maddison‚Äôs 2023/24 \\(PR_p\\) ranks as the 15th lowest.\nFor context, here‚Äôs a look at the 10 players who underperformed the most in the 2023/24 season.\n\n\nApproach 1 output, top 10 underperforming players\nall_prp_approach1 |&gt; head(10) |&gt; dplyr::select(player, prior_pr, target_pr, prp)\n#&gt; # A tibble: 10 √ó 4\n#&gt;    player              prior_pr target_pr     prp\n#&gt;    &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n#&gt;  1 Ciro Immobile          1.23      0.503 0      \n#&gt;  2 Giovanni Simeone       1.03      0.306 0.00166\n#&gt;  3 Nabil Fekir            1.14      0.490 0.00333\n#&gt;  4 Wahbi Khazri           1.11      0.322 0.00499\n#&gt;  5 Kevin Volland          1.18      0.388 0.00666\n#&gt;  6 Adrien Thomasson       1.18      0.282 0.00832\n#&gt;  7 Timo Werner            0.951     0.543 0.00998\n#&gt;  8 Ga√´tan Laborde         1.02      0.546 0.0116 \n#&gt;  9 Fabi√°n Ruiz Pe√±a       1.67      0.510 0.0133 \n#&gt; 10 Benjamin Bourigeaud    1.12      0.503 0.0150\n\n\nCiro Immobile tops the list, with several other notable attacking players who had less than stellar seasons.\n\n\nOverall, I‚Äôd say that this methodology is straightforward and seems to generate fairly reasonable results. However, it certainly has its flaws.\n\nSubjectivity in weighting: The choice to weight the difference in performance ratios by pre-2023/24 xG is inherently subjective. While it‚Äôs important to have some form of weighting‚Äìso as to avoid disproportionately emphasizing players with a shorter history of past shots or who shoot relatively few shots in the target season‚Äìalternative weighting strategies could lead to significantly different rankings.\nSensitivity to player pool: The percentile ranking of a player‚Äôs \\(PR_{p,\\text{target}}\\) is highly sensitive to the comparison group. For instance, comparing forwards to defenders could skew results due to generally higher variability in defenders‚Äô goal-to-expected goals ratios. Moreover, if we chose to evaluate a set of players from lower-tier leagues who generally score fewer goals than their expected goals, even players who can simply maintain a \\(PR_p = 1\\) might appear more favorably than they would when compared to Big Five league players. This potential for selection bias underlines the importance of carefully choosing the comparison set of players.\n\n\n\n\n\nThere‚Äôs only so much you can do with player-season-level data; shot-level data can help us more robustly understand and quantify the uncertainty of shooting outcomes.\nHere‚Äôs a ‚Äúresampling‚Äù approach to quantify the performance ratio percentile \\(PRP_p\\) of a player in the target season:\n\nSample \\(N_{p,\\text{target}}\\) shots from a player‚Äôs past shots \\(S_{p,\\text{target}'}\\). Repeat this for \\(R\\) resamples.5\nCount the number of resamples \\(r\\) in which the performance ratio \\(\\hat{PR}_{p,\\text{target}'}\\) of the sampled shots is less than or equal to the observed \\(PR_{p,\\text{target}}\\) in the target season for the player. The proportion \\(PRP_p = \\frac{r}{R}\\) represents the unlikeness of a given player‚Äôs observed \\(PR_{p,\\text{target}}\\) (or worse) in the target season.\n\nHere‚Äôs how that looks in code.\n\n\nApproach 2 implementation\nR &lt;- 1000\nresample_player_shots &lt;- function(\n    shots, \n    n_shots_to_sample, \n    n_sims = R,\n    replace = TRUE,\n    seed = 42\n) {\n  \n  withr::local_seed(seed)\n  purrr::map_dfr(\n    1:n_sims,\n    \\(.sim) {\n      sampled_shots &lt;- shots |&gt; \n        slice_sample(n = n_shots_to_sample, replace = replace)\n      \n      list(\n        sim = .sim,\n        xg = sum(sampled_shots$xg),\n        g = sum(sampled_shots$g),\n        pr = sum(sampled_shots$g) / sum(sampled_shots$xg)\n      )\n    }\n  )\n}\n\nresample_one_player_pr &lt;- function(shots, target_season_end_year) {\n  target_shots &lt;- shots |&gt;\n    dplyr::filter(season_end_year == target_season_end_year)\n  \n  prior_shots &lt;- shots |&gt;\n    dplyr::filter(season_end_year &lt; target_season_end_year)\n  \n  prior_shots |&gt; \n    resample_player_shots(\n      n_shots_to_sample = nrow(target_shots)\n    )\n}\n\nresample_player_pr &lt;- function(shots, players, target_season_end_year = TARGET_SEASON_END_YEAR) {\n  purrr::map_dfr(\n    players,\n    \\(.player) {\n      shots |&gt; \n        dplyr::filter(player == .player) |&gt; \n        resample_one_player_pr(\n          target_season_end_year = target_season_end_year\n        ) |&gt; \n        dplyr::mutate(\n          player = .player\n        )\n    }\n  )\n}\n\nmaddison_resampled_pr &lt;- np_shots |&gt; \n  resample_player_pr(\n    players = 'James Maddison'\n  ) |&gt; \n  dplyr::inner_join(\n    wide_player_np_shots |&gt; \n      dplyr::select(\n        player,\n        prior_pr,\n        target_pr\n      ),\n    by = dplyr::join_by(player)\n  ) |&gt; \n  dplyr::arrange(player)\n\nmaddison_prp_approach2 &lt;- maddison_resampled_pr |&gt;\n  dplyr::summarize(\n    .by = c(player, prior_pr, target_pr),\n    prp = sum(pr &lt;= target_pr) / n()\n  ) |&gt; \n  dplyr::arrange(player)\n\n\n\n\nApproach 2 output for Maddison\nmaddison_prp_approach2 |&gt; dplyr::select(player, prior_pr, target_pr, prp)\n#&gt; # A tibble: 1 √ó 4\n#&gt;   player         prior_pr target_pr   prp\n#&gt;   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 James Maddison     1.38     0.797 0.109\n\n\nThe plot below should provide a bit of visual intuition as to what‚Äôs going on.\n\nThese results imply that Maddison‚Äôs 2023/24 \\(G / xG\\) ratio of 0.797 (or worse) occurs in 11% of simulations, i.e.¬†an 11th percentile outcome. That‚Äôs a bit higher than what the first approach showed.\nHow can we feel confident about this approach? Well, in the first approach, we implicitly assumed that the performance ratio percentiles should be uniform across all players, hence the percentile ranking. We should see if the same bears out with this second approach.\nThe plot below shows a histogram of the performance ratio percentile across all players, where each player‚Äôs estimated performance ratio percentile is grouped into a decile.\n\n\nApproach 2 implementation for all players\nall_resampled_pr &lt;- np_shots |&gt; \n  resample_player_pr(\n    players = all_players_to_evaluate$player\n  ) |&gt; \n  dplyr::inner_join(\n    wide_player_np_shots |&gt; \n      ## to make sure we just one Rodri, Danilo, and Nicol√°s Gonz√°lez \n      dplyr::filter(player_id %in% all_players_to_evaluate$player_id) |&gt; \n      dplyr::select(\n        player,\n        prior_pr,\n        target_pr,\n        prior_shots,\n        target_shots\n      ),\n    by = dplyr::join_by(player)\n  ) |&gt; \n  dplyr::arrange(player, player)\n\nall_prp_approach2 &lt;- all_resampled_pr |&gt;\n  dplyr::summarize(\n    .by = c(player, prior_pr, target_pr, prior_shots, target_shots),\n    prp = sum(pr &lt;= target_pr) / dplyr::n()\n  ) |&gt; \n  dplyr::arrange(prp)\n\n\n\nIndeed, the histogram shows a fairly uniform distribution, with a bit of irregularity at the upper end.\nLooking at who is in the lower end of the leftmost decile, we see some of the same names‚ÄìImmobile and Savanier‚Äìamong the ten underperformers. Withholding judgment on the superiority of any methodology, we can find some solace in seeing some of the same names among the most unlikely underperformers here as we did with approach 1.\n\n\nApproach 2 output, top 10 underperforming players\nall_prp_approach2 |&gt; head(10) |&gt; dplyr::select(player, prior_pr, target_pr, prp)\n#&gt; # A tibble: 10 √ó 4\n#&gt;    player                    prior_pr target_pr   prp\n#&gt;    &lt;chr&gt;                        &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1 Pierre-Emerick Aubameyang     1.07     0.636 0.009\n#&gt;  2 Alex Baena                    1.54     0.326 0.01 \n#&gt;  3 Amine Harit                   1.27     0.262 0.01 \n#&gt;  4 Erling Haaland                1.26     0.897 0.013\n#&gt;  5 Kevin Volland                 1.18     0.388 0.015\n#&gt;  6 Antonio Sanabria              1.05     0.380 0.018\n#&gt;  7 Kevin Behrens                 1.39     0.673 0.019\n#&gt;  8 Elye Wahi                     1.38     0.770 0.021\n#&gt;  9 Ansu Fati                     1.31     0.430 0.024\n#&gt; 10 M'Bala Nzola                  1.10     0.274 0.025\n\n\nOne familiar face in the printout above is Manchester City‚Äôs striker Erling Haaland, whose underperformance this season has been called out among fans and the media. His sub-par performance this year ranked as a 9th percentile outcome by approach 1, which is very low, but not quite as low as what this approach finds (1st percentile).\n\n\n\nAssumption of shot profile consistency: We assume that a player‚Äôs past shot behavior accurately predicts their future performance. This generally holds unless a player changes their role or team, or is recovering from an injury. But there are other exceptions as well. For example, Haaland has taken a lot more headed shots this season, despite playing effectively the same role on mostly the same team from last season The change in Haaland‚Äôs shot profile this year conflicts with the assumption of a consistent shot profile, perhaps explaining why this resampling approach finds Haaland‚Äôs shooting performance to be more unlikely the percentile ranking approach.\nNon-parametric nature: This method does not assume any specific distribution for a player‚Äôs performance ratios; instead, it relies on the stability of a player‚Äôs performance over time. The resampling process itself shapes the outcome distribution, which can vary significantly between players with different shooting behaviors, such as a forward versus a defender.\nComputational demands: The resampling approach requires relatively more computational resources than the prior approach, especially without parallel processing. Even a relatively small number of resamples, such as \\(R=1000\\), can take a few seconds per player to compute.\n\n\n\n\n\nIf we assume that the set of goals-to-xG ratios come from a Gamma data-generating process, then we can leverage the properties of a player-level Gamma distribution to assess the likelihood of a player‚Äôs observed goals-to-xG ratio.\nTo calculate the performance ratio percentile \\(PRP_p\\):\n\nEstimate a Gamma distribution \\(\\Gamma_{p,\\text{target}'}\\) to model a player‚Äôs true outperformance ratio \\(PR_{p,\\text{target}'}\\) across all prior shots, excluding those in the target season‚Äì\\(\\hat{PR}_{p,\\text{target}'}\\).\nCalculate the probability that \\(\\hat{PR}_{p,\\text{target}'}\\) is less than or equal to the player‚Äôs observed \\(PR_{p,\\text{target}}\\) in the target season using the Gamma distribution‚Äôs cumulative distribution function (CDF).\n\nWhile that may sound daunting, I promise that it‚Äôs not.\n\n\nApproach 3 implementation\nSHOT_TO_SHAPE_MAPPING &lt;- list(\n  'from' = c(50, 750),\n  'to' = c(1, 25)\n)\n## Fix the gamma distribution's \"median\" to be shaped around the player's past\n##   historical G/xG ratio (with minimum shape value of 1, so as to prevent a \n##   monotonically decreasing distribution function).\n## Fit larger shape and rate parameters when the player has a lot of prior shots, \n##   so as to create a tighter Gamma distibution.\nestimate_one_gamma_distributed_pr  &lt;- function(\n    shots,\n    target_season_end_year\n) {\n  player_np_shots &lt;- shots |&gt; \n    dplyr::mutate(is_target = season_end_year == target_season_end_year)\n  \n  prior_player_np_shots &lt;- player_np_shots |&gt; \n    dplyr::filter(!is_target)\n  \n  target_player_np_shots &lt;- player_np_shots |&gt; \n    dplyr::filter(is_target)\n  \n\n  agg_player_np_shots &lt;- player_np_shots |&gt;\n    dplyr::summarize(\n      .by = c(is_target),\n      shots = dplyr::n(),\n      dplyr::across(c(g, xg), \\(.x) sum(.x))\n    ) |&gt; \n    dplyr::mutate(pr = g / xg)\n  \n  agg_prior_player_np_shots &lt;- agg_player_np_shots |&gt; \n    dplyr::filter(!is_target)\n  \n  agg_target_player_np_shots &lt;- agg_player_np_shots |&gt; \n    dplyr::filter(is_target)\n\n  shape &lt;- dplyr::case_when(\n    agg_prior_player_np_shots$shots &lt; SHOT_TO_SHAPE_MAPPING$from[1] ~ SHOT_TO_SHAPE_MAPPING$to[2],\n    agg_prior_player_np_shots$shots &gt; SHOT_TO_SHAPE_MAPPING$from[2] ~ SHOT_TO_SHAPE_MAPPING$to[2],\n    TRUE ~ scales::rescale(\n      agg_prior_player_np_shots$shots, \n      from = SHOT_TO_SHAPE_MAPPING$from, \n      to = SHOT_TO_SHAPE_MAPPING$to\n    )\n  )\n  list(\n    'shape' = shape,\n    'rate' = shape / agg_prior_player_np_shots$pr\n  )\n}\n\nestimate_gamma_distributed_pr &lt;- function(\n    shots,\n    players,\n    target_season_end_year\n) {\n  \n  purrr::map_dfr(\n    players,\n    \\(.player) {\n      params &lt;- shots |&gt; \n        dplyr::filter(player == .player) |&gt; \n        estimate_one_gamma_distributed_pr(\n          target_season_end_year = target_season_end_year\n        )\n      \n      list(\n        'player' = .player,\n        'params' = list(params)\n      )\n    }\n  )\n}\n\nmaddison_gamma_pr &lt;- np_shots |&gt; \n  estimate_gamma_distributed_pr(\n    players = 'James Maddison',\n    target_season_end_year = TARGET_SEASON_END_YEAR\n  ) |&gt; \n  dplyr::inner_join(\n    wide_player_np_shots |&gt; \n      dplyr::select(\n        player,\n        prior_pr,\n        target_pr\n      ),\n    by = dplyr::join_by(player)\n  ) |&gt; \n  dplyr::arrange(player)\n\nmaddison_prp_approach3 &lt;- maddison_gamma_pr |&gt; \n  dplyr::mutate(\n    prp = purrr::map2_dbl(\n      target_pr,\n      params,\n      \\(.target_pr, .params) {\n        pgamma(\n          .target_pr, \n          shape = .params$shape, \n          rate = .params$rate,\n          lower.tail = TRUE\n        )\n      }\n    )\n  ) |&gt; \n  tidyr::unnest_wider(params)\n\n\n\n\nApproach 3 output for Maddison\nmaddison_prp_approach3 |&gt; dplyr::select(player, prior_pr, target_pr, prp)\n#&gt; # A tibble: 1 √ó 4\n#&gt;   player         prior_pr target_pr    prp\n#&gt;   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 James Maddison     1.38     0.797 0.0469\n\n\nWe see that Maddison‚Äôs 2023/24 \\(PR_{p,\\text{target}}\\) ratio of 0.797 (or worse) is about a 5th percentile outcome given his prior shot history.\nTo gain some intuition around this approach, we can plot out the Gamma distributed estimate of Maddison‚Äôs \\(PR_p\\). The result is a histogram that looks not all that dissimilar to the one from before with resampled shots, just much smoother (since this is a ‚Äúparametric‚Äù approach).\n\nAs with approach 2, we should check to see what the distribution of performance ratio percentiles looks like‚Äìwe should expect to see a somewhat uniform distribution.\n\n\nApproach 3 for all players\nall_gamma_pr &lt;- np_shots |&gt; \n  estimate_gamma_distributed_pr(\n    players = all_players_to_evaluate$player,\n    target_season_end_year = TARGET_SEASON_END_YEAR\n  ) |&gt; \n  dplyr::inner_join(\n    wide_player_np_shots |&gt; \n      dplyr::filter(\n        player_id %in% all_players_to_evaluate$player_id\n      ) |&gt; \n      dplyr::select(\n        player,\n        prior_pr,\n        target_pr\n      ),\n    by = dplyr::join_by(player)\n  ) |&gt; \n  dplyr::arrange(player)\n\nall_prp_approach3 &lt;- all_gamma_pr |&gt; \n  dplyr::mutate(\n    prp = purrr::map2_dbl(\n      target_pr,\n      params,\n      \\(.target_pr, .params) {\n        pgamma(\n          .target_pr, \n          shape = .params$shape, \n          rate = .params$rate,\n          lower.tail = TRUE\n        )\n      }\n    )\n  ) |&gt; \n  tidyr::unnest_wider(params) |&gt; \n  dplyr::arrange(prp)\n\n\n\nThis histogram has a bit more distortion than our resampling approach, so perhaps it‚Äôs a little less calibrated.\nLooking at the top 10 strongest underperformers, 2 of the names here‚ÄìVolland Sanabria‚Äìare shared with approach 2‚Äôs top 10, and 7 are shared with approach 1‚Äôs top 10.\n\n\nApproach 3 output, top 10 underperforming players\nall_prp_approach3 |&gt; head(10) |&gt; dplyr::select(player, prior_pr, target_pr, prp)\n#&gt; # A tibble: 10 √ó 4\n#&gt;    player           prior_pr target_pr      prp\n#&gt;    &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n#&gt;  1 Ciro Immobile        1.23     0.503 0.000238\n#&gt;  2 Giovanni Simeone     1.03     0.306 0.000248\n#&gt;  3 Adrien Thomasson     1.18     0.282 0.000346\n#&gt;  4 Wahbi Khazri         1.11     0.322 0.000604\n#&gt;  5 Kevin Volland        1.18     0.388 0.00132 \n#&gt;  6 Nabil Fekir          1.14     0.490 0.00256 \n#&gt;  7 Fabi√°n Ruiz Pe√±a     1.67     0.510 0.00271 \n#&gt;  8 Antonio Sanabria     1.05     0.380 0.00796 \n#&gt;  9 T√©ji Savanier        1.42     0.548 0.0103  \n#&gt; 10 Jordan Veretout      1.16     0.360 0.0105\n\n\nWe can visually check the consistency of the results from this method with the prior two with scatter plots of the estimated performance ratio percentile from each.\n\nIf two of the approaches were perfectly in agreement, then each point‚Äìrepresenting one of the 602 evaluated players‚Äìwould fall along the 45-degree slope 1 line.\nWith that in mind, we can see that approach 3 more precisely agrees with approach 1, although approach 3 tends to assign slightly higher percentiles to players on the whole. The results from approaches 2 and 3 also have a fair degree of agreement, and the results are more uniformly calibrated.\n\n\n\nParametric nature: The reliance on a Gamma distribution for modeling a player‚Äôs performance is both a strength and a limitation. The Gamma distribution is apt for positive, skewed continuous variables, making it suitable for modeling goals-to-xG ratios. And we can leverage the properties of the distribution to calculate a likelihood percentile directly. However, the dependency on a distribution is sort of an ‚Äúall-or-nothing‚Äù endeavor‚Äìif we don‚Äôt estimate an appropriate distribution, then we can under- or over-estimate the likelihood of individual player outcomes.\nSensitivity to distribution parameters: The outcomes of this methodology are highly sensitive to the parameters defining each player‚Äôs Gamma distribution. Small adjustments in shape or rate parameters can significantly alter the distribution, causing substantial shifts in the percentile outcomes of player performances. This sensitivity underscores the need for careful parameter selection and calibration."
  },
  {
    "objectID": "posts/xg-likelihood/index.html#footnotes",
    "href": "posts/xg-likelihood/index.html#footnotes",
    "title": "Estimating Shooting Performance Unlikeliness",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI only consider non-penalty xG and goals for this post. The ability to score penalties at a high success rate is generally seen as a different skill set than the ability to score goals in open play.‚Ü©Ô∏é\nThe raw difference between goals and xG is a reasonable measure of shooting performance, but it can ‚Äúhide‚Äù shot volume. Is it fair to compare a player who takes 100 shots in a year and scores 12 goals on 10 xG with a player who takes 10 shots and scores 3 goals on 1 xG? The raw difference is +2 in both cases, indicating no difference in the shooting performance for the two players. However, their \\(PR_p\\) would be 1.2 and 3 respectively, hinting at the former player‚Äôs small sample size.‚Ü©Ô∏é\nThe weighting emphasizes scenarios where a veteran player, typically overperforming or at worst neutral, suddenly underperforms, as opposed to a second-year player experiencing similar downturns.‚Ü©Ô∏é\nPercentiles greater than 50% generally correspond with players who have overperformed, so really the bottom 50% are the players we‚Äôre looking at when we‚Äôre considering underperformance.‚Ü©Ô∏é\n\\(N_p\\) should be set equal to the number of shots a player has taken in the target season. \\(R\\) should be set to some fairly large number, so as to achieve stability in the results.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/xg-likelihood/index.html#potential-future-research",
    "href": "posts/xg-likelihood/index.html#potential-future-research",
    "title": "Estimating Shooting Performance Unlikeliness",
    "section": "Potential Future Research",
    "text": "Potential Future Research\n\nCan these approaches be applied to teams or managers to understand the unlikeliness of their season-long outcomes?\n\nI think the answer is ‚Äúyes‚Äù, for the resampling approach. The non-parametric nature of resampling makes it easy to translate to other ‚Äúlevels of aggregation‚Äù, i.e.¬†a set of players under a manager or playing as a team.\n\nCan we accurately attribute a percentage of the underperformance to skill and luck?\n\nEh, I don‚Äôt know about ‚Äúaccurately‚Äù, especially at the player level.The R-squared of year-over-year player-level G / xG ratios is nearly zero. If we equate ‚Äúskill‚Äù to ‚Äúpercent of variance explained in year-over-year correlations of a measure (i.e.¬†G / xG)‚Äù, then I suppose the answer is that basically 0% of seasonal over- or under-performance is due to innate factors; rather, we‚Äôd attribute all variation to ‚Äúluck‚Äù (assuming that their ‚Äúskill‚Äù and ‚Äúluck‚Äù are the only factors that can explain residuals). That‚Äôs not all that compelling, although it may be the reality.\nMy prior work on ‚Äúmeta-metrics‚Äù for soccer perhaps has a more compelling answer. The ‚Äústability‚Äù measure defined in that post for \\(G / xG\\) comes out to about 70% (out of 100%). If we say that ‚Äústability‚Äù is just another word for ‚Äúskill‚Äù, then we could attribute about 70% of a player‚Äôs seasonal over- or under-performance to innate factors, on average."
  },
  {
    "objectID": "index.html#previously",
    "href": "index.html#previously",
    "title": "Tony ElHabr",
    "section": "Previously",
    "text": "Previously\nDecision Scientist at H-E-B Data Scientist at CollegeVine Power Analyst at EDF Trading Engineer at ERCOT"
  },
  {
    "objectID": "index.html#interests",
    "href": "index.html#interests",
    "title": "Tony ElHabr",
    "section": "Interests",
    "text": "Interests\nsports analyticsdata sciencedata engineeringmemes"
  },
  {
    "objectID": "index.html#other",
    "href": "index.html#other",
    "title": "Tony ElHabr",
    "section": "Other",
    "text": "Other\n2021 NFL Big Data Bowl Finalist Contestant on seasons 0 and 1 of ‚ÄúSliced‚ÄùRWeekly newsletter curator"
  },
  {
    "objectID": "posts/xg-likelihood/index.html#data",
    "href": "posts/xg-likelihood/index.html#data",
    "title": "Estimating Shooting Performance Unlikeliness",
    "section": "",
    "text": "I‚Äôll be using public data from FBref for the 2018/19 - 2023/24 seasons of the the Big Five European soccer leagues, updated through May 7. Fake data is nice for examples, but ultimately we want to test our methods on real data. Our intuition about the results can be a useful caliber of the sensibility of our results.\n\n\nGet shot data\nraw_shots &lt;- worldfootballR::load_fb_match_shooting(\n  country = COUNTRIES,\n  tier = TIERS,\n  gender = GENDERS,\n  season_end_year = SEASON_END_YEARS\n)\n#&gt; ‚Üí Data last updated 2024-05-07 17:52:59 UTC\n\nnp_shots &lt;- raw_shots |&gt; \n  ## Drop penalties\n  dplyr::filter(\n    !dplyr::coalesce((Distance == '13' & round(as.double(xG), 2) == 0.79), FALSE)\n  ) |&gt; \n  dplyr::transmute(\n    season_end_year = Season_End_Year,\n    team = Squad,\n    player_id = Player_Href |&gt; dirname() |&gt; basename(),\n    player = Player,\n    match_date = lubridate::ymd(Date),\n    match_id = MatchURL |&gt; dirname() |&gt; basename(),\n    minute = Minute,\n    g = as.integer(Outcome == 'Goal'),\n    xg = as.double(xG)\n  ) |&gt; \n  ## A handful of scored shots with empty xG\n  dplyr::filter(!is.na(xg)) |&gt; \n  dplyr::arrange(season_end_year, player_id, match_date, minute)\n\n## Use the more commonly used name when a player ID is mapped to multiple names\n##   (This \"bug\" happens because worldfootballR doesn't go back and re-scrape data\n##   when fbref makes a name update.)\nplayer_name_mapping &lt;- np_shots |&gt; \n  dplyr::count(player_id, player) |&gt; \n  dplyr::group_by(player_id) |&gt; \n  dplyr::slice_max(n, n = 1, with_ties = FALSE) |&gt; \n  dplyr::ungroup() |&gt; \n  dplyr::distinct(player_id, player)\n\nplayer_season_np_shots &lt;- np_shots |&gt; \n  dplyr::summarize(\n    .by = c(player_id, season_end_year), \n    shots = dplyr::n(),\n    dplyr::across(c(g, xg), sum)\n  ) |&gt; \n  dplyr::mutate(\n    pr = g / xg\n  ) |&gt; \n  dplyr::left_join(\n    player_name_mapping,\n    by = dplyr::join_by(player_id)\n  ) |&gt; \n  dplyr::relocate(player, .after = player_id) |&gt; \n  dplyr::arrange(player_id, season_end_year)\nplayer_season_np_shots\n#&gt; # A tibble: 15,327 √ó 7\n#&gt;    player_id player          season_end_year shots     g    xg    pr\n#&gt;    &lt;chr&gt;     &lt;chr&gt;                     &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1 0000acda  Marco Benassi              2018    70     5  4.01 1.25 \n#&gt;  2 0000acda  Marco Benassi              2019    59     7  5.61 1.25 \n#&gt;  3 0000acda  Marco Benassi              2020    20     1  1.01 0.990\n#&gt;  4 0000acda  Marco Benassi              2022    10     0  0.99 0    \n#&gt;  5 0000acda  Marco Benassi              2023    19     0  1.35 0    \n#&gt;  6 000b3da6  Manuel Iturra              2018     2     0  0.41 0    \n#&gt;  7 00242715  Moussa Niakhate            2018    16     0  1.43 0    \n#&gt;  8 00242715  Moussa Niakhate            2019    10     1  1.5  0.667\n#&gt;  9 00242715  Moussa Niakhate            2020    11     1  1.02 0.980\n#&gt; 10 00242715  Moussa Niakhate            2021     9     2  1.56 1.28 \n#&gt; # ‚Ñπ 15,307 more rows\n\n\nFor illustrative purposes, we‚Äôll focus on one player in particular‚ÄìJames Maddison. Maddison has had a sub-par 2023/2024 season by his own standards, underperforming his xG for the first time since he started playing in the Premier League in 2018/19.\n\n\nMaddison‚Äôs season-by-season data\nplayer_season_np_shots |&gt; dplyr::filter(player == 'James Maddison')\n#&gt; # A tibble: 6 √ó 7\n#&gt;   player_id player         season_end_year shots     g    xg    pr\n#&gt;   &lt;chr&gt;     &lt;chr&gt;                    &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 ee38d9c5  James Maddison            2019    81     6  5.85 1.03 \n#&gt; 2 ee38d9c5  James Maddison            2020    74     6  5.36 1.12 \n#&gt; 3 ee38d9c5  James Maddison            2021    75     8  3.86 2.07 \n#&gt; 4 ee38d9c5  James Maddison            2022    72    12  7.56 1.59 \n#&gt; 5 ee38d9c5  James Maddison            2023    83     9  7.12 1.26 \n#&gt; 6 ee38d9c5  James Maddison            2024    55     4  5.02 0.797\n\n\n\n\nMore variables useful for the rest of the post\nTARGET_SEASON_END_YEAR &lt;- 2024\n\nplayer_np_shots &lt;- player_season_np_shots |&gt; \n  dplyr::mutate(\n    is_target = season_end_year == TARGET_SEASON_END_YEAR\n  ) |&gt; \n  dplyr::summarize(\n    .by = c(is_target, player_id, player),\n    dplyr::across(\n      c(shots, g, xg),\n      \\(.x) sum(.x, na.rm = TRUE)\n    )\n  ) |&gt; \n  dplyr::mutate(pr = g / xg) |&gt; \n  dplyr::arrange(player, player_id, is_target)\n\nwide_player_np_shots &lt;- player_np_shots |&gt;\n  dplyr::transmute(\n    player_id, \n    player,\n    which = ifelse(is_target, 'target', 'prior'), \n    shots, g, xg, pr\n  ) |&gt; \n  tidyr::pivot_wider(\n    names_from = which, \n    values_from = c(shots, g, xg, pr), \n    names_glue = '{which}_{.value}'\n  )\n\nall_players_to_evaluate &lt;- wide_player_np_shots |&gt; \n  tidyr::drop_na(prior_pr, target_pr) |&gt; \n  dplyr::filter(\n    prior_shots &gt;= 50,\n    target_shots &gt;= 10,\n    prior_g &gt; 0, \n    target_g &gt; 0\n  )"
  },
  {
    "objectID": "posts/xg-predictor-future-results/index.html",
    "href": "posts/xg-predictor-future-results/index.html",
    "title": "Expected goals, gamestate, and predictiveness",
    "section": "",
    "text": "What is a ‚Äòneutral‚Äô gamestate?\n\n\n\nThe two most common definitions I‚Äôve seen for ‚Äúneutral‚Äù gamestate are periods of match time when\n\nthe score is tied, i.e.¬†goal difference of 0\nthe absolute goal difference (GD) is 0 or 1\n\nI investigate both.\n\n\n\n\n\n\n\n\ntldr: The answer to the header question\n\n\n\n\n\nThe answer to the question is ‚Äúno‚Äù‚Äìexpected goals (xG) subset to neutral gamestates does not end up being a better predictor of future performance than overall xG. Nonetheless, I do find that there is barely any reduction in the predictiveness in xG when using the latter definition of ‚Äúneutral‚Äù gamestate (absolute GD &lt;= 1). This itself is an interesting finding, as it indirectly validates the common notion that xG accumulated in lopsided gamestates (e.g.¬†2-0, 1-4) adds litle value in terms of forecasting rest-of-season performance.\n\n\n\n‚ÄúThe best predictor for future performance is Expected Goals‚Äù, authored by Sander Ijtsma of 11tegen11 in 2015, is one of the most influential pieces of writing in the public soccer analytics sphere. The write-up provided compelling evidence for the superiority of expected goals (xG) in terms of forecasting team-season outcomes on a game-by-game basis.12 Ijtsma drew out curves for running \\(R^2\\) values of a handful of season-to-date (‚Äúpast‚Äù) metrics with respect to rest-of-season (‚Äúfuture‚Äù) performance indicators‚Äìgoals ratio and points per game‚Äìand found that xG ratio3 tended to be the most predictive.\n\nIn 2022, Eliot McKinley wrote a cool piece for American Soccer Analytics (ASA) where he replicated and extended Ijtsma‚Äôs analysis. He used a larger data set of more recent seasons‚Äì2018 through 2022‚Äì and added a look at Major League Soccer (MLS) in addition to the the Big 5 European Leagues, and adding. Further, Eliot presented a novel aspect where he bootstrapped the procedure for generating running \\(R^2\\) values to generate smoother, more interpretable curves.\n\n\n\n\n\n\n\nSmaller \\(R^2\\) values in the MLS\n\n\n\n\n\nMike Imburgio deduced that the parity (partially due to the salary cap) in the MLS relative is probably to blame for the diluted predictiveness of metrics like xG ratio relative to the Big 5.\n\n\n\nIn this post I extend this prior art in two ways:\n\nI expand the data set range to 2018 through 2024, sourcing from a reputed public data source‚ÄìFBref.4 The intention is to make the analysis even more robust and reproducible.\nI include additional curves for ‚Äúgamestate-aware‚Äù goals and xG, in an effort to see if one can achieve metrics that are perhaps even more predictive than xG itself.\n\nOn the second point, Ijtsma noted that subsetting the past performance tallies to gamestates when the score is either tied or within 1 goal might abate the potential biases introduced by within-match complacency (manifesting in conservative tactics). In fact, he said he had evaluated these things.\n\nI‚Äôve looked at this, but I have decided not to include that subanalysis in this post, for the sake of accessibility‚Ä¶ I think this method will show that the phenomenon [where teams exert more or less effort in matches against particular opposition] either does not truly exist, or that its effect is so small that correcting for this will allow more noise and thereby weaken the model.\n\nUnfortunately, I couldn‚Äôt find any subsequent blog post where Ijtsma writes about the role of gamestate in the context of his \\(R^2\\) analysis, so, alas, I‚Äôm here to do exactly that.\nNow, my point isn‚Äôt necessarily to find the absolute best metric possible to use to forecast future team performance. Tiotal Football (with assistance from Eliot) showed that comprehensive event-based metrics like goals added (light blue), completed passes into the area in front of the box (yellow), etc. are even more predictive of future points per game than xG, at least in the MLS.\n\nI do look at a few metrics that go beyond Ijtsma‚Äôs set in the Appendix, but that is not the primary focus of this post."
  },
  {
    "objectID": "posts/xg-predictor-future-results/index.html#replicating-prior-art",
    "href": "posts/xg-predictor-future-results/index.html#replicating-prior-art",
    "title": "Expected goals, gamestate, and predictiveness",
    "section": "Replicating prior art",
    "text": "Replicating prior art\nLet‚Äôs begin with replicating Ijtsma‚Äôs visualization where he evaluated the running \\(R^2\\) of the five measures of past performance with respect to two measures of future performance. The five measures of past performance are:\n\npoints per game\ngoal ratio\ntotal shots ratio\nshots on target ratio\nxG ratio\n\nThe two measures of future performance are:\n\ngoals ratio\npoints per game\n\n\n\nCalculate rolling \\(R^2\\) for season-to-date and rest-of-season performance measures\naccumulate_team_summary &lt;- function(df, op, .prefix) {\n  df |&gt; \n    dplyr::mutate(\n      xg_nonneutral = xg_trailing + xg_leading,\n      xg_conceded_nonneutral = xg_conceded_trailing + xg_conceded_leading\n    ) |&gt; \n    dplyr::arrange(team, season, op(game_idx)) |&gt; \n    dplyr::group_by(season, team) |&gt; \n    dplyr::mutate(\n      dplyr::across(\n        c(\n          shots, shots_conceded, \n          sot, sot_conceded,\n          g, g_conceded, \n          xg, xg_conceded,\n          xgot, xgot_conceded,\n          \n          g_neutral, g_conceded_neutral, \n          xg_neutral, xg_conceded_neutral, \n          xg_nonneutral, xg_conceded_nonneutral,\n          xg_trailing, xg_conceded_trailing, \n          xg_leading, xg_conceded_leading, \n          \n          ppa, ppa_conceded, \n          xag, xag_conceded,\n          xg_xag, xg_xag_conceded,\n          \n          duration_leading,\n          pts, pts_conceded\n        ),\n        \\(.x) cumsum(dplyr::coalesce(.x, 0))\n      )\n    ) |&gt; \n    dplyr::ungroup() |&gt; \n    dplyr::mutate(\n      shot_ratio = shots / (shots + shots_conceded),\n      sot_ratio = sot / (sot + sot_conceded),\n      g_ratio = g / (g + g_conceded),\n      xg_ratio = xg / (xg + xg_conceded),\n      xgot_ratio = xgot / (xgot + xgot_conceded),\n      \n      shot_neutral_ratio = shots_neutral / (shots_neutral + shots_conceded_neutral),\n      sot_neutral_ratio = sot_neutral / (sot_neutral + sot_conceded_neutral),\n      g_neutral_ratio = g_neutral / (g_neutral + g_conceded_neutral),\n      xg_neutral_ratio = xg_neutral / (xg_neutral + xg_conceded_neutral),\n      xg_nonneutral_ratio = xg_nonneutral / (xg_nonneutral + xg_conceded_nonneutral),\n      xg_trailing_ratio = xgot_trailing / (xgot_trailing + xgot_conceded_trailing),\n      xg_leading_ratio = xgot_leading / (xgot_leading + xgot_conceded_leading),\n\n      ppa_ratio = ppa / (ppa + ppa_conceded),\n      xag_ratio = xag / (xag + xag_conceded),\n      xg_xag_ratio = xg_xag / (xg_xag + xg_xag_conceded),\n      \n      duration_leading_per_game = duration_leading / game_idx,\n      ppg = pts / game_idx\n    ) |&gt; \n    dplyr::mutate(\n      ## replace NaNs with NAs for the cor calculation\n      dplyr::across(\n        dplyr::ends_with('ratio'),\n        \\(.x) tidyr::replace_na(.x, 0.5)\n      )\n    ) |&gt; \n    dplyr::rename_with(\n      .fn = \\(.x) paste0(.prefix, '_', .x),\n      .cols = c(\n        shots, shots_conceded, \n        sot, sot_conceded,\n        g, g_conceded, \n        xg, xg_conceded,\n        xgot, xgot_conceded,\n        \n        shots_neutral, shots_conceded_neutral, \n        sot_neutral, sot_conceded_neutral,\n        g_neutral, g_conceded_neutral, \n        xg_neutral, xg_conceded_neutral, \n        xg_nonneutral, xg_conceded_nonneutral,\n        xg_trailing, xg_conceded_trailing,\n        xg_leading, xg_conceded_leading,\n        \n        ppa, ppa_conceded, \n        xag, xag_conceded,\n        xg_xag, xg_xag_conceded,\n        \n        duration_leading,\n        pts, pts_conceded,\n        \n        dplyr::ends_with('ratio'),\n        \n        duration_leading_per_game,\n        ppg\n      )\n    )\n}\n\ncalculate_nested_r2 &lt;- function(data, col, target_col) {\n  purrr::map_dbl(\n    data,\n    \\(.x) {\n      cor(\n        .x[[col]],\n        .x[[target_col]],\n        use = 'complete.obs'\n      )^2\n    }\n  )\n}\n\ncalculate_rolling_r2s &lt;- function(combined_df) {\n  \n  past_team_summary &lt;- combined_df |&gt; \n    accumulate_team_summary(`+`, .prefix = 'past')\n  \n  future_team_summary &lt;- combined_df |&gt; \n    accumulate_team_summary(`-`, .prefix = 'future')\n  \n  accumulated_df &lt;- dplyr::inner_join(\n    past_team_summary |&gt; \n      dplyr::select(\n        season,\n        team,\n        country,\n        game_idx,\n        dplyr::starts_with('past')\n      ),\n    future_team_summary |&gt; \n      dplyr::select(\n        season,\n        team,\n        game_idx,\n        dplyr::starts_with('future')\n      ),\n    by = dplyr::join_by(season, team, game_idx)\n  ) |&gt; \n    dplyr::mutate(\n      league_group = ifelse(country == 'USA', 'MLS', 'Big 5'),\n      .keep = 'unused',\n      .before = 1\n    )\n  \n  accumulated_df |&gt; \n    tidyr::nest(data = -c(league_group, game_idx)) |&gt; \n    dplyr::mutate(\n      past_shot_ratio__future_g_ratio = calculate_nested_r2(data, 'past_shot_ratio', 'future_g_ratio'),\n      past_sot_ratio__future_g_ratio = calculate_nested_r2(data, 'past_sot_ratio', 'future_g_ratio'),\n      past_g_ratio__future_g_ratio = calculate_nested_r2(data, 'past_g_ratio', 'future_g_ratio'),\n      past_xg_ratio__future_g_ratio = calculate_nested_r2(data, 'past_xg_ratio', 'future_g_ratio'),\n      past_xgot_ratio__future_g_ratio = calculate_nested_r2(data, 'past_xgot_ratio', 'future_g_ratio'),\n      \n      past_shot_neutral_ratio__future_g_ratio = calculate_nested_r2(data, 'past_shot_neutral_ratio', 'future_g_ratio'),\n      past_sot_neutral_ratio__future_g_ratio = calculate_nested_r2(data, 'past_sot_neutral_ratio', 'future_g_ratio'),\n      past_g_neutral_ratio__future_g_ratio = calculate_nested_r2(data, 'past_g_neutral_ratio', 'future_g_ratio'),\n      past_xg_neutral_ratio__future_g_ratio = calculate_nested_r2(data, 'past_xg_neutral_ratio', 'future_g_ratio'),\n      past_xg_nonneutral_ratio__future_g_ratio = calculate_nested_r2(data, 'past_xg_nonneutral_ratio', 'future_g_ratio'),\n      past_xg_trailing_ratio__future_g_ratio = calculate_nested_r2(data, 'past_xg_trailing_ratio', 'future_g_ratio'),\n      past_xg_leading_ratio__future_g_ratio = calculate_nested_r2(data, 'past_xg_leading_ratio', 'future_g_ratio'),\n      \n      ## Bonus 1: Non-gamestate, non-shot features\n      past_ppa_ratio__future_g_ratio = calculate_nested_r2(data, 'past_ppa_ratio', 'future_g_ratio'),\n      past_xag_ratio__future_g_ratio = calculate_nested_r2(data, 'past_xag_ratio', 'future_g_ratio'),\n      past_xg_xag_ratio__future_g_ratio = calculate_nested_r2(data, 'past_xg_xag_ratio', 'future_g_ratio'),\n      \n      ## Bonus 2: duration leading per game\n      past_duration_leading_per_game__future_g_ratio = calculate_nested_r2(data, 'past_duration_leading_per_game', 'future_g_ratio'),\n      past_ppg__future_g_ratio = calculate_nested_r2(data, 'past_ppg', 'future_g_ratio'),\n      \n      past_shot_ratio__future_ppg = calculate_nested_r2(data, 'past_shot_ratio', 'future_ppg'),\n      past_sot_ratio__future_ppg = calculate_nested_r2(data, 'past_sot_ratio', 'future_ppg'),\n      past_g_ratio__future_ppg = calculate_nested_r2(data, 'past_g_ratio', 'future_ppg'),\n      past_xg_ratio__future_ppg = calculate_nested_r2(data, 'past_xg_ratio', 'future_ppg'),\n      past_xgot_ratio__future_ppg = calculate_nested_r2(data, 'past_xgot_ratio', 'future_ppg'),\n      \n      past_shot_neutral_ratio__future_ppg = calculate_nested_r2(data, 'past_shot_neutral_ratio', 'future_ppg'),\n      past_sot_neutral_ratio__future_ppg = calculate_nested_r2(data, 'past_sot_neutral_ratio', 'future_ppg'),\n      past_g_neutral_ratio__future_ppg = calculate_nested_r2(data, 'past_g_neutral_ratio', 'future_ppg'),\n      past_xg_neutral_ratio__future_ppg = calculate_nested_r2(data, 'past_xg_neutral_ratio', 'future_ppg'),\n      past_xg_nonneutral_ratio__future_ppg = calculate_nested_r2(data, 'past_xg_nonneutral_ratio', 'future_ppg'),\n      past_xg_trailing_ratio__future_ppg = calculate_nested_r2(data, 'past_xg_trailing_ratio', 'future_ppg'),\n      past_xg_leading_ratio__future_ppg = calculate_nested_r2(data, 'past_xg_leading_ratio', 'future_ppg'),\n      \n      past_ppa_ratio__future_ppg = calculate_nested_r2(data, 'past_ppa_ratio', 'future_ppg'),\n      past_xag_ratio__future_ppg = calculate_nested_r2(data, 'past_xag_ratio', 'future_ppg'),\n      past_xg_xag_ratio__future_ppg = calculate_nested_r2(data, 'past_xg_xag_ratio', 'future_ppg'),\n      \n      past_duration_leading_per_game__future_ppg = calculate_nested_r2(data, 'past_duration_leading_per_game', 'future_ppg'),\n      past_ppg__future_ppg = calculate_nested_r2(data, 'past_ppg', 'future_ppg')\n    ) |&gt; \n    dplyr::select(-data)\n}\n\npivot_rolling_r2s &lt;- function(df) {\n  df |&gt; \n    tidyr::pivot_longer(\n      -c(league_group, game_idx),\n      names_pattern = '(^.*)__(.*$)',\n      names_to = c('predictor', 'target'),\n      values_to = 'r2'\n    )\n}\n\ndo_calculate_rolling_r2s &lt;- purrr::compose( \n  calculate_rolling_r2s,\n  pivot_rolling_r2s,\n  .dir = 'forward'\n)\n\njoin_team_summary_and_gamestate_dfs &lt;- function(team_summary_df, gamestate_df) {\n  team_summary_df |&gt; \n    dplyr::left_join(\n      gamestate_df |&gt; dplyr::select(-c(country, gender, tier, date)),\n      by = dplyr::join_by(season, match_id, team)\n    ) |&gt; \n    ## since we didn't have xgot at the match-level, we need to create it from the shot-level data\n    dplyr::mutate(\n      xgot = xgot_trailing + xgot_neutral + xgot_leading,\n      xgot_conceded = xgot_conceded_trailing + xgot_conceded_neutral + xgot_conceded_leading\n    )\n}\n\ncombined_df_gd0 &lt;- join_team_summary_and_gamestate_dfs(\n  combined_team_summary,\n  gamestate_gd0_by_game\n)\ncombined_df_abs_gd1 &lt;- join_team_summary_and_gamestate_dfs(\n  combined_team_summary,\n  gamestate_abs_gd1_by_game\n)\n\n## both dfs have the same format\nrolling_r2s_gd0 &lt;- combined_df_gd0 |&gt; do_calculate_rolling_r2s()\nrolling_r2s_abs_gd1 &lt;- combined_df_abs_gd1 |&gt; do_calculate_rolling_r2s()\nrolling_r2s_abs_gd1\n#&gt; # A tibble: 2,448 √ó 5\n#&gt;    league_group game_idx predictor               target             r2\n#&gt;    &lt;chr&gt;           &lt;int&gt; &lt;chr&gt;                   &lt;chr&gt;           &lt;dbl&gt;\n#&gt;  1 Big 5               1 past_shot_ratio         future_g_ratio 0.187 \n#&gt;  2 Big 5               1 past_sot_ratio          future_g_ratio 0.170 \n#&gt;  3 Big 5               1 past_g_ratio            future_g_ratio 0.165 \n#&gt;  4 Big 5               1 past_xg_ratio           future_g_ratio 0.182 \n#&gt;  5 Big 5               1 past_xgot_ratio         future_g_ratio 0.158 \n#&gt;  6 Big 5               1 past_shot_neutral_ratio future_g_ratio 0.188 \n#&gt;  7 Big 5               1 past_sot_neutral_ratio  future_g_ratio 0.0206\n#&gt;  8 Big 5               1 past_g_neutral_ratio    future_g_ratio 0.166 \n#&gt;  9 Big 5               1 past_xg_neutral_ratio   future_g_ratio 0.183 \n#&gt; 10 Big 5               1 past_xgot_neutral_ratio future_g_ratio 0.155 \n#&gt; # ‚Ñπ 2,294 more rows\n#&gt; # ‚Ñπ Use `print(n = ...)` to see more rows\n\n\n\n\nPlotting the \\(R^2\\) values\nTAG_LABEL &lt;- htmltools::tagList(\n  htmltools::tags$span(htmltools::HTML(enc2utf8(\"&#xf099;\")), style = 'font-family:fb'),\n  htmltools::tags$span(\"@TonyElHabr\"),\n)\nCAPTION_LABEL &lt;- '**Data**: Opta via fbref. 2018 - 2024 seasons, excluding 2020.&lt;br/&gt;**Definitions**: Ratio = team value / (team value + opponent value).&lt;br/&gt;**Inspiration**: 11tegen, American Soccer Analysis'\n\nPLOT_RESOLUTION &lt;- 300\nWHITISH_FOREGROUND_COLOR &lt;- 'white'\nCOMPLEMENTARY_FOREGROUND_COLOR &lt;- '#cbcbcb' # '#f1f1f1'\nBLACKISH_BACKGROUND_COLOR &lt;- '#1c1c1c'\nCOMPLEMENTARY_BACKGROUND_COLOR &lt;- '#4d4d4d'\nFONT &lt;- 'Titillium Web'\nsysfonts::font_add_google(FONT, FONT)\n## https://github.com/tashapiro/tanya-data-viz/blob/main/chatgpt-lensa/chatgpt-lensa.R for twitter logo\nsysfonts::font_add('fb', 'Font Awesome 6 Brands-Regular-400.otf')\nshowtext::showtext_auto()\nshowtext::showtext_opts(dpi = PLOT_RESOLUTION)\n\nggplot2::theme_set(ggplot2::theme_minimal())\nggplot2::theme_update(\n  text = ggplot2::element_text(family = FONT),\n  title = ggplot2::element_text(size = 16, color = WHITISH_FOREGROUND_COLOR),\n  plot.title = ggtext::element_markdown(face = 'bold', size = 16, color = WHITISH_FOREGROUND_COLOR),\n  plot.title.position = 'plot',\n  plot.subtitle = ggtext::element_markdown(size = 16, color = COMPLEMENTARY_FOREGROUND_COLOR),\n  axis.text = ggplot2::element_text(color = WHITISH_FOREGROUND_COLOR, size = 11),\n  legend.text = ggplot2::element_text(size = 12, color = WHITISH_FOREGROUND_COLOR, face = 'plain'),\n  legend.title = ggplot2::element_text(size = 12, color = WHITISH_FOREGROUND_COLOR, face = 'bold'),\n  axis.title.x = ggtext::element_markdown(size = 14, color = WHITISH_FOREGROUND_COLOR, face = 'bold', hjust = 0.99),\n  axis.title.y = ggtext::element_markdown(size = 14, color = WHITISH_FOREGROUND_COLOR, face = 'bold', hjust = 0.99),\n  axis.ticks = ggplot2::element_line(color = WHITISH_FOREGROUND_COLOR),\n  axis.line = ggplot2::element_blank(),\n  strip.text = ggplot2::element_text(size = 14, color = WHITISH_FOREGROUND_COLOR, face = 'bold', hjust = 0),\n  panel.grid.major = ggplot2::element_line(color = COMPLEMENTARY_BACKGROUND_COLOR),\n  panel.grid.minor = ggplot2::element_line(color = COMPLEMENTARY_BACKGROUND_COLOR),\n  panel.grid.minor.x = ggplot2::element_blank(),\n  panel.grid.minor.y = ggplot2::element_blank(),\n  plot.margin = ggplot2::margin(10, 20, 10, 20),\n  plot.background = ggplot2::element_rect(fill = BLACKISH_BACKGROUND_COLOR, color = BLACKISH_BACKGROUND_COLOR),\n  plot.caption = ggtext::element_markdown(color = WHITISH_FOREGROUND_COLOR, hjust = 0, size = 10, face = 'plain', lineheight = 1.1),\n  plot.caption.position = 'plot',\n  plot.tag = ggtext::element_markdown(size = 10, color = WHITISH_FOREGROUND_COLOR, hjust = 1),\n  plot.tag.position = c(0.99, 0.01),\n  panel.spacing.x = grid::unit(2, 'lines'),\n  panel.spacing.y = grid::unit(1, 'lines'),\n  # panel.background = ggplot2::element_rect(fill = BLACKISH_BACKGROUND_COLOR, color = BLACKISH_BACKGROUND_COLOR)\n  panel.background = ggplot2::element_rect(fill = BLACKISH_BACKGROUND_COLOR, color = WHITISH_FOREGROUND_COLOR)\n)\n\nBASE_PRETTY_PREDICTOR_NAMES &lt;- c(\n  'past_ppg' = 'Points per Game',\n  'past_g_ratio' = 'Goal Ratio',\n  'past_shot_ratio' = 'Total Shots Ratio',\n  'past_sot_ratio' = 'Shots on Target Ratio',\n  'past_xg_ratio' = 'xG Ratio'\n)\nPRETTY_TARGET_NAMES &lt;- c(\n  'future_ppg' = 'Future Points per Game',\n  'future_g_ratio' = 'Future Goals Ratio'\n)\nXG_RATIO_COLOR &lt;- '#f44336'\nBASE_PREDICTOR_PALETTE &lt;- c(\n  'Points per Game' = '#cad2c5',\n  'Goal Ratio' = '#8bc34a',\n  'Total Shots Ratio' = '#ffc107',\n  'Shots on Target Ratio' = '#448aff',\n  'xG Ratio' = XG_RATIO_COLOR\n)\n\nCROSSHAIR_LABEL &lt;- '&lt;i&gt;Crosshairs mark first match week where R&lt;sup&gt;2&lt;/sup&gt;&gt;0.5.&lt;/i&gt;'\nplotting_rolling_r2s &lt;- function(rolling_r2s, pretty_predictor_names, predictor_palette) {\n  prettified_rolling_r2s &lt;- rolling_r2s |&gt; \n    dplyr::filter(\n      predictor %in% names(pretty_predictor_names)\n    ) |&gt; \n    dplyr::mutate(\n      pretty_predictor = factor(pretty_predictor_names[predictor], rev(pretty_predictor_names)),\n      pretty_target = PRETTY_TARGET_NAMES[target]\n    ) |&gt; \n    ## put xG on top\n    dplyr::arrange(league_group, pretty_target, dplyr::desc(pretty_predictor))\n  \n  min_prettified_rolling_r2s &lt;- prettified_rolling_r2s |&gt; \n    dplyr::filter(r2 &gt; 0.5) |&gt; \n    dplyr::group_by(league_group, pretty_target, pretty_predictor) |&gt; \n    dplyr::slice_min(game_idx, n = 1) |&gt; \n    dplyr::ungroup()\n  \n  prettified_rolling_r2s |&gt; \n    ggplot2::ggplot() +\n    ggplot2::aes(\n      x = game_idx,\n      y = r2\n    ) +\n    ggplot2::geom_segment(\n      data = min_prettified_rolling_r2s,\n      ggplot2::aes(\n        color = pretty_predictor,\n        x = 0,\n        xend = game_idx,\n        y = r2,\n        yend = r2\n      ),\n      linetype = 2,\n      linewidth = 0.5\n    ) +\n    ggplot2::geom_segment(\n      data = min_prettified_rolling_r2s,\n      ggplot2::aes(\n        color = pretty_predictor,\n        x = game_idx,\n        xend = game_idx,\n        y = 0,\n        yend = r2\n      ),\n      linetype = 2,\n      linewidth = 0.5\n    ) +\n    ggplot2::geom_line(\n      linewidth = 0.75,\n      ggplot2::aes(color = pretty_predictor)\n    ) +\n    ggplot2::scale_color_manual(\n      values = predictor_palette\n    ) +\n    ggplot2::guides(\n      color = ggplot2::guide_legend(\n        title = NULL,\n        position = 'inside',\n        label.theme = ggplot2::element_text(color = WHITISH_FOREGROUND_COLOR, size = 11, FONT),\n        override.aes = list(linewidth = 2)\n      )\n    ) +\n    ggplot2::scale_y_continuous(\n      expand = c(0, 0),\n      limits = c(0, 1),\n      breaks = seq(0, 1, by = 0.2),\n      labels = scales::number_format(accuracy = 0.1)\n    ) +\n    ggplot2::scale_x_continuous(\n      expand = c(0, 0),\n      limits = c(0, 38),\n      breaks = seq.int(0, 35, by = 5),\n      labels = seq.int(0, 35, by = 5)\n    ) +\n    ggtext::geom_richtext(\n      data = tibble::tibble(\n        league_group = 'Big 5',\n        pretty_target = sort(unname(PRETTY_TARGET_NAMES))[1]\n      ),\n      ggplot2::aes(\n        x = 1,\n        y = 0.7,\n        label = CROSSHAIR_LABEL\n      ),\n      fill = NA, label.color = NA,\n      label.padding = grid::unit(rep(0, 4), 'pt'),\n      color = WHITISH_FOREGROUND_COLOR,\n      family = FONT,\n      size = 10 / .pt,\n      hjust = 0,\n      vjust = 0.5\n    ) +\n    ggplot2::theme(\n      legend.position.inside = c(0.82, 0.35),\n      legend.key.spacing.y = ggplot2::unit(-4, 'pt'),\n      panel.grid.major = ggplot2::element_blank()\n    ) +\n    ggplot2::facet_grid(league_group~pretty_target) +\n    ggplot2::labs(\n      title = 'Predictiveness of Season-to-Date Metrics on Rest-of-Season Performance',\n      x = 'Match Day',\n      y = 'R&lt;sup&gt;2&lt;/sup&gt;',\n      caption = CAPTION_LABEL,\n      tag = TAG_LABEL\n    )\n}\n\n## Can pick either rolling_r2s_gd0 or rolling_r2s_abs_gd1 to plot since we're looking at just\n##   non-gamestate features here\nrolling_r2s_plot &lt;- rolling_r2s_abs_gd1 |&gt; \n  plotting_rolling_r2s(\n    pretty_predictor_names = BASE_PRETTY_PREDICTOR_NAMES, \n    predictor_palette = BASE_PREDICTOR_PALETTE\n  )\n\nggplot2::ggsave(\n  rolling_r2s_plot,\n  filename = file.path(PROJ_DIR, 'rolling-r2s.png'),\n  width = 9,\n  height = 9 / 1.5\n)\n\n\n\nThere‚Äôs a bit of visual noise here that can make it hard to differentiate traces. This can be smoothed out by resampling.\nSpecifically, if we randomly reorder matchweeks before calculating cumulative in-season measures, and do that 100 times, we end up with a plot that looks like this.\n\n\nCalculating bootstrapped \\(R^2\\) values\ncalculate_resampled_rolling_r2s &lt;- function(\n    combined_df, \n    resamples = 10, \n    seed = 42,\n    parallel_seed = 117, \n    cores_prop = 0.5\n  ) {\n\n  match_idx_grid &lt;- combined_df |&gt; dplyr::distinct(season, country, gender, tier, game_idx)\n  n_cores &lt;- parallel::detectCores()\n  cores_for_parallel &lt;- ceiling(n_cores * cores_prop)\n  future::plan(\n    future::multisession,\n    workers = cores_for_parallel\n  )\n  withr::local_seed(seed)\n  resampled_rolling_df &lt;- furrr::future_map(\n    1:resamples,\n    .progress = TRUE,\n    .options = furrr::furrr_options(\n      seed = parallel_seed\n    ),\n    \\(.i) {\n      resampled_match_idx_grid &lt;- match_idx_grid |&gt; \n        dplyr::rename(orig_game_idx = game_idx) |&gt; \n        dplyr::slice_sample(n = nrow(match_idx_grid), replace = FALSE) |&gt; \n        dplyr::group_by(country, gender, tier, season) |&gt;\n        dplyr::mutate(\n          game_idx = dplyr::row_number(),\n        ) |&gt; \n        dplyr::ungroup()\n      \n      combined_df |&gt; \n        dplyr::rename(orig_game_idx = game_idx) |&gt; \n        dplyr::left_join(\n          resampled_match_idx_grid,\n          by = dplyr::join_by(season, country, gender, tier, orig_game_idx)\n        ) |&gt; \n        calculate_rolling_r2s()\n    }\n  )  |&gt; \n    purrr::list_rbind()\n  future::plan(future::sequential)\n  \n  resampled_rolling_df |&gt; \n    dplyr::summarize(\n      .by = c(league_group, game_idx),\n      dplyr::across(\n        dplyr::matches('__'),\n        \\(.x) mean(.x, na.rm = TRUE)\n      )\n    )\n}\n\ndo_calculate_resampled_rolling_r2s &lt;- purrr::compose( \n  calculate_resampled_rolling_r2s,\n  pivot_rolling_r2s,\n  .dir = 'forward'\n)\n\nN_RESAMPLES &lt;- 100\nresampled_rolling_r2s_gd0 &lt;- combined_df_gd0 |&gt;\n  do_calculate_resampled_rolling_r2s(resamples = N_RESAMPLES)\nresampled_rolling_r2s_abs_gd1 &lt;- combined_df_abs_gd1 |&gt; \n  do_calculate_resampled_rolling_r2s(resamples = N_RESAMPLES)\n\n\n\n\nPlotting the bootstrapped \\(R^2\\) values\nBOOTSTRAPPED_CAPTION_LABEL &lt;- stringr::str_replace(\n  CAPTION_LABEL, \n  '&lt;br\\\\/&gt;', \n  glue::glue(' Data resampled by match day {N_RESAMPLES} times.&lt;br/&gt;')\n)\nrelabel_for_bootstrap_plot &lt;- function(...) {\n  list(\n    ...,\n    ggplot2::labs(\n      y = 'Average Bootstrapped R&lt;sup&gt;2&lt;/sup&gt;',\n      caption = BOOTSTRAPPED_CAPTION_LABEL,\n      x = 'Randomized Match Day'\n    )\n  )\n}\n\nresampled_rolling_r2s_plot &lt;- resampled_rolling_r2s_gd0 |&gt; \n  plotting_rolling_r2s(\n    pretty_predictor_names = BASE_PRETTY_PREDICTOR_NAMES, \n    predictor_palette = BASE_PREDICTOR_PALETTE\n  ) +\n  relabel_for_bootstrap_plot()\n\nggplot2::ggsave(\n  resampled_rolling_r2s_plot,\n  filename = file.path(PROJ_DIR, 'bootstrapped-rolling-r2s.png'),\n  width = 9,\n  height = 9 / 1.5\n)\n\n\n\nIndeed, this looks like the bootstrapped plot from ASA.5 Note that,\n\n\n\n\n\n\nEffect of bootstrapping\n\n\n\n\n\nFor a given metric, the bootstrapped \\(R^2\\) values are slightly smaller across the board compared to the non-bootstrapped values.6 This is perhaps not surprising, as resampling tends to have a ‚Äúshrinking‚Äù effect on figures. Intuitively, this noise could be associated with scheduling bias, injuries, etc."
  },
  {
    "objectID": "posts/xg-predictor-future-results/index.html#extending-prior-art",
    "href": "posts/xg-predictor-future-results/index.html#extending-prior-art",
    "title": "Expected goals, gamestate, and predictiveness",
    "section": "Extending prior art",
    "text": "Extending prior art\nNow we incorporate gamestate-aware measures.7 In this case, we‚Äôre mostly interested game time where the score is close.\nA ‚Äúclose‚Äù match can be strictly defined as one with a tied score, whether it‚Äôs 0-0, 1-1, etc. The indirect assumption is that teams start to play more conservatively (if at all) when leading, thereby distorting the ratio of shots, goals, and xG that we might otherwise expect given the relative quality of the teams. By excluding events when the game is not tied, we might achieve more ‚Äúsignal‚Äù in our measures of performance.\nWe might also define ‚Äúclose‚Äù as periods when the absolute difference in goals is 0 or 1, so 0-1, 1-2, 3-2, 5-4, etc. in addition to 0-0, 1-1, etc. This definition indirectly assumes that teams don‚Äôt start to play more complacently (if at all) when ahead until they have a 2 goal lead. This approach would capture more game time (and have higher tallies of goals, shots, etc.) compared to the gd = 0 approach, although less time than just using the whole match.\nIn the plot below, I‚Äôve split out each the xG and goals measures of past performance into their own facets, paired with tied gamestate analogue (in a brighter blue shade). Note that I‚Äôm showing just points per game as the sole measure of future performance.8\n\n\nPlotting the bootstrapped \\(R^2\\) values, including the neutral gamestate metrics\nBASE_AND_NEUTRAL_PRETTY_PREDICTOR_LOOKUP &lt;- list(\n  'xG Ratio' = c(\n    'past_xg_ratio',\n    'past_xg_neutral_ratio'\n  ),\n  'Goal Ratio' = c(\n    'past_g_ratio',\n    'past_g_neutral_ratio'\n  )\n)\nBASE_AND_NEUTRAL_TARGET_PREDICTOR &lt;- 'future_ppg'\nunlist_and_invert &lt;- function(x) {\n  unlisted &lt;- unlist(x)\n  names(unlisted) &lt;- rep(names(x), lengths(x))\n  setNames(names(unlisted), unname(unlisted))\n}\nBASE_AND_NEUTRAL_PRETTY_PREDICTOR_GROUP_NAMES &lt;- unlist_and_invert(\n  BASE_AND_NEUTRAL_PRETTY_PREDICTOR_LOOKUP\n)\n\nXG_RATIO_GD0_COLOR &lt;- '#00b4d8'\nIS_NEUTRAL_PALETTE &lt;- c(\n  `TRUE` =  XG_RATIO_GD0_COLOR,\n  `FALSE` = '#caf0f8'\n)\n\nplotting_rolling_r2s_with_neutral &lt;- function(rolling_r2s, gamestate_description) {\n  prettified_rolling_r2s &lt;- rolling_r2s |&gt; \n    dplyr::filter(\n      predictor %in% names(BASE_AND_NEUTRAL_PRETTY_PREDICTOR_GROUP_NAMES),\n      target == BASE_AND_NEUTRAL_TARGET_PREDICTOR\n    ) |&gt; \n    dplyr::mutate(\n      is_neutral = stringr::str_detect(predictor, 'neutral'),\n      pretty_predictor_group = factor(\n        BASE_AND_NEUTRAL_PRETTY_PREDICTOR_GROUP_NAMES[predictor],\n        levels = names(BASE_AND_NEUTRAL_PRETTY_PREDICTOR_LOOKUP)\n      )\n    ) |&gt; \n    ## put xG on top\n    dplyr::arrange(league_group, pretty_predictor_group, is_neutral)\n  \n  min_rolling_r2s &lt;- prettified_rolling_r2s |&gt; \n    dplyr::filter(r2 &gt; 0.5) |&gt; \n    dplyr::group_by(league_group, predictor) |&gt; \n    dplyr::slice_min(game_idx, n = 1) |&gt; \n    dplyr::ungroup()\n\n  prettified_rolling_r2s |&gt; \n    ggplot2::ggplot() +\n    ggplot2::aes(\n      x = game_idx,\n      y = r2,\n      group = predictor\n    ) +\n    ggplot2::geom_segment(\n      data = min_rolling_r2s,\n      ggplot2::aes(\n        color = is_neutral,\n        x = 0,\n        xend = game_idx,\n        y = r2,\n        yend = r2\n      ),\n      linetype = 2,\n      linewidth = 0.5\n    ) +\n    ggplot2::geom_segment(\n      data = min_rolling_r2s,\n      ggplot2::aes(\n        color = is_neutral,\n        x = game_idx,\n        xend = game_idx,\n        y = 0,\n        yend = r2\n      ),\n      linetype = 2,\n      linewidth = 0.5\n    ) +\n    ggplot2::geom_line(\n      linewidth = 1,\n      ggplot2::aes(color = is_neutral)\n    ) +\n    ggplot2::scale_color_manual(\n      values = IS_NEUTRAL_PALETTE\n    ) +\n    ggplot2::guides(\n      color = 'none'\n    ) +\n    ggplot2::scale_y_continuous(\n      limits = c(0, 1),\n      breaks = seq(0, 1, by = 0.2),\n      labels = scales::number_format(accuracy = 0.1)\n    ) +\n    ggplot2::scale_x_continuous(\n      breaks = seq.int(0, 35, by = 5),\n      labels = seq.int(0, 35, by = 5)\n    ) +\n    ggplot2::theme(\n      panel.grid.major = ggplot2::element_blank(),\n      plot.subtitle = ggtext::element_markdown(size = 12)\n    ) +\n    ggplot2::facet_grid(league_group~pretty_predictor_group) +\n    ggplot2::labs(\n      title = 'Predictiveness of Season-to-Date Metrics on Rest-of-Season Points Per Game',\n      subtitle = glue::glue('Are &lt;b&gt;&lt;span style=\"color:{IS_NEUTRAL_PALETTE[[\"FALSE\"]]}\"&gt;full match&lt;/span&gt;&lt;/b&gt; metrics more predictive when subset to &lt;b&gt;&lt;span style=\"color:{IS_NEUTRAL_PALETTE[[\"TRUE\"]]}\"&gt;{gamestate_description}&lt;/span&gt;&lt;/b&gt;?'),\n      x = 'Randomized Match Day',\n      y = 'Average Bootstrapped R&lt;sup&gt;2&lt;/sup&gt;',\n      caption = BOOTSTRAPPED_CAPTION_LABEL,\n      tag = TAG_LABEL\n    )\n}\n\nrolling_r2s_with_neutral_gd0_plot &lt;- resampled_rolling_r2s_gd0 |&gt;\n  plotting_rolling_r2s_with_neutral('tied gamestates')\n\nrolling_r2s_with_neutral_abs_gd1_plot &lt;- resampled_rolling_r2s_abs_gd1 |&gt; \n  plotting_rolling_r2s_with_neutral('gamestates with absolute goal difference &lt;= 1')\n\nggplot2::ggsave(\n  rolling_r2s_with_neutral_gd0_plot,\n  filename = file.path(PROJ_DIR, 'bootstrapped-rolling-r2s-with-neutral-gd-0.png'),\n  width = 8,\n  height = 8 / 1.5\n)\n\nggplot2::ggsave(\n  rolling_r2s_with_neutral_abs_gd1_plot,\n  filename = file.path(PROJ_DIR, 'bootstrapped-rolling-r2s-with-neutral-abs-gd-1.png'),\n  width = 8,\n  height = 8 / 1.5\n)\n\n\n\nSo we see that subsetting xG and goals to tied gamestates reduces how predictive they are of future points per game, both for the Big 5 leagues and the MLS. Perhaps this is not surprising, as we‚Äôre disregarding a lot of data (over 50%!) that is included in full match sums.\nLet‚Äôs see if the same plot using the alternative definition for neutral gamestates‚Äìtime when the absolute GD is 0 or 1‚Äìlooks any different.\n\nAh, so using the alternative, more relaxed definition of neutral gamestate, the predictiveness of the season-to-date xG and goal ratios is much closer to the full match analogues. Nonetheless, it‚Äôs evident that the neutral gamestate xG and goal ratios are no better than the full match ratios, indicating that there is no incremental value to be had with focusing on neutral gamestates. This falls in line with what Ijtsma hypothesized‚Äìthat the effect of within-match complacency is minimal, and accounting for it (by reducing tallies to neutral gamestates) is more than likely to reduce predictiveness.\nLet‚Äôs put our two neutral gamestate xG traces alongside the full match xG trace for one complete picture.\n\n\nPlotting full-match and neutral xG ratios\nmini_rolling_r2s_neutral &lt;- dplyr::bind_rows(\n  rolling_r2s_gd0 |&gt; \n    dplyr::filter(\n      league_group == 'Big 5',\n      predictor %in% c('past_xg_ratio', 'past_xg_neutral_ratio'),\n      target == 'future_ppg'\n    ) |&gt; \n    dplyr::mutate(\n      predictor = ifelse(predictor == 'past_xg_neutral_ratio', 'past_xg_gd0_ratio', predictor)\n    ),\n  rolling_r2s_abs_gd1 |&gt; \n    dplyr::filter(\n      league_group == 'Big 5',\n      predictor == 'past_xg_neutral_ratio',\n      target == 'future_ppg'\n    ) |&gt; \n    dplyr::mutate(\n      predictor = 'past_xg_abs_gd1_ratio'\n    )\n)\n\nNEUTRAL_PRETTY_PREDICTOR_NAMES &lt;- c(\n  'past_xg_gd0_ratio' = 'xG Ratio when tied',\n  'past_xg_abs_gd1_ratio' = 'xG Ratio when abs(GD) &lt;= 1',\n  'past_xg_ratio' = 'xG Ratio'\n)\n\nXG_RATIO_ABS_GD1_COLOR &lt;- '#b4e600'\nNEUTRAL_COLOR_PALETTE &lt;- c(\n  'xG Ratio when tied' = XG_RATIO_GD0_COLOR,\n  'xG Ratio when abs(GD) &lt;= 1' = XG_RATIO_ABS_GD1_COLOR,\n  'xG Ratio' = XG_RATIO_COLOR\n)\n\nprettified_rolling_r2s_neutral &lt;- mini_rolling_r2s_neutral |&gt; \n  dplyr::filter(\n    predictor %in% names(NEUTRAL_PRETTY_PREDICTOR_NAMES),\n    target == 'future_ppg',\n    league_group == 'Big 5'\n  ) |&gt; \n  dplyr::mutate(\n    pretty_predictor = factor(NEUTRAL_PRETTY_PREDICTOR_NAMES[predictor], rev(NEUTRAL_PRETTY_PREDICTOR_NAMES))\n  ) |&gt; \n  ## put xG on top\n  dplyr::arrange(dplyr::desc(pretty_predictor))\n\nmin_prettified_rolling_r2s_neutral &lt;- prettified_rolling_r2s_neutral |&gt; \n  dplyr::filter(r2 &gt; 0.5) |&gt; \n  dplyr::group_by(pretty_predictor) |&gt; \n  dplyr::slice_min(game_idx, n = 1) |&gt; \n  dplyr::ungroup()\n\nrolling_r2s_neutral_plot &lt;- prettified_rolling_r2s_neutral |&gt; \n  ggplot2::ggplot() +\n  ggplot2::aes(\n    x = game_idx,\n    y = r2\n  ) +\n  ggplot2::geom_segment(\n    data = min_prettified_rolling_r2s_neutral,\n    ggplot2::aes(\n      color = pretty_predictor,\n      x = 0,\n      xend = game_idx,\n      y = r2,\n      yend = r2\n    ),\n    linetype = 2,\n    linewidth = 0.5\n  ) +\n  ggplot2::geom_segment(\n    data = min_prettified_rolling_r2s_neutral,\n    ggplot2::aes(\n      color = pretty_predictor,\n      x = game_idx,\n      xend = game_idx,\n      y = 0,\n      yend = r2\n    ),\n    linetype = 2,\n    linewidth = 0.5\n  ) +\n  ggplot2::geom_line(\n    linewidth = 0.75,\n    ggplot2::aes(color = pretty_predictor)\n  ) +\n  ggplot2::scale_color_manual(\n    values = NEUTRAL_COLOR_PALETTE\n  ) +\n  ggplot2::guides(\n    color = ggplot2::guide_legend(\n      title = NULL,\n      position = 'inside',\n      label.theme = ggplot2::element_text(color = WHITISH_FOREGROUND_COLOR, size = 14, FONT),\n      override.aes = list(linewidth = 2)\n    )\n  ) +\n  ggplot2::scale_y_continuous(\n    expand = c(0, 0),\n    limits = c(0, 1),\n    breaks = seq(0, 1, by = 0.2),\n    labels = scales::number_format(accuracy = 0.1)\n  ) +\n  ggplot2::scale_x_continuous(\n    expand = c(0, 0),\n    limits = c(0, 38),\n    breaks = seq.int(0, 35, by = 5),\n    labels = seq.int(0, 35, by = 5)\n  ) +\n  ggtext::geom_richtext(\n    data = tibble::tibble(),\n    ggplot2::aes(\n      x = 1,\n      y = 0.6,\n      label = CROSSHAIR_LABEL\n    ),\n    fill = NA, label.color = NA,\n    label.padding = grid::unit(rep(0, 4), 'pt'),\n    color = WHITISH_FOREGROUND_COLOR,\n    family = FONT,\n    size = 14 / .pt,\n    hjust = 0,\n    vjust = 0.5\n  ) +\n  ggplot2::theme(\n    legend.position.inside = c(0.7, 0.8),\n    legend.key.spacing.y = ggplot2::unit(-2, 'pt'),\n    panel.grid.major = ggplot2::element_blank(),\n    plot.title = ggtext::element_markdown(margin = ggplot2::margin(0, 0, 10, 0))\n  ) +\n  ggplot2::labs(\n    title = 'Predictiveness of Season-to-Date xG on&lt;br/&gt;Rest-of-Season Points per Game in the Big 5 leagues',\n    x = 'Match Day',\n    y = 'R&lt;sup&gt;2&lt;/sup&gt;',\n    caption = CAPTION_LABEL,\n    tag = TAG_LABEL\n  )\nrolling_r2s_neutral_plot\nggplot2::ggsave(\n  rolling_r2s_neutral_plot,\n  filename = file.path(PROJ_DIR, 'rolling-r2s-neutral.png'),\n  width = 6,\n  height = 6 \n)"
  },
  {
    "objectID": "posts/xg-predictor-future-results/index.html#footnotes",
    "href": "posts/xg-predictor-future-results/index.html#footnotes",
    "title": "Expected goals, gamestate, and predictiveness",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you know anything about the broader soccer analytics discourse, you‚Äôve more than likely heard about one of the Ijtsma‚Äôs findings regarding the predictiveness of xG. As Eliot puts it: ‚ÄúIf you‚Äôve ever heard or read someone say that it takes 5-10 (4 in the article) games for xG to be predictive, they probably read this article or got it second hand from someone who has.‚Äù‚Ü©Ô∏é\nNote that there are some critics of the choice to rely on \\(R^2\\) as a measure of predictiveness. For the purpose of this post, we‚Äôll take that criticism on the chin.‚Ü©Ô∏é\nA ‚Äúratio‚Äù here is broadly defined in pseudo-code as team's value / (team's value + opponent's value). Ijtsma‚Äôs notes in a reply to a comment that one might arguably use a difference formula, i.e.¬†team's value - opponent's value, as ratios are susceptible to noise when values are themselves fractional. This is most relevant for xG, and not so relevant for shots and goals, which are inherently discrete.‚Ü©Ô∏é\nNotably, I exclude the COVID-19 influenced 2020 season, so as to reduce a bit of noise in the data. Although bootstrapping games within-season should reduce much of this noise, in practice, FBref is missing some game-level xG data for 2020 matches, so this choice is sort of just a pragmatic one.‚Ü©Ô∏é\nI only perform 100 bootstraps instead of 1,000 like Eliot did since I found 100 sufficient for creating visual clarity.‚Ü©Ô∏é\nThis may not be easy to see directly since the bootstrapped and non-bootstrapped curves are not plotted altogether. Nonetheless, one can compare the peaks of the curves against the y-axis in this plot compared to the prior one to verify the suppressing effect of the resampling.‚Ü©Ô∏é\nIf you‚Äôve been following the code, these measures have already been calculated.‚Ü©Ô∏é\nThere are similar trends with future goal ratio. I‚Äôve opted for visual brevity here.‚Ü©Ô∏é\nThe tables that follow show non-bootstrapped \\(R^2\\) values. The bootstrapped \\(R^2\\) values only exceed 0.5 for a handful of measures, typically in the 15-20 game range.‚Ü©Ô∏é\nNote that Ijtsma says that xG ratio becomes useful as early as the fourth match week. However, his statement is based purely on a visual assessment of when the \\(R^2\\) curve generally ‚Äúsettles down‚Äù after the expected early season up-tick. Looking at his graph, the \\(R^2\\) values for xG don‚Äôt actually exceed the 0.5 threshold until some point between the 10th and 15th match days.‚Ü©Ô∏é\nFBref‚Äôs explanation regarding how xAG differs from expected assists (xA): ‚ÄúxA, or expected assists, is the likelihood that a given completed pass will become a goal assist‚Ä¶ Players receive xA for every completed pass regardless of whether a shot occurred or not‚Ä¶ [Expected Assisted Goals (xAG)] indicates a player‚Äôs ability to set up scoring chances‚Ä¶ Players receive xAG only when a shot is taken after a completed pass.‚Äù‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/xg-predictor-future-results/index.html#additional-past-metrics",
    "href": "posts/xg-predictor-future-results/index.html#additional-past-metrics",
    "title": "Expected goals, gamestate, and predictiveness",
    "section": "Additional Past Metrics",
    "text": "Additional Past Metrics\nI was interested in evaluating running in-season \\(R^2\\) values for a few more measures of past performance:\n\npasses into the penalty area ratio\nexpected goals on target (xGOT) ratio\nxG + expected assisted goals11 (xG+xAG) ratio\n\nBelow is the same running \\(R^2\\) plot shown before, just with these new measures, along side our tried and true xG ratio measure.\n\nAha! It seems that we have identified a measure‚ÄìxG+xAG ratio (purple above)‚Äìthat becomes reliable for forecasting rest-of-season performance even earlier than xG ratio. Specifically, it exceeds the reliability threshold at the 7 and 10 game marks for future goal ratio and future points per game respectively. That‚Äôs a non-trivial improvement compared to the 9 and 13 match day marks for xG ratio.\nAlas, xG+xAG metaphorically stands on the shoulders of xG, adding in the component of expected assists (xA) on shots. Assuming that the underlying xA model is well calibrated, perhaps we should not be surprised to see that the composite xG+xAG measure outperforms all others.\n\n\n\n\n\n\nxG+xAG usage\n\n\n\n\n\nxG+xAG is not something I typically see used at the team-level, presumably because it effectively ‚Äúdouble counts‚Äù goal contributions (goals + assists). Rather, it‚Äôs more commonly used to describe expected individual player contributions, where the intention is not to roll-up to the team-level. Nonetheless, it‚Äôs interesting to see that one can achieve slightly better predictiveness with team xG+xAG.\n\n\n\nOn the other hand, I suppose I am a bit surprised that xGOT ratio does not seem to do nearly as well as xG ratio in terms of forecasting rest-of-season performance. The implication is that there is value in the xG of off-target shots for projecting the future. By comparison, shots on target ratio tend to be more predictive than just the shots ratio, meaning that including off-target shots introduces noise that reduces predictiveness. That‚Äôs an interesting difference in shots and xG!"
  },
  {
    "objectID": "posts/xg-predictor-future-results/index.html#non-neutral-xg-ratio",
    "href": "posts/xg-predictor-future-results/index.html#non-neutral-xg-ratio",
    "title": "Expected goals, gamestate, and predictiveness",
    "section": "Non-Neutral xG Ratio",
    "text": "Non-Neutral xG Ratio\nFinally, I was interested in verifying that there‚Äôs not a lot of value in non-neutral gamestate xG ratios. The plot below shows the running \\(R^2\\) values for two measures already visualized:\n\nxG ratio\nxG ratio when the absolute GD is less than or equal to 1 (83.1% of total minutes).\n\nBut, in additional to these, I also plot the xG ratio when the absolute goal difference is greater than 1 (16.1% of total minutes)\n\nWe see that that the \\(R^2\\) values for absolute GD &gt;1 gamestates basically hover around 0 across all match days, for both the Big 5 and the MLS. This isn‚Äôt all that surprising given how small a percentage of full matches occur in &gt;1 GD gamestates. But it also backs up our intuition that stats (i.e.¬†shots, xG) accumulated in such gamestates do not strongly reflect how the teams will perform for the rest-of-the-season."
  }
]